list_df <- list()
list_df_name <- c()
mode_selection <- ""
default_folder_load <- "C:/Users/User/Documents/R Executables Folder"

#########################################################################################################################
############################################# Analysis Function Pipeline ################################################
#########################################################################################################################

#------------------------------------------------------------------------------------------------------------------------
################################# X) Utilities / Preprocessing ##########################################################
#------------------------------------------------------------------------------------------------------------------------
#https://www.statisticshowto.com/unimodal-distribution-2/
#----------------------------- Data Reshaping, Wide->Long, Long->Wide --------------------------------------
data_reshaping <- function(data, directional="long",
                           wide_to_long_value_label="",
                           wide_to_long_rename_category=list(),
                           idvars=c(),
                           long_to_wide_varname="",
                           long_to_wide_value="",
                           idvar_unique_adjustment=FALSE,
                           reshape_guide=FALSE){ 
  library(rlang)
  library(tidyr)
  library(rlang)
  if(reshape_guide){
    writeLines("================= To transform to long data ===========================")
    writeLines("required a data with most commonly a repeated columns, example as such census data from 2010-2015 as columns")
    writeLines("that can be concated into single column which will resulting long data")
    writeLines("================= To transform to wide data ===========================")
    writeLines("required a data with most commonly a repetitive rows which follows general->detail hierarchy, example such as Indometh data")
    writeLines("or required a unique idvars for every single row")
    writeLines("that can be viewed independently into multiple columns which will resulting wide data")
  }
  if(idvar_unique_adjustment){
    for(f in 1:length(idvars)){
      sym_idvar <- sym(idvars[f])
      #) Sort first then Group by to see the count
      data <- data %>% arrange(!!sym_idvar)
      idvar_count <- data %>% group_by(!!sym_idvar) %>% summarise(count = n()) %>% as.data.frame()
      new_idvar <- c()
      for(h in 1:nrow(idvar_count)){
        vector_unique <- paste0(idvar_count[h,1], "_", seq(1, idvar_count[h,2], by=1))
        new_idvar <- c(new_idvar, vector_unique)
      }
      data[[idvars[f]]] <- new_idvar
    }
  }
  if(directional == "long"){
    long_data <- data %>%
      gather(Var, Value, -{{idvars}}) %>%
      mutate(Category = Var) %>%
      select(-Var) 
    
    if(wide_to_long_value_label != ""){
      sym_name <- enquo(wide_to_long_value_label)
      long_data <- long_data %>% rename(!!sym_name := Value)
    }
    if(length(wide_to_long_rename_category) > 0){
      for(a in 1:length(wide_to_long_rename_category)){
        value_change <- wide_to_long_rename_category[[a]]
        long_data <- long_data %>%
          mutate(Category = ifelse(Category == value_change[1], value_change[2], Category))
      }
    }
    return(long_data)
  }else if(directional == "long_sequence"){
    #the reshape ways to long transformation has weaknesses: 
    #Cannot transform variable to long data which has a different length of varying
    #therefore using tidyr ways
    #long_data <- reshape(data, idvar=idvars, varying=wide_to_long_idx,
    #                     v.names=var_names, direction="long")
    long_data <- data %>%
      gather(Var, Value, -{{idvars}}) %>%
      mutate(Time = sub("[A-Za-z]+", "", Var), Type = sub("[\\_0-9]+", "", Var)) %>%
      select(-Var) %>%
      spread(Type, Value) 
    return(long_data)
    
  }else if(directional == "wide"){
    variable_name <- enquo(long_to_wide_varname)
    value_name <- enquo(long_to_wide_value)
    #wide_data <- reshape(data, idvar=idvars, timevar=long_to_wide_timevar,
    #                     v.names=var_names, direction="wide")
    wide_data <- data %>%
      spread(!!variable_name, !!value_name)
    length_unique_varname <- length(unique(data[[long_to_wide_varname]]))
    wide_ncol <- ncol(wide_data)
    colnames(wide_data)[(wide_ncol - length_unique_varname):wide_ncol] <- paste0(long_to_wide_value,"_", colnames(wide_data)[(wide_ncol - length_unique_varname):wide_ncol])
    return(wide_data)
  }
}

#example implementation of data reshaping
data("Indometh")
Indometh <- as.data.frame(Indometh)
indometh_wide <- data_reshaping(Indometh, directional="wide", idvars=c("Subject"),
                                long_to_wide_value = "conc", long_to_wide_varname = "time")
census_data <- read.csv(file.choose())
colnames(census_data)[8] <- "CENSUSPOP2010"
census_wide <- data_reshaping(census_data, directional="long", 
                              idvars=c("SUMLEV","REGION","DIVISION","STATE",
                                       "COUNTY","STNAME","CTYNAME"))

#https://ademos.people.uic.edu/Chapter9.#3_separate()
#if method=separate, separate1_name & separate2_name become output object name
#if method=unite, separate1_name & separate2_name become input object name
columns_reshaping <- function(data, columnname, method="separate", separator=".",
                              separate1_name="Object", separate2_name="Number", 
                              united_name="Full_Object"){
  library(dplyr)
  library(tidyr)
  library(rlang)
  if(method=="separate"){
    sym_source <- sym(columnname)
    sym_separate1 <- sym(separate1_name)
    sym_separate2 <- sym(separate2_name)
    data <- data %>% separate(!!sym_source, !!sym_separate1, !!sym_separate2, sep=separator)
  }else if(method=="unite"){
    sym_unitedname <- sym(united_name)
    sym_separate1 <- sym(separate1_name)
    sym_separate2 <- sym(separate2_name)
    data <- data %>% unite(!!sym_unitedname, !!sym_separate1, !!sym_separate2, sep=separator)
  }
  return(data)
}

spread_by_multiple_var <- function(df,spread_key, spread_var=c(), 
                                   spread_key_from_name=c(), spread_key_to_name=c()){
  library(tidyr)
  library(dplyr)
  library(rlang)
  library(lubridate)
  spread_result <- NULL
  reorder_bool <- FALSE
  spread_result_reorder <- NULL 
  independent_names <- c()
  length_independent <- 0 
  time_vector <- c()
  
  if(class(df[[spread_key]]) %in% c("POSIXct","Date")){
    reorder_bool <- TRUE
    sym_spread_key <- sym(spread_key)
    df <- df %>% arrange(!!sym_spread_key)
    df[[spread_key]] <- paste0(year(df[[spread_key]]),"_",month.name[month(df[[spread_key]])])
    time_vector <- unique(df[[spread_key]])
  }
  
  if(class(df[[spread_key]]) %in% c("character") && length(spread_key_from_name) != 0 && 
     length(spread_key_to_name) != 0 && length(spread_key_from_name) == length(spread_key_to_name)){
    for(a in 1:length(spread_key_from_name)){
      df[[spread_key]][which(df[[spread_key]] %in% spread_key_from_name[a])] <- spread_key_to_name[a] 
    }
  }
  
  for(x in 1:length(spread_var)){
    sym_spread_key <- sym(spread_key)
    sym_spread_var <- sym(spread_var[x])
    spread_ind_df <- df[,-which(colnames(df) %in% spread_var)]
    independent_names <- colnames(spread_ind_df)
    length_spread_ind <- length(colnames(spread_ind_df))
    length_independent <- length_spread_ind
    spread_var_name <- spread_var[x]
    spread_var_df <- df[,which(colnames(df) %in% spread_var_name)]
    spread_df <- spread_ind_df
    spread_df[[spread_var_name]] <- spread_var_df
    spread_df <- spread_df %>% spread(!!sym_spread_key, !!sym_spread_var) %>% as.data.frame()
    colnames(spread_df)[length_spread_ind:length(colnames(spread_df))] <- paste0(toupper(spread_var_name),"_",colnames(spread_df)[length_spread_ind:length(colnames(spread_df))])
    if(x == 1){
      spread_result <- spread_df
    }
    else if(x > 1){
      spread_result <- cbind(spread_result, spread_df[length_spread_ind:length(colnames(spread_df))])
    }
  }
  
  #change na values to 0
  spread_result[is.na(spread_result)] <- 0  
  
  if(reorder_bool){
    for(x in 1:length(time_vector)){
      spread_temp <- spread_result %>% select(contains(time_vector[x])) 
      reorder_eval <- paste0('spread_temp %>% select(contains("')
      for(g in 1:length(spread_var)){
        if(g != length(spread_var)){
          reorder_eval <- paste0(reorder_eval,spread_var[g],'"),contains("')
        }
        else{
          reorder_eval <- paste0(reorder_eval,spread_var[g],'"))')
        }
      }
      spread_temp <- eval(parse(text = reorder_eval))
      if(x == 1){
        spread_result_reorder <- spread_result[,1:(length_independent - 1)]
        spread_result_reorder <- cbind(spread_result_reorder, spread_temp)
        colnames(spread_result_reorder)[1:(length_independent - 1)] <- colnames(spread_result)[1:(length_independent - 1)]
      }
      else if(x > 1){
        spread_result_reorder <- cbind(spread_result_reorder, spread_temp)
      }
    }
    return(spread_result_reorder)
  }
  else{
    return(spread_result)
  }
}

gather_by_multiple_var <- function(df, idvars=c(), category_name = ""){
  library(tidyr)
  library(dplyr)
  library(rlang)
  library(lubridate)
  
  all_gather <- df %>%
    gather(Var, Value, -{{idvars}}) %>%
    mutate(Category = Var) %>%
    select(-Var)
  
  gather_organized <- NULL
  
  library(stringr)
  value_and_category <- as.data.frame(str_split_fixed(all_gather$Category, "_", 2))
  colnames(value_and_category) <- c("Value", "Category")
  all_gather$filter_by <- value_and_category$Value
  all_gather$sub_category <- value_and_category$Category
  
  #validate category if it dates then transform into dates
  date_validate <- FALSE
  check_year <- str_detect(value_and_category$Category, "[0-9]{4}")
  check_month <- rep(FALSE, length(check_year))
  
  capwords <- function(s, strict = FALSE) {
    cap <- function(s) paste(toupper(substring(s, 1, 1)),
                             {s <- substring(s, 2); if(strict) tolower(s) else s},
                             sep = "", collapse = " " )
    sapply(strsplit(s, split = " "), cap, USE.NAMES = !is.null(names(s)))
  }
  
  for(a in 1:nrow(value_and_category)){
    for(b in 1:12){
      check_month[a] <- str_detect(value_and_category$Category[a], month.name[b]) 
      if(check_month[a]){
        break
      }
    }
  }

  if(all(check_year) && all(check_month)){
    date_validate <- TRUE
    writeLines("Performing Date Extraction as well. . .")
    unique_columns <- unique(value_and_category$Value)
    for(h in 1:length(unique_columns)){
      gather_filtered <- all_gather %>% filter(filter_by == unique_columns[h]) %>% as.data.frame()
      value_category_table <- value_and_category %>% filter(Value == unique_columns[h]) %>% as.data.frame()
      if(h==1){
        yearly_date <- as.vector(str_match(value_category_table$Category, "[0-9]{4}"))
        monthly_name_date <- as.vector(str_match(value_category_table$Category, "[A-Za-z]+"))
        monthly_date <- c()
        for(f in 1:length(monthly_name_date)){
          monthly_date[f] <- which(month.name == monthly_name_date[f])
        }
        monthly_date <- ifelse(monthly_date < 10, paste0(0,monthly_date), paste0(monthly_date))
        complete_date <- as.Date(paste0(yearly_date, "-", monthly_date, "-01"))
        if(category_name == ""){
          gather_filtered$extracted_date <- complete_date
        }
        else{
          gather_filtered[[category_name]] <- complete_date
        }
        colnames(gather_filtered)[which(colnames(gather_filtered) == "Value")] <- unique_columns[h]
        gather_filtered <- gather_filtered[-which(colnames(gather_filtered) %in% c("Category","filter_by","sub_category"))] 
        gather_organized <- gather_filtered
      }
      else if(h > 1){
        colnames(gather_filtered)[which(colnames(gather_filtered) == "Value")] <- unique_columns[h]
        gather_organized[[unique_columns[h]]] <- gather_filtered[[unique_columns[h]]]
      }
    }
  }
  else{
    unique_columns <- unique(value_and_category$Value)
    for(h in 1:length(unique_columns)){
      gather_filtered <- all_gather %>% filter(filter_by == unique_columns[h]) %>% as.data.frame()
      value_category_table <- value_and_category %>% filter(Value == unique_columns[h]) %>% as.data.frame()
      if(h==1){
        if(category_name == ""){
          colnames(gather_filtered)[which(colnames(gather_filtered) == "sub_category")] <- "extracted_category"
        }
        else{
          colnames(gather_filtered)[which(colnames(gather_filtered) == "sub_category")] <- category_name
        }
        colnames(gather_filtered)[which(colnames(gather_filtered) == "Value")] <- unique_columns[h]
        gather_filtered <- gather_filtered[-which(colnames(gather_filtered) %in% c("filter_by"))]
        gather_organized <- gather_filtered
      }
      else if(h > 1){
        colnames(gather_filtered)[which(colnames(gather_filtered) == "Value")] <- unique_columns[h]
        gather_organized[[unique_columns[h]]] <- gather_filtered[[unique_columns[h]]]
      }
    }
  }
  
  #finally, order the category to first column
  sym_first_pos <- sym(colnames(gather_organized)[1])
  if(date_validate){
    if(category_name == ""){
      gather_organized <- gather_organized %>% relocate(extracted_date, .before = !!sym_first_pos)
    }
    else{
      sym_category <- sym(category_name)
      gather_organized <- gather_organized %>% relocate(!!sym_category, .before = !!sym_first_pos)
    }
  }
  else{
    if(category_name == ""){
      gather_organized <- gather_organized %>% relocate(extracted_category, .before = !!sym_first_pos)
    }
    else{
      sym_category <- sym(category_name)
      gather_organized <- gather_organized %>% relocate(!!sym_category, .before = !!sym_first_pos)
    }
  }

  return(gather_organized)
}

no_duplicate_ignore_column_order <- function(df, columns_to_consider=c()){
  return(df[!duplicated(apply(df[,columns_to_consider], 1, function(row) paste(sort(row), collapse=""))),])
}

#test_fungsi_pg_wide <- spread_by_multiple_var(fungsi_pg, "month_date", c("trx","sv","merchant"))
#test_va_permata <- spread_by_multiple_var(va_permata, "month_date", c("trx","sv"))
#idvars_fungsi_pg <- colnames(test_fungsi_pg_wide)[1]
#idvars_va_permata <- colnames(test_va_permata)[1:6]
#backward_fungsi_pg <- gather_by_multiple_var(test_fungsi_pg_wide, idvars_fungsi_pg, category_name = "month_date")
#backward_va_permata <- gather_by_multiple_var(test_va_permata, idvars_va_permata, category_name = "month_date")

#----------------------------- Activity Rate Monitoring by Periods-----------------------------
active_rate_monitoring <- function(data, date_col="", subject_col=c(), details=FALSE, 
                                   by_period="month", seq_label_period=FALSE){
  library(dplyr)
  library(tidyr)
  library(lubridate)
  date_sym <- sym(date_col)
  subject_sym <- NULL
  arrange_sym <- NULL
  if(length(subject_col) == 1){
    subject_sym <- sym(subject_col[1])
  }else if(length(subject_col) > 1){
    subject_sym <- syms(subject_col)
    arrange_sym <- sym(subject_col[1])
    writeLines('Using first Subject Var to Arrange in group by.. ')
  }
  
  data_monitor <- NULL
  period_length <- NULL
  period_vector <- NULL
  
  #Primary Functions to compute Active rate monitoring
  active_rate_count_freq <- function(vector, by_n_period=2){
    count <- 0
    g <- 1
    while(g < length(vector)){
      length_to <- (by_n_period+(g-1))
      if(length_to > length(vector)){
        break
      }
      vec <- vector[g:length_to]
      if(any(is.na(vec)) == FALSE){
        count <- count + 1
      }
      g <- g+1
    }
    return(count)
  }
  active_rate_string <- function(vector, by_n_period=2){
    string <- ""
    g <- 1
    while(g < length(vector)){
      length_to <- (by_n_period+(g-1))
      if(length_to > length(vector)){
        break
      }
      vec <- vector[g:length_to]
      if(any(is.na(vec)) == FALSE){
        if(string != ""){
          string <- paste0(string,", ",names(vec)[1], " => ", names(vec)[length(vec)])
        }else{
          string <- paste0(string, names(vec)[1], " => ", names(vec)[length(vec)])
        }
      }
      g <- g+1
    }
    return(string)
  }
  active_rate_period_over_period <- function(vector, start_period=1, end_period=2){
    bool <- FALSE
    vector_check <- vector[start_period:end_period]
    if(any(is.na(vector_check))){
      return(bool)
    }else{
      bool <- TRUE
      return(bool)
    }
  }
  
  if(by_period=="year"){
    if(length(subject_col) == 1){
      data_monitor <- data %>% group_by(!!subject_sym,
                                        year=floor_date(!!date_sym, "year")) %>% 
        summarise(count=n()) %>%
        arrange(!!subject_sym) %>%
        spread(year, count)
      
      period_length <- length(colnames(data_monitor[-1]))
      if(seq_label_period){
        colnames(data_monitor)[-1] <- paste0("Year_", seq(1,period_length, 1))
        period_vector <- paste0("Year_", seq(1,period_length, 1))
      }
    }else if(length(subject_col) > 1){
      data_monitor <- data %>% group_by(!!!subject_sym,
                                        year=floor_date(!!date_sym, "year")) %>% 
        summarise(count=n()) %>%
        arrange(!!arrange_sym) %>%
        spread(year, count)
      
      period_length <- length(colnames(data_monitor[-c(seq(1,length(subject_col),1))]))
      if(seq_label_period){
        colnames(data_monitor)[-c(seq(1,length(subject_col),1))] <- paste0("Year_", seq(1,period_length, 1))
        period_vector <- paste0("Year_", seq(1,period_length, 1))
      }
    }
  }
  else if(by_period=="month"){
    if(length(subject_col) == 1){
      data_monitor <- data %>% group_by(!!subject_sym,
                                        month=floor_date(!!date_sym, "month")) %>% 
        summarise(count=n()) %>%
        arrange(!!subject_sym) %>%
        spread(month, count)
      
      period_length <- length(colnames(data_monitor[-1]))
      if(seq_label_period){
        colnames(data_monitor)[-1] <- paste0("Month_", seq(1,period_length, 1))
        period_vector <- paste0("Month_", seq(1,period_length, 1))
      }
    }else if(length(subject_col) > 1){
      data_monitor <- data %>% group_by(!!!subject_sym,
                                        month=floor_date(!!date_sym, "month")) %>% 
        summarise(count=n()) %>%
        arrange(!!arrange_sym) %>%
        spread(month, count)
      
      period_length <- length(colnames(data_monitor[-c(seq(1,length(subject_col),1))]))
      if(seq_label_period){
        colnames(data_monitor)[-c(seq(1,length(subject_col),1))] <- paste0("Month_", seq(1,period_length, 1))
        period_vector <- paste0("Month_", seq(1,period_length, 1))
      }
    }
  }
  else if(by_period=="week"){
    if(length(subject_col) == 1){
      data_monitor <- data %>% group_by(!!subject_sym,
                                        week=floor_date(!!date_sym, "week")) %>% 
        summarise(count=n()) %>%
        arrange(!!subject_sym) %>%
        spread(week, count)
      
      period_length <- length(colnames(data_monitor[-1]))
      if(seq_label_period){
        colnames(data_monitor)[-1] <- paste0("Week_", seq(1,period_length, 1))
        period_vector <- paste0("Week_", seq(1,period_length, 1))
      }
    }else if(length(subject_col) > 1){
      data_monitor <- data %>% group_by(!!!subject_sym,
                                        week=floor_date(!!date_sym, "week")) %>% 
        summarise(count=n()) %>%
        arrange(!!arrange_sym) %>%
        spread(week, count)
      
      period_length <- length(colnames(data_monitor[-c(seq(1,length(subject_col),1))]))
      if(seq_label_period){
        colnames(data_monitor)[-c(seq(1,length(subject_col),1))] <- paste0("Week_", seq(1,period_length, 1))
        period_vector <- paste0("Week_", seq(1,period_length, 1))
      }
    }
  }
  
  #----------------- Make Matrix of Combination period over period -----------------------------
  writeLines("Computing Period Over Period Tables")
  period_combination <- as.data.frame(t(combn(period_vector, 2)))
  colnames(period_combination) <- c("From","To")
  period_combination$from_integer <- lapply(period_combination[,1], FUN=function(x) substr(x, nchar(x), nchar(x)))
  period_combination$to_integer <- lapply(period_combination[,2], FUN=function(x) substr(x, nchar(x), nchar(x)))
  period_combination$from_integer <- as.numeric(period_combination$from_integer)
  period_combination$to_integer <- as.numeric(period_combination$to_integer)
  period_combination$statement <- paste0(period_combination[,1], "=>",period_combination[,2])
  
  if(length(subject_col)==1){
    data_period_to_period <- data_monitor 
    for(period_collection in 1:nrow(period_combination)){
      from_index <- period_combination$from_integer[period_collection]
      to_index <- period_combination$to_integer[period_collection]
      varname <- period_combination$statement[period_collection]
      data_period_to_period[[varname]] <- apply(data_monitor[,-1], 
                                                FUN=function(x) active_rate_period_over_period(x,from_index,to_index), 
                                                MARGIN=1)
    }
  }
  else if(length(subject_col) > 1){
    data_period_to_period <- data_monitor
    for(period_collection in 1:nrow(period_combination)){
      from_index <- period_combination$from_integer[period_collection]
      to_index <- period_combination$to_integer[period_collection]
      varname <- period_combination$statement[period_collection]
      data_period_to_period[[varname]] <- apply(data_monitor[,-c(seq(1,length(subject_col),1))], 
                                                FUN=function(x) active_rate_period_over_period(x,from_index,to_index), 
                                                MARGIN=1)
    }
  }
  
  #----------------- Make Active Rate Monitoring by Counting Availability ----------------------
  writeLines("Computing Count Active rate in Subject over Periods...")
  if(length(subject_col)==1){
    data_monitor$total <- rowSums(data_monitor[-1], na.rm=TRUE)
    if(period_length == 1){
      writeLines("Cannot Make Active Rate Comparison with Period Length of 1! No Results!")
    }
    else{
      data_monitor_backup <- data_monitor
      for(b in 1:(period_length-1)){
        varname_freq <- paste0("P",b+1,"_active_freq")
        varname_maxperiod_possible <- paste0("P",b+1,"_active_max_possible")
        varname_active_pct <- paste0("P",b+1,"_active_percentage")
        varname_detail <- paste0("P",b+1,"_active_detail")
        varname_available <- paste0("P",b+1,"_availability")
        active_freq <- apply(data_monitor_backup[,-1], FUN=function(x) active_rate_count_freq(x, (b+1)), MARGIN=1)
        data_monitor[[varname_available]] <- ifelse(active_freq > 0, TRUE, FALSE)
        if(details){
          data_monitor[[varname_freq]] <- active_freq
          data_monitor[[varname_maxperiod_possible]] <- period_length-b
          data_monitor[[varname_active_pct]] <- round(data_monitor[[varname_freq]] / data_monitor[[varname_maxperiod_possible]],2)
          data_monitor[[varname_detail]] <- apply(data_monitor_backup[,-1], FUN=function(x) active_rate_string(x, (b+1)), MARGIN=1) 
        }
      }
    }
    data_monitor <- as.data.frame(data_monitor)
  }
  else if(length(subject_col) > 1){
    data_monitor$total <- rowSums(data_monitor[-c(seq(1,length(subject_col),1))], na.rm=TRUE)
    if(period_length == 1){
      writeLines("Cannot Make Active Rate Comparison with Period Length of 1! No Results!")
    }
    else{
      data_monitor_backup <- data_monitor
      for(b in 1:(period_length-1)){
        varname_freq <- paste0("P",b+1,"_active_freq")
        varname_maxperiod_possible <- paste0("P",b+1,"_active_max_possible")
        varname_active_pct <- paste0("P",b+1,"_active_percentage")
        varname_detail <- paste0("P",b+1,"_active_detail")
        varname_available <- paste0("P",b+1,"_availability")
        active_freq <- apply(data_monitor_backup[,-c(seq(1,length(subject_col),1))], FUN=function(x) active_rate_count_freq(x, (b+1)), MARGIN=1)
        data_monitor[[varname_available]] <- ifelse(active_freq > 0, TRUE, FALSE)
        if(details){
          data_monitor[[varname_freq]] <- active_freq
          data_monitor[[varname_maxperiod_possible]] <- period_length-b
          data_monitor[[varname_active_pct]] <- round(data_monitor[[varname_freq]] / data_monitor[[varname_maxperiod_possible]],2)
          data_monitor[[varname_detail]] <- apply(data_monitor_backup[,-c(seq(1,length(subject_col),1))], FUN=function(x) active_rate_string(x, (b+1)), MARGIN=1) 
        }
      }
    }
    data_monitor <- as.data.frame(data_monitor)
  }
  return(list(data_monitor, data_period_to_period))
}

#------------- example 
monitor_all_year <- active_rate_monitoring(merchant_trx_all, "transaction_time", 
                                           c("merchant_id","merchant_company"), details=FALSE,
                                           by_period = "year", seq_label_period = TRUE)
monitor_year_activerate <- monitor_all_year[[1]]
monitor_year_period2 <- monitor_all_year[[2]]
write.csv2(monitor_year_activerate, paste0(default_folder_path, "active_rate_timeline_yearly.csv"))
write.csv2(monitor_year_period2, paste0(default_folder_path, "active_rate_period_to_period_yearly.csv"))

monitor_all_year_detail <- active_rate_monitoring(merchant_trx_all, "transaction_time", 
                                                  c("merchant_id","merchant_company"), details=TRUE,
                                                  by_period = "year", seq_label_period = TRUE)
monitor_year_activerate <- monitor_all_year_detail[[1]]
monitor_year_period2 <- monitor_all_year_detail[[2]]
write.csv2(monitor_year_activerate, paste0(default_folder_path, "active_rate_timeline_yearly.csv"))
write.csv2(monitor_year_period2, paste0(default_folder_path, "active_rate_period_to_period_yearly.csv"))

monitor_all_month <- active_rate_monitoring(merchant_trx_all, "transaction_time", 
                                            c("merchant_id","merchant_company"), details=FALSE,
                                            by_period = "month", seq_label_period = TRUE)
monitor_month_activerate <- monitor_all_month[[1]]
monitor_month_period2 <- monitor_all_month[[2]]
write.csv2(monitor_month_activerate, paste0(default_folder_path, "active_rate_timeline_monthly.csv"))
write.csv2(monitor_month_period2, paste0(default_folder_path, "active_rate_period_to_period_monthly.csv"))

monitor_all_month_detail <- active_rate_monitoring(merchant_trx_all, "transaction_time", 
                                                   c("merchant_id","merchant_company"), details=FALSE,
                                                   by_period = "month", seq_label_period = TRUE)
monitor_month_activerate <- monitor_all_month_detail[[1]]
monitor_month_period2 <- monitor_all_month_detail[[2]]
write.csv2(monitor_month_activerate, paste0(default_folder_path, "active_rate_timeline_monthly.csv"))
write.csv2(monitor_month_period2, paste0(default_folder_path, "active_rate_period_to_period_monthly.csv"))

monitor_all_week <- active_rate_monitoring(merchant_trx_all, "transaction_time", 
                                           c("merchant_id","merchant_company"), details=FALSE,
                                           by_period="week", seq_label_period = TRUE)
monitor_week_activerate <- monitor_all_week[[1]]
monitor_week_period2 <- monitor_all_week[[2]]
write.csv2(monitor_week_activerate, paste0(default_folder_path, "active_rate_timeline_weekly.csv"))
write.csv2(monitor_week_period2, paste0(default_folder_path, "active_rate_period_to_period_weekly.csv"))

rm(list=ls()[-which(ls() %in% c("merchant_trx_all","default_folder_path","active_rate_monitoring"))])
monitor_all_week_detail <- active_rate_monitoring(merchant_trx_all, "transaction_time", 
                                                  c("merchant_id","merchant_company"), details=FALSE,
                                                  by_period="week", seq_label_period = TRUE)
monitor_week_activerate <- monitor_all_week_detail[[1]]
monitor_week_period2 <- monitor_all_week_detail[[2]]
write.csv2(monitor_week_activerate, paste0(default_folder_path, "active_rate_timeline_weekly.csv"))
write.csv2(monitor_week_period2, paste0(default_folder_path, "active_rate_period_to_period_weekly.csv"))

#----------------------------- Factor Counting in Dataset ----------------------
factor_availability_count <- function(data, sorted_count=FALSE){
  library(dplyr)
  every_class <- unlist(lapply(data, class))
  names(every_class) <- NULL
  excluded <- c()
  included <- c()
  for(f in 1:length(every_class)){
    if(every_class[f] %in% c("numeric","integer")){
      excluded <- c(excluded, f)
    }
  }
  all_column_index <- 1:length(colnames(data))
  if(length(excluded) > 0){
    included <- all_column_index[-which(all_column_index %in% excluded)]
  }else if(length(excluded) == 0){
    included <- all_column_index
  }
  if(length(included)==0){
    writeLines("There is no Factor/Character Columns in this dataset")
  }else{
    collected_groupby <- NULL
    colname_included <- colnames(data)[included]
    for(t in 1:length(colname_included)){
      sym_colname <- sym(colname_included[t])
      value <- colname_included[t]
      grouped <- data %>% group_by(!!sym_colname) %>% summarise(count = n()) %>% 
        mutate(column_names = value) %>% as.data.frame()
      colnames(grouped)[1] <- "factor_names"
      grouped <- grouped %>% select(column_names, factor_names, count) %>% as.data.frame()
      collected_groupby <- rbind(collected_groupby, grouped)
    }
    if(sorted_count){
      collected_groupby <- collected_groupby %>% arrange(desc(count))
    }
    return(collected_groupby)
  }
}

#----------------------------- Numerical Counting in Dataset ---------------------
custom_sequence_count <- function(data, column_sequence="", custom_sequence=c(), custom_label=c()){
  vector <- data[[column_sequence]]
  if(length(custom_sequence) > 0 && length(custom_label) > 0){
    if(length(custom_sequence) != length(custom_label)){
      message("Value and Label Parameter length is not equal, please fix parameter!")
      break
    } 
  }
  if(length(custom_label) > 0){
    sequence_countset <- c()
    #lower than minimal custom sequence
    sequence_countset <- c(sequence_countset, length(vector[vector < custom_sequence[1]]))
    names(sequence_countset)[1] <- paste0(column_sequence,"_lower_than_",custom_label[1])
    #in between custom sequence
    for(k in 1:(length(custom_sequence)-1)){
      sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[k] & vector <= custom_sequence[k+1]]))
      names(sequence_countset)[k+1] <- paste0(column_sequence,"_",custom_label[k]," to ",custom_label[k+1])
    }
    #higher than maximal custom sequence
    sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[length(custom_sequence)]]))
    names(sequence_countset)[length(sequence_countset)] <- paste0(column_sequence,"_higher_than_",
                                                                  custom_label[length(custom_label)])
    #add more attributes
    percentage_countset <- round(sequence_countset/sum(sequence_countset),3)
    names(percentage_countset) <- paste0(names(percentage_countset), " (pct)")
    order_countset <- order(percentage_countset, decreasing = TRUE)
    percentage_sort <- sort(percentage_countset, decreasing = TRUE)
    
    interval_description <- ""
    collected_pops <- 0
    idx <- 1
    bounds_before <- 0
    bounds_after <- 0
    while(collected_pops <= 0.95){
      if(collected_pops > 0){
        interval_description <- paste0(interval_description, ", ")
      }
      collected_pops <- collected_pops + percentage_sort[idx]
      percent_collected <- collected_pops * 100
      if(order_countset[idx] == 1){
        if(idx == 1){
          bounds_before <- min(custom_sequence)
          bounds_after <- custom_sequence[1]
        }else if(idx > 1){
          new_before <- min(custom_sequence)
          new_after <- custom_sequence[1]
          bounds_before <- min(c(bounds_before, new_before))
          bounds_after <- max(c(bounds_after, new_after))
        }
      }else if(order_countset[idx] > 1 && order_countset[idx] < length(order_countset)){
        if(idx == 1){
          bounds_before <- custom_sequence[order_countset[idx]-1]
          bounds_after <- custom_sequence[order_countset[idx]] 
        }else if(idx > 1){
          new_before <- custom_sequence[order_countset[idx]-1]
          new_after <- custom_sequence[order_countset[idx]]
          bounds_before <- min(c(bounds_before, new_before))
          bounds_after <- max(c(bounds_after, new_after))
        }
      }else if(order_countset[idx] == length(order_countset[idx])){
        if(idx == 1){
          bounds_before <- custom_sequence[length(custom_sequence)]
          bounds_after <- max(custom_sequence)
        }else if(idx > 1){
          new_before <- custom_sequence[length(custom_sequence)]
          new_after <- max(custom_sequence)
          bounds_before <- min(c(bounds_before, new_before))
          bounds_after <- max(c(bounds_after, new_after))
        }
      }
      interval_description <- paste0(interval_description, percent_collected, "% Population Achieved in the interval Bounds of ",
                                     bounds_before," to ",bounds_after)
      idx <- idx + 1
    }
    interval_description <- paste0(interval_description, ".")
    names(interval_description) <- "interval_desc"
    writeLines(paste0("Selected Sequence Bounds of ",column_sequence))
    print(custom_sequence)
    writeLines("Count Dataset within Bounds")
    print(sequence_countset)
    writeLines("Percentage Dataset within Bounds")
    print(percentage_countset)
    print(interval_description)
    all_info <- c(sequence_countset, percentage_countset, interval_description)
    
  }else if(length(custom_label) == 0){
    sequence_countset <- c()
    #lower than minimal custom sequence
    sequence_countset <- c(sequence_countset, length(vector[vector < custom_sequence[1]]))
    names(sequence_countset)[1] <- paste0(column_sequence,"_lower_than_",custom_sequence[1])
    #in between custom sequence
    for(k in 1:(length(custom_sequence)-1)){
      sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[k] & vector <= custom_sequence[k+1]]))
      names(sequence_countset)[k+1] <- paste0(column_sequence,"_",custom_sequence[k]," to ",custom_sequence[k+1])
    }
    #higher than maximal custom sequence
    sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[length(custom_sequence)]]))
    names(sequence_countset)[length(sequence_countset)] <- paste0(column_sequence,"_higher_than_",
                                                                  custom_sequence[length(custom_sequence)])
    #add more attributes
    percentage_countset <- round(sequence_countset/sum(sequence_countset),3)
    names(percentage_countset) <- paste0(names(percentage_countset), " (pct)")
    order_countset <- order(percentage_countset, decreasing = TRUE)
    percentage_sort <- sort(percentage_countset, decreasing = TRUE)
    
    interval_description <- ""
    collected_pops <- 0
    idx <- 1
    bounds_before <- 0
    bounds_after <- 0
    while(collected_pops <= 0.95){
      if(collected_pops > 0){
        interval_description <- paste0(interval_description, ",")
      }
      collected_pops <- collected_pops + percentage_sort[idx]
      percent_collected <- collected_pops * 100
      if(order_countset[idx] == 1){
        if(idx == 1){
          bounds_before <- min(custom_sequence)
          bounds_after <- custom_sequence[1]
        }else if(idx > 1){
          new_before <- min(custom_sequence)
          new_after <- custom_sequence[1]
          bounds_before <- min(c(bounds_before, new_before))
          bounds_after <- max(c(bounds_after, new_after))
        }
      }else if(order_countset[idx] > 1 && order_countset[idx] < length(order_countset)){
        if(idx == 1){
          bounds_before <- custom_sequence[order_countset[idx]-1]
          bounds_after <- custom_sequence[order_countset[idx]] 
        }else if(idx > 1){
          new_before <- custom_sequence[order_countset[idx]-1]
          new_after <- custom_sequence[order_countset[idx]]
          bounds_before <- min(c(bounds_before, new_before))
          bounds_after <- max(c(bounds_after, new_after))
        }
      }else if(order_countset[idx] == length(order_countset[idx])){
        if(idx == 1){
          bounds_before <- custom_sequence[length(custom_sequence)]
          bounds_after <- max(custom_sequence)
        }else if(idx > 1){
          new_before <- custom_sequence[length(custom_sequence)]
          new_after <- max(custom_sequence)
          bounds_before <- min(c(bounds_before, new_before))
          bounds_after <- max(c(bounds_after, new_after))
        }
      }
      interval_description <- paste0(interval_description, percent_collected, "% Population Achieved in the interval Bounds of ",
                                     bounds_before," to ",bounds_after)
      idx <- idx + 1
    }
    interval_description <- paste0(interval_description, ".")
    names(interval_description) <- "interval_desc"
    writeLines(paste0("Selected Sequence Bounds of ",column_sequence))
    print(custom_sequence)
    writeLines("Count Dataset within Bounds")
    print(sequence_countset)
    writeLines("Percentage Dataset within Bounds")
    print(percentage_countset)
    print(interval_description)
    all_info <- c(sequence_countset, percentage_countset, interval_description)
  }
  return(all_info)
}

numerical_sequence_count <- function(data, sequence_length=5, construct_df_format=TRUE){
  library(dplyr)
  every_class <- unlist(lapply(data, class))
  names(every_class) <- NULL
  included <- c()
  for(f in 1:length(every_class)){
    if(every_class[f] %in% c("numeric","integer")){
      included <- c(included, f)
    }
  }
  numerical_sequence_list <- list()
  all_column_index <- 1:length(colnames(data))
  if(length(included) > 0){
    for(h in 1:length(included)){
      writeLines(paste0("Counting Column ", colnames(data)[included[h]]))
      vector <- data[,included[h]]
      min_vector <- min(vector, na.rm=TRUE)
      max_vector <- max(vector, na.rm=TRUE)
      sequence_point <- seq(min_vector, max_vector, length.out=sequence_length)
      sequence_countset <- c()
      for(k in 1:(length(sequence_point)-1)){
        sequence_countset <- c(sequence_countset, length(vector[vector >= sequence_point[k] & vector <= sequence_point[k+1]]))
        names(sequence_countset)[k] <- paste0(colnames(data)[included[h]],"_",sequence_point[k]," to ",sequence_point[k+1])
      }
      #Add more attributes
      percentage_countset <- round(sequence_countset/sum(sequence_countset),3)
      names(percentage_countset) <- paste0(names(percentage_countset), " (pct)")
      order_countset <- order(percentage_countset, decreasing = TRUE)
      percentage_sort <- sort(percentage_countset, decreasing = TRUE)
      print(order_countset)
      print(percentage_sort)
      
      interval_description <- ""
      collected_pops <- 0
      idx <- 1
      bounds_before <- 0
      bounds_after <- 0
      while(collected_pops <= 0.95){
        if(collected_pops > 0){
          interval_description <- paste0(interval_description, ", ")
        }
        collected_pops <- collected_pops + percentage_sort[idx]
        percent_collected <- collected_pops * 100
        interval_description <- paste0(interval_description, percent_collected, 
                                       "% Population Achieved in the interval Bounds of ",
                                       names(percentage_sort[idx]))
        idx <- idx + 1
      }
      interval_description <- paste0(interval_description, ".")
      names(interval_description) <- "interval_desc"
      writeLines(paste0("Sequence Bounds of ",colnames(data)[included[h]]))
      print(sequence_point)
      writeLines("Count Dataset within Bounds")
      print(sequence_countset)
      writeLines("Percentage Dataset within Bounds")
      print(percentage_countset)
      print(interval_description)
      numerical_sequence_list <- c(numerical_sequence_list, 
                                   list(c(sequence_countset, percentage_countset, interval_description)))
    }
    if(construct_df_format){
      sequence_df <- NULL
      description_df <- NULL
      for(h in 1:length(numerical_sequence_list)){
        if(is.null(sequence_df)){
          sequence_df <- suppressWarnings(data.frame(variable_sequence_name = names(numerical_sequence_list[[h]])[1:(sequence_length-1)],
                                                     variable_sequence_count = as.numeric(numerical_sequence_list[[h]])[1:(sequence_length-1)],
                                                     variable_sequence_pct = as.numeric(numerical_sequence_list[[h]])[sequence_length:(2*(sequence_length-1))]))
          description_df <- data.frame(sequence_description = numerical_sequence_list[[h]][length(numerical_sequence_list[[h]])])
        }else{
          sequence_add <- suppressWarnings(data.frame(variable_sequence_name = names(numerical_sequence_list[[h]])[1:(sequence_length-1)],
                                                      variable_sequence_count = as.numeric(numerical_sequence_list[[h]])[1:(sequence_length-1)],
                                                      variable_sequence_pct = as.numeric(numerical_sequence_list[[h]])[sequence_length:(2*(sequence_length-1))]))
          description_add <- data.frame(sequence_description = numerical_sequence_list[[h]][length(numerical_sequence_list[[h]])])
          sequence_df <- rbind(sequence_df, sequence_add)
          description_df <- rbind(description_df, description_add)
        }
      }
      row.names(description_df) <- NULL
      return(list(sequence_df, description_df))
    }
    else{
      return(numerical_sequence_list) 
    }
  }else if(length(included) == 0){
    writeLines("There is no Numerical/Integer Columns in this dataset")
  }
}

quantile_count <- function(data, quantile_set = seq(0,1,0.1)){
  library(dplyr)
  every_class <- unlist(lapply(data, class))
  names(every_class) <- NULL
  included <- c()
  for(f in 1:length(every_class)){
    if(every_class[f] %in% c("numeric","integer")){
      included <- c(included, f)
    }
  }
  quantile_countset_list <- list()
  all_column_index <- 1:length(colnames(data))
  if(length(included) > 0){
    for(h in 1:length(included)){
      vector <- data[,included[h]]
      quantile_column <- quantile(vector, quantile_set)
      quantile_countset <- c()
      for(k in 1:(length(quantile_column)-1)){
        quantile_countset <- c(quantile_countset, length(vector[vector >= quantile_column[k] & vector <= quantile_column[k+1]]))
        names(quantile_countset)[k] <- paste0(colnames(data)[included[h]],"_",names(quantile_column[k]),"-",names(quantile_column[k+1]))
      }
      writeLines(paste0("Quantile Bounds of ",colnames(data)[included[h]]))
      print(quantile_column)
      writeLines("Count Dataset within Bounds")
      print(quantile_countset)
      quantile_countset_list <- c(quantile_countset_list, list(quantile_countset))
    }
    return(quantile_countset_list)
  }else if(length(included) == 0){
    writeLines("There is no Numerical/Integet Columns in this dataset")
  }
}

quantile_distribution <- function(value_vec, quantile_gap=0.1, value_abs = F,
                                  use_custom=F, custom_quantile=c(), 
                                  rounding_number = 2){
  library(dplyr)
  if(value_abs){
    value_vec <- abs(value_vec)
  }
  
  if(use_custom){
    quantile_value <- quantile(value_vec, custom_quantile)
  }else{
    quantile_set <- seq(0, 1, quantile_gap)
    quantile_value <- quantile(value_vec, quantile_set)
  }
  
  quantile_ranges <- c()
  quantile_dist <- c()
  for(b in 1:(length(quantile_value) - 1)){
    quantile_ranges <- c(quantile_ranges, paste0(round(quantile_value[b],rounding_number), "-",round(quantile_value[b+1],rounding_number)))
    quantile_dist <- c(quantile_dist, length(value_vec[which(value_vec >= quantile_value[b] & value_vec <= quantile_value[b+1])]))
    names(quantile_dist)[length(quantile_dist)] <- paste0(names(quantile_value[b]),"-",names(quantile_value[b+1]))
  }
  quantile_df <- data.frame(quantile_pct = names(quantile_dist),
                            quantile_value = quantile_ranges,
                            quantile_dist = as.numeric(quantile_dist))
  quantile_df <- quantile_df %>% arrange(desc(quantile_dist))
  return(quantile_df)
}

quantile_flexible_pct_combination <- function(quantile_pct_df, quantile_flex_length = 4){
  library(stringr)
  quantile_pct_df$quantile_value <- ifelse(grepl("\\-\\-", quantile_pct_df$quantile_value), 
                                           str_replace(quantile_pct_df$quantile_value, "\\-\\-", ":-"),
                                           str_replace(quantile_pct_df$quantile_value, "\\-", ":"))
  
  quantile_pct_from <- as.numeric(str_replace(unlist(lapply(strsplit(quantile_pct_df$quantile_pct,"\\-"), FUN = function(x) x[[1]])), "%", ""))
  quantile_pct_to <- as.numeric(str_replace(unlist(lapply(strsplit(quantile_pct_df$quantile_pct,"\\-"), FUN = function(x) x[[2]])), "%", ""))
  
  flex_diff <- quantile_pct_to[1] - quantile_pct_from[1]
  quantile_flex_df <- NULL
  for(f in 1:nrow(quantile_pct_df)){
    combn_seq <- list()
    for(g in 1:(quantile_flex_length + 1)){
      seq_now <- seq(quantile_pct_from[f] - quantile_flex_length + (g-1), quantile_pct_from[f] + (g-1), flex_diff)
      if(all(seq_now >= 0) && all(seq_now <= 100)){
        combn_seq <- c(combn_seq, list(seq_now))
      }
    }
    
    for(h in 1:length(combn_seq)){
      which_rows_to_sum <- which(quantile_pct_from %in% combn_seq[[h]])
      which_min_quantile <- which(quantile_pct_from == min(combn_seq[[h]]))
      which_max_quantile <- which(quantile_pct_to == max(combn_seq[[h]]))
      quantile_dist_sum <- sum(quantile_pct_df$quantile_dist[which_rows_to_sum])
      quantile_pct_name <- paste0(min(combn_seq[[h]]), "-", max(combn_seq[[h]]))
      quantile_value_name <- paste0(unlist(strsplit(quantile_pct_df$quantile_value[which_min_quantile], "\\:"))[1], ":",
                                    unlist(strsplit(quantile_pct_df$quantile_value[which_max_quantile], "\\:"))[2])
      if(is.null(quantile_flex_df)){
        quantile_flex_df <- as.data.frame(cbind(quantile_pct = quantile_pct_name, 
                                                quantile_value = quantile_value_name,
                                                quantile_dist = quantile_dist_sum))
      }else{
        quantile_flex_df_add <- as.data.frame(cbind(quantile_pct = quantile_pct_name, 
                                                    quantile_value = quantile_value_name,
                                                    quantile_dist = quantile_dist_sum))
        quantile_flex_df <- rbind(quantile_flex_df, quantile_flex_df_add)
      }
    }
  }
  quantile_flex_df <- quantile_flex_df[!duplicated(quantile_flex_df),]
  quantile_flex_df <- quantile_flex_df %>% arrange(desc(quantile_dist))
  return(quantile_flex_df)
}

quantile_find_minmax_std_deviance <- function(value_vec, 
                                             try_gap = c(0.33/seq(1,10,1),
                                                      0.25/seq(1,10,1),
                                                      0.2/seq(1,10,1),
                                                      0.1428/seq(1,10,1),
                                                      0.1111/seq(1,10,1)),
                                             priority = "max"){
  try_gap <- unique(try_gap)
  try_gap <- sort(try_gap, decreasing = T)
  std_value <- c()
  quantile_distribution <- function(value_vec, quantile_gap=0.1, 
                                    use_custom=F, custom_quantile=c()){
    library(dplyr)
    if(use_custom){
      quantile_value <- quantile(abs(value_vec), custom_quantile)
    }else{
      quantile_set <- seq(0, 1, quantile_gap)
      quantile_value <- quantile(abs(value_vec), quantile_set)
    }
    
    quantile_ranges <- c()
    quantile_dist <- c()
    for(b in 1:(length(quantile_value) - 1)){
      quantile_ranges <- c(quantile_ranges, paste0(round(quantile_value[b],2), "-",round(quantile_value[b+1],2)))
      quantile_dist <- c(quantile_dist, length(value_vec[which(value_vec >= quantile_value[b] & value_vec <= quantile_value[b+1])]))
      names(quantile_dist)[length(quantile_dist)] <- paste0(names(quantile_value[b]),"-",names(quantile_value[b+1]))
    }
    quantile_df <- data.frame(quantile_pct = names(quantile_dist),
                              quantile_value = quantile_ranges,
                              quantile_dist = as.numeric(quantile_dist))
    quantile_df <- quantile_df %>% arrange(desc(quantile_dist))
    return(quantile_df)
  }
  for(a in 1:length(try_gap)){
    x <- quantile_distribution(value_vec, try_gap[a])
    std_value <- c(std_value, sqrt(var(x$quantile_dist)))
  }
  
  quantile_dev_df <- data.frame(try_gap, std_value)
  
  if(priority == "max"){
    quantile_dev_df <- quantile_dev_df %>% arrange(desc(std_value))
  }else if(priority == "min"){
    quantile_dev_df <- quantile_dev_df %>% arrange(std_value)
  }
  return(quantile_dev_df)
}
#--------------------------- Precedence Vector Mapping ---------------------------
precedence_sequences <- function(precedence_vector, precedence_sequence=10, exact_sequence = FALSE, 
                                 var_name = "", lag_oriented_naming=T){
  precedence_table_list <- list()
  precedence_list <- list()
  for(x in 2:precedence_sequence){
    precedence_df <- data.frame(no = seq(1,(length(precedence_vector) - (x-1)),1))
    for(y in 1:x){
      limit_rows <- length(precedence_vector) - (x-y)
      seq_idx <- seq(y,limit_rows,1)
      precedence_df[[paste0("P",y)]] <- precedence_vector[seq_idx]
    }
    precedence_df_string <- paste0("precedence_df$P", seq(1,x,1), collapse=",' => ',")
    precedence_df_string <- paste0("paste0(",precedence_df_string,")")
    precedence_df$relationship <- eval(parse(text = precedence_df_string))
    precedence_df <- precedence_df[,-1]
    if(var_name != ""){
      library(stringr)
      colnames(precedence_df)[1:x] <- str_replace(colnames(precedence_df)[1:x], "P", var_name)
    }
    if(lag_oriented_naming){
      no_numbers <- str_replace(colnames(precedence_df)[1:x], "[0-9]", "")
      number_to_attach <- rev(seq(1,(x-1),1))
      colnames(precedence_df)[1:(x-1)] <-  paste0(no_numbers[1:(x-1)],"_",number_to_attach)
      colnames(precedence_df)[x] <- no_numbers[x]
    }
    precedence_list <- c(precedence_list, list(precedence_df))
    names(precedence_list)[x-1] <- paste0("Precedence_Pattern_",x)
    precedence_table_list <- c(precedence_table_list, list(sort(table(precedence_df$relationship), decreasing = TRUE)))
    names(precedence_table_list)[x-1] <- paste0("Precedence_Table_Pattern_",x)
  }
  if(exact_sequence){
    exact_df <- precedence_list[[(precedence_sequence-1)]]
    exact_table <- precedence_table_list[[(precedence_sequence-1)]]
    return(list(exact_df, exact_table))
  }else{
    return(list(precedence_list, precedence_table_list))
  }
}

#----------------------------- Experimental Designs ---------------------------------
experimental_design_model <- function(n_experiment, block, 
                                      treatment_vector=c(), 
                                      treatment2_vector=c(),
                                      seed=56, design_type="", randomize=TRUE, 
                                      note=TRUE, plot_number=0){
  library(agricolae)
  if(note){
    writeLines("====================== Types of Experimental Design: =========================")
    writeLines("1) RCBD (Randomized Complete Block Design), Complete Means Every")
    writeLines("Randomized Means Every treatment may appear in random orders")
    writeLines("Complate Means Every treatment must appear in block")
    writeLines("2) BIBD (Balanced Incomplete Block Design)")
    writeLines("Balanced Means Each Pair of Treatments occur together in a block")
    writeLines("Incomplete Means Not every treatment will appear in Block")
    writeLines('Block Means Experimental Group which is similiar to every group')
    writeLines("3) CRD (Complete Randomized Design with Equal or Different Repetition)")
    writeLines("4) Cyclic (Incomplete Block Design with Size k)")
    writeLines("5) Dau (Augmented Block Design, 2 Treatments Randomized Block)")
    writeLines("6) Alpha (Alpha Design similiar to Lattice,  but the tables are rectangular s by k (with s blocks and k<s columns)")
    writeLines("7) Lattice (Randomize Treatment in KxK Lattice)")
    writeLines("8) Split Plot Design (3 sub types design available = RCBD, CRD, LSD)")
    writeLines("9) Strip Plot Design")
    writeLines("10) Youden Design (Incomplete Latin Square Design)")
    writeLines("11) Factorial Design (Randomize Block Latin Square for N Factors) (3 sub types design available = RCBD, CRD, LSD)")
    writeLines("12) Latin Square (LSD), Similiar to RCBD but have 2 Block Factors")
    writeLines("All Factors must have the same number of levels")
    writeLines("Treatment and 2 Blocking Factors dont interact")
    writeLines("13) Graeco Latin Square, Similiar like Latin Square, but use 3 Blocking Factors")
  }
  if(design_type=="rcbd"){
    design <- design.rcbd(treatment_vector, r=n_experiment, seed=seed, serie=plot_number)
  }
  else if(design_type=="bibd"){
    bibd_lambda_calculator <- function(t, k, r){
      writeLines(paste0('Using Number of Treatments = ', t))
      writeLines(paste0('Using Number of Treatments per block= ', k))
      writeLines(paste0('Using Number of Repetitions = ', r))
      writeLines(paste0('Resulting Lambda of: ',(r*(k-1)) / (t-1)))
      lambda <- (r*(k-1)) / (t-1)
      return(lambda)
    }
    bibd_lambda <- bibd_lambda_calculator(length(treatment_vector), bibd_block, n_experiment)
    if(bibd_lambda == round(bibd_lambda)){
      design <- design.bib(treatment_vector, k=block, r=n_experiment, seed=seed)
    }else{
      message("Not Possible to make BIBD with the Lambda Provided")
    }
  }
  else if(design_type=="crd"){
    design <- design.crd(treatment_vector, r=n_experiment, serie = plot_number, seed = seed, 
                         kinds = "Super-Duper", randomization=randomize)
  }
  else if(design_type=="cyclic"){
    design <- design.cyclic(treatment_vector, k=block, r=n_experiment, 
                            serie = plot_number, seed=seed, randomization=randomize)
  }
  else if(design_type=="dau"){
    design <- design.dau(treatment_vector, treatment2_vector, r=n_experiment, 
                         serie = plot_number, seed=seed, randomization=randomize)
  }
  else if(design_type=="alpha"){
    design <- design.alpha(treatment_vector, k=block, r=n_experiment, 
                           serie = plot_number, seed = seed, randomization=randomize)
  }
  else if(design_type=="lattice"){
    design <- design.lattice(treatment_vector, r=n_experiment, 
                             serie = plot_number, seed = seed, randomization=randomize)
  }
  else if(design_type=="split_plot_rcbd"){
    design <- design.split(treatment_vector, treatment_vector2, r=n_experiment, design="rcbd",serie = plot_number,
                           seed = seed, kinds = "Super-Duper", first=TRUE, randomization=randomize)
  }
  else if(design_type=="split_plot_crd"){
    design <- design.split(treatment_vector, treatment_vector2, r=n_experiment, design="crd",serie = plot_number,
                           seed = seed, kinds = "Super-Duper", first=TRUE, randomization=randomize)
  }
  else if(design_type=="split_plot_lsd"){
    design <- design.split(treatment_vector, treatment_vector2, r=n_experiment, design="lsd",serie = plot_number,
                           seed = seed, kinds = "Super-Duper", first=TRUE, randomization=randomize)
  }
  else if(design_type=="strip_plot"){
    design <- design.strip(treatment_vector, treatment_vector2, r=n_experiment, 
                           serie = plot_number, seed = seed, randomization=randomize)
  }
  else if(design_type=="youden"){
    design <- design.youden(treatment_vector, r=n_experiment, 
                            serie = plot_number, seed = seed, 
                            first=TRUE,randomization=randomize)
  }
  else if(design_type=="factorial_ab_rcbd"){
    design <- design.ab(treatment_vector, r=n_experiment, serie = plot_number, 
                        design="rcbd",seed = seed, first=TRUE, randomization=randomize)
  }
  else if(design_type=="factorial_ab_crd"){
    design <- design.ab(treatment_vector, r=n_experiment, serie = plot_number, 
                        design="crd",seed = seed, first=TRUE, randomization=randomize)
  }
  else if(design_type=="factorial_ab_lsd"){
    design <- design.ab(treatment_vector, r=n_experiment, serie = plot_number, 
                        design="lsd",seed = seed, first=TRUE, randomization=randomize)
  }
  else if(design_type=="latin_square"){
    design <- design.lsd(treatment_vector, seed=seed)
  }
  else if(design_type=="graeco_latin_square"){
    design_graeco_custom <- function(trt1, trt2, serie = 2, seed = 0, kinds = "Super-Duper", 
                                     randomization = TRUE){
      number <- 10
      if (serie > 0) 
        number <- 10^serie
      r <- length(trt1)
      if (seed == 0) {
        genera <- runif(1)
        seed <- .Random.seed[3]
      }
      set.seed(seed, kinds)
      parameters <- list(design = "graeco", trt1 = trt1, 
                         trt2 = trt2, r = r, serie = serie, seed = seed, kinds = kinds, 
                         randomization)
      col <- rep(gl(r, 1), r)
      fila <- gl(r, r)
      fila <- as.character(fila)
      fila <- as.numeric(fila)
      plots <- fila * number + (1:r)
      C1 <- data.frame(plots, row = factor(fila), col)
      
      C2 <- C1
      a <- 1:(r * r)
      dim(a) <- c(r, r)
      for (i in 1:r) {
        for (j in 1:r) {
          k <- i + j - 1
          if (k > r) 
            k <- i + j - r - 1
          a[i, j] <- k
        }
      }
      m <- trt1
      if (randomization) 
        m <- sample(trt1, r)
      C1 <- data.frame(C1, m[a])
      m <- trt2
      if (randomization) 
        m <- sample(trt2, r)
      C2 <- data.frame(C2, m[a])
      ntr <- length(trt1)
      C1 <- data.frame(C1, B = 0)
      for (k in 1:r) {
        x <- C1[k, 4]
        i <- 1
        for (j in 1:(r^2)) {
          y <- C2[(k - 1) * r + i, 4]
          if (C1[j, 4] == x) {
            C1[j, 5] <- y
            i <- i + 1
          }
        }
      }
      
      C1[, 4] <- as.factor(C1[, 4])
      C1[, 5] <- as.factor(C1[, 5])
      names(C1)[4] <- c(paste(deparse(substitute(trt1))))
      names(C1)[5] <- c(paste(deparse(substitute(trt2))))
      outdesign <- list(parameters = parameters, 
                        sketch = matrix(paste(C1[,4], C1[,5]), 
                                        byrow = TRUE, ncol = r), book = C1)
      return(outdesign)
    }
    
    design <- design_graeco_custom(treatment_vector, treatment2_vector, serie=plot_number, 
                                   seed=seed, randomization=randomize)
  }
  return(design)
}

#----------------------------- Sampling Techniques with R ------------------------------------
sampling_technique <- function(data, n=50, frac=0.5, k_cluster,
                               use="n", method="simple",
                               random_systematic_start_pos=TRUE,
                               stratify_group_col="",
                               quota_specification = list(),
                               scaled_column_focus = "",
                               cluster_group_col="", 
                               save_row_index=TRUE){
  library(dplyr)
  library(rlang)
  library(caret)
  library(sampling)
  #https://stats.stackexchange.com/questions/117171/smart-sampling-techniques-in-r
  writeLines("====================================")
  writeLines("Sampling Technique included is: ")
  writeLines("1) Simple Random Sampling")
  writeLines("2) Stratified Random Sampling")
  writeLines("3) Systematic Random Sampling")
  writeLines("4) Convenient Random Sampling")
  writeLines("5) Quota Random Sampling")
  writeLines("6) Scaled Random Sampling")
  writeLines("7) Cluster Sampling")
  writeLines("====================================")
  n_by_frac <- round(frac * nrow(data),0)
  if(method == "simple"){
    writeLines('Sampling by Simple Random, Sampling pure random to data until N sample')
    if(use=="n"){
      sampling <- data[sample(1:nrow(data), n),]
    }else if(use=="frac"){
      sampling <- data[sample(1:nrow(data), n_by_frac),]
    }
    if(save_row_index){
      sampling$row_index <- as.numeric(row.names(sampling))
      row.names(sampling) <- NULL
    }
  }
  else if(method == "stratified"){
    writeLines('Sampling by Stratified, Sampling Each Group by an N/Group Sample')
    sym_col <- sym(stratify_group_col)
    if(use == "n"){
      sampling <- data %>% group_by(!!sym_col) %>% sample_n(n)
    }else if(use == "frac"){
      sampling <- data %>% group_by(!!sym_col) %>% sample_n(n_by_frac) 
    }
    if(save_row_index){
      sampling$row_index <- as.numeric(row.names(sampling))
      row.names(sampling) <- NULL
    }
  }
  else if(method == "systematic"){
    writeLines('Sampling by Systematic Order, Take each sample of every N Order')
    starting_point <- NULL
    n_to_select <- NULL
    if(random_systematic_start_pos){
      if(use == "n"){
        starting_point <- sample(1:n, 1)
        n_to_select <- (n-1)
      }else if(use == "frac"){
        starting_point <- sample(1:n_by_frac, 1)
        n_to_select <- (n_by_frac-1) 
      }
      move_forward <- nrow(data) - starting_point
      index_to_select <- round(seq(from=starting_point, to=nrow(data), length.out=move_forward))
      sampling <- data[index_to_select,]
    }
    else{
      if(use == "n"){
        index_to_select <- round(seq(from=1, to=nrow(data), length.out=n))
        sampling <- data[index_to_select,]
      }else if(use == "frac"){
        index_to_select <- round(seq(from=1, to=nrow(data), length.out=n_by_frac))
        sampling <- data[index_to_select,]
      }
    }
    if(save_row_index){
      sampling$row_index <- as.numeric(row.names(sampling))
      row.names(sampling) <- NULL
    }
  }
  else if(method == "convenient"){
    writeLines('Sampling by Convenient Methods, Take N Data start by random Position')
    if(use=="n"){
      starting_point <- sample(1:(nrow(data) - n), 1)
      sampling <- data[starting_point:(starting_point:(starting_point+n)),]
    }else if(use=="frac"){
      starting_point <- sample(1:(nrow(data) - n), 1)
      sampling <- data[starting_point:(starting_point:(starting_point+n)),]
    }
    if(save_row_index){
      sampling$row_index <- as.numeric(row.names(sampling))
      row.names(sampling) <- NULL
    }
  }
  else if(method == "quota"){
    writeLines('Sampling by Matching Multiple Criteria Groups on Datasets')
    column_names <- quota_specification[[1]]
    factor_to_focus <- quota_specification[[2]]
    take_sample <- quota_specification[[3]]
    sampling <- NULL
    row_index_collected <- NULL
    for(g in 1:length(column_names)){
      row_index <- which(data[[column_names[g]]] %in% c(factor_to_focus[g]))
      if(is.null(row_index_collected)){
        row_index_collected <- row_index
        samples <- sample(row_index, take_sample[g])
      }else{
        row_index_collected <- sampling$row_index
        validate <- which(row_index %in% row_index_collected)
        row_index <- row_index[-validate]
        samples <- sample(row_index, take_sample[g])
      }
      subsampling <- data[samples,]
      subsampling$row_index <- samples
      row.names(subsampling) <- NULL
      sampling <- rbind(sampling, subsampling)
      #print(head(sort(table(sampling$row_index), decreasing = TRUE)))
      writeLines(paste0(nrow(subsampling), " data related with ", column_names[g]," = ",factor_to_focus[g], " is sampled"))
    }
    if(save_row_index){
      return(sampling)
    }else{
      sampling <- sampling[,!colnames(sampling) %in% c("row_index")]
      return(sampling)
    }
  }
  else if(method == "scaled"){
    writeLines("Sample a Proportion of Data by Scaling Sample to the True Population of Groups")
    sym_scale <- sym(scaled_column_focus)
    scaled_unit <- data %>% group_by(!!sym_scale) %>% summarise(population_count = n()) %>%
      mutate(props = population_count/sum(population_count)) %>%
      mutate(applied_sample = round(props * n, 0)) %>% as.data.frame()
    print(head(scaled_unit))
    sampling <- NULL
    factor_info <- scaled_unit[,1]
    sample_info <- scaled_unit$applied_sample
    for(h in 1:length(sample_info)){
      row_index <- which(data[[scaled_column_focus]] %in% c(factor_info[h]))
      if(sample_info[h] > 0){
        subsample <- data[sample(row_index, sample_info[h]),]
        if(save_row_index){
          subsample$row_index <- as.numeric(row.names(subsample))
          row.names(subsample) <- NULL
        }
        sampling <- rbind(sampling, subsample)
      }
    }
  }
  else if(method == "cluster"){
    writeLines("Sample a Proportion of Data by selecting Several Groups as Clusters")
    clusters <- sample(unique(data[[cluster_group_col]]), size=k_cluster, replace=F)
    sampling <- data[data[[cluster_group_col]] %in% clusters, ]
    if(save_row_index){
      sampling$row_index <- as.numeric(row.names(sampling))
      row.names(sampling) <- NULL
    }
  }
  return(sampling)
}

#Example implementation of sampling technique using Mushroom Datasets
test_data <- read.csv(file.choose())
simple_samp <- sampling_technique(test_data, method="simple",n=200)
stratify_samp <- sampling_technique(test_data, method="stratified",
                                    stratify_group_col="stalk.surface.above.ring",n=200)
systematic_samp <- sampling_technique(test_data, method="systematic",n=200)
convinient_samp <- sampling_technique(test_data, method="convenient",n=200)
scaled_samp <- sampling_technique(test_data, method="scaled", 
                                  scaled_column_focus = "stalk.surface.above.ring",n=200)
quota_samp <- sampling_technique(test_data, method="quota",
                                 quota_specification = list(c("stalk.surface.above.ring",
                                                              "stalk.surface.below.ring",
                                                              "stalk.color.above.ring",
                                                              "stalk.color.below.ring"),
                                                            c("s","s","w","w"),
                                                            c(100,100,150,150)))
cluster_samp <- sampling_technique(test_data, k_cluster=10, 
                                   cluster_group_col = "CT", method="cluster")


#----------------------------- Regex Helper in R --------------------------------------------
regex_manipulation <- function(string_vector, regex_pattern, replace_pattern,
                               mode="find", return_mode="table"){
  library(stringr)
  finder_table <- cbind(str_match(string_vector, regex_pattern), string_vector)
  if(mode=="find"){
    if(return_mode=="table"){
      finder_table <- as.data.frame(finder_table)
      colnames(finder_table)[1] <- "matched"
      #finder_table <- finder_table[which(rowSums(is.na(finder_table))==0),]
      return(finder_table)
    }else if(return_mode=="boolean"){
      bool_vector <- ifelse(rowSums(is.na(finder_table))==0, TRUE, FALSE)
      return(bool_vector)
    }else if(return_mode=="index"){
      finder_table <- as.data.frame(finder_table)
      finder_table <- finder_table[which(rowSums(is.na(finder_table))==0),]
      return(as.numeric(row.names(finder_table)))
    }else if(return_mode=="vector"){
      finder_table <- finder_table[which(rowSums(is.na(finder_table))==0)]
      return(finder_table)
    }
  }else if(mode=="replace"){
    return(str_replace_all(string_vector, regex_pattern, replace_pattern))
  }
}

email <- c("xyz@yahoo.com","xyz@miaow",
           "xyz@gmail.com","xyz@yahoo@com",
           "xyz@ymail.com","xyz@GMAIL.com",
           "xyz@fas.edu","xyz@yahoo.COM")
regexemail <- "[a-z]+@[a-z]+\\.[a-z]{2,3}"

test <- regex_manipulation(email, regexemail, mode="find")
test <- regex_manipulation(email, regexemail, mode="find", return_mode="vector")
test <- regex_manipulation(email, regexemail, mode="find", return_mode="boolean")
test <- regex_manipulation(email, regexemail, mode="find", return_mode="index")

#---------------------> About Distribution, if it left skewed then Mean > Median
#---------------------> About Distribution, if it right skewed then Median > Mean
#----------------------------- Color Utility ----------------------------------
color_identity_hexadecimal <- function(color_vector){
  library(viridis)
  library(scales)
  show_col(color_vector)
}

color_palette_utility <- function(n, color_mode="", custom_focus="", note=TRUE){
  
  if(note){
    writeLines("Color Mode Option can be either rainbow, viridis, magma, plasma, inferno, and custom")
    writeLines("n is how many distinctive colour will be used in color mode selected")
    writeLines("For color mode = custom, the n parameter didnt give effect, take these notable options:")
    writeLines("custom_focus='' will return all the possible colors from colors()")
    writeLines("custom_focus='red' will return all the possible colors from colors() that most likely in red familiar")
    writeLines("custom_focus='yellow' will return all the possible colors from colors() that most likely in yellow familiar")
    writeLines("custom_focus='orange' will return all the possible colors from colors() that most likely in orange familiar")
    writeLines("custom_focus='light_chocolate' will return all the possible colors from colors() that most likely in light chocolate familiar")
    writeLines("custom_focus='dark_chocolate' will return all the possible colors from colors() that most likely in dark chocolate familiar")
    writeLines("custom_focus='blue' will return all the possible colors from colors() that most likely in blue familiar")
    writeLines("custom_focus='green' will return all the possible colors from colors() that most likely in green familiar")
    writeLines("custom_focus='white' will return all the possible colors from colors() that most likely in white familiar")
    writeLines("custom_focus='pink' will return all the possible colors from colors() that most likely in pink familiar")
    writeLines("custom_focus='violet' will return all the possible colors from colors() that most likely in violet familiar")
    writeLines("custom_focus='black' will return all the possible colors from colors() that most likely in black familiar")
  }
  
  library(viridis)
  library(RColorBrewer)
  
  color_filter <- colors()
  
  if(color_mode=="rainbow"){
    return(rainbow(n))
  }else if(color_mode=="viridis"){
    return(viridis(n))
  }else if(color_mode=="magma"){
    return(magma(n))
  }else if(color_mode=="plasma"){
    return(plasma(n))
  }else if(color_mode=="inferno"){
    return(inferno(n))
  }else if(color_mode=="custom"){
    regex_manipulation <- function(string_vector, regex_pattern, replace_pattern,
                                   mode="find", return_mode="table"){
      library(stringr)
      finder_table <- cbind(str_match(string_vector, regex_pattern), string_vector)
      if(mode=="find"){
        if(return_mode=="table"){
          finder_table <- as.data.frame(finder_table)
          colnames(finder_table)[1] <- "matched"
          #finder_table <- finder_table[which(rowSums(is.na(finder_table))==0),]
          return(finder_table)
        }else if(return_mode=="boolean"){
          bool_vector <- ifelse(rowSums(is.na(finder_table))==0, TRUE, FALSE)
          return(bool_vector)
        }else if(return_mode=="index"){
          finder_table <- as.data.frame(finder_table)
          finder_table <- finder_table[which(rowSums(is.na(finder_table))==0),]
          return(as.numeric(row.names(finder_table)))
        }else if(return_mode=="vector"){
          finder_table <- finder_table[which(rowSums(is.na(finder_table))==0)]
          return(finder_table)
        }
      }else if(mode=="replace"){
        return(str_replace_all(string_vector, regex_pattern, replace_pattern))
      }
    }
    
    if(custom_focus==""){
      return(color_filter)
    }
    else if(custom_focus=="red"){
      writeLines("Gathering all color with red family")
      red_family1 <- color_filter[regex_manipulation(color_filter, "^red", return_mode = "index")]
      red_family2 <- color_filter[regex_manipulation(color_filter, "^brown", return_mode = "index")]
      red_family3 <- color_filter[regex_manipulation(color_filter, "^firebrick", return_mode = "index")]
      red_family4 <- color_filter[regex_manipulation(color_filter, "^indianred", return_mode = "index")]
      red_family5 <- color_filter[regex_manipulation(color_filter, "^tomato", return_mode = "index")]
      red_family <- c(red_family1, red_family2, red_family3, red_family4, red_family5)
      return(red_family)
    }
    else if(custom_focus=="orange"){
      writeLines("Gathering all color with orange family")
      orange_family1 <- color_filter[regex_manipulation(color_filter, "^darkorange", return_mode = "index")]
      orange_family2 <- color_filter[regex_manipulation(color_filter, "^orange", return_mode = "index")]
      orange_family <- c(orange_family1, orange_family2)
      return(orange_family)
    }
    else if(custom_focus=="yellow"){
      writeLines("Gathering all color with yellow family")
      yellow_family1 <- color_filter[regex_manipulation(color_filter, "^darkgoldenrod", return_mode = "index")]
      yellow_family2 <- color_filter[regex_manipulation(color_filter, "^gold", return_mode = "index")]
      yellow_family3 <- color_filter[regex_manipulation(color_filter, "^lightgoldenrod", return_mode = "index")]
      yellow_family4 <- color_filter[regex_manipulation(color_filter, "^khaki", return_mode = "index")]
      yellow_family5 <- color_filter[regex_manipulation(color_filter, "^yellow", return_mode = "index")]
      
      yellow_family <- c(yellow_family1,yellow_family2,yellow_family3,yellow_family4, yellow_family5)
      return(yellow_family)
    }
    else if(custom_focus=="light_chocolate"){
      writeLines("Gathering all color with light chocolate family")
      light_chocolate_family1 <- color_filter[regex_manipulation(color_filter, "^antiquewhite", return_mode = "index")]
      light_chocolate_family2 <- color_filter[regex_manipulation(color_filter, "^bisque", return_mode = "index")]
      light_chocolate_family3 <- color_filter[regex_manipulation(color_filter, "^blanchedalmond", return_mode = "index")]
      light_chocolate_family4 <- color_filter[regex_manipulation(color_filter, "^beige", return_mode = "index")]
      light_chocolate_family5 <- color_filter[regex_manipulation(color_filter, "^burlywood", return_mode = "index")]
      light_chocolate_family6 <- color_filter[regex_manipulation(color_filter, "^lightsalmon", return_mode = "index")]
      light_chocolate_family7 <- color_filter[regex_manipulation(color_filter, "^navajowhite", return_mode = "index")]
      light_chocolate_family8 <- color_filter[regex_manipulation(color_filter, "^moccasin", return_mode = "index")]
      light_chocolate_family9 <- color_filter[regex_manipulation(color_filter, "^papayawhip", return_mode = "index")]
      light_chocolate_family10<- color_filter[regex_manipulation(color_filter, "^peachpuff", return_mode = "index")]
      light_chocolate_family11<- color_filter[regex_manipulation(color_filter, "^wheat", return_mode = "index")]
      light_chocolate_family12<- color_filter[regex_manipulation(color_filter, "^rosybrown", return_mode = "index")]
      light_chocolate <- c(light_chocolate_family1, light_chocolate_family2, light_chocolate_family3,
                           light_chocolate_family4, light_chocolate_family5, light_chocolate_family6,
                           light_chocolate_family7, light_chocolate_family8, light_chocolate_family9,
                           light_chocolate_family10, light_chocolate_family11, light_chocolate_family12)
      return(light_chocolate)
    }
    else if(custom_focus=="dark_chocolate"){
      writeLines("Gathering all color with dark chocolate family")
      dark_chocolate1 <- color_filter[regex_manipulation(color_filter, "^chocolate", return_mode = "index")]
      dark_chocolate2 <- color_filter[regex_manipulation(color_filter, "^coral", return_mode = "index")]
      dark_chocolate3 <- color_filter[regex_manipulation(color_filter, "^salmon", return_mode = "index")]
      dark_chocolate4 <- color_filter[regex_manipulation(color_filter, "^sienna", return_mode = "index")]
      dark_chocolate5 <- color_filter[regex_manipulation(color_filter, "^tan", return_mode = "index")]
      dark_chocolate <- c(dark_chocolate1, dark_chocolate2, dark_chocolate3, 
                          dark_chocolate4, dark_chocolate5)
    }
    else if(custom_focus=="blue"){
      writeLines("Gathering all color with blue family")
      blue1 <- color_filter[regex_manipulation(color_filter, "^blue", return_mode = "index")]
      blue2 <- color_filter[regex_manipulation(color_filter, "^darkblue", return_mode = "index")]
      blue3 <- color_filter[regex_manipulation(color_filter, "^cornflowerblue", return_mode = "index")]
      blue4 <- color_filter[regex_manipulation(color_filter, "^cyan", return_mode = "index")]
      blue5 <- color_filter[regex_manipulation(color_filter, "^darkcyan", return_mode = "index")]
      blue6 <- color_filter[regex_manipulation(color_filter, "^darkslateblue", return_mode = "index")]
      blue7 <- color_filter[regex_manipulation(color_filter, "^darkcyan", return_mode = "index")]
      blue8 <- color_filter[regex_manipulation(color_filter, "^darkturquoise", return_mode = "index")]
      blue9 <- color_filter[regex_manipulation(color_filter, "^dodgerblue", return_mode = "index")]
      blue10 <- color_filter[regex_manipulation(color_filter, "^lavenderblush1", return_mode = "index")]
      blue11 <- color_filter[regex_manipulation(color_filter, "^lightblue", return_mode = "index")]
      blue12 <- color_filter[regex_manipulation(color_filter, "^lightcyan", return_mode = "index")]
      blue13 <- color_filter[regex_manipulation(color_filter, "^lightseagreen", return_mode = "index")]
      blue14 <- color_filter[regex_manipulation(color_filter, "^lightskyblue", return_mode = "index")]
      blue15 <- color_filter[regex_manipulation(color_filter, "^lightsteelblue", return_mode = "index")]
      blue16 <- color_filter[regex_manipulation(color_filter, "^lightslateblue", return_mode = "index")]
      blue17 <- color_filter[regex_manipulation(color_filter, "^mediumblue", return_mode = "index")]
      blue18 <- color_filter[regex_manipulation(color_filter, "^midnightblue", return_mode = "index")]
      blue19 <- color_filter[regex_manipulation(color_filter, "^mediumslateblue", return_mode = "index")]
      blue20 <- color_filter[regex_manipulation(color_filter, "^navy", return_mode = "index")]
      blue21 <- color_filter[regex_manipulation(color_filter, "^paleturquoise", return_mode = "index")]
      blue22 <- color_filter[regex_manipulation(color_filter, "^powderblue", return_mode = "index")]
      blue23 <- color_filter[regex_manipulation(color_filter, "^royalblue", return_mode = "index")]
      blue24 <- color_filter[regex_manipulation(color_filter, "^skyblue", return_mode = "index")]
      blue25 <- color_filter[regex_manipulation(color_filter, "^slateblue", return_mode = "index")]
      blue26 <- color_filter[regex_manipulation(color_filter, "^steelblue", return_mode = "index")]
      blue27 <- color_filter[regex_manipulation(color_filter, "^turquoise", return_mode = "index")]
      blue <- c(blue1, blue2, blue3, blue4, blue5, blue6, blue7, blue8, blue9, blue10, blue11, blue12,
                blue13, blue14, blue15, blue16, blue17, blue18, blue19, blue20, blue21, blue22, blue23, 
                blue24, blue25, blue26, blue27)
      return(blue)
    }
    else if(custom_focus=="green"){
      writeLines("Gathering all color with green family")
      green1 <- color_filter[regex_manipulation(color_filter, "^aquamarine", return_mode = "index")]
      green2 <- color_filter[regex_manipulation(color_filter, "^azure", return_mode = "index")]
      green3 <- color_filter[regex_manipulation(color_filter, "^chartreuse", return_mode = "index")]
      green4 <- color_filter[regex_manipulation(color_filter, "^azure", return_mode = "index")]
      green5 <- color_filter[regex_manipulation(color_filter, "^darkgreen", return_mode = "index")]
      green6 <- color_filter[regex_manipulation(color_filter, "^darkolivegreen", return_mode = "index")]
      green7 <- color_filter[regex_manipulation(color_filter, "^darkseagreen", return_mode = "index")]
      green8 <- color_filter[regex_manipulation(color_filter, "^forestgreen", return_mode = "index")]
      green9 <- color_filter[regex_manipulation(color_filter, "^green", return_mode = "index")]
      green10 <- color_filter[regex_manipulation(color_filter, "^lawngreen", return_mode = "index")]
      green11 <- color_filter[regex_manipulation(color_filter, "^limegreen", return_mode = "index")]
      green12 <- color_filter[regex_manipulation(color_filter, "^mediumspringgreen", return_mode = "index")]
      green13 <- color_filter[regex_manipulation(color_filter, "^olivedrab", return_mode = "index")]
      green14 <- color_filter[regex_manipulation(color_filter, "^mediumaquamarine", return_mode = "index")]
      green15 <- color_filter[regex_manipulation(color_filter, "^mediumseagreen", return_mode = "index")]
      green16 <- color_filter[regex_manipulation(color_filter, "^palegreen", return_mode = "index")]
      green17 <- color_filter[regex_manipulation(color_filter, "^seagreen", return_mode = "index")]
      green18 <- color_filter[regex_manipulation(color_filter, "^springgreen", return_mode = "index")]
      green19 <- color_filter[regex_manipulation(color_filter, "^yellowgreen", return_mode = "index")]
      green <- c(green1, green2, green3, green4, green5, green6, green7, green8, green9, green10, green11, green12,
                 green13, green14, green15, green16, green17, green18, green19, green20, green21, green22, green23, 
                 green24, green25, green26, green27)
      return(green)
    }
    else if(custom_focus=="white"){
      writeLines("Gathering all color with white family")
      white1 <- color_filter[regex_manipulation(color_filter, "^white", return_mode = "index")]
      white2 <- color_filter[regex_manipulation(color_filter, "^cornsilk", return_mode = "index")]
      white3 <- color_filter[regex_manipulation(color_filter, "^floralwhite", return_mode = "index")]
      white4 <- color_filter[regex_manipulation(color_filter, "^gainsboro", return_mode = "index")]
      white5 <- color_filter[regex_manipulation(color_filter, "^ghostwhite", return_mode = "index")]
      white6 <- color_filter[regex_manipulation(color_filter, "^honeydew", return_mode = "index")]
      white7 <- color_filter[regex_manipulation(color_filter, "^cornsilk", return_mode = "index")]
      white8 <- color_filter[regex_manipulation(color_filter, "^ivory", return_mode = "index")]
      white9 <- color_filter[regex_manipulation(color_filter, "^lightgoldenrodyellow", return_mode = "index")]
      white10 <- color_filter[regex_manipulation(color_filter, "^lightyellow", return_mode = "index")]
      white11 <- color_filter[regex_manipulation(color_filter, "^linen", return_mode = "index")]
      white12 <- color_filter[regex_manipulation(color_filter, "^seashell", return_mode = "index")]
      white13 <- color_filter[regex_manipulation(color_filter, "^snow", return_mode = "index")]
      white14 <- color_filter[regex_manipulation(color_filter, "^whitesmoke", return_mode = "index")]
      white <- c(white1, white2, white3, white4, white5,
                 white6, white7, white8, white9, white10,
                 white11, white12, white13, white14)
      return(white)
    }
    else if(custom_focus=="pink"){
      writeLines("Gathering all color with pink family")
      pink1 <- color_filter[regex_manipulation(color_filter, "^deeppink", return_mode = "index")]
      pink2 <- color_filter[regex_manipulation(color_filter, "^hotpink", return_mode = "index")]
      pink3 <- color_filter[regex_manipulation(color_filter, "^palevioletred", return_mode = "index")]
      pink4 <- color_filter[regex_manipulation(color_filter, "^pink", return_mode = "index")]
      pink5 <- color_filter[regex_manipulation(color_filter, "^violetred", return_mode = "index")]
      pink6 <- color_filter[regex_manipulation(color_filter, "^maroon", return_mode = "index")]
      pink <- c(pink1, pink2, pink3, pink4, pink5, pink6)
      return(pink)
    }
    else if(custom_focus=="violet"){
      writeLines("Gathering all color with violet family")
      violet1 <- color_filter[regex_manipulation(color_filter, "^darkmagenta", return_mode = "index")]
      violet2 <- color_filter[regex_manipulation(color_filter, "^darkorchid", return_mode = "index")]
      violet3 <- color_filter[regex_manipulation(color_filter, "^darkviolet", return_mode = "index")]
      violet4 <- color_filter[regex_manipulation(color_filter, "^darkmagenta", return_mode = "index")]
      violet5 <- color_filter[regex_manipulation(color_filter, "^magenta", return_mode = "index")]
      violet6 <- color_filter[regex_manipulation(color_filter, "^mediumorchid", return_mode = "index")]
      violet7 <- color_filter[regex_manipulation(color_filter, "^mediumpurple", return_mode = "index")]
      violet8 <- color_filter[regex_manipulation(color_filter, "^orchid", return_mode = "index")]
      violet9 <- color_filter[regex_manipulation(color_filter, "^mediumviolet", return_mode = "index")]
      violet10 <- color_filter[regex_manipulation(color_filter, "^plum", return_mode = "index")]
      violet11 <- color_filter[regex_manipulation(color_filter, "^purple", return_mode = "index")]
      violet12 <- color_filter[regex_manipulation(color_filter, "^thistle", return_mode = "index")]
      violet13 <- color_filter[regex_manipulation(color_filter, "^violet", return_mode = "index")]
      violet14 <- color_filter[regex_manipulation(color_filter, "^violetred", return_mode = "index")]
      violet <- c(violet1, violet2, violet3, violet4, violet5, violet6,
                  violet7, violet8, violet9, violet10, violet11, violet12, violet13, violet14)
      return(violet)
    }
    else if(custom_focus=="black"){
      writeLines("Gathering all color with Black & Gray family")
      black1 <- color_filter[regex_manipulation(color_filter, "^black", return_mode = "index")]
      black2 <- color_filter[regex_manipulation(color_filter, "^grey", return_mode = "index")]
      black3 <- color_filter[regex_manipulation(color_filter, "^gray", return_mode = "index")]
      black4 <- color_filter[regex_manipulation(color_filter, "^azure4", return_mode = "index")]
      black5 <- color_filter[regex_manipulation(color_filter, "^antiquewhite4", return_mode = "index")]
      black6 <- color_filter[regex_manipulation(color_filter, "^cornsilk4", return_mode = "index")]
      black7 <- color_filter[regex_manipulation(color_filter, "^honeydew4", return_mode = "index")]
      black8 <- color_filter[regex_manipulation(color_filter, "^ivory4", return_mode = "index")]
      black9 <- color_filter[regex_manipulation(color_filter, "^lavenderblush4", return_mode = "index")]
      black10 <- color_filter[regex_manipulation(color_filter, "^lightcyan44", return_mode = "index")]
      black11 <- color_filter[regex_manipulation(color_filter, "^lightyellow4", return_mode = "index")]
      black12 <- color_filter[regex_manipulation(color_filter, "^peachpuff4", return_mode = "index")]
      black13 <- color_filter[regex_manipulation(color_filter, "^seashell4", return_mode = "index")]
      black14 <- color_filter[regex_manipulation(color_filter, "^wheat4", return_mode = "index")]
      black <- c(black1, black2, black3, black4, black5, black6,
                 black7, black8, black9, black10, black11, black12,
                 black13, black14)
    }
  }
}

#----------------------------- Convert list data type to matrix ----------------------------------------------
convert_list_to_matrix <- function(big_list){
  matrix_bind <- NULL
  for(x in 1:length(big_list)){
    matrix_bind <- cbind(matrix_bind, big_list[[x]])
  }
  return(matrix_bind)
}

#----------------------------- Legend symbols guideline ---------------------------------------------------
pch_plot_legend_guideline <- function(){
  writeLines("Source References: https://www.datanovia.com/en/blog/pch-in-r-best-tips/")
  writeLines("pch = 0, square")
  writeLines("pch = 1, circle")
  writeLines("pch = 2, triangle point up")
  writeLines("pch = 3, plus")
  writeLines("pch = 4, cross")
  writeLines("pch = 5, diamond")
  writeLines("pch = 6, triangle point down")
  writeLines("pch = 7, square cross")
  writeLines("pch = 8, star")
  writeLines("pch = 9, diamond plus")
  writeLines("pch = 10, circle plus")
  writeLines("pch = 11, triangles up and down")
  writeLines("pch = 12, square plus")
  writeLines("pch = 13, circle cross")
  writeLines("pch = 14, square and triangle down")
  writeLines("pch = 15, filled square")
  writeLines("pch = 16, filled circle")
  writeLines("pch = 17, filled triangle point-up")
  writeLines("pch = 18, filled diamond")
  writeLines("pch = 19, solid circle")
  writeLines("pch = 20, bullet (smaller circle)")
  writeLines("pch = 21, filled circle blue")
  writeLines("pch = 22, filled square blue")
  writeLines("pch = 23, filled diamond blue")
  writeLines("pch = 24, filled triangle point-up blue")
  writeLines("pch = 25, filled triangle point down blue")
}

#----------------------------- Calculate Character Occurences in a given String -------------------------------------
#mode can be either max or vector
calculate_max_char_occurences <- function(string_vector, search_for=".", mode="max"){
  library(stringr)
  num_vector <- c()
  if(grepl('[^[:punct:]]', search_for) == FALSE){
    search_for <- paste0("\\",search_for)
  }
  for(a in 1:length(string_vector)){
    num_vector <- c(num_vector, str_count(string_vector[a], search_for))
  }
  if(mode=="max"){
    return(max(num_vector))
  }
  else if(mode=="vector"){
    return(num_vector)
  }
}

#----------------------------- Return a character that repeated the most in string --------------------------
most_repeated_character <- function(vector_string, filter_max=1){
  most_repeat <- c()
  for(g in 1:length(vector_string)){
    if(filter_max > 0){
      tab <- table(strsplit(vector_string[g], '')[[1]])
      max_tab <- max(tab)
      if(max_tab <= filter_max){
        most_repeat <- c(most_repeat, "")
      }
      else{
        most_repeat <- c(most_repeat, names(tab)[tab == max(tab)])
      }
    }
    else{
      tab <- table(strsplit(vector_string[g], '')[[1]])
      most_repeat <- c(most_repeat, names(tab)[tab == max(tab)])
    }
  }
  return(most_repeat) 
}

#----------------------------- Reverse a String name by seperator -------------------------
reverse_string_by_sep <- function(vector_string, sep="[.]"){
  splitting = strsplit(vector_string, sep)
  for(h in 1:length(splitting)){
    if(length(splitting[[h]]) == 1){
      splitting[[h]] = splitting[[h]][1]
    }
    else if(length(splitting[[h]]) == 2){
      splitting[[h]] = paste0(splitting[[h]][2],".",splitting[[h]][1])
    }
  }
  splitting = unlist(splitting)
  return(splitting)
}

#example implementation
library(MNP)
data(detergent)
mldata2 <- detergent
colnames(mldata2) <- str_replace(colnames(mldata2), "Price", ".Price")
colnames(mldata2) <- reverse_concat_string(colnames(mldata2))

#----------------------------- Combination and Permutation -----------------------------------------------
factorial_next_limit_searcher <- function(n_var, by_limit=c(170)){
  fact_results <- NA
  for(x in 1:n_var){
    if(x == 1){
      fact_results <- data.frame(n_var = x, factorial_var = factorial(x), over_limit=FALSE)
    }
    else if(x > 1){
      if(factorial(x) == Inf){
        if(length(by_limit) == 1){
          if(prod((by_limit+1):x) != Inf){
            fact_results <- rbind(fact_results, data.frame(n_var = x, factorial_var = prod((by_limit+1):x), over_limit=TRUE))
          }else{
            writeLines(paste0('Next Limit Factorial is ', nrow(fact_results)))
            return(fact_results)
          }
        }else{
          limiter <- which(by_limit < x)
          limiter <- limiter[length(limiter)]
          if(prod(((by_limit[limiter])+1):x) != Inf){
            fact_results <- rbind(fact_results, data.frame(n_var = x, factorial_var = prod((by_limit[limiter]+1):x), over_limit=TRUE))
          }else{
            writeLines(paste0('Next Limit Factorial is ', nrow(fact_results)))
            return(fact_results)
          }
        }
      }
      else{
        fact_results <- rbind(fact_results, data.frame(n_var = x, factorial_var = factorial(x), over_limit=FALSE))
      }
    }
  }
  writeLines("Still Not Found any limit with give n_var!")
  return(fact_results)
}

limit1 <- factorial_next_limit_searcher(305,c(170))
limit2 <- factorial_next_limit_searcher(500,c(170,300))
limit3 <- factorial_next_limit_searcher(605,c(170,300,420))
limit4 <- factorial_next_limit_searcher(705,c(170,300,420,535))
limit5 <- factorial_next_limit_searcher(805,c(170,300,420,535,646))
limit6 <- factorial_next_limit_searcher(905,c(170,300,420,535,646,754))
limit7 <- factorial_next_limit_searcher(1005,c(170,300,420,535,646,754,860))
limit8 <- factorial_next_limit_searcher(1105,c(170,300,420,535,646,754,860,964))
limit9 <- factorial_next_limit_searcher(1200,c(170,300,420,535,646,754,860,964,1066))
limit10 <- factorial_next_limit_searcher(1305,c(170,300,420,535,646,754,860,964,1066,1167))
limit11 <- factorial_next_limit_searcher(1405,c(170,300,420,535,646,754,860,964,1066,1167,1266))
limit12 <- factorial_next_limit_searcher(1505,c(170,300,420,535,646,754,860,964,1066,1167,1266,1364))
limit13 <- factorial_next_limit_searcher(1605,c(170,300,420,535,646,754,860,964,1066,1167,1266,1364,1461))
limit14 <- factorial_next_limit_searcher(2005,c(170,300,420,535,646,754,860,964,1066,1167,1266,1364,1461,1557))
limit15 <- factorial_next_limit_searcher(2005,c(170,300,420,535,646,754,860,964,1066,1167,1266,1364,1461,1557,1653))
limit16 <- factorial_next_limit_searcher(2005,c(170,300,420,535,646,754,860,964,1066,1167,1266,1364,1461,1557,1653,1748))
limit17 <- factorial_next_limit_searcher(2005,c(170,300,420,535,646,754,860,964,1066,1167,1266,1364,1461,1557,1653,1748,1842))
limit18 <- factorial_next_limit_searcher(2205,c(170,300,420,535,646,754,860,964,1066,1167,1266,1364,1461,1557,1653,1748,1842,1936,2029))

calc_combination <- function(n,r,limit_breaker_factorial = c(170,300,420,535,646,754,860,964,1066,1167,
                                                             1266,1364,1461,1557,1653,1748,1842,1936,2029)){
  if(n <= 170){
    return(factorial(n) / (factorial(r) * factorial(n-r)))
  }else{
    #save it in list of vectors for indirect computation
    prod_main_list <- list()
    prod_divider_list <- list()
    prod1_length <- sum(limit_breaker_factorial < n) + 1
    prod2_length <- sum(limit_breaker_factorial < r) + 1
    prod3_length <- sum(limit_breaker_factorial < (n-r)) + 1
    for(x in 1:prod1_length){
      if(x < prod1_length){
        starter <- 0
        if(x==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[x-1] + 1
        }
        prod_main_list <- append(prod_main_list, list(starter:limit_breaker_factorial[x]))
      }else if(x == prod1_length){
        starter <- 0
        if(x==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[x-1] + 1
        }
        prod_main_list <- append(prod_main_list, list(starter:n))
      }
    }
    for(y in 1:prod2_length){
      if(y < prod2_length){
        starter <- 0
        if(y==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[y-1] + 1
        }
        prod_divider_list <- append(prod_divider_list, list(starter:limit_breaker_factorial[y]))
      }else if(y == prod2_length){
        starter <- 0
        if(y==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[y-1] + 1
        }
        prod_divider_list <- append(prod_divider_list, list(starter:r))
      }
    }
    for(z in 1:prod3_length){
      if(z < prod3_length){
        starter <- 0
        if(z==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[z-1] + 1
        }
        prod_divider_list <- append(prod_divider_list, list(starter:limit_breaker_factorial[z]))
      }
      else if(z == prod3_length){
        starter <- 0
        if(z==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[z-1] + 1
        }
        prod_divider_list <- append(prod_divider_list, list(starter:(n-r)))
      }
    }
    
    prod_main_remain <- list()
    prod_divider_remain <- list()
    first_fac_idx <- which(unlist(lapply(prod_divider_list, FUN = function(x) any(x == 1))))
    length_main <- length(prod_main_list)
    length_divider <- length(prod_divider_list)
    length_nr <- length_divider - first_fac_idx[2] + 1
    length_r <- length_divider - length_nr
    if(length_nr >= length_r){
      nr_starter <- first_fac_idx[2]
      for(divide in 1:length_nr){
        remainer_main <- which(!prod_main_list[[divide]] %in% prod_divider_list[[divide + nr_starter - 1]])
        if(length(remainer_main) != 0){
          prod_main_remain <- append(prod_main_remain, list(prod_main_list[[divide]][remainer_main]))
        }
      }
      if(length_main > length_nr){
        for(undivide in (length_nr+1):length_main){
          prod_main_remain <- append(prod_main_remain, list(prod_main_list[[undivide]]))
        }
      }
      for(divide_remain in 1:(nr_starter - 1)){
        prod_divider_remain <- append(prod_divider_remain, list(prod_divider_list[[divide_remain]]))
      }
    }
    else if(length_nr < length_r){
      r_starter <- first_fac_idx[1]
      for(divide in 1:length_r){
        remainer_main <- which(!prod_main_list[[divide]] %in% prod_divider_list[[divide + r_starter - 1]])
        if(length(remainer_main) != 0){
          prod_main_remain <- append(prod_main_remain, list(prod_main_list[[divide]][remainer_main]))
        }
      }
      if(length_main > length_r){
        for(undivide in (length_r+1):length_main){
          prod_main_remain <- append(prod_main_remain, list(prod_main_list[[undivide]]))
        }
      }
      for(divide_remain in (length_r+1):length_divider){
        prod_divider_remain <- append(prod_divider_remain, list(prod_divider_list[[divide_remain]]))
      }
    }
    
    prod_main_vec <- unlist(lapply(prod_main_remain, FUN = function(x) prod(x)))
    prod_divider_vec <- unlist(lapply(prod_divider_remain, FUN = function(x) prod(x)))
    
    for(grand_divide in 1:length(prod_divider_vec)){
      prod_main_vec[grand_divide] <- prod_main_vec[grand_divide] / prod_divider_vec[grand_divide]
    }
    
    grand_result <- prod(prod_main_vec)
    #for debug
    #return(list(prod_main_list = prod_main_list, prod_divider_list = prod_divider_list, 
    #            prod_main_remain = prod_main_remain, prod_divider_remain = prod_divider_remain, 
    #            grand_result = grand_result))
    if(grand_result == Inf){
      return(prod_main_vec)
    }else{
      return(grand_result)
    }
    
    
  }
}
calc_permutation <- function(n,r,limit_breaker_factorial = c(170,300,420,535,646,754,860,964,1066,1167,
                                                             1266,1364,1461,1557,1653,1748,1842,1936,2029)){
  if(n <= 170){
    return(factorial(n) / (factorial(n-r)))
  }else{
    #save it in list of vectors for indirect computation
    prod_main_list <- list()
    prod_divider_list <- list()
    prod1_length <- sum(limit_breaker_factorial < n) + 1
    prod2_length <- sum(limit_breaker_factorial < (n-r)) + 1
    for(x in 1:prod1_length){
      if(x < prod1_length){
        starter <- 0
        if(x==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[x-1] + 1
        }
        prod_main_list <- append(prod_main_list, list(starter:limit_breaker_factorial[x]))
      }else if(x == prod1_length){
        starter <- 0
        if(x==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[x-1] + 1
        }
        prod_main_list <- append(prod_main_list, list(starter:n))
      }
    }
    for(z in 1:prod2_length){
      if(z < prod2_length){
        starter <- 0
        if(z==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[z-1] + 1
        }
        prod_divider_list <- append(prod_divider_list, list(starter:limit_breaker_factorial[z]))
      }
      else if(z == prod2_length){
        starter <- 0
        if(z==1){
          starter <- 1
        }else{
          starter <- limit_breaker_factorial[z-1] + 1
        }
        prod_divider_list <- append(prod_divider_list, list(starter:(n-r)))
      }
    }
    
    prod_main_remain <- list()
    prod_divider_remain <- list()
    length_main <- length(prod_main_list)
    length_divider <- length(prod_divider_list)
    
    for(divide in 1:length_divider){
      remainer_main <- which(!prod_main_list[[divide]] %in% prod_divider_list[[divide]])
      if(length(remainer_main) != 0){
        prod_main_remain <- append(prod_main_remain, list(prod_main_list[[divide]][remainer_main]))
      }
    }
    if(length_main > length_divider){
      for(undivide in (length_divider+1):length_main){
        prod_main_remain <- append(prod_main_remain, list(prod_main_list[[undivide]]))
      }
    }
    
    prod_main_vec <- unlist(lapply(prod_main_remain, FUN = function(x) prod(x)))
    grand_result <- prod(prod_main_vec)
    if(grand_result == Inf){
      return(prod_main_vec)
    }else{
      return(grand_result)
    }
  }
}
big_number_interpretation <- function(numbers, truncated_degree_show=4, show_as="string"){
  library(english)
  library(gmp) 
  digit_interpretation <- c()
  interpretation <- ""
  below_100_degree <- c("Thousand","Million","Billion","Trillion","Quadrillion","Quintillion",
                        "Sextillion","Septillion","Octillion","Nonillion","Decillion","Undecillion",
                        "Tredecillion","Quattuordecillion","Quindecillion", "Sexdecillion", "Septemdecillion", 
                        "Octodecillion","Novemdecillion","Vigintillion","Unvigintillion",
                        "Duovigintillion", "Trevigintillion", "Quattuorvigintillion",
                        "Quinvigintillion","Sexvigintillion", "Septivigintillion",
                        "Octovigintillion","Nonvigintillion", "Trigintillion", "Untrigintillion", "Duotrigintillion")
  below_100_nominal <- 10^seq(3, 99, 3)
  to_306_degree <- seq(102, 306, 3)
  above_100_degree <- paste0("Googol_Rank_",seq(1, length(to_306_degree), 1)) #max 69
  above_100_nominal <- 10^seq(102, 306, 3)
  
  all_degree <- c(below_100_degree, above_100_degree)
  all_nominal <- c(below_100_nominal, above_100_nominal)
  
  locate_digits <- function(big_numbers, division){
    options(scipen=20)
    num_char <- as.character(big_numbers)
    div_char <- as.character(division)
    num_nchar <- nchar(num_char) - 1
    
    if(grepl("e", div_char) == FALSE){
      div_nchar <- nchar(div_char) - 1
    }else{
      div_nchar <- as.numeric(unlist(strsplit(div_char, "\\+", ""))[2])
    }
    
    diff_nchar <- num_nchar - div_nchar
    starter_digit <- floor(diff_nchar / 3)
    digit_cursor <- (diff_nchar %% 3) + 2
    if(starter_digit == 1){
      get_digit <- substr(num_char, digit_cursor, (diff_nchar + 1))
      return(get_digit)
    }
    else if(starter_digit > 1){
      get_digit <- substr(num_char, digit_cursor + 3*(starter_digit-1), (diff_nchar + 1))
      return(get_digit)
    }
    else if(starter_digit == 0){
      get_digit <- substr(num_char, 1, (diff_nchar + 1))
      return(get_digit)
    }
    
  }
  real_numbers <- as.bigz(numbers)
  
  if(all(real_numbers > below_100_nominal)){
    idx_degree <- which(real_numbers > above_100_nominal)
    if(length(idx_degree) == 0){
      idx_degree <- which(real_numbers > below_100_nominal)
      idx_degree <- length(idx_degree)
      pointer <- idx_degree
      while(pointer >= 1){
        digit_actual <- as.numeric(locate_digits(numbers, below_100_nominal[pointer]))
        digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", below_100_degree[pointer]))
        pointer <- pointer - 1
      }
    }else{
      idx_degree <- which(real_numbers > all_nominal)
      idx_degree <- length(idx_degree)
      pointer <- idx_degree
      while(pointer >= 1){
        digit_actual <- as.numeric(locate_digits(numbers, all_nominal[pointer]))
        digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", all_degree[pointer]))
        pointer <- pointer - 1
      }
    }
  }
  else{
    idx_degree <- which(real_numbers > below_100_nominal)
    if(length(idx_degree) == 0){
      idx_degree <- 0
      interpretation <- english(as.numeric(real_numbers))
    }else{
      idx_degree <- length(idx_degree)
      pointer <- idx_degree
      while(pointer >= 1){
        digit_actual <- as.numeric(locate_digits(numbers, below_100_nominal[pointer]))
        digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", below_100_degree[pointer]))
        pointer <- pointer - 1
      }
    }
  }
  
  length_interpretation <- length(digit_interpretation)
  if(!is.na(truncated_degree_show)){
    if(length(digit_interpretation) < truncated_degree_show){
      interpretation <- paste0(digit_interpretation, collapse = " and ")
    }else{
      for(v in 1:truncated_degree_show){
        if(v != truncated_degree_show){
          interpretation <- paste0(interpretation, digit_interpretation[v], " and ")
        }else{
          interpretation <- paste0(interpretation, digit_interpretation[v])
        }
      }
    }
    
  }else{
    interpretation <- paste0(digit_interpretation, collapse = " and ")
  }
  
  if(show_as == "string"){
    return(interpretation)
  }else if(show_as == "vector"){
    return(digit_interpretation)
  }
}
calc_nvar_select_combination <- function(n_var, limit_n_combination=NA, only_exact_n_combination=FALSE){
  options(scipen=306)
  calc_combination <- function(n,r,limit_breaker_factorial = c(170,300,420,535,646,754,860,964,1066,1167,
                                                               1266,1364,1461,1557,1653,1748,1842,1936,2029)){
    if(n <= 170){
      return(factorial(n) / (factorial(r) * factorial(n-r)))
    }else{
      #save it in list of vectors for indirect computation
      prod_main_list <- list()
      prod_divider_list <- list()
      prod1_length <- sum(limit_breaker_factorial < n) + 1
      prod2_length <- sum(limit_breaker_factorial < r) + 1
      prod3_length <- sum(limit_breaker_factorial < (n-r)) + 1
      for(x in 1:prod1_length){
        if(x < prod1_length){
          starter <- 0
          if(x==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[x-1] + 1
          }
          prod_main_list <- append(prod_main_list, list(starter:limit_breaker_factorial[x]))
        }else if(x == prod1_length){
          starter <- 0
          if(x==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[x-1] + 1
          }
          prod_main_list <- append(prod_main_list, list(starter:n))
        }
      }
      for(y in 1:prod2_length){
        if(y < prod2_length){
          starter <- 0
          if(y==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[y-1] + 1
          }
          prod_divider_list <- append(prod_divider_list, list(starter:limit_breaker_factorial[y]))
        }else if(y == prod2_length){
          starter <- 0
          if(y==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[y-1] + 1
          }
          prod_divider_list <- append(prod_divider_list, list(starter:r))
        }
      }
      for(z in 1:prod3_length){
        if(z < prod3_length){
          starter <- 0
          if(z==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[z-1] + 1
          }
          prod_divider_list <- append(prod_divider_list, list(starter:limit_breaker_factorial[z]))
        }
        else if(z == prod3_length){
          starter <- 0
          if(z==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[z-1] + 1
          }
          prod_divider_list <- append(prod_divider_list, list(starter:(n-r)))
        }
      }
      
      prod_main_remain <- list()
      prod_divider_remain <- list()
      first_fac_idx <- which(unlist(lapply(prod_divider_list, FUN = function(x) any(x == 1))))
      length_main <- length(prod_main_list)
      length_divider <- length(prod_divider_list)
      length_nr <- length_divider - first_fac_idx[2] + 1
      length_r <- length_divider - length_nr
      if(length_nr >= length_r){
        nr_starter <- first_fac_idx[2]
        for(divide in 1:length_nr){
          remainer_main <- which(!prod_main_list[[divide]] %in% prod_divider_list[[divide + nr_starter - 1]])
          if(length(remainer_main) != 0){
            prod_main_remain <- append(prod_main_remain, list(prod_main_list[[divide]][remainer_main]))
          }
        }
        if(length_main > length_nr){
          for(undivide in (length_nr+1):length_main){
            prod_main_remain <- append(prod_main_remain, list(prod_main_list[[undivide]]))
          }
        }
        for(divide_remain in 1:(nr_starter - 1)){
          prod_divider_remain <- append(prod_divider_remain, list(prod_divider_list[[divide_remain]]))
        }
      }
      else if(length_nr < length_r){
        r_starter <- first_fac_idx[1]
        for(divide in 1:length_r){
          remainer_main <- which(!prod_main_list[[divide]] %in% prod_divider_list[[divide + r_starter - 1]])
          if(length(remainer_main) != 0){
            prod_main_remain <- append(prod_main_remain, list(prod_main_list[[divide]][remainer_main]))
          }
        }
        if(length_main > length_r){
          for(undivide in (length_r+1):length_main){
            prod_main_remain <- append(prod_main_remain, list(prod_main_list[[undivide]]))
          }
        }
        for(divide_remain in (length_r+1):length_divider){
          prod_divider_remain <- append(prod_divider_remain, list(prod_divider_list[[divide_remain]]))
        }
      }
      
      prod_main_vec <- unlist(lapply(prod_main_remain, FUN = function(x) prod(x)))
      prod_divider_vec <- unlist(lapply(prod_divider_remain, FUN = function(x) prod(x)))
      
      for(grand_divide in 1:length(prod_divider_vec)){
        prod_main_vec[grand_divide] <- prod_main_vec[grand_divide] / prod_divider_vec[grand_divide]
      }
      
      grand_result <- prod(prod_main_vec)
      #for debug
      #return(list(prod_main_list = prod_main_list, prod_divider_list = prod_divider_list, 
      #            prod_main_remain = prod_main_remain, prod_divider_remain = prod_divider_remain, 
      #            grand_result = grand_result))
      if(grand_result == Inf){
        return(prod_main_vec)
      }else{
        return(grand_result)
      }
      
      
    }
    
  }
  big_number_interpretation <- function(numbers, truncated_degree_show=4, show_as="vector"){
    library(english)
    library(gmp) 
    digit_interpretation <- c()
    interpretation <- ""
    below_100_degree <- c("Thousand","Million","Billion","Trillion","Quadrillion","Quintillion",
                          "Sextillion","Septillion","Octillion","Nonillion","Decillion","Undecillion",
                          "Tredecillion","Quattuordecillion","Quindecillion", "Sexdecillion", "Septemdecillion", 
                          "Octodecillion","Novemdecillion","Vigintillion","Unvigintillion",
                          "Duovigintillion", "Trevigintillion", "Quattuorvigintillion",
                          "Quinvigintillion","Sexvigintillion", "Septivigintillion",
                          "Octovigintillion","Nonvigintillion", "Trigintillion", "Untrigintillion", "Duotrigintillion")
    below_100_nominal <- 10^seq(3, 99, 3)
    to_306_degree <- seq(102, 306, 3)
    above_100_degree <- paste0("Googol_Rank_",seq(1, length(to_306_degree), 1)) #max 69
    above_100_nominal <- 10^seq(102, 306, 3)
    
    all_degree <- c(below_100_degree, above_100_degree)
    all_nominal <- c(below_100_nominal, above_100_nominal)
    
    locate_digits <- function(big_numbers, division){
      options(scipen=20)
      num_char <- as.character(big_numbers)
      div_char <- as.character(division)
      num_nchar <- nchar(num_char) - 1
      
      if(grepl("e", div_char) == FALSE){
        div_nchar <- nchar(div_char) - 1
      }else{
        div_nchar <- as.numeric(unlist(strsplit(div_char, "\\+", ""))[2])
      }
      
      diff_nchar <- num_nchar - div_nchar
      starter_digit <- floor(diff_nchar / 3)
      digit_cursor <- (diff_nchar %% 3) + 2
      if(starter_digit == 1){
        get_digit <- substr(num_char, digit_cursor, (diff_nchar + 1))
        return(get_digit)
      }
      else if(starter_digit > 1){
        get_digit <- substr(num_char, digit_cursor + 3*(starter_digit-1), (diff_nchar + 1))
        return(get_digit)
      }
      else if(starter_digit == 0){
        get_digit <- substr(num_char, 1, (diff_nchar + 1))
        return(get_digit)
      }
      
    }
    real_numbers <- as.bigz(numbers)
    
    if(all(real_numbers > below_100_nominal)){
      idx_degree <- which(real_numbers > above_100_nominal)
      if(length(idx_degree) == 0){
        idx_degree <- which(real_numbers > below_100_nominal)
        idx_degree <- length(idx_degree)
        pointer <- idx_degree
        while(pointer >= 1){
          digit_actual <- as.numeric(locate_digits(numbers, below_100_nominal[pointer]))
          digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", below_100_degree[pointer]))
          pointer <- pointer - 1
        }
      }else{
        idx_degree <- which(real_numbers > all_nominal)
        idx_degree <- length(idx_degree)
        pointer <- idx_degree
        while(pointer >= 1){
          digit_actual <- as.numeric(locate_digits(numbers, all_nominal[pointer]))
          digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", all_degree[pointer]))
          pointer <- pointer - 1
        }
      }
    }
    else{
      idx_degree <- which(real_numbers > below_100_nominal)
      if(length(idx_degree) == 0){
        idx_degree <- 0
        interpretation <- english(as.numeric(real_numbers))
      }else{
        idx_degree <- length(idx_degree)
        pointer <- idx_degree
        while(pointer >= 1){
          digit_actual <- as.numeric(locate_digits(numbers, below_100_nominal[pointer]))
          digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", below_100_degree[pointer]))
          pointer <- pointer - 1
        }
      }
    }
    
    length_interpretation <- length(digit_interpretation)
    if(!is.na(truncated_degree_show)){
      if(length(digit_interpretation) < truncated_degree_show){
        interpretation <- paste0(digit_interpretation, collapse = " and ")
      }else{
        for(v in 1:truncated_degree_show){
          if(v != truncated_degree_show){
            interpretation <- paste0(interpretation, digit_interpretation[v], " and ")
          }else{
            interpretation <- paste0(interpretation, digit_interpretation[v])
          }
        }
      }
      
    }else{
      interpretation <- paste0(digit_interpretation, collapse = " and ")
    }
    
    if(show_as == "string"){
      return(interpretation)
    }else if(show_as == "vector"){
      return(digit_interpretation)
    }
    
  }
  combination_vec <- list()
  if(is.na(limit_n_combination)){
    for(a in 1:n_var){
      combination_vec <- append(combination_vec, list(calc_combination(n_var, a)))
      names(combination_vec)[a] <- paste0("var_combination_",a)
    }
    check_factors <- any(unlist(lapply(combination_vec, FUN = function(x) length(x))) > 1)
    if(check_factors){
      writeLines(paste0("Combination by ", n_var, " Total Variable, Resulting a possibility of near Infinite!"))
      return(combination_vec)
    }else{
      writeLines(paste0("Combination by ", n_var, " Total Variable, Resulting a possibility of ", format(sum(unlist(combination_vec)), big.mark=","), " Combination"))
      writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
      print(big_number_interpretation(paste0(floor(sum(unlist(combination_vec)))), truncated_degree_show=4, show_as="vector"))
      return(combination_vec)
    }
  }else{
    for(a in 1:limit_n_combination){
      combination_vec <- append(combination_vec, list(calc_combination(n_var, a)))
      names(combination_vec)[a] <- paste0("var_combination_",a)
    }
    if(only_exact_n_combination){
      if(length(combination_vec[[limit_n_combination]]) == 1){
        writeLines(paste0("Combination by ", n_var, " Total Variable, Limited by ",limit_n_combination,
                          " Combination Only, Resulting a possibility of ", format(combination_vec[[limit_n_combination]], big.mark=","), " Combination"))
        writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
        print(big_number_interpretation(paste0(floor(combination_vec[[limit_n_combination]])), truncated_degree_show=4, show_as="vector"))
        return(combination_vec[limit_n_combination])
      }else{
        writeLines(paste0("Combination by ", n_var, " Total Variable, Limited by ",limit_n_combination,
                          " Combination Only, Resulting a possibility of ", length(combination_vec[[limit_n_combination]]), " Factors Combination"))
        for(printing in 1:length(combination_vec[[limit_n_combination]])){
          writeLines(paste0("Factor ",printing, " Combination: ",format(combination_vec[[limit_n_combination]][printing], big.mark=","), " Combination"))
          writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
          print(big_number_interpretation(paste0(floor(combination_vec[[limit_n_combination]][printing])), truncated_degree_show=4, show_as="vector"))
        }
        
        return(combination_vec[limit_n_combination])
      }
      
    }else{
      check_factors <- any(unlist(lapply(combination_vec, FUN = function(x) length(x))) > 1)
      if(check_factors){
        writeLines(paste0("Combination by ", n_var, " Total Variable, Limited by ",limit_n_combination,
                          " Streak Combination, Resulting a possibility of near Infinite!"))
        return(combination_vec)
      }else{
        writeLines(paste0("Combination by ", n_var, " Total Variable, Limited by ",limit_n_combination,
                          " Streak Combination, Resulting a possibility of ", format(sum(unlist(combination_vec)), big.mark=","), " Combination"))
        writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
        print(big_number_interpretation(paste0(floor(sum(unlist(combination_vec)))), truncated_degree_show=4, show_as="vector"))
        return(combination_vec)
      }
    }
  }
}
calc_nvar_select_permutation <- function(n_var, limit_n_permutation=NA, only_exact_n_permutation=FALSE){
  options(scipen=306)
  calc_permutation <- function(n,r,limit_breaker_factorial = c(170,300,420,535,646,754,860,964,1066,1167,
                                                               1266,1364,1461,1557,1653,1748,1842,1936,2029)){
    if(n <= 170){
      return(factorial(n) / (factorial(n-r)))
    }else{
      #save it in list of vectors for indirect computation
      prod_main_list <- list()
      prod_divider_list <- list()
      prod1_length <- sum(limit_breaker_factorial < n) + 1
      prod2_length <- sum(limit_breaker_factorial < (n-r)) + 1
      for(x in 1:prod1_length){
        if(x < prod1_length){
          starter <- 0
          if(x==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[x-1] + 1
          }
          prod_main_list <- append(prod_main_list, list(starter:limit_breaker_factorial[x]))
        }else if(x == prod1_length){
          starter <- 0
          if(x==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[x-1] + 1
          }
          prod_main_list <- append(prod_main_list, list(starter:n))
        }
      }
      for(z in 1:prod2_length){
        if(z < prod2_length){
          starter <- 0
          if(z==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[z-1] + 1
          }
          prod_divider_list <- append(prod_divider_list, list(starter:limit_breaker_factorial[z]))
        }
        else if(z == prod2_length){
          starter <- 0
          if(z==1){
            starter <- 1
          }else{
            starter <- limit_breaker_factorial[z-1] + 1
          }
          prod_divider_list <- append(prod_divider_list, list(starter:(n-r)))
        }
      }
      
      prod_main_remain <- list()
      prod_divider_remain <- list()
      length_main <- length(prod_main_list)
      length_divider <- length(prod_divider_list)
      
      for(divide in 1:length_divider){
        remainer_main <- which(!prod_main_list[[divide]] %in% prod_divider_list[[divide]])
        if(length(remainer_main) != 0){
          prod_main_remain <- append(prod_main_remain, list(prod_main_list[[divide]][remainer_main]))
        }
      }
      if(length_main > length_divider){
        for(undivide in (length_divider+1):length_main){
          prod_main_remain <- append(prod_main_remain, list(prod_main_list[[undivide]]))
        }
      }
      
      prod_main_vec <- unlist(lapply(prod_main_remain, FUN = function(x) prod(x)))
      grand_result <- prod(prod_main_vec)
      if(grand_result == Inf){
        return(prod_main_vec)
      }else{
        return(grand_result)
      }
    }
  }
  big_number_interpretation <- function(numbers, truncated_degree_show=4, show_as="vector"){
    library(english)
    library(gmp) 
    digit_interpretation <- c()
    interpretation <- ""
    below_100_degree <- c("Thousand","Million","Billion","Trillion","Quadrillion","Quintillion",
                          "Sextillion","Septillion","Octillion","Nonillion","Decillion","Undecillion",
                          "Tredecillion","Quattuordecillion","Quindecillion", "Sexdecillion", "Septemdecillion", 
                          "Octodecillion","Novemdecillion","Vigintillion","Unvigintillion",
                          "Duovigintillion", "Trevigintillion", "Quattuorvigintillion",
                          "Quinvigintillion","Sexvigintillion", "Septivigintillion",
                          "Octovigintillion","Nonvigintillion", "Trigintillion", "Untrigintillion", "Duotrigintillion")
    below_100_nominal <- 10^seq(3, 99, 3)
    to_306_degree <- seq(102, 306, 3)
    above_100_degree <- paste0("Googol_Rank_",seq(1, length(to_306_degree), 1)) #max 69
    above_100_nominal <- 10^seq(102, 306, 3)
    
    all_degree <- c(below_100_degree, above_100_degree)
    all_nominal <- c(below_100_nominal, above_100_nominal)
    
    locate_digits <- function(big_numbers, division){
      options(scipen=20)
      num_char <- as.character(big_numbers)
      div_char <- as.character(division)
      num_nchar <- nchar(num_char) - 1
      
      if(grepl("e", div_char) == FALSE){
        div_nchar <- nchar(div_char) - 1
      }else{
        div_nchar <- as.numeric(unlist(strsplit(div_char, "\\+", ""))[2])
      }
      
      diff_nchar <- num_nchar - div_nchar
      starter_digit <- floor(diff_nchar / 3)
      digit_cursor <- (diff_nchar %% 3) + 2
      if(starter_digit == 1){
        get_digit <- substr(num_char, digit_cursor, (diff_nchar + 1))
        return(get_digit)
      }
      else if(starter_digit > 1){
        get_digit <- substr(num_char, digit_cursor + 3*(starter_digit-1), (diff_nchar + 1))
        return(get_digit)
      }
      else if(starter_digit == 0){
        get_digit <- substr(num_char, 1, (diff_nchar + 1))
        return(get_digit)
      }
      
    }
    real_numbers <- as.bigz(numbers)
    
    if(all(real_numbers > below_100_nominal)){
      idx_degree <- which(real_numbers > above_100_nominal)
      if(length(idx_degree) == 0){
        idx_degree <- which(real_numbers > below_100_nominal)
        idx_degree <- length(idx_degree)
        pointer <- idx_degree
        while(pointer >= 1){
          digit_actual <- as.numeric(locate_digits(numbers, below_100_nominal[pointer]))
          digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", below_100_degree[pointer]))
          pointer <- pointer - 1
        }
      }else{
        idx_degree <- which(real_numbers > all_nominal)
        idx_degree <- length(idx_degree)
        pointer <- idx_degree
        while(pointer >= 1){
          digit_actual <- as.numeric(locate_digits(numbers, all_nominal[pointer]))
          digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", all_degree[pointer]))
          pointer <- pointer - 1
        }
      }
    }
    else{
      idx_degree <- which(real_numbers > below_100_nominal)
      if(length(idx_degree) == 0){
        idx_degree <- 0
        interpretation <- english(as.numeric(real_numbers))
      }else{
        idx_degree <- length(idx_degree)
        pointer <- idx_degree
        while(pointer >= 1){
          digit_actual <- as.numeric(locate_digits(numbers, below_100_nominal[pointer]))
          digit_interpretation <- c(digit_interpretation, paste0(digit_actual, " ", below_100_degree[pointer]))
          pointer <- pointer - 1
        }
      }
    }
    
    length_interpretation <- length(digit_interpretation)
    if(!is.na(truncated_degree_show)){
      if(length(digit_interpretation) < truncated_degree_show){
        interpretation <- paste0(digit_interpretation, collapse = " and ")
      }else{
        for(v in 1:truncated_degree_show){
          if(v != truncated_degree_show){
            interpretation <- paste0(interpretation, digit_interpretation[v], " and ")
          }else{
            interpretation <- paste0(interpretation, digit_interpretation[v])
          }
        }
      }
      
    }else{
      interpretation <- paste0(digit_interpretation, collapse = " and ")
    }
    
    if(show_as == "string"){
      return(interpretation)
    }else if(show_as == "vector"){
      return(digit_interpretation)
    }
    
  }
  permutation_vec <- list()
  if(is.na(limit_n_permutation)){
    for(a in 1:n_var){
      permutation_vec <- append(permutation_vec, list(calc_permutation(n_var, a)))
      names(permutation_vec)[a] <- paste0("var_permutation_",a)
    }
    check_factors <- any(unlist(lapply(permutation_vec, FUN = function(x) length(x))) > 1)
    if(check_factors){
      writeLines(paste0("Permutation by ", n_var, " Total Variable, Resulting a possibility of near Infinite!"))
      return(permutation_vec)
    }else{
      writeLines(paste0("Permutation by ", n_var, " Total Variable, Resulting a possibility of ", format(sum(unlist(permutation_vec)), big.mark=","), " Permutation"))
      writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
      print(big_number_interpretation(paste0(floor(sum(unlist(permutation_vec)))), truncated_degree_show=4, show_as="vector"))
      return(permutation_vec)
    }
  }else{
    for(a in 1:limit_n_permutation){
      permutation_vec <- append(permutation_vec, list(calc_permutation(n_var, a)))
      names(permutation_vec)[a] <- paste0("var_permutation_",a)
    }
    if(only_exact_n_permutation){
      if(length(permutation_vec[[limit_n_permutation]]) == 1){
        writeLines(paste0("Permutation by ", n_var, " Total Variable, Limited by ",limit_n_permutation,
                          " Permutation Only, Resulting a possibility of ", format(permutation_vec[[limit_n_permutation]], big.mark=","), " Permutation"))
        writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
        print(big_number_interpretation(paste0(floor(permutation_vec[[limit_n_permutation]])), truncated_degree_show=4, show_as="vector"))
        return(permutation_vec[limit_n_permutation])
      }else{
        writeLines(paste0("Permutation by ", n_var, " Total Variable, Limited by ",limit_n_permutation,
                          " Permutation Only, Resulting a possibility of ", length(permutation_vec[[limit_n_permutation]]), " Factors Permutation"))
        for(printing in 1:length(permutation_vec[[limit_n_permutation]])){
          writeLines(paste0("Factor ",printing, " Permutation: ",format(permutation_vec[[limit_n_permutation]][printing], big.mark=","), " Permutation"))
          writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
          print(big_number_interpretation(paste0(floor(permutation_vec[[limit_n_permutation]][printing])), truncated_degree_show=4, show_as="vector"))
        }
        
        return(permutation_vec[limit_n_permutation])
      }
      
    }else{
      check_factors <- any(unlist(lapply(permutation_vec, FUN = function(x) length(x))) > 1)
      if(check_factors){
        writeLines(paste0("Permutation by ", n_var, " Total Variable, Limited by ",limit_n_permutation,
                          " Streak Permutation, Resulting a possibility of near Infinite!"))
        return(permutation_vec)
      }else{
        writeLines(paste0("Permutation by ", n_var, " Total Variable, Limited by ",limit_n_permutation,
                          " Streak Permutation, Resulting a possibility of ", format(sum(unlist(permutation_vec)), big.mark=","), " Permutation"))
        writeLines("Number Spelling (Warnings Big Numbers may lose precision): ")
        print(big_number_interpretation(paste0(floor(sum(unlist(permutation_vec)))), truncated_degree_show=4, show_as="vector"))
        return(permutation_vec)
      }
    }
  }
}
permutation_element <- function(elements, pick_r, no_repeat=TRUE){
  eval_string <- "expand.grid("
  sort_string <- "element_df %>% arrange("
  for(a in 1:pick_r){
    if(a < pick_r){
      eval_string <- paste0(eval_string, "elements,")
      sort_string <- paste0(sort_string, "elements_",a,",")
    }
    else if(a == pick_r){
      eval_string <- paste0(eval_string, "elements)")
      sort_string <- paste0(sort_string, "elements_",a,")")
    }
  }
  element_df <- eval(parse(text = eval_string))
  colnames(element_df) <- paste0("elements_", 1:pick_r)
  
  if(no_repeat){
    library(dplyr)
    element_df$unique_labeler <- apply(element_df, FUN = function(x) length(unique(x)), MARGIN = 1)
    element_df <- element_df[which(element_df$unique_labeler == pick_r),]
    element_df <- element_df[,-ncol(element_df)]
    rownames(element_df) <- NULL
    element_df <- eval(parse(text = sort_string))
  }
  
  return(element_df)
}
combination_element <- function(elements, pick_r){
  element_df <- data.frame(t(combn(elements, pick_r)))
  colnames(element_df) <- paste0("elements_", 1:pick_r)
  return(element_df)
}
expand_repeat_element <- function(elements, repeat_n=3, calc_total_element=F){
    eval_expand <- paste0("expansion <- expand.grid(")
    for(a in 1:repeat_n){
      if(a==repeat_n){
        eval_expand <- paste0(eval_expand, "elements)")
      }else{
        eval_expand <- paste0(eval_expand, "elements,")
      }
    }
    eval(parse(text = eval_expand))
    if(calc_total_element){
      return(nrow(expansion))
    }else{
      return(expansion)
    }
  }

#----------------------------- Dealing Missing Values in DataFrame -------------------------------------------------------
inspect_NA <- function(df, check_str_summary=TRUE, return_colname=FALSE,
                       return_colnames_threshold=1){
  library(naniar)
  na_table <- data.frame(miss_var_summary(df))
  na_table <- na_table[na_table$n_miss >= return_colnames_threshold,]
  colname_with_miss_value <- as.character(na_table$variable)
  if(check_str_summary){
    if(nrow(na_table) == 0){
      writeLines("There is No Missing Values found in this dataset!")
      na_table <- 0
    }
    else{
      print(na_table)
      writeLines('Checking Structures of Dataframe that have Missing Values')
      print(str(df[,which(colnames(df) %in% colname_with_miss_value)]))
      print(summary(df[,which(colnames(df) %in% colname_with_miss_value)])) 
    }
  }
  if(return_colname){
    return(colname_with_miss_value)
  }else{
    return(na_table) 
  }
}

impute_NA_Inf_fix <- function(df, value_impute = 0){
    for(x in 1:ncol(df)){
      target <- df[,x]
      if(any(is.na(target))){
        target[which(is.na(target))] <- value_impute
        df[,x] <- target
      }
      if(any(target == Inf) || any(target == -Inf)){
        target[which(target == Inf)] <- value_impute
        target[which(target == -Inf)] <- value_impute
        df[,x] <- target
      }
    }
    return(df)
}

clean_NA <- function(df, method="mean", knn_method="mean",k_knn=5,
                     impute_model_return=FALSE, multivar_n_impute=1, 
                     mice_iteration=10, seeds=42){
  if(method == "mean"){
    writeLines("Use Mean to impute in every variable")
    for(h in 1:ncol(df)){
      if(class(df[,h]) %in% c("numeric","integer")){
        df[which(is.na(df[,h])),h] <- mean(df[,h], na.rm=TRUE)
      }
    }
  }else if(method == "median"){
    writeLines("Use Median to impute in every variable")
    for(h in 1:ncol(df)){
      if(class(df[,h]) %in% c("numeric","integer")){
        df[which(is.na(df[,h])),h] <- median(df[,h], na.rm=TRUE)
      }
    }
  }else if(method == "mode"){
    writeLines("Use Highest Densities in every variable to calculate Mode")
    for(h in 1:ncol(df)){
      k <- NULL
      if(class(df[,h]) %in% c("integer")){
        k <- table(df[,h])
      }
      else if(class(df[,h]) %in% c("numeric")){
        k <- table(round(df[,h]))
      }
      modes <- as.numeric(names(which(k==max(k))))
      if(length(modes) > 1){
        df[which(is.na(df[,h])),h] <- mean(modes)
      }else if(length(modes) == 1){
        df[which(is.na(df[,h])),h] <- modes
      }
    }
  }
  else if(method == "zero"){
    for(h in 1:ncol(df)){
      if(class(df[,h]) %in% c("numeric","integer")){
        df[which(is.na(df[,h])),h] <- 0
      }
    }
  }
  else if(method == "mice"){
    library(mice)
    writeLines("MICE Stands for: Multivariate Imputation by Chained Equations")
    mice_imputes <- mice(df, m=multivar_n_impute, maxit=mice_iteration, printFlag=TRUE) 
    which_m_impute <- sample(1:multivar_n_impute, 1)
    df_imputed <- complete(mice_imputes, which_m_impute)
    if(impute_model_return){
      return(list(df_imputed, mice_imputes))
    }else{
      return(df_imputed)
    }
  }
  else if(method == "hmisc"){
    library(hmisc)
    var_formula_length <- 0
    var_names <- c()
    hmisc_input_formula <- "~ "
    for(h in 1:ncol(df)){
      if(class(df[,h]) %in% c("numeric","integer")){
        if(h < ncol(df)){
          formula_string <- paste0(formula_string, colnames(df)[h], " + ")
          var_formula_length <- var_formula_length + 1
          var_names <- c(var_names, colnames(df)[h])
        }else if(h == ncol(df)){
          formula_string <- paste0(formula_string, colnames(df)[h])
          var_formula_length <- var_formula_length + 1
          var_names <- c(var_names, colnames(df)[h])
        }
      }
    }
    writeLines("Using Formula: ")
    print(hmisc_input_formula)
    hmisc_input_formula <- as.formula(hmisc_input_formula)
    writeLines("Imputation using Additive Regression, Bootstrapping, and Predictive Mean Matching (PMM).")
    hmisc_imputes <- aregImpute(hmisc_input_formula, data = df, n.impute = multivar_n_impute)
    for(x in 1:var_formula_length){
      which_m_impute <- sample(1:multivar_n_impute, 1)
      impute_value <- hmisc_imputes$imputed[[var_names[x]]][,which_m_impute]
      names(impute_value) <- NULL
      df[which(is.na(df[,x])),x] <- impute_value
    }
    if(impute_model_return){
      return(list(df, hmisc_imputes))
    }else{
      return(df)
    }
  }else if(method == "amelia"){
    library(Amelia)
    writeLines("Imputation based by Multivariate Normal Distribution (MVN) Distribution using EMB algorithm")
    writeLines('These Imputation need to use Full data (Cannot use Subset data)')
    writeLines("Note: Need to ensure first all the variable follows Normal Distribution")
    logistic_varname <- c()
    factor_varname <- c()
    ordered_factor_varname <- c()
    for(h in 1:ncol(df)){
      if(class(df[,h]) %in% c("integer")){
        unique_value <- unique(df[,h])
        if(all(unique_value %in% c(0,1))){
          logistic_varname <- c(logistic_varname, colnames(df)[h])
        }
      }
      else if(class(df[,h]) %in% c("ordered")){
        ordered_factor_varname <- c(ordered_factor_varname, colnames(df)[h])
      }
      else if(class(df[,h]) %in% c("factor")){
        factor_varname <- c(factor_varname, colnames(df)[h])
      }
    }
    
    which_m_impute <- sample(1:multivar_n_impute, 1)
    boolA <- length(logistic_varname)==0
    boolB <- length(factor_varname)==0
    boolC <- length(ordered_factor_varname)==0
    
    if(boolA && boolB && boolC){
      amelia_fit <- amelia(df, m=multivar_n_impute, 
                           parallel = "multicore") 
    }else{
      if(boolA && boolB){
        amelia_fit <- amelia(df, m=multivar_n_impute, 
                             parallel = "multicore",
                             ords=ordered_factor_varname) 
      }
      else if(boolB && boolC){
        amelia_fit <- amelia(df, m=multivar_n_impute, 
                             parallel = "multicore",
                             lgstc=logistic_varname) 
      }
      else if(boolA && boolC){
        amelia_fit <- amelia(df, m=multivar_n_impute, 
                             parallel = "multicore",
                             noms=factor_varname) 
      }
      else if(boolA){
        amelia_fit <- amelia(df, m=multivar_n_impute, 
                             parallel = "multicore",
                             ords=ordered_factor_varname,
                             noms=factor_varname) 
      }
      else if(boolB){
        amelia_fit <- amelia(df, m=multivar_n_impute, 
                             parallel = "multicore",
                             lgstc=logistic_varname,
                             noms=factor_varname) 
      }
      else if(boolC){
        amelia_fit <- amelia(df, m=multivar_n_impute, 
                             parallel = "multicore",
                             lgstc = logistic_varname,
                             noms=factor_varname) 
      }
      else{
        amelia_fit <- amelia(df, m=multivar_n_impute, 
                             parallel = "multicore",
                             lgstc = logistic_varname,
                             noms=factor_varname,
                             ords=ordered_factor_varname) 
      }
    }
    
    imputed_df <- amelia_fit$imputations[[which_m_impute]]
    if(impute_model_return){
      return(list(imputed_df, amelia_fit))
    }else{
      return(imputed_df)
    }
    
  }else if(method == "missforest"){
    library(missForest)
    writeLines("Imputation using Random Forest Algorithm")
    random_forest_fit <- missForest(iris.mis)
    writeLines("Error Measurements")
    writeLines('NRMSE (Normalized Mean Square Error)')
    writeLines('PFC (Proportion of Falsely Classified)')
    print(random_forest_fit$OOBrrror)
    imputed_df <- random_forest_fit$ximp
    if(impute_model_return){
      return(list(imputed_df, random_forest_fit))
    }else{
      return(imputed_df)
    }
  }else if(method == "mi"){
    library(mi)
    writeLines("Imputation using Diagnostics in Regression and also Summary, and detecting irregular values")
    mi_data <- mi(df, seed = seeds)
    mi_imputation <- mi::complete(mi_data, length(mi_data))
    which_m_impute <- sample(1:length(mi_data), 1)
    imputed_df <- mi_imputation[[which_m_impute]]
    return(imputed_df)
  }else if(method == "knn"){
    writeLines(paste0("Imputation using KNN, with k = ",k_knn))
    library(caret)
    if(knn_method=="mean"){
      preProcValues <- preProcess(df,
                                  method = c("knnImpute"),
                                  k = k_knn,
                                  knnSummary = mean)
      impute_data <- predict(preProcValues, df, na.action = na.pass)
    }else if(knn_method=="median"){
      preProcValues <- preProcess(df,
                                  method = c("knnImpute"),
                                  k = k_knn,
                                  knnSummary = median)
      impute_data <- predict(preProcValues, df, na.action = na.pass)
    }
    return(impute_data)
  }
  else if(method=="EM"){
    writeLines("Imputation using Expectation-Maximization (EM)")
    all_cols <- colnames(df)
    formula_em <- paste0("~",paste0(all_cols, collapse="+"))
    em_impute <- impute_em(nhanes, formula_em)
    return(em_impute)
  }
  
}

#----------------------------- Extract information from R help into an object ---------------------------------------
r_help_extractor <- function(function_name, from_package, flat_argument_to_one=TRUE){
  Rd2list <- function(Rd){
    names(Rd) <- substring(sapply(Rd, attr, "Rd_tag"),2)
    temp_args <- Rd$arguments
    
    Rd$arguments <- NULL
    myrd <- lapply(Rd, unlist)
    myrd <- lapply(myrd, paste, collapse="")
    
    temp_args <- temp_args[sapply(temp_args , attr, "Rd_tag") == "\\item"]
    temp_args <- lapply(temp_args, lapply, paste, collapse="")
    temp_args <- lapply(temp_args, "names<-", c("arg", "description"))
    myrd$arguments <- temp_args
    return(myrd)
  }
  
  getHelpList <- function(...){
    thefile <- help(...)
    myrd <- utils:::.getHelpFile(thefile)
    Rd2list(myrd)
  }

  convert_arglist_to_dataframe <- function(help_list){
    arg_list <- help_list$arguments
    arg_name <- c()
    arg_desc <- c()
    for(x in 1:length(arg_list)){
      arg_name <- c(arg_name, arg_list[[x]]$arg)
      arg_desc <- c(arg_desc, arg_list[[x]]$description)
    }
    arg_df <- data.frame(callname=help_list$name, model_name=help_list$title, 
                         description=help_list$description, argument=arg_name, 
                         argument_description=arg_desc)
    return(arg_df)
  }
  myhelp <- getHelpList(function_name, package=toString(from_package))
  df_arg <- NULL
  if("arguments" %in% names(myhelp)){
    df_arg <- convert_arglist_to_dataframe(myhelp)  
  }
  else{
    df_arg <- data.frame(callname=myhelp$name, model_name=myhelp$title, 
                         description=myhelp$description, argument=NULL, 
                         argument_description=NULL)
  }
  if(flat_argument_to_one){
    df_arg <- df_arg[,-ncol(df_arg)]
    argument_all <- paste0(as.character(df_arg$argument),collapse=", ")
    df_arg <- df_arg[1,]
    df_arg$argument[1] <- argument_all
  }
  
  regex_manipulation <- function(string_vector, regex_pattern, replace_pattern,
                                 mode="find", return_mode="table"){
    library(stringr)
    finder_table <- cbind(str_match(string_vector, regex_pattern), string_vector)
    if(mode=="find"){
      if(return_mode=="table"){
        finder_table <- as.data.frame(finder_table)
        colnames(finder_table)[1] <- "matched"
        #finder_table <- finder_table[which(rowSums(is.na(finder_table))==0),]
        return(finder_table)
      }else if(return_mode=="boolean"){
        bool_vector <- ifelse(rowSums(is.na(finder_table))==0, TRUE, FALSE)
        return(bool_vector)
      }else if(return_mode=="index"){
        finder_table <- as.data.frame(finder_table)
        finder_table <- finder_table[which(rowSums(is.na(finder_table))==0),]
        return(as.numeric(row.names(finder_table)))
      }else if(return_mode=="vector"){
        finder_table <- finder_table[which(rowSums(is.na(finder_table))==0)]
        return(finder_table)
      }
    }else if(mode=="replace"){
      return(str_replace_all(string_vector, regex_pattern, replace_pattern))
    }
  } 
  
  df_arg$description <- regex_manipulation(as.character(df_arg$description), "^[\\s+\\n]", "", mode="replace")
  df_arg$description <- regex_manipulation(as.character(df_arg$description), "[\\n]", " ", mode="replace")
  return(df_arg)
}

#----------------------------- Code Generator with same pattern of string -------------------------------------
code_generator_one_to_many <- function(code_string, from_replace=c(), to_replace=list()){
  library(stringr)
  if(length(from_replace) > 1){
    text <- c()
    for(f in 1:length(to_replace[[1]])){
      sub_text <- ""
      for(g in 1:length(from_replace)){
        if(g == 1){
          sub_text <- str_replace_all(code_string, from_replace[g], to_replace[[g]][[f]])
        }else if(g > 1){
          sub_text <- str_replace_all(sub_text, from_replace[g], to_replace[[g]][[f]])
        }
      }
      text <- c(text, sub_text)
    }
    return(cat(text, sep="\n"))
  }else{
    return(cat(str_replace_all(code_string, from_replace, to_replace[[1]]), sep="\n"))
  }
}

#example from one to many
#c("gbpjpy","GBPJPY"),
#               list(c("usdchf", "usdcad", "gbpusd", "audusd", "audjpy", "cadjpy", "chfjpy", "eurjpy",
#                      "nzdjpy", "eurchf", "eurgbp", "nzdusd", "audnzd", "cadchf", "eurcad", "gbpchf",
#                      "nzdcad", "euraud", "eurnzd"),
#                    toupper(c("usdchf", "usdcad", "gbpusd", "audusd", "audjpy", "cadjpy", "chfjpy", "eurjpy",
#                              "nzdjpy", "eurchf", "eurgbp", "nzdusd", "audnzd", "cadchf", "eurcad", "gbpchf",
#                              "nzdcad", "euraud", "eurnzd")))

code_generator_one_to_one <- function(code_string, from_replace=c(), to_replace=list(), debug = F){
  library(stringr)
  to_inner_length <- length(to_replace[[1]])
  to_outer_length <- length(to_replace)
  
  if(length(from_replace) > 1){
    text <- c()
    sub_text <- ""
    for(f in 1:to_outer_length){
      sub_text <- ""
      for(g in 1:length(from_replace)){
        if(sub_text == ""){
          if(debug) writeLines(paste0("from_replace[",g,"] = ", from_replace[g]))
          if(debug) writeLines(paste0("to_replace[[",f,"]][[",g,"]] = ", to_replace[[f]][[g]]))
          sub_text <- str_replace_all(code_string, from_replace[g], to_replace[[f]][[g]])
        }else{
          if(debug) writeLines(paste0("from_replace[",g,"] = ", from_replace[g]))
          if(debug) writeLines(paste0("to_replace[[",f,"]][[",g,"]] = ", to_replace[[f]][[g]]))
          sub_text <- str_replace_all(sub_text, from_replace[g], to_replace[[f]][[g]])
        }
      }
      text <- c(text, sub_text)
    }
    return(cat(text, sep="\n"))
  }else{
    return(cat(str_replace_all(code_string, from_replace, to_replace[[1]]), sep="\n"))
  }
}

#example of one to one (from_replace --> to_replace)
#("eurusd",'\\("EUR"','"US"\\)','merge_pair1 = "eur"','merge_pair2 = "us"'),
#               list(c("usdjpy",'\\("US"','"JPY"\\)','merge_pair1 = "us"','merge_pair2 = "jpy"'),
#                    c("usdchf",'\\("US"','"CHF"\\)','merge_pair1 = "us"','merge_pair2 = "chf"'),
#                    c("usdcad",'\\("US"','"CAD"\\)','merge_pair1 = "us"','merge_pair2 = "cad"'),
#                    c("gbpusd",'\\("UK"','"US"\\)','merge_pair1 = "uk"','merge_pair2 = "us"'),
#                    c("audusd",'\\("AUD"','"US"\\)','merge_pair1 = "aud"','merge_pair2 = "us"'),
#                    c("audjpy",'\\("AUD"','"JPY"\\)','merge_pair1 = "aud"','merge_pair2 = "jpy"'),
#                    c("cadjpy",'\\("CAD"','"JPY"\\)','merge_pair1 = "cad"','merge_pair2 = "jpy"'),
#                    c("chfjpy",'\\("CHF"','"JPY"\\)','merge_pair1 = "chf"','merge_pair2 = "jpy"'),
#                    c("eurjpy",'\\("EUR"','"JPY"\\)','merge_pair1 = "eur"','merge_pair2 = "jpy"'),
#                    c("gbpjpy",'\\("UK"','"JPY"\\)','merge_pair1 = "uk"','merge_pair2 = "jpy"'),
#                    c("nzdjpy",'\\("NZD"','"JPY"\\)','merge_pair1 = "nzd"','merge_pair2 = "jpy"'),
#                    c("eurchf",'\\("EUR"','"CHF"\\)','merge_pair1 = "eur"','merge_pair2 = "chf"'),
#                    c("eurgbp",'\\("EUR"','"UK"\\)','merge_pair1 = "eur"','merge_pair2 = "uk"'),
#                    c("nzdusd",'\\("NZD"','"US"\\)','merge_pair1 = "nzd"','merge_pair2 = "us"'),
#                    c("audnzd",'\\("AUD"','"NZD"\\)','merge_pair1 = "aud"','merge_pair2 = "nzd"'),
#                    c("cadchf",'\\("CAD"','"CHF"\\)','merge_pair1 = "cad"','merge_pair2 = "chf"'),
#                    c("eurcad",'\\("EUR"','"CAD"\\)','merge_pair1 = "eur"','merge_pair2 = "cad"'),
#                    c("gbpchf",'\\("UK"','"CHF"\\)','merge_pair1 = "uk"','merge_pair2 = "chf"'),
#                    c("nzdcad",'\\("NZD"','"CAD"\\)','merge_pair1 = "nzd"','merge_pair2 = "cad"'),
#                    c("euraud",'\\("EUR"','"AUD"\\)','merge_pair1 = "eur"','merge_pair2 = "aud"'),
#                    c("eurnzd",'\\("EUR"','"NZD"\\)','merge_pair1 = "eur"','merge_pair2 = "nzd"'))

#----------------------------- Check Dataframe Family Distribution --------------------------------------------------
check_dist_df <- function(feature){
  library(fitdistrplus)
  library(logspline)
  #Must install JAGS first from: https://sourceforge.net/projects/mcmc-jags/
  library(rjags) 
  library(prevalence)
  
  normalized_feature = feature / max(feature)
  x11()
  plot_dist <- descdist(feature, discrete = FALSE)
  getmode <- function(v) {
    uniqv <- unique(v)
    uniqv[which.max(tabulate(match(v, uniqv)))]
  }
  
  feature_mode = getmode(feature)
  feature_mode_percent = round(feature_mode / max(feature), 2)
  feature_min = min(feature)
  feature_min_percent = round(feature_min / max(feature), 2)
  writeLines(paste0("Using Feature with Mode Percent: ", feature_mode_percent))
  writeLines(paste0("Using Feature with Min Percent: ", feature_min_percent))
  
  #https://rdrr.io/cran/prevalence/man/betaExpert. -> Find Beta Parameter
  param = betaExpert(best = feature_mode_percent, 
                     lower = feature_min_percent, upper = 0.95, p = 0.95)
  
  fit.normal <- fitdist(feature, "norm")
  x11()
  plot(fit.normal, sub="Fit Data to Normal Distribution")
  fit.uniform <- fitdist(feature, "unif")
  x11()
  plot(fit.uniform, sub="Fit Data to Uniform Distribution")
  fit.exponential <- fitdist(feature, "exp")
  x11()
  plot(fit.exponential, sub="Fit Data to Exponential Distribution")
  fit.logistic <- fitdist(feature, "logis")
  x11()
  plot(fit.logistic, sub="Fit Data to Logistic Distribution")
  
  print("Using Alpha as first parameter to fit beta distribution")
  print(round(param$alpha),0)
  print("Using Beta as first parameter to fit beta distribution")
  print(round(param$beta),0)
  
  fit.beta <- fitdist(normalized_feature, "beta", 
                      start=list(shape1=round(param$alpha,0), 
                                 shape2=round(param$beta,0)), method="mme")
  x11()
  plot(fit.beta, sub=paste0("Fit Data to Beta Distribution"))
  fit.lognormal <- fitdist(feature, "lnorm")
  x11()
  plot(fit.lognormal, sub="Fit Data to Log Normal Distribution")
  fit.gamma <- fitdist(feature, "gamma")
  x11()
  plot(fit.gamma, sub="Fit Data to Gamma Distribution")
  return(list(plot_dist, fit.normal, fit.uniform, fit.exponential, 
              fit.logistic, fit.beta, fit.lognormal, fit.gamma))
}

mixture_prop_assignment <- function(data){
  library(flexmix)
  data("NPreg", package = "flexmix")
}

library(flexmix)
data("NPreg", package = "flexmix")
ex1 <- flexmix(yn ~ x + I(x^2), data = NPreg, k = 2,
               control = list(verb = 5, iter = 100))
ex2 <- flexmix(yn ~ x, data = NPreg, k = 2,
               model = list(FLXMRglm(yn ~ . + I(x^2)), 
                            FLXMRglm(yp ~ ., family = "poisson")))


#----------------------------- Merging 2 Factor data type into one ------------------------------------------------------------ 
factor_merge <- function(df, factor_col1, factor_col2, sep="-",
                         merge_name="merged_factor"){
  factor_vector <- paste0(as.character(df[[factor_col1]]), sep, 
                          as.character(df[[factor_col2]]))
  df[[merge_name]] <- as.factor(factor_vector)
  return(df)
}

#----------------------------- Categorize Numeric Column into an n-factor Column ----------------------------------------------
#range_label = n+1 vector of label
cut_numeric_to_factor <- function(object, specific=FALSE, column, range_label=c(), 
                                  label=c(), n_factor=5){
  
  if(specific==TRUE && length(range_label) == 0){
    stop("When Specific set to TRUE, please input desired range label")
  }
  
  if(specific==TRUE && length(label) == 0 && length(range_label) > 0){
    label <- rep("", length(range_label)-1)
    for(f in 1:(length(range_label)-1)){
      label[f] <- paste0(range_label[f],"-",range_label[f+1]) 
    }
  }
  
  if(class(object) == "data.frame"){
    if(specific==FALSE){
      object <- object %>%
        mutate_if(is.numeric, funs(cut_number(., n=n_factor)))
    }
    else if(specific==TRUE){
      if(length(label) > 0){
        new_column <- paste0("factor_label_",column)
        object[[new_column]] <- cut(
          object[[column]],
          breaks = range_label,
          labels = label
        )
      }else{
        new_column <- paste0("factor_",column)
        object[[new_column]] <- cut(
          object[[column]],
          breaks = range_label
        )
      }
    }
    return(object) 
  }
  else if(class(object) != "data.frame"){
    if(specific==FALSE){
      object <- object %>%
        mutate_if(is.numeric, funs(cut_number(., n=n_factor)))
    }
    else if(specific==TRUE){
      if(length(label) > 0){
        new_column <- paste0("factor_label_",column)
        object[[new_column]] <- cut(
          object,
          breaks = range_label,
          labels = label
        )
      }else{
        new_column <- paste0("factor_",column)
        object[[new_column]] <- cut(
          object,
          breaks = range_label
        )
      }
    }
    return(object) 
  }
}

quantile_cut_df_num <- function(df, use_quantile_range = c(0,0.01,0.25,0.5,0.75,0.9, 0.95, 0.99, 0.999, 1), decimal_supress = 6){
  
  decimalplaces <- function(x) {
    if (abs(x - round(x)) > .Machine$double.eps^0.5) {
      nchar(strsplit(sub('0+$', '', as.character(x)), ".", fixed = TRUE)[[1]][[2]])
    } else {
      return(0)
    }
  }
  
  for(a in 1:ncol(df)){
    if(is.numeric(df[,a])){
      quantile_info <- as.numeric(quantile(df[,a], use_quantile_range))
      for(check_quantile in 1:length(quantile_info)){
        if(decimalplaces(quantile_info[check_quantile]) >= decimal_supress && max(quantile_info) < 1){
          quantile_info[check_quantile] <- round(quantile_info[check_quantile], decimal_supress)
        }else{
          quantile_info[check_quantile] <- round(quantile_info[check_quantile], 2)
        }
      }
      quantile_label <- c()
      for(b in 1:(length(quantile_info) - 1)){
        quantile_label <- c(quantile_label, paste0(quantile_info[b], "-", quantile_info[b+1]))
      }
      df[,a] <- cut(df[,a], breaks = quantile_info, labels = quantile_label)
    }
  }
  return(df)
}

summary_count <- function(df, column){
  library(rlang)
  cut_numeric_to_factor <- function(object, specific=FALSE, column, range_label=c(), 
                                    label=c(), n_factor=5){
    if(class(object) == "data.frame"){
      if(specific==FALSE){
        object <- object %>%
          mutate_if(is.numeric, funs(cut_number(., n=n_factor)))
      }
      else if(specific==TRUE){
        if(length(label) > 0){
          new_column <- paste0("factor_label_",column)
          object[[new_column]] <- cut(
            object[[column]],
            breaks = range_label,
            labels = label
          )
        }else{
          new_column <- paste0("factor_",column)
          object[[new_column]] <- cut(
            object[[column]],
            breaks = range_label
          )
        }
      }
      return(object) 
    }
    else if(class(object) != "data.frame"){
      if(specific==FALSE){
        object <- object %>%
          mutate_if(is.numeric, funs(cut_number(., n=n_factor)))
      }
      else if(specific==TRUE){
        if(length(label) > 0){
          new_column <- paste0("factor_label_",column)
          object[[new_column]] <- cut(
            object,
            breaks = range_label,
            labels = label
          )
        }else{
          new_column <- paste0("factor_",column)
          object[[new_column]] <- cut(
            object,
            breaks = range_label
          )
        }
      }
      return(object) 
    }
  }
  
  df_sum <- cut_numeric_to_factor(df, column=column, specific=TRUE, 
                                  range_label=fivenum(df[[column]]))
  df_sum <- cut_numeric_to_factor(df_sum, column=column, specific=TRUE, 
                                  range_label=fivenum(df_sum[[column]]),
                              label=c("Minimum-1st Qu","1st Qu-Median","Median-3rd Qu","3rd Qu-Maximum"))
  label_col <- colnames(df_sum)[(ncol(df_sum)-1):ncol(df_sum)]
  label_syms <- syms(label_col)
  df_sum %>% group_by(!!!label_syms) %>% summarise(count=n())
}

summary_format_hour <- function(numeric_days){
  x <- summary(numeric_days)
  x_1 <- floor(x)
  x_2 <- round(abs(x - floor(x)) * 24)
  x_format <- paste0(x_1, " Days ", x_2, " Hours")
  names(x_format) <- names(x)
  return(x_format)
}

dataset3 <- cut_numeric_to_factor(dataset3, column="teachers",specific=TRUE,
                                  range_label=fivenum(dataset3$teachers),
                                  label=c("Minimum-1st Qu","1st Qu-Median",
                                          "Median-3rd Qu","3rd Qu-Maximum"))
dataset3 %>% group_by(factor_teachers) %>% summarise(count = n())
dataset3 <- cut_numeric_to_factor(dataset3, column="read",specific=TRUE,
                                  range_label=fivenum(dataset3$read),
                                  label=c("Minimum-1st Qu","1st Qu-Median",
                                          "Median-3rd Qu","3rd Qu-Maximum"))
dataset3 %>% group_by(factor_read) %>% summarise(count = n())

#----------------------------- Set R Viewers new Window --------------------------------------
new_r_viewer <- function(){
  options(viewer = function(url, height = NULL)
  {
    if (!is.character(url) || (length(url) != 1))
      stop("url must be a single element character vector.", call. = FALSE)

    if (identical(height, "maximize"))
      height <- -1

    if (!is.null(height) && (!is.numeric(height) || (length(height) != 1)))
      stop("height must be a single element numeric vector or 'maximize'.", call. = FALSE)

    invisible(.Call("rs_showPageViewer", url, title = "RStudio", self_contained = FALSE))  
  })
}

#----------------------------- Run Length Encoding (To Check Homogenous value occurences in a vector) --------------------------------------
run_length_encoding <- function(vector, add_cumsum=TRUE,  make_table=FALSE){
  writeLines('Checking for Count of Homogen Value Occurences before meet a different value in a Vector')
  rle_values <- rle(as.character(vector))$values
  rle_length <- rle(as.character(vector))$length
  rle_df <- data.frame(rle_values, rle_length)
  if(add_cumsum){
    rle_df$rle_cummulative <- cumsum(rle_df$rle_length) 
  }
  if(make_table){
    writeLines("Distribution of Homogenous Value occurences")
    print(table(rle_values))
    table_rle <- table(rle_length)
    return(list(rle_df, table_rle))
  }
  return(rle_df)
}

value_grouping_by_run_length <- function(vector, value_monitor, date_vector, 
                                           add_cumsum=TRUE, value_name = ""){
    writeLines('Checking for Count of Homogen Value Occurences before meet a different value in a Vector')
    date_vector <- as.character(date_vector)
    rle_values <- rle(as.character(vector))$values
    rle_length <- rle(as.character(vector))$length
    rle_df <- data.frame(rle_values, rle_length)
    rle_df$rle_cummulative <- cumsum(rle_df$rle_length) 
    total_value <- c()
    date_value <- c()
    
    for(x in 1:nrow(rle_df)){
      if(x == 1){
        total_value <- c(total_value, sum(value_monitor[1:rle_df$rle_cummulative[x]]))
        if(rle_df$rle_cummulative[x] > 1){
          date_value <- c(date_value, paste0(date_vector[1], " - ", date_vector[rle_df$rle_cummulative[x]]))
        }
        else if(rle_df$rle_cummulative[x] == 1){
          date_value <- c(date_value, date_vector[1])
        }
      }
      else if(x > 1){
        total_value <- c(total_value, sum(value_monitor[(rle_df$rle_cummulative[x-1] + 1):rle_df$rle_cummulative[x]]))
        if((rle_df$rle_cummulative[x-1] + 1) < rle_df$rle_cummulative[x]){
          date_value <- c(date_value, paste0(date_vector[(rle_df$rle_cummulative[x-1] + 1)], " - ", date_vector[rle_df$rle_cummulative[x]]))
        }
        else if((rle_df$rle_cummulative[x-1] + 1) == rle_df$rle_cummulative[x]){
          date_value <- c(date_value, date_vector[rle_df$rle_cummulative[x]])
        }
      }
    }
    
    if(value_name == ""){
      rle_df$value_cummulative <- total_value
    }
    else{
      rle_df[[value_name]] <- total_value
    }
    
    rle_df$datetime_range <- date_value
    
    return(rle_df)
  }

aggregate_by_n_row <- function(asset_df, labels="rle_values", 
                                 variable="rle_length", by=3){
    
    if(nrow(asset_df) %% by == 0){
      rowsums_df_list <- list()
      if(by > 2){
        for(g in 1:by){
          if(g == 1){
            row_label <- asset_df[[labels]]
            seq_list <- list()
            eval_paste <- "paste0("
            for(f in 1:by){
              s_idx <- seq(f, length(row_label), by=by)
              seq_list <- c(seq_list, list(s_idx))
              if(f < by){
                eval_paste <- paste0(eval_paste, 'row_label[seq_list[[',f,']]]," -> ",')
              }
              else if(f == by){
                eval_paste <- paste0(eval_paste, 'row_label[seq_list[[',f,']]])')
              }
            }
            row_label_n <- eval(parse(text = eval_paste))
            rowsum_n <- rowsum(asset_df[[variable]], rep(seq_len(length(asset_df[[variable]])/by), each=by))
            rowsum_df <- data.frame(label=row_label_n, aggregate_value=rowsum_n)
            rowsums_df_list <- c(rowsums_df_list, list(rowsum_df))
          }
          else if(g > 1){
            exclude_obs <- c(seq(1,(g-1),1))
            remain_exclude_idx <- nrow(asset_df) - (by + 1 + length(exclude_obs))
            exclude_obs <- c(exclude_obs, seq(remain_exclude_idx, nrow(asset_df), 1))
            asset_df_excluded <- asset_df[-exclude_obs,]
            row_label <- asset_df[[labels]]
            seq_list <- list()
            eval_paste <- "paste0("
            for(f in 1:by){
              s_idx <- seq(f, length(row_label), by=by)
              seq_list <- c(seq_list, list(s_idx))
              if(f < by){
                eval_paste <- paste0(eval_paste, 'row_label[seq_list[[',f,']]]," -> ",')
              }
              else if(f == by){
                eval_paste <- paste0(eval_paste, 'row_label[seq_list[[',f,']]])')
              }
            }
            row_label_n <- eval(parse(text = eval_paste))
            rowsum_n <- rowsum(asset_df[[variable]], rep(seq_len(length(asset_df[[variable]])/by), each=by))
            rowsum_df <- data.frame(label=row_label_n, aggregate_value=rowsum_n)
            rowsums_df_list <- c(rowsums_df_list, list(rowsum_df))
          }
        }
        return(rowsums_df_list)
      }
      else if(by == 2){
        row_label <- asset_df[[labels]]
        seq_list <- list()
        eval_paste <- "paste0("
        for(f in 1:by){
          s_idx <- seq(f, length(row_label), by=by)
          seq_list <- c(seq_list, list(s_idx))
          if(f < by){
            eval_paste <- paste0(eval_paste, 'row_label[seq_list[[',f,']]]," -> ",')
          }
          else if(f == by){
            eval_paste <- paste0(eval_paste, 'row_label[seq_list[[',f,']]])')
          }
        }
        row_label_n <- eval(parse(text = eval_paste))
        rowsum_n <- rowsum(asset_df[[variable]], rep(seq_len(length(asset_df[[variable]])/by), each=by))
        rowsum_df <- data.frame(label=row_label_n, aggregate_value=rowsum_n)
        return(rowsum_df)
      }
    }
    else{
      messages(paste0("Given Dataset Not Possible to Aggregate row by ", by))
    }
  }

x <- round(runif(3000, 0, 1))
#these will check if there is an index with a neighbour homogenous count greater than 3
v <- run_length_encoding(x, filter_value=3)
#these will check how much evidences occur that have a neighbour homogenous count greater than 3
length(v[[2]])
v <- run_length_encoding(x, filter_value=10)
length(v[[2]])

#----------------------------- Probability of a pattern by row-wise ----------------------
row_wise_pattern_count <- function(df, col_search, pattern_search=c(), 
                                   pattern_target=c(), mode="count"){
  row_vector <- df[, which(colnames(df)==col_search)]
  count_pattern <- 0
  for(x in 1:length(row_vector)){
    if(row_vector[x] %in% pattern_search && row_vector[x+1] %in% pattern_target){
      count_pattern <- count_pattern + 1
    }
  }
  if(mode=="count"){
    return(count_pattern) 
  }
  else if(mode=="prob"){
    return(count_pattern / (length(row_vector) - 1))
  }
}

#----------------------------- Numerical Characteristic Identifier
numerical_identity <- function(df){
  library(dplyr)
  vector_num_list <- c()
  vector_identity_list <- list()
  numerical_df <- Filter(is.numeric, df)
  for(h in 1:ncol(numerical_df)){
    vector_name <- colnames(numerical_df)[h]
    vectors_num <- numerical_df[,h]
    vector_identity <- c()
    if(min(vectors_num) == 0 && max(vectors_num) == 1){
      vector_identity <- c(vector_identity, "Standardized Numeric Value")
    }
    if(is.integer(vectors_num)){
      vector_identity <- c(vector_identity, "Count Data/Rounded/Integer Value")
    }else{
      vector_identity <- c(vector_identity, "Real/Float/Numerical Value")
    }
    if(any(vectors_num) < 0){
      vector_identity <- c(vector_identity, "Value Varies between Negative and Positives")
    }else{
      vector_identity <- c(vector_identity, "Nonnegative Values")
    }
    if(max(vectors_num) > 1000000){
      vector_identity <- c(vector_identity, "Big Numbers Value/Possible Rp Currency Variable")
    }
    most_diff_value_tables <- table(diff(vectors_num))
    most_diff_numeric <- as.numeric(names(most_diff_value_tables == max(most_diff_value_tables)))
    if(most_diff_numeric == 1){
      vector_identity <- c(vector_identity, "Sequential Numeric Variable")
    }
    unique_values <- unique(vectors_num)
    if(all(unique_values >= 1) && all(unique_values <= 12)){
      table_unique <- table(vectors_num)
      if(length(table_unique) == 12){
        vector_identity <- c(vector_identity, "Possible Month or Hour with (12 Digit Format) as Numeric Variable ") 
      }
    }
    if(all(unique_values >= 1) && all(unique_values <= 24) |
       (all(unique_values >= 0) && all(unique_values <= 23))){
      table_unique <- table(vectors_num)
      if(length(table_unique) == 24){
        vector_identity <- c(vector_identity, "Possible Hour with (24 Digit Format) as Numeric Variable") 
      }
    }
    if((all(unique_values >= 0) && all(unique_values <= 59))){
      table_unique <- table(vectors_num)
      if(length(table_unique) == 60){
        vector_identity <- c(vector_identity, "Possible Minute or Second as Numeric Variable") 
      }
    }
    if(all(unique_values >= 1900) && all(unique_values <= 2100)){
      vector_identity <- c(vector_identity, "Possible Year as Numeric Variable") 
    }
    less_unique_more_length <- FALSE
    vector_length <- length(vectors_num)
    unique_length <- length(unique_values)
    proportion_unique_length <- unique_length / vector_length
    if(vector_length < 100){
      if(proportion_unique_length <= 0.3){
        less_unique_more_length <- TRUE
      }
    }else if(vector_length >= 100 && vector_length < 1000){
      if(proportion_unique_length <= 0.11){
        less_unique_more_length <- TRUE
      }
    }else if(vector_length >= 1000 && vector_length < 10000){
      if(proportion_unique_length <= 0.015){
        less_unique_more_length <- TRUE
      }
    }else if(vector_length >= 10000 && vector_length < 100000){
      if(proportion_unique_length <= 0.0015){
        less_unique_more_length <- TRUE
      }
    }
    if(less_unique_more_length){
      vector_identity <- c(vector_identity, "Recommended to convert to Factors if necessary") 
    }
    vector_identity <- c(vector_identity, paste0("Value Ranges between ", min(vectors_num), " to ", max(vectors_num)))
    writeLines("====================================================")
    writeLines(paste0(h,") Variable Name = ", vector_name))
    writeLines('Variable Identity')
    print(vector_identity)
    writeLines("====================================================")
    vector_num_list <- c(vector_num_list, vector_name)
    vector_identity_list <- c(vector_identity_list, list(vector_identity))
  }
  return(list(vector_num_list, vector_identity_list))
}

#----------------------------- Currency Rates Data Mining --------------------------------
all_conversion_rate_viewer <- function(mode="today", unit=1, perspective="Customer", add_symbol_legend=TRUE,
                                       use_filter=TRUE, symbol_filter=c("AUD","EUR","USD","MYR"), 
                                       conversion_type="Bank_Notes"){
  library(stringr)
  library(tidyverse)
  library(rvest)
  library(httr)
  library(RCurl)
  
  curlSetOpt(timeout = 200)
  
  kurs_bi <- "https://kursdollar.org/bank/bi.php"
  kurs_mandiri <- "https://kursdollar.org/bank/mandiri.php"
  kurs_bca <- "https://kursdollar.org/bank/bca.php"
  kurs_bni <- "https://kursdollar.org/bank/bni.php"
  kurs_hsbc <- "https://kursdollar.org/bank/hsbc.php"
  kurs_panin <- "https://kursdollar.org/bank/panin.php"
  kurs_cimb <- "https://kursdollar.org/bank/cimb.php"
  kurs_ocbc <- "https://kursdollar.org/bank/ocbc.php"
  kurs_bri <- "https://kursdollar.org/bank/bri.php"
  kurs_uob <- "https://kursdollar.org/bank/uob.php"
  kurs_maybank <- 'https://kursdollar.org/bank/maybank.php'
  kurs_permata <- "https://kursdollar.org/bank/permata.php"
  kurs_mega <- "https://kursdollar.org/bank/mega.php"
  kurs_danamon <- "https://kursdollar.org/bank/danamon.php"
  kurs_btn <- "https://kursdollar.org/bank/btn.php"
  kurs_mayapada <- "https://kursdollar.org/bank/mayapada.php"
  kurs_muamalat <- "https://kursdollar.org/bank/muamalat.php"
  kurs_bukopin <- "https://kursdollar.org/bank/bukopin.php"
  
  bank_name_list <- c("Bank Indonesia", "Bank Mandiri", "Bank Central Asia (BCA)", "Bank Negara Indonesia (BNI)",
                 "Bank HSBC", "Bank Panin", "Bank CIMB Niaga", "Bank OCBC", "Bank Rakyat Indonesia (BRI)",
                 "Bank UOB", "Maybank", "Bank Permata","Bank Mega", "Bank Danamon", "Bank BTN (Bank Tabungan Negara)",
                 "Bank Mayapada", "Bank Muamalat", "Bank Bukopin")
  
  link_kurs <- c(kurs_bi, kurs_mandiri, kurs_bca, kurs_bni, kurs_hsbc, kurs_panin, kurs_cimb, kurs_ocbc, 
                 kurs_bri, kurs_uob, kurs_maybank, kurs_permata, kurs_mega, kurs_danamon, kurs_btn, kurs_mayapada,
                 kurs_muamalat, kurs_bukopin)
  
  include_bank_notes <- c(seq(1,18,1))
  include_tt_counter <- c(2,3,4,8,9,11,12,13,16,17)
  include_e_rate <- c(2,3,9)
  
  if(mode == "today"){
    writeLines(paste0("Processed Conversion Rate Date: ", Sys.Date(), " Conversion Type = ", conversion_type))
    collected_kurs <- NULL
    
    #1) Validate Bank which include Bank Notes, TT Counter, and E-Rate
    idx_read <- c()
    if(conversion_type == "Bank_Notes"){
      idx_read <- include_bank_notes
    }
    else if(conversion_type == "TT_Counter"){
      idx_read <- include_tt_counter
    }
    else if(conversion_type == "E_Rate"){
      idx_read <- include_e_rate
    }
    
    #2) Read All the Table from each Bank and Concate into 1
    extract_df <- NULL
    for(v in idx_read){
      writeLines(paste0(v,') Read Table on ', link_kurs[v]))
      while(TRUE){
        tryCatch({
          open_url <- url(link_kurs[v], "rb")
          extract_df <- read_(open_url) 
          close(open_url)
          extract_df <- extract_df %>%
            _nodes("table") %>% 
            _table(fill = T) %>% as.data.frame()
          writeLines("Read Success!") 
          break
        }, error=function(e){
          message("Read Timeout")
          message("Retrying. .")
        })
      }
      
      title <- as.character(extract_df[2,3:ncol(extract_df)])
      separate <- str_split(title, "\\)")
      symbol_name <- unlist(lapply(separate, FUN=function(x) x[1]))
      symbol_desc <- unlist(lapply(separate, FUN=function(x) x[2]))
      symbol_name <- str_replace_all(symbol_name, "\\(", "")
      
      buy_today <- NULL
      sell_today <- NULL
      
      if(conversion_type == "Bank_Notes"){
        row_count <- nrow(extract_df)
        bank_note_raw_start <- 3
        bank_note_raw_end <- row_count
        
        buy_today <- as.character(extract_df[bank_note_raw_start,3:ncol(extract_df)])
        sell_today <- as.character(extract_df[bank_note_raw_start+1,3:ncol(extract_df)])
      }
      else if(conversion_type == "TT_Counter"){
        row_count <- nrow(extract_df)
        raw_data_count <- (row_count - 5)/2
        bank_note_raw_start <- 3
        bank_note_raw_end <- bank_note_raw_start + (raw_data_count - 1)
        tt_counter_raw_start <- 4 + bank_note_raw_end
        tt_counter_raw_end <- row_count
        
        buy_today <- as.character(extract_df[tt_counter_raw_start,3:ncol(extract_df)])
        sell_today <- as.character(extract_df[tt_counter_raw_start+1,3:ncol(extract_df)])
      }
      else if(conversion_type == "E_Rate"){
        row_count <- nrow(extract_df)
        raw_data_count <- (row_count - 8)/3
        bank_note_raw_start <- 3
        bank_note_raw_end <- bank_note_raw_start + (raw_data_count - 1)
        tt_counter_raw_start <- 4 + bank_note_raw_end
        tt_counter_raw_end <- tt_counter_raw_start + (raw_data_count - 1)
        e_rate_raw_start <- 4 + tt_counter_raw_end
        e_rate_raw_end <- row_count
        
        buy_today <- as.character(extract_df[e_rate_raw_start,3:ncol(extract_df)])
        sell_today <- as.character(extract_df[e_rate_raw_start+1,3:ncol(extract_df)])
      }
      
      buy_today <- unlist(lapply(str_split(buy_today, "\\("), FUN=function(x) x[1]))
      sell_today <- unlist(lapply(str_split(sell_today, "\\("), FUN=function(x) x[1]))
      buy_today <- buy_today %>% str_replace_all("\\.","") %>% str_replace_all("\\,",".")
      sell_today <- sell_today %>% str_replace_all("\\.","") %>% str_replace_all("\\,",".")
      buy_today <- as.numeric(buy_today)
      sell_today <- as.numeric(sell_today)
      
      if(v == 1){
        bank_add <- rep(bank_name_list[v], length(symbol_name))
        collected_kurs <- data.frame(bank_name = bank_add, symbol_name, symbol_desc, buy_today, sell_today)
      }
      else if(v > 1){
        bank_add <- rep(bank_name_list[v], length(symbol_name))
        add_kurs <- data.frame(bank_name = bank_add, symbol_name, symbol_desc, buy_today, sell_today)
        collected_kurs <- rbind(collected_kurs, add_kurs)
      }
    }
    
    #3 Change the Big Table into different Perspectives
    
    if(perspective == "Bank"){
      collected_kurs_buy <- collected_kurs %>% select(bank_name, symbol_name, buy_today) %>% spread(symbol_name, buy_today)
      collected_kurs_buy["price_type"] <- "Buy"
      collected_kurs_sell <- collected_kurs %>% select(bank_name, symbol_name, sell_today) %>% spread(symbol_name, sell_today)
      collected_kurs_sell["price_type"] <- "Sell"
      collected_kurs_all <- rbind(collected_kurs_buy, collected_kurs_sell)
      collected_kurs_all <- collected_kurs_all %>% arrange(bank_name, price_type)
      collected_kurs_asset <- colnames(collected_kurs_all)[2:(ncol(collected_kurs_all)-1)]
      collected_kurs_all["currency_unit"] <- rep(unit,nrow(collected_kurs_all))
      
      if(use_filter){
        collected_kurs_all <- collected_kurs_all %>% select(bank_name, price_type, currency_unit, !!!symbol_filter)
        collected_kurs_all[,3:ncol(collected_kurs_all)] <- collected_kurs_all[,3:ncol(collected_kurs_all)] * unit
        symbol_legend <- collected_kurs %>% select(symbol_name, symbol_desc) %>% unique
        if(add_symbol_legend){
          return(list(collected_kurs_all, symbol_legend))
        }
        else{
          return(collected_kurs_all)
        }
      }
      else{
        collected_kurs_all <- collected_kurs_all %>% select(bank_name, price_type, currency_unit, !!!collected_kurs_asset)
        collected_kurs_all[,3:ncol(collected_kurs_all)] <- collected_kurs_all[,3:ncol(collected_kurs_all)] * unit
        symbol_legend <- collected_kurs %>% select(symbol_name, symbol_desc) %>% unique
        if(add_symbol_legend){
          return(list(collected_kurs_all, symbol_legend))
        }
        else{
          return(collected_kurs_all)
        }
      }
    }
    else if(perspective == "Customer"){
      collected_kurs_buy <- collected_kurs %>% select(bank_name, symbol_name, buy_today) %>% spread(symbol_name, buy_today)
      collected_kurs_buy["price_type"] <- "Sell"
      collected_kurs_sell <- collected_kurs %>% select(bank_name, symbol_name, sell_today) %>% spread(symbol_name, sell_today)
      collected_kurs_sell["price_type"] <- "Buy"
      collected_kurs_all <- rbind(collected_kurs_buy, collected_kurs_sell)
      collected_kurs_all <- collected_kurs_all %>% arrange(bank_name, price_type)
      collected_kurs_asset <- colnames(collected_kurs_all)[2:(ncol(collected_kurs_all)-1)]
      collected_kurs_all["currency_unit"] <- rep(unit,nrow(collected_kurs_all))
      
      if(use_filter){
        collected_kurs_all <- collected_kurs_all %>% select(bank_name, price_type, currency_unit, !!!symbol_filter)
        collected_kurs_all[,3:ncol(collected_kurs_all)] <- collected_kurs_all[,3:ncol(collected_kurs_all)] * unit
        symbol_legend <- collected_kurs %>% select(symbol_name, symbol_desc) %>% unique
        if(add_symbol_legend){
          return(list(collected_kurs_all, symbol_legend))
        }
        else{
          return(collected_kurs_all)
        }
      }
      else{
        collected_kurs_all <- collected_kurs_all %>% select(bank_name, price_type, currency_unit, !!!collected_kurs_asset)
        collected_kurs_all[,3:ncol(collected_kurs_all)] <- collected_kurs_all[,3:ncol(collected_kurs_all)] * unit
        symbol_legend <- collected_kurs %>% select(symbol_name, symbol_desc) %>% unique
        if(add_symbol_legend){
          return(list(collected_kurs_all, symbol_legend))
        }
        else{
          return(collected_kurs_all)
        }
      }
    }
    else if(perspective == "Both"){
      bank_collected_kurs_buy <- collected_kurs %>% select(bank_name, symbol_name, buy_today) %>% spread(symbol_name, buy_today)
      bank_collected_kurs_buy["price_type"] <- "Buy"
      bank_collected_kurs_sell <- collected_kurs %>% select(bank_name, symbol_name, sell_today) %>% spread(symbol_name, sell_today)
      bank_collected_kurs_sell["price_type"] <- "Sell"
      bank_collected_kurs_all <- rbind(bank_collected_kurs_buy, bank_collected_kurs_sell)
      bank_collected_kurs_all <- bank_collected_kurs_all %>% arrange(bank_name, price_type)
      bank_collected_kurs_asset <- colnames(bank_collected_kurs_all)[2:(ncol(bank_collected_kurs_all)-1)]
      
      if(use_filter){
        bank_collected_kurs_all <- bank_collected_kurs_all %>% select(bank_name, price_type, !!!symbol_filter)
        bank_collected_kurs_all[,3:ncol(bank_collected_kurs_all)] <- bank_collected_kurs_all[,3:ncol(bank_collected_kurs_all)] * unit
      }
      else{
        bank_collected_kurs_all <- bank_collected_kurs_all %>% select(bank_name, price_type, !!!bank_collected_kurs_asset)
        bank_collected_kurs_all[,3:ncol(bank_collected_kurs_all)] <- bank_collected_kurs_all[,3:ncol(bank_collected_kurs_all)] * unit
      }
      
      cust_collected_kurs_buy <- collected_kurs %>% select(bank_name, symbol_name, buy_today) %>% spread(symbol_name, buy_today)
      cust_collected_kurs_buy["price_type"] <- "Sell"
      cust_collected_kurs_sell <- collected_kurs %>% select(bank_name, symbol_name, sell_today) %>% spread(symbol_name, sell_today)
      cust_collected_kurs_sell["price_type"] <- "Buy"
      cust_collected_kurs_all <- rbind(cust_collected_kurs_buy, cust_collected_kurs_sell)
      cust_collected_kurs_all <- cust_collected_kurs_all %>% arrange(bank_name, price_type)
      cust_collected_kurs_asset <- colnames(cust_collected_kurs_all)[2:(ncol(cust_collected_kurs_all)-1)]
      
      if(use_filter){
        cust_collected_kurs_all <- cust_collected_kurs_all %>% select(bank_name, price_type, !!!symbol_filter)
        cust_collected_kurs_all[,3:ncol(cust_collected_kurs_all)] <- cust_collected_kurs_all[,3:ncol(cust_collected_kurs_all)] * unit
      }
      else{
        cust_collected_kurs_all <- cust_collected_kurs_all %>% select(bank_name, price_type, !!!cust_collected_kurs_asset)
        cust_collected_kurs_all[,3:ncol(cust_collected_kurs_all)] <- cust_collected_kurs_all[,3:ncol(cust_collected_kurs_all)] * unit
      }
      
      symbol_legend <- collected_kurs %>% select(symbol_name, symbol_desc) %>% unique
      if(add_symbol_legend){
        return(list(bank_view = bank_collected_kurs_all, cust_view = cust_collected_kurs_all, symbol_legend)) 
      }
      else{
        return(list(bank_view = bank_collected_kurs_all, cust_view = cust_collected_kurs_all)) 
      }
      
    }
      
  }
  else if(mode == "historical"){
    writeLines("Collect All Historical Kurs available from https://kursdollar.org/")
    collected_kurs <- NULL
    full_symbol <- c("USD","SGD","AUD","EUR","CNY",
                     "HKD","GBP","JPY","CAD","NZD",
                     "MYR","THB","SAR","PHP","KRW",
                     "VND","PGK","LAK","KWD","BND")
    
    extract_df <- NULL
    for(v in 1:length(link_kurs)){
      writeLines(paste0(v,') Read Table on ', link_kurs[v]))
      while(TRUE){
        tryCatch({
          open_url <- url(link_kurs[v], "rb")
          extract_df <- read_(open_url) 
          close(open_url)
          extract_df <- extract_df %>%
            _nodes("table") %>% 
            _table(fill = T) %>% as.data.frame()
          writeLines("Read Success!") 
          break
        }, error=function(e){
          message("Read Timeout")
          message("Retrying. .")
        })
      }
      
      title <- as.character(extract_df[2,3:ncol(extract_df)])
      separate <- str_split(title, "\\)")
      symbol_name <- unlist(lapply(separate, FUN=function(x) x[1]))
      symbol_desc <- unlist(lapply(separate, FUN=function(x) x[2]))
      symbol_name <- str_replace_all(symbol_name, "\\(", "")
      
      #bank notes row 3-18, tt counter row 22-37, e-rate row 41-56
      if(v %in% include_e_rate){
        row_count <- nrow(extract_df)
        raw_data_count <- (row_count - 8)/3
        bank_note_raw_start <- 3
        bank_note_raw_end <- bank_note_raw_start + (raw_data_count - 1)
        tt_counter_raw_start <- 4 + bank_note_raw_end
        tt_counter_raw_end <- tt_counter_raw_start + (raw_data_count - 1)
        e_rate_raw_start <- 4 + tt_counter_raw_end
        e_rate_raw_end <- row_count
        
        notitle_df <- extract_df[c(bank_note_raw_start:bank_note_raw_end, 
                                   tt_counter_raw_start:tt_counter_raw_end, 
                                   e_rate_raw_start:e_rate_raw_end),]
        rownames(notitle_df) <- NULL
        colnames(notitle_df)[2] <- "price_type"
        colnames(notitle_df)[3:ncol(extract_df)] <- symbol_name
        
        date_start_substring_idx <- str_locate(notitle_df[,1], " ")[,1] + 1
        date_end_substring_idx <- str_locate(notitle_df[,1],":")[,1] - 3
        day_filtered <- str_sub(notitle_df[,1], 1, date_start_substring_idx - 2)
        date_filtered <- str_sub(notitle_df[,1], date_start_substring_idx, date_end_substring_idx)
        hour_filtered <- str_sub(notitle_df[,1], date_end_substring_idx+1, nchar(notitle_df[,1]))
        
        for(f in 3:ncol(notitle_df)){
          notitle_df[,f] <- unlist(lapply(str_split(as.character(notitle_df[,f]), "\\("), FUN=function(x) x[1]))
          notitle_df[,f] <- notitle_df[,f] %>% str_replace_all("\\.","") %>% str_replace_all("\\,",".")
          notitle_df[,f] <- as.numeric(notitle_df[,f])
        }
        
        if(length(symbol_name) < length(full_symbol)){
          writeLines('Currently Available Symbol Name: ')
          print(symbol_name)
          writeLines('Full Symbol')
          print(full_symbol)
          writeLines("Which Symbol is not Available (will add later)")
          not_available_symbol <- full_symbol[which(!(full_symbol %in% symbol_name) == TRUE)]
          print(not_available_symbol)
          for(add in 1:length(not_available_symbol)){
            notitle_df[[not_available_symbol[add]]] <- NA
          }
        }
        
        notitle_df["Date"] <- date_filtered
        notitle_df["Day_Date"] <- day_filtered
        notitle_df["Hour_Date"] <- hour_filtered
        notitle_df <- notitle_df[,-1]
        notitle_df["conversion_type"] <- ""
        notitle_df[1:raw_data_count, which(colnames(notitle_df) %in% "conversion_type")] <- "Bank Notes"
        notitle_df[(raw_data_count + 1):(raw_data_count * 2), which(colnames(notitle_df) %in% "conversion_type")] <- "TT Counter"
        notitle_df[(raw_data_count * 2 + 1):nrow(notitle_df), which(colnames(notitle_df) %in% "conversion_type")] <- "E-Rate"
        
      }
      else if(v %in% include_tt_counter){
        row_count <- nrow(extract_df)
        raw_data_count <- (row_count - 5) / 2
        bank_note_raw_start <- 3
        bank_note_raw_end <- bank_note_raw_start + (raw_data_count - 1)
        tt_counter_raw_start <- 4 + bank_note_raw_end
        tt_counter_raw_end <- row_count
        
        notitle_df <- extract_df[c(bank_note_raw_start:bank_note_raw_end, 
                                   tt_counter_raw_start:tt_counter_raw_end),]
        rownames(notitle_df) <- NULL
        colnames(notitle_df)[2] <- "price_type"
        colnames(notitle_df)[3:ncol(extract_df)] <- symbol_name
        
        date_start_substring_idx <- str_locate(notitle_df[,1], " ")[,1] + 1
        date_end_substring_idx <- str_locate(notitle_df[,1],":")[,1] - 3
        day_filtered <- str_sub(notitle_df[,1], 1, date_start_substring_idx - 2)
        date_filtered <- str_sub(notitle_df[,1], date_start_substring_idx, date_end_substring_idx)
        hour_filtered <- str_sub(notitle_df[,1], date_end_substring_idx+1, nchar(notitle_df[,1]))
        
        for(f in 3:ncol(notitle_df)){
          notitle_df[,f] <- unlist(lapply(str_split(as.character(notitle_df[,f]), "\\("), FUN=function(x) x[1]))
          notitle_df[,f] <- notitle_df[,f] %>% str_replace_all("\\.","") %>% str_replace_all("\\,",".")
          notitle_df[,f] <- as.numeric(notitle_df[,f])
        }
        
        if(length(symbol_name) < length(full_symbol)){
          writeLines('Currently Available Symbol Name: ')
          print(symbol_name)
          writeLines('Full Symbol')
          print(full_symbol)
          writeLines("Which Symbol is not Available (will add later)")
          not_available_symbol <- full_symbol[which(!(full_symbol %in% symbol_name) == TRUE)]
          print(not_available_symbol)
          for(add in 1:length(not_available_symbol)){
            notitle_df[[not_available_symbol[add]]] <- NA
          }
        }
        
        notitle_df["Date"] <- date_filtered
        notitle_df["Day_Date"] <- day_filtered
        notitle_df["Hour_Date"] <- hour_filtered
        notitle_df <- notitle_df[,-1]
        notitle_df["conversion_type"] <- ""
        notitle_df[1:raw_data_count, which(colnames(notitle_df) %in% "conversion_type")] <- "Bank Notes"
        notitle_df[(raw_data_count + 1):nrow(notitle_df), which(colnames(notitle_df) %in% "conversion_type")] <- "TT Counter"
      }
      else{
        row_count <- nrow(extract_df)
        bank_note_raw_start <- 3
        bank_note_raw_end <- row_count
        
        notitle_df <- extract_df[c(bank_note_raw_start:bank_note_raw_end),]
        rownames(notitle_df) <- NULL
        colnames(notitle_df)[2] <- "price_type"
        colnames(notitle_df)[3:ncol(extract_df)] <- symbol_name
        
        date_start_substring_idx <- str_locate(notitle_df[,1], " ")[,1] + 1
        date_end_substring_idx <- str_locate(notitle_df[,1],":")[,1] - 3
        day_filtered <- str_sub(notitle_df[,1], 1, date_start_substring_idx - 2)
        date_filtered <- str_sub(notitle_df[,1], date_start_substring_idx, date_end_substring_idx)
        hour_filtered <- str_sub(notitle_df[,1], date_end_substring_idx+1, nchar(notitle_df[,1]))
        
        for(f in 3:ncol(notitle_df)){
          notitle_df[,f] <- unlist(lapply(str_split(as.character(notitle_df[,f]), "\\("), FUN=function(x) x[1]))
          notitle_df[,f] <- notitle_df[,f] %>% str_replace_all("\\.","") %>% str_replace_all("\\,",".")
          notitle_df[,f] <- as.numeric(notitle_df[,f])
        }
        
        if(length(symbol_name) < length(full_symbol)){
          writeLines('Currently Available Symbol Name: ')
          print(symbol_name)
          writeLines('Full Symbol')
          print(full_symbol)
          writeLines("Which Symbol is not Available (will add later)")
          not_available_symbol <- full_symbol[which(!(full_symbol %in% symbol_name) == TRUE)]
          print(not_available_symbol)
          for(add in 1:length(not_available_symbol)){
            notitle_df[[not_available_symbol[add]]] <- NA
          }
        }
        
        notitle_df["Date"] <- date_filtered
        notitle_df["Day_Date"] <- day_filtered
        notitle_df["Hour_Date"] <- hour_filtered
        notitle_df <- notitle_df[,-1]
        notitle_df["conversion_type"] <- ""
        notitle_df[1:nrow(notitle_df), which(colnames(notitle_df) %in% "conversion_type")] <- "Bank Notes"
      }
      
      print(head(notitle_df))
      
      if(v == 1){
        collected_kurs <- data.frame(Bank_Name = rep(bank_name_list[v], nrow(notitle_df)))
        notitle_df <- notitle_df %>% select(Date, Day_Date, Hour_Date, price_type, conversion_type, !!!full_symbol)
        collected_kurs <- cbind(collected_kurs, notitle_df)
      }
      else if(v > 1){
        add_kurs <- data.frame(Bank_Name = rep(bank_name_list[v], nrow(notitle_df)))
        notitle_df <- notitle_df %>% select(Date, Day_Date, Hour_Date, price_type, conversion_type, !!!full_symbol)
        add_kurs <- cbind(add_kurs, notitle_df)
        collected_kurs <- rbind(collected_kurs, add_kurs)
      }
    }
    
    collected_kurs$currency_unit <- rep(unit, nrow(collected_kurs))
    collected_kurs$price_type <- ifelse(collected_kurs$price_type == "Beli", "Buy", "Sell")
    
    if(perspective == "Bank"){
      if(use_filter){
        collected_kurs <- collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!symbol_filter)
        collected_kurs[,9:ncol(collected_kurs)] <- collected_kurs[,9:ncol(collected_kurs)] * unit
        return(collected_kurs)
      }
      else{
        collected_kurs <- collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!full_symbol)
        collected_kurs[,9:ncol(collected_kurs)] <- collected_kurs[,9:ncol(collected_kurs)] * unit
        return(collected_kurs)
      }
    }
    else if(perspective == "Customer"){
      collected_kurs$price_type <- ifelse(collected_kurs$price_type == "Buy", "Sell", "Buy")
      if(use_filter){
        collected_kurs <- collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!symbol_filter)
        collected_kurs[,9:ncol(collected_kurs)] <- collected_kurs[,9:ncol(collected_kurs)] * unit
        return(collected_kurs)
      }
      else{
        collected_kurs <- collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!full_symbol)
        collected_kurs[,9:ncol(collected_kurs)] <- collected_kurs[,9:ncol(collected_kurs)] * unit
        return(collected_kurs)
      }
    }
    else if(perspective == "Both"){
      bank_collected_kurs <- collected_kurs
      cust_collected_kurs <- collected_kurs
      cust_collected_kurs$price_type <- ifelse(cust_collected_kurs$price_type == "Buy", "Sell", "Buy")
      if(use_filter){
        bank_collected_kurs <- bank_collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!symbol_filter)
        bank_collected_kurs[,9:ncol(bank_collected_kurs)] <- bank_collected_kurs[,9:ncol(bank_collected_kurs)] * unit
        cust_collected_kurs <- cust_collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!symbol_filter)
        cust_collected_kurs[,9:ncol(cust_collected_kurs)] <- cust_collected_kurs[,9:ncol(cust_collected_kurs)] * unit
        return(list(bank_view=bank_collected_kurs, cust_view=cust_collected_kurs))
      }
      else{
        bank_collected_kurs <- bank_collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!full_symbol)
        bank_collected_kurs[,9:ncol(bank_collected_kurs)] <- bank_collected_kurs[,9:ncol(bank_collected_kurs)] * unit
        cust_collected_kurs <- cust_collected_kurs %>% select(Bank_Name, price_type, Date, Day_Date, Hour_Date, price_type, conversion_type, currency_unit, !!!full_symbol)
        cust_collected_kurs[,9:ncol(cust_collected_kurs)] <- cust_collected_kurs[,9:ncol(cust_collected_kurs)] * unit
        return(list(bank_view=bank_collected_kurs, cust_view=cust_collected_kurs))
      }
    }
  }
}

bank_notes <- all_conversion_rate_viewer(conversion_type = "Bank_Notes", mode="today", use_filter=FALSE)
tt_counter <- all_conversion_rate_viewer(conversion_type = "TT_Counter", mode="today", use_filter=FALSE)
e_rate <- all_conversion_rate_viewer(conversion_type = "E_Rate", mode="today", use_filter=FALSE)
hist_rate <- all_conversion_rate_viewer(mode="historical", use_filter = FALSE)

#----------------------------- RData Saver ----------------------------------
#4type of save, all, many, rdata-one, rds-one
rdata_saver <- function(target="rdata-one", obj_list=list(), rdata_name=list(), directory=""){
  if(target=="all"){
    save.image(file=paste0(directory, rdata_name, ".RData")) 
  }
  else if(target=="many"){
    for(a in 1:length(obj_list)){
      filename <- paste0(directory, rdata_name[a], ".RData")
      print(paste0("Save to ",filename))
      save(get(obj_list[[a]]), file=paste0(directory, rdata_name[a], ".RData"))
    }
  }
  else if(target=="rdata-one"){
    if(length(obj_list) == 1){
      filename <- paste0(directory, rdata_name, ".RData")
      print(paste0("Save to ",filename))
      save(get(obj_list[[1]]), file=paste0(directory, rdata_name, ".RData"))
    }
  }
  else if(target=="rds-one"){
    filename <- paste0(directory, rdata_name, ".rds")
    print(paste0("Save to ",filename))
    saveRDS(get(obj_list[[1]]), file=paste0(directory, rdata_name[a], ".rds"))
  }
}

rdata_saver(target="rdata-one", obj_list=list(a), rdata_name=list("testrdata"), 
            directory="C:/Users/user/desktop/")

#------------------------------------------------------------------------------------------------------------------------
################################# 1) Common & Advanced Plotting #########################################################
#------------------------------------------------------------------------------------------------------------------------
#----------------------------- Univariate and Multivariate Generic Plots -----------------------------
univariate_plot <- function(x, density_col="orange"){
  library(EnvStats)
  # set up a matrix layout for multiple plots
  x11()
  mat <- rbind(1:3, 4:6, 7:9)
  layout(mat)
  writeLines(" 1) Define a Scatter Plot") 
  plot(x, main='scatter plot')
  writeLines(" 2) Define a Normality Plot")
  qqPlot(x, main='QQ Normality Plot')
  writeLines(" 3) Define a Strip Chart Plot")
  stripchart(x, main="Strip Chart plot")
  writeLines(" 4) Define a Stack Strip Chart Plot")
  stripchart(x, method="stack", main="Stack Strip Chart Plot")   # stack points to reduce overlap
  writeLines(" 5) Define a Histogram Plot")
  hist(x, main="Histogram Plot")
  writeLines(" 6) Define a Box Plot")
  boxplot(x, main='Box Plot')
  writeLines(" 7) Define a Dot Chart")
  dotchart(x, main='dot chart')
  rug(x)
  writeLines(" 8) Define a Density Plot")
  x.density <- density(x)
  plot(x.density, main="density")
  polygon(x.density, col=density_col, border="black")
  writeLines(" 9) Define an Empirical CDF Plot")
  plot(ecdf(x),main='empirical CDF')
  #reset plot layout
  layout(c(1,1))
}

univariate_density_plot_by_group <- function(data, value, group, title="Density plot by Group", 
                                             plot_point_density=FALSE){
  library(ggplot2)
  library(ggridges)
  library(rlang)
  
  value_sym <- sym(value)
  group_sym <- sym(group)
  length_group <- length(unique(data[[group]]))
  
  #View Density Ridgeline
  x11()
  a <- ggplot(data, aes(x = !!value_sym, y = !!group_sym, fill = stat(x))) + 
    geom_density_ridges_gradient(scale = 12, size = 0.3, rel_min_height = 0.01) +
    scale_fill_viridis_c(name = group, option = "C") +
    labs(title = title) 
  print(a)
  
  #Highlight the tails of the distributions
  x11()
  b <- ggplot(data, aes(x = !!value_sym, y = !!group_sym, fill = factor(stat(quantile)))) +
    stat_density_ridges(
      geom = "density_ridges_gradient",
      calc_ecdf = TRUE,
      quantiles = c(0.025, 0.975)
    ) +
    scale_fill_manual(
      name = "Probability", values = c("#FF0000A0", "#A0A0A0A0", "#0000FFA0"),
      labels = c("(0, 0.025]", "(0.025, 0.975]", "(0.975, 1]")
    ) +
    ggtitle(paste0(title, " with Distribution Tails Highlight"))
  print(b)
  
  #Color the density curves by probabilities
  x11()
  c <- ggplot(data, aes(x = !!value_sym, y = !!group_sym, fill = 0.5 - abs(0.5 - stat(ecdf)))) +
    stat_density_ridges(geom = "density_ridges_gradient", calc_ecdf = TRUE) +
    scale_fill_viridis_c(name = "Tail probability", direction = -1) +
    ggtitle(paste0(title, " with Probabilites on Density Curves"))
  print(c)
  
  #Styling jittered points
  if(plot_point_density){
    x11()
    d <- ggplot(data, aes(x = !!value_sym, y = !!group_sym, fill = !!group_sym)) +
      geom_density_ridges(
        aes(point_color = !!group_sym, point_fill = !!group_sym, point_shape = !!group_sym),
        alpha = .2, point_alpha = 1, jittered_points = TRUE
      ) +
      scale_point_color_hue(l = 40) +
      scale_discrete_manual(aesthetics = "point_shape", 
                            values = seq(21,by=1,length.out=length_group)) +
      ggtitle(paste0(title, " with Styled Jittered points"))
    print(d) 
  }
}

lasagna_plot <- function(individual_matrix, times_matrix, time_unit="Days",
                         color_scheme = "rainbow", limit_individual=0, 
                         width_lasagna_graph=8){
  library(colorspace)
  library(plotrix)
  writeLines("Perform Lasagna Plot")
  x11()
  #some times matrix validation
  ncol_times_matrix <- ncol(times_matrix)
  a <- which(times_matrix == apply(times_matrix, FUN=function(x) max(x), MARGIN=1), arr.ind=TRUE)
  if(!all(a[,2] == ncol_times_matrix)){
    writeLines("Preprocess Times Matrix to be Cumulative")
    times_matrix <- t(apply(times_matrix, 1, cumsum))
  }
  if(!((dim(individual_matrix)[2] + 1) == dim(times_matrix)[2])){
    stop("Dimension of Individual Matrix is not match with Times Matrix, (Usually Columns of Individual Matrix are -1 than Times Matrix)")
  }
  
  x11()
  lc <- NULL
  if(limit_individual > 0){
    lc <- longCat(individual_matrix[1:limit_individual,], times=times_matrix[1:limit_individual,])
  }
  else{
    lc <- longCat(individual_matrix, times=times_matrix)
  }
  #can be gray, rainbow, heat, terrain, topo, cm
  cols <- longCatPlot(lc,  colScheme=color_scheme, legendBuffer=0, 
                      lwd=width_lasagna_graph, xlab=time_unit,
                      groupBuffer=0, main='Individually test Varying Times of Observation')
  plotrix::color.legend(xl=par('usr')[1]+0.975*(par('usr')[2]-par('usr')[1]),
                        xr=par('usr')[1]+1*(par('usr')[2]-par('usr')[1]),
                        yb=par('usr')[3]+0.20*(par('usr')[4]-par('usr')[3]),
                        yt=par('usr')[3]+0.80*(par('usr')[4]-par('usr')[3]),
                        rect.col = cols, gradient="y",
                        legend = lc$Labels[seq(1,length(lc$Labels),3)])
}

#set.seed(642531)
#y <- matrix(sample(1:24, 500, replace=TRUE), 100, 5)
#set.seed(963854)
#times <- matrix(runif(600, 1, 3), 100, 6)
#lasagna_plot(y, times, width_lasagna_graph = 2)

multivariate_scatterplot <- function(df){
  library(car)
  library(dplyr)
  x11()
  df <- df %>%
    select_if(is.numeric)
  scatterplotMatrix(df)
}
#No Non-Numeric Allowed except Class
multivariate_scatterplot_by_class <- function(df, class_name){
  library(mclust)
  x11()
  class <- df[class_name]
  independent <- df[,-which(colnames(df) != class_name)]
  clPairs(independent, class)
}

#Multivariate Corrplot References
#http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram#:~:text=R%20corrplot%20function%20is%20used,graph%20of%20the%20correlation%20matrix.&text=The%20correlation%20matrix%20to%20visualize,general%20matrix%2C%20please%20use%20is.&text=The%20visualization%20method%20%3A%20%E2%80%9Ccircle%E2%80%9D,%2C%20%E2%80%9Cnumber%E2%80%9D%2C%20etc.
multivariate_corrplot <- function(df, significance_level=0.01){
  library(corrplot)
  m <- cor(df)
  x11()
  corrplot(m, method="circle", title = "Basic Correlation with Circle", type="upper")
  x11()
  corrplot(m, method="number", title = "Basic Correlation with Numbers", type="upper")
  x11()
  corrplot(m, method="color", title = "Basic Correlation with Colors", type="upper")
  
  cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
      for (j in (i + 1):n) {
        tmp <- cor.test(mat[, i], mat[, j], ...)
        p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
      }
    }
    colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
    p.mat
  }
  p.mat <- cor.mtest(m)
  
  x11()
  col<- colorRampPalette(c("red", "white", "blue"))(20)
  corrplot(m, method="circle", order="hclust", col=col, 
           p.mat = p.mat, sig.level = significance_level, insig="blank",
           title= paste0("Reordering Correlation of Circle with Significance of ",significance_level), 
           type="upper")
  x11()
  corrplot(m, method="number", order="hclust", col=col, 
           p.mat = p.mat, sig.level = significance_level, insig="blank",
           title= paste0("Reordering Correlation of Number with Significance of ",significance_level), 
           type="upper")
  x11()
  corrplot(m, method="color", order="hclust", col=col, 
           p.mat = p.mat, sig.level = significance_level, insig="blank",
           title= paste0("Reordering Correlation of Color with Significance of ",significance_level), 
           type="upper")
}

multivariate_density_plot <- function(df){
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  x11()
  df %>%
    select_if(is.numeric) %>%
    gather(metric, value) %>%
    ggplot(aes(value, fill = metric)) +
    geom_density(show.legend = FALSE) +
    facet_wrap( ~ metric, scales = "free") 
}

#https://community.rstudio.com/t/select-if-all-numeric-columns-and-one-character-column/36862/4
multivariate_partition_matrix <- function(df, coldependent){
  library(car)
  library(rattle)
  library(klaR)
  library(MASS)
  library(tidyverse)
  x11()
  cols_applied <- df %>% {
    bind_cols(
      select_if(., is.numeric),
      select_at(., coldependent)
    )
  }
  df <-
    formula_plot <- paste0(coldependent,"~.")
  partimat(eval(parse(text=formula_plot)), data=cols_applied, plot.matrix=TRUE)
}

#Available Method: lda, qda, rpart, naiveBayes, rda, sknn, svmLight
multivariate_partition_matrix_break <- function(df, coldependent, method_choose="lda", by=4){
  library(car)
  library(rattle)
  library(klaR)
  library(MASS)
  df <- df %>%
    select_if(is.numeric)
  df_column_independent = colnames(df)
  df_column = df_column_independent[which(df_column_independent != coldependent)]
  pos_combi = expand.grid(df_column, df_column)
  pos_combi = pos_combi[which(pos_combi$Var1 != pos_combi$Var2),]
  row.names(pos_combi) <- NULL
  #https://stackoverflow.com/questions/9028369/removing-duplicate-combinations-irrespective-of-order
  pos_combi_real <- t(apply(pos_combi, 1, sort))
  pos_combi_real <- pos_combi_real[!duplicated(pos_combi_real),]
  pos_combi_real <- as.data.frame(pos_combi_real)
  pos_combi_real$formula <- paste0(coldependent, " ~ ", pos_combi_real$V1, " + ", pos_combi_real$V2)
  pos_combi_real$error_rate <- NULL
  for(error_iter in 1:nrow(pos_combi_real)){
    v1 = pos_combi_real$V1[error_iter]
    v2 = pos_combi_real$V2[error_iter]
    x1 <- df[,which(df_column_independent==v1)]
    y1 <- df[,which(df_column_independent==v2)]
    pos_combi_real$error_rate[error_iter] <- discriminant_analysis_error_rate(grouping = df[,coldependent], x=x1, y=y1,
                                                                              method=method_choose, xlab=v1, ylab=v2)
  }
  iteration_stop = nrow(pos_combi_real)
  iteration_partimat = round(nrow(pos_combi_real) / by, 0)
  iter = 0
  #print(head(df))
  #print(head(pos_combi_real))
  
  for(possible in 1:iteration_partimat){
    x11()
    par(mfrow=c(2,2))
    v1_1st_explain = pos_combi_real$V1[1 + 4*(possible-1)]
    v2_1st_explain = pos_combi_real$V2[1 + 4*(possible-1)]
    v1_2nd_explain = pos_combi_real$V1[2 + 4*(possible-1)]
    v2_2nd_explain = pos_combi_real$V2[2 + 4*(possible-1)]
    v1_3rd_explain = pos_combi_real$V1[3 + 4*(possible-1)]
    v2_3rd_explain = pos_combi_real$V2[3 + 4*(possible-1)]
    v1_4th_explain = pos_combi_real$V1[4 + 4*(possible-1)]
    v2_4th_explain = pos_combi_real$V2[4 + 4*(possible-1)]
    
    x1 <- df[,which(df_column_independent==v1_1st_explain)]
    y1 <- df[,which(df_column_independent==v2_1st_explain)]
    x2 <- df[,which(df_column_independent==v1_2nd_explain)]
    y2 <- df[,which(df_column_independent==v2_2nd_explain)]
    x3 <- df[,which(df_column_independent==v1_3rd_explain)]
    y3 <- df[,which(df_column_independent==v2_3rd_explain)]
    x4 <- df[,which(df_column_independent==v1_4th_explain)]
    y4 <- df[,which(df_column_independent==v2_4th_explain)]
    
    drawparti(grouping = df[,coldependent], x=x1, y=y1,
              method=method_choose, xlab=v1_1st_explain, ylab=v2_1st_explain)
    iter = iter + 1
    if(iter == iteration_stop){break}
    drawparti(grouping = df[,coldependent], x=x2, y=y2,
              method=method_choose, xlab=v1_2nd_explain, ylab=v2_2nd_explain)
    iter = iter + 1
    if(iter == iteration_stop){break}
    drawparti(grouping = df[,coldependent], x=x3, y=y3,
              method=method_choose, xlab=v1_3rd_explain, ylab=v2_3rd_explain)
    iter = iter + 1
    if(iter == iteration_stop){break}
    drawparti(grouping = df[,coldependent], x=x4, y=y4,
              method=method_choose, xlab=v1_4th_explain, ylab=v2_4th_explain)
    iter = iter + 1
    if(iter == iteration_stop){break}
  }
  return(pos_combi_real)
}

multivariate_parallel_coordination <- function(df, independent_col_index, group_col_index, 
                                               scale="globalminmax", boxplot_attach=TRUE,
                                               missing_options="exclude", 
                                               title_plot="Parallel Coordination Plot",
                                               short_independent_name=TRUE, 
                                               unique_colname_method="numberin"){
  library(GGally)
  library(parcoords)
  library(plotly)
  library(tidyverse)
  
  writeLines("")
  writeLines("Scaling Options Include:")
  writeLines("1) std: univariately, subtract mean and divide by standard deviation")
  writeLines("2) robust: univariately, subtract median and divide by median absolute deviation")
  writeLines("3) uniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one")
  writeLines("4) globalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum")
  writeLines("5) center: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param")
  writeLines("6) centerObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param")
  writeLines(paste0("Choosen Scaling: ", scale))
  writeLines("")
  
  group_name <- colnames(df)[group_col_index]
  facet_eval <- paste0("facet_wrap(~ ", group_name, ")")
  
  writeLines("Missing Data Options Include:")
  writeLines("1) exclude: remove all cases with missing values")
  writeLines("2) mean: set missing values to the mean of the variable")
  writeLines("3) median: set missing values to the median of the variable")
  writeLines("4) min10: set missing values to 10% below the minimum of the variable")
  writeLines("5) random: set missing values to value of randomly chosen observation on that variable")
  writeLines(paste0("Choosen Missing Options: ", missing_options))
  
  if(short_independent_name){
    short_independent <- c()
    library(stringr)
    length_independent <- length(independent_col_index)
    target_independent <- colnames(df)[independent_col_index]
    for(w in 1:length(target_independent)){
      if(grepl("\\_", target_independent[w])){
        writeLines("Take 2 first Letter from beginning and after symbol")
        string_sep <- unlist(str_split(target_independent[w], "\\_"))
        char_vector <- c()
        for(f in 1:length(string_sep)){
          char_vector <- c(char_vector, substr(string_sep[f],1,1))
        }
        symbols <- toupper(paste0(char_vector, collapse=""))
        short_independent <- c(short_independent, symbols)
        writeLines(paste0("Variable ",target_independent[w],"--> is changed to -->",symbols))
        writeLines("")
      }
      else if(grepl("\\.", target_independent[w])){
        writeLines("Take 2 first Letter from beginning and after symbol")
        string_sep <- unlist(str_split(target_independent[w], "\\."))
        char_vector <- c()
        for(f in 1:length(string_sep)){
          char_vector <- c(char_vector, substr(string_sep[f],1,1))
        }
        symbols <- toupper(paste0(char_vector, collapse=""))
        short_independent <- c(short_independent, symbols)
        writeLines(paste0("Variable ",target_independent[w],"--> is changed to -->",symbols))
        writeLines("")
      }
      else{
        writeLines("Take 3 first Letter and Uppercase")
        symbols <- substr(target_independent[w], 1, 3)
        short_independent <- c(short_independent, symbols)
        writeLines(paste0("Variable ",target_independent[w],"--> is changed to -->",symbols))
        writeLines("")
      }
    }
    
    #ensure that all simplified symbols is unique! if not replay the short independent process with concating last string
    if(length(unique(short_independent)) < length_independent){
      writeLines("==================================================================================")
      writeLines("========================= Remake Unique Symbols ==================================")
      writeLines("==================================================================================")
      short_independent <- c()
      library(stringr)
      length_independent <- length(independent_col_index)
      target_independent <- colnames(df)[independent_col_index]
      for(w in 1:length(target_independent)){
        if(grepl("\\_", target_independent[w])){
          writeLines("Take 2 first Letter from beginning and after symbol")
          string_sep <- unlist(str_split(target_independent[w], "\\_"))
          char_vector <- c()
          for(f in 1:(length(string_sep)-1)){
            char_vector <- c(char_vector, substr(string_sep[f],1,1))
          }
          symbols <- paste0(toupper(paste0(char_vector, collapse="")),string_sep[length(string_sep)])
          short_independent <- c(short_independent, symbols)
          writeLines(paste0("Variable ",target_independent[w],"--> is changed to -->",symbols))
          writeLines("")
        }
        else if(grepl("\\.", target_independent[w])){
          writeLines("Take 2 first Letter from beginning and after symbol")
          string_sep <- unlist(str_split(target_independent[w], "\\."))
          char_vector <- c()
          for(f in 1:(length(string_sep)-1)){
            char_vector <- c(char_vector, substr(string_sep[f],1,1))
          }
          symbols <- paste0(toupper(paste0(char_vector, collapse="")),string_sep[length(string_sep)])
          short_independent <- c(short_independent, symbols)
          writeLines(paste0("Variable ",target_independent[w],"--> is changed to -->",symbols))
          writeLines("")
        }
        else{
          writeLines("Take 3 first Letter and Uppercase")
          symbols <- substr(target_independent[w], 1, 3)
          short_independent <- c(short_independent, symbols)
          writeLines(paste0("Variable ",target_independent[w],"--> is changed to -->",symbols))
          writeLines("")
        }
      }
    }
    colnames(df)[independent_col_index] <- short_independent
  }
  
  ggparcoord(data = df, 
             columns = independent_col_index, 
             groupColumn = group_col_index,
             scale = scale,
             missing = missing_options,
             boxplot = boxplot_attach, 
             title = title_plot) + 
    eval(parse(text=facet_eval))
}

#It is not compatible with Big Range Values of data
multivariate_forest_plot <- function(df, factor_col, value_col){
  library(forestplot)
  library(dplyr)
  library(lattice)
  
  df <- df %>% 
    group_by(eval(parse(text=factor_col))) %>% 
    summarise(total = sum(eval(parse(text=value_col)), na.rm=TRUE), 
              average = round(mean(eval(parse(text=value_col)), na.rm=TRUE),0),
              minimum = min(eval(parse(text=value_col)), na.rm=TRUE),
              maximum = max(eval(parse(text=value_col)), na.rm=TRUE),
              variance = var(eval(parse(text=value_col)), na.rm=TRUE),
              stddev = sd(eval(parse(text=value_col)), na.rm=TRUE),
              count = n()
    )
  
  df <- data.frame(df)
  colnames(df)[1] <- factor_col
  print(head(df))
  
  #1) Calculate Mean, Lower and Upper from the value columns
  mean_value <- df$average
  lower_value <- df$minimum
  upper_value <- df$maximum
  
  writeLines("mean value")
  print(mean_value)
  writeLines("lower value")
  print(lower_value)
  writeLines("upper value")
  print(upper_value)
  
  value_most_minimum <- min(lower_value, na.rm=TRUE)
  value_most_maximum <- max(upper_value, na.rm=TRUE)
  
  value_df <- data.frame(mean=mean_value,
                         min=lower_value,
                         max=upper_value)
  value_df <- rbind(NA,value_df) #for header
  value_df <- rbind(value_df, NA) #for space before grand summary
  average_average <- mean(value_df$mean, na.rm=TRUE)
  average_min <- mean(value_df$min, na.rm=TRUE)
  average_max <- mean(value_df$max, na.rm=TRUE)
  grand_value <- data.frame(mean=average_average,
                            min=average_min,
                            max=average_max)
  value_df <- rbind(value_df, grand_value)
  
  factor_table <- c()
  avg_value_table <- c()
  min_value_table <- c()
  max_value_table <- c()
  
  #2) Make Table Structure
  table_length <- nrow(df) + 3
  for(tab in 1:table_length){
    if(tab==1){
      #Header Colname
      factor_table <- c(factor_table, as.character(factor_col))
      avg_value_table <- c(avg_value_table, paste0(as.character(value_col), "(mean)"))
      min_value_table <- c(min_value_table, paste0(as.character(value_col), "(min)"))
      max_value_table <- c(max_value_table, paste0(as.character(value_col), "(max)"))
    }
    else if(tab > 1 && tab <= (table_length-2)){
      #Main Body
      factor_table <- c(factor_table, as.character(df[tab-1, 1]))
      avg_value_table <- c(avg_value_table, as.character(value_df$mean[tab]))
      min_value_table <- c(min_value_table, as.character(value_df$min[tab]))
      max_value_table <- c(max_value_table, as.character(value_df$max[tab]))
    }
    else if(tab > (table_length-2)){
      #Grand Summary
      if(tab == (table_length-1)){
        factor_table <- c(factor_table, NA)
        avg_value_table <- c(avg_value_table, NA)
        min_value_table <- c(min_value_table, NA)
        max_value_table <- c(max_value_table, NA)
      }
      else if(tab == (table_length)){
        factor_table <- c(factor_table, "Summary Grand Average")
        avg_value_table <- c(avg_value_table, as.character(value_df$mean[tab]))
        min_value_table <- c(min_value_table, as.character(value_df$min[tab]))
        max_value_table <- c(max_value_table, as.character(value_df$max[tab]))
      }
    }
  }
  tabletext <- cbind(factor_table, avg_value_table,
                     min_value_table, max_value_table)
  
  writeLines("")
  writeLines("====================Value DF Provided=============================")
  writeLines("")
  print(value_df)
  
  writeLines("")
  writeLines("====================Table Text Provided=============================")
  writeLines("")
  print(tabletext)
  
  trellis.device(device = "windows", 
                 height = 600, width = 1200, color = TRUE)
  
  print(paste0("most maximum value = ", value_most_maximum))
  print(paste0("most minimum value = ", value_most_minimum))
  
  forestplot(tabletext, 
             value_df,
             new_page = TRUE,
             is.summary=c(TRUE,rep(FALSE,nrow(df)),TRUE,TRUE),
             clip=c(value_most_minimum, value_most_maximum), 
             xlog=TRUE,
             col=fpColors(box="royalblue",line="darkblue", 
                          summary="royalblue", hrz_lines = "#444444"),
             vertices = TRUE)
}

multivariate_tree_percent_plot <- function(df, var_vector=c(), palette_vector=c(), 
                                  customcolor_vector=c(), guide=TRUE, orientation="horizontal"){
  x11()
  library(vtree)
  if(guide){
    writeLines("Construct Tree Diagram consisting of Data Composition of Factors")
    writeLines("If Using Pallette, Here is Palette Number to Choose From: ")
    writeLines("1	Reds		4	Oranges		7	PuBu		10	PuBuGn		13	RdYlGn")
    writeLines("2	Blues		5	Purples		8	PuRd		11	BuPu		14	Set1")
    writeLines("3	Greens		6	YlGn		9	YlOrBr		12	YlOrRd		")
    writeLines("If Using Custom Color, Provide Variable Name for each Color to use as Named Vector")
    writeLines("Fold       Clap       Exer      Smoke ")
    writeLines("#e7d4e8   #99db8c9    #9ecae1   #a4e12f")
  }
  orient <- NULL
  if(orientation=="horizontal"){
    orient <- TRUE
  }else if(orientation=="vertical"){
    orient <- FALSE
  }
  if(length(palette_vector) > 0){
    vtree(df, var_vector, 
          horiz=orient, 
          palette = palette_vector)
  }else if(length(customcolor_vector) > 0){
    vtree(df, var_vector, 
          horiz=orient, 
          fillcolor = customcolor_vector) 
  }
}

#----------------------------- Use Radar Chart to compare comparable Individuals ---------------------------
radar_chart_autoplot <- function(df, label_legend=c(), 
                                 title_plot="Radar Plot", segment_length=7){
  # Radar chart with fmsb package
  library(fmsb)
  maxvalue <- max(df)
  minvalue <- min(df)
  
  # Construct the data set
  observation_len = nrow(df)
  column_len = length(colnames(df))
  data <- data.frame(rbind(rep(maxvalue, column_len), 
                           rep(minvalue, column_len),
                           df))
  
  # Define fill colors
  colors_fill <- c(scales::alpha("firebrick1", 0.2),
                   scales::alpha("chocolate1", 0.2),
                   scales::alpha("darkgoldenrod1", 0.2),
                   scales::alpha("khaki1", 0.2),
                   scales::alpha("lawngreen", 0.2),
                   scales::alpha("green4", 0.2),
                   scales::alpha("lightseagreen", 0.2),
                   scales::alpha("lightcyan", 0.2),
                   scales::alpha("lightblue2", 0.2),
                   scales::alpha("deepskyblue2", 0.2),
                   scales::alpha("deepskyblue4", 0.2),
                   scales::alpha("burlywood1", 0.2),
                   scales::alpha("darkorchid1", 0.2),
                   scales::alpha("darkmagenta", 0.2),
                   scales::alpha("moccasin", 0.2),
                   scales::alpha("olivedrab1", 0.2),
                   scales::alpha("orchid3", 0.2),
                   scales::alpha("pink1", 0.2),
                   scales::alpha("tomato1", 0.2),
                   scales::alpha("tomato4", 0.2),
                   scales::alpha("gold1", 0.2))
  
  # Define line colors
  colors_line <- c(scales::alpha("firebrick1", 0.9),
                   scales::alpha("chocolate1", 0.9),
                   scales::alpha("darkgoldenrod1", 0.9),
                   scales::alpha("khaki1", 0.9),
                   scales::alpha("lawngreen", 0.9),
                   scales::alpha("green4", 0.9),
                   scales::alpha("lightseagreen", 0.9),
                   scales::alpha("lightcyan", 0.9),
                   scales::alpha("lightblue2", 0.9),
                   scales::alpha("deepskyblue2", 0.9),
                   scales::alpha("deepskyblue4", 0.9),
                   scales::alpha("burlywood1", 0.9),
                   scales::alpha("darkorchid1", 0.9),
                   scales::alpha("darkmagenta", 0.9),
                   scales::alpha("moccasin", 0.9),
                   scales::alpha("olivedrab1", 0.9),
                   scales::alpha("orchid3", 0.9),
                   scales::alpha("pink1", 0.9),
                   scales::alpha("tomato1", 0.9),
                   scales::alpha("tomato4", 0.9),
                   scales::alpha("gold1", 0.9))
  
  color_random_index = round(runif(observation_len, 1, 21))
  color_fill_select = colors_fill[color_random_index]
  color_line_select = colors_line[color_random_index]
  segment_radar <- segment_length
  axis_labels <- round(seq(0, maxvalue, length=segment_radar+1))
  # Create plot
  x11()
  radarchart(data, 
             seg = segment_radar,  # Number of axis segments
             title = title_plot,
             axistype = 1,
             pcol = color_line_select,
             pfcol = color_fill_select,
             plwd = 5, cglty=1, plty=1, 
             axislabcol = "black",
             cglcol="black",
             caxislabels = axis_labels)
  
  if(length(label_legend) > 0){
    # Add a legend
    legend("topright", legend = label_legend, 
           bty = "n", pch=20 , col = color_line_select, 
           cex = 1.05, pt.cex = 3) 
  }
  
}

#----------------------------- sample usage radar chart autoplot -------------------------------------------
sample_vec = Vehicle[sample(1:nrow(Vehicle), 5), which(colnames(Vehicle)!="Class")]
radar_chart_autoplot(sample_vec, label_legend = rownames(sample_vec), "Sample Vehicle Radar Plot")

#----------------------------- Use Sunburst Plot (Level path is index columns to be Level 1-n in order (categories)) -----------------------
sunburst_autoplot <- function(df, level_path=c(), value_observe="",
                              mode_explanation="percentage"){
  library(sunburstR)
  writeLines("1) Transform the DF first to appropriate DF to be processed with sunburst R")
  string_level_path = c()
  length_level = length(level_path)
  df_selection = df[,level_path]
  df_value = df[,value_observe]
  
  for(x in 1:nrow(df_selection)){
    string_level_temp = ""
    for(y in 1:length_level){
      if(y != length_level){
        string_level_temp <- paste0(string_level_temp, df_selection[x,y], "-")
      }
      else if(y == length_level){
        string_level_temp <- paste0(string_level_temp, df_selection[x,y])
      }
    }
    string_level_path <- c(string_level_path, string_level_temp)
  }
  df_sunburst <- data.frame(cbind(string_level_path, df_value))
  df_sunburst$string_level_path <- as.character(df_sunburst$string_level_path)
  print(head(df_sunburst))
  writeLines("2) Plot sunburst R")
  if(mode_explanation=="percentage"){
    writeLines("Creating Sunburst with Percentage Explanation")
    print(sunburst(df_sunburst))
  }
  else if(mode_explanation=="nominal"){
    writeLines("Creating Sunburst with Nominal Explanation")
    print(sunburst(df_sunburst, count=TRUE))
  }
  else if(mode_explanation=="category"){
    writeLines("Creating Sunburst with Category Explanation")
    print(sunburst(df_sunburst, explanation = "function(d){return d.data.name}"))
  }
  return(df_sunburst)
}

#----------------------------- sample usage Sunburst  ---------------------------------------------
pokemon_sunburst <- sunburst_autoplot(pokemon, c("egg_group_1","egg_group_2","type_1","type_2"), "weight", "nominal")
pokemon_sunburst <- sunburst_autoplot(pokemon, c("egg_group_1","egg_group_2","type_1","type_2"), "weight", "percentage")
pokemon_sunburst <- sunburst_autoplot(pokemon, c("egg_group_1","egg_group_2","type_1","type_2"), "weight", "category")
pokemon_sunburst <- sunburst_autoplot(pokemon, c("egg_group_1","egg_group_2","type_1","type_2"), "attack", "nominal")
pokemon_sunburst <- sunburst_autoplot(pokemon, c("egg_group_1","egg_group_2","type_1","type_2"), "attack", "percentage")
pokemon_sunburst <- sunburst_autoplot(pokemon, c("egg_group_1","egg_group_2","type_1","type_2"), "attack", "category")

wind_rose_autoplot <- function(df, timeframeplot=FALSE, selected_pollutant="All",
                               angle_plot = 30, width_plot=0.2, grid_length=5){
  library(openair)
  library(dplyr)
  
  if("ws" %in% colnames(df) == FALSE || "wd" %in% colnames(df) == FALSE){
    stop("Dataset Doesn't Contain an Approproate Wind Strength and Directional! Add it First before Plotting")
  }
  df_numeric <- df[,-which(colnames(df)=="ws" | colnames(df)=="wd")]
  df_numeric <- df_numeric %>%
    select_if(is.numeric)
  if(selected_pollutant=="All"){
    if(timeframeplot==FALSE){
      col_length = length(colnames(df_numeric))
      a <- windRose(df, main="Wind Rose Plot Frequency of Counts", angle = angle_plot,
                    width = width_plot, grid.line = grid_length, sub="")
      b <- polarPlot(df, main="Wind Polarity Plot", angle = angle_plot,
                     width = width_plot, grid.line = grid_length)
      for(i in 1:col_length){
        x11()
        c <- polarFreq(df, pollutant = colnames(df_numeric)[i], ws.int = 30, statistic = "weighted.mean",
                       offset = 20, trans = FALSE, col = "heat",main=paste("Wind Polar Frequency by",colnames(df_numeric)[i]))
        d <- pollutionRose(df, pollutant = colnames(df_numeric)[i], angle = angle_plot,
                           width = width_plot, grid.line = grid_length, statistic = "prop.mean",
                           main=paste("Pollution Rose Plot by ", colnames(df_numeric)[i]))
        print(a, split = c(1, 1, 2, 2))
        print(b, split = c(2, 1, 2, 2), newpage = FALSE)
        print(c, split = c(1, 2, 2, 2), newpage = FALSE)
        print(d, split = c(2, 2, 2, 2), newpage = FALSE)
      }
      
    }
    else{
      col_length = length(colnames(df_numeric))
      a <- windRose(df, main="Wind Rose Plot Frequency of Counts", angle = angle_plot,
                    width = width_plot, grid.line = grid_length, sub="")
      b <- polarPlot(df, main="Wind Polarity Plot", angle = angle_plot,
                     width = width_plot, grid.line = grid_length)
      for(i in 1:col_length){
        x11()
        c <- polarFreq(df, pollutant = colnames(df_numeric)[i], ws.int = 30, statistic = "weighted.mean",
                       offset = 20, trans = FALSE, col = "heat",main=paste("Wind Polar Frequency by",colnames(df_numeric)[i]))
        print(a, split = c(1, 1, 3, 1))
        print(b, split = c(2, 1, 3, 1), newpage = FALSE)
        print(c, split = c(3, 1, 3, 1), newpage = FALSE)
        x11()
        d <- pollutionRose(df, pollutant = colnames(df_numeric)[i], angle = angle_plot,
                           width = width_plot, grid.line = grid_length, type = "year", statistic = "prop.mean",
                           main=paste("Yearly Pollution Rose Plot by ", colnames(df_numeric)[i]))
        x11()
        e <- pollutionRose(df, pollutant = colnames(df_numeric)[i], angle = angle_plot,
                           width = width_plot, grid.line = grid_length, type = "season", statistic = "prop.mean",
                           main=paste("Season Pollution Rose Plot by ", colnames(df_numeric)[i]))
        x11()
        f <- pollutionRose(df, pollutant = colnames(df_numeric)[i], angle = angle_plot,
                           width = width_plot, grid.line = grid_length, type = "weekday", statistic = "prop.mean",
                           main=paste("Weekday Pollution Rose Plot by ", colnames(df_numeric)[i]))
        x11()
        h <- timeVariation(df)
      }
      
    }
  }
  else{
    if(timeframeplot==FALSE){
      col_length = length(colnames(df_numeric))
      a <- windRose(df, main="Wind Rose Plot Frequency of Counts", angle = angle_plot,
                    width = width_plot, grid.line = grid_length, sub="")
      b <- polarPlot(df, main="Wind Polarity Plot", angle = angle_plot,
                     width = width_plot, grid.line = grid_length)
      c <- polarFreq(df, pollutant = selected_pollutant, ws.int = 30, statistic = "weighted.mean",
                     offset = 20, trans = FALSE, col = "heat",main=paste("Wind Polar Frequency by",selected_pollutant))
      d <- pollutionRose(df, pollutant = selected_pollutant, angle = angle_plot,
                         width = width_plot, grid.line = grid_length, statistic = "prop.mean",
                         main=paste("Pollution Rose Plot by ", selected_pollutant))
      x11()
      print(a, split = c(1, 1, 2, 2))
      print(b, split = c(2, 1, 2, 2), newpage = FALSE)
      print(c, split = c(1, 2, 2, 2), newpage = FALSE)
      print(d, split = c(2, 2, 2, 2), newpage = FALSE)
      
      
    }
    else{
      col_length = length(colnames(df_numeric))
      a <- windRose(df, main="Wind Rose Plot Frequency of Counts", angle = angle_plot,
                    width = width_plot, grid.line = grid_length, sub="")
      b <- polarPlot(df, main="Wind Polarity Plot", angle = angle_plot,
                     width = width_plot, grid.line = grid_length)
      c <- polarFreq(df, pollutant = selected_pollutant, ws.int = 30, statistic = "weighted.mean",
                     offset = 20, trans = FALSE, col = "heat",main=paste("Wind Polar Frequency by",selected_pollutant))
      x11()
      print(a, split = c(1, 1, 3, 1))
      print(b, split = c(2, 1, 3, 1), newpage = FALSE)
      print(c, split = c(3, 1, 3, 1), newpage = FALSE)
      x11()
      d <- pollutionRose(df, pollutant = selected_pollutant, angle = angle_plot,
                         width = width_plot, grid.line = grid_length, type = "year", statistic = "prop.mean",
                         main=paste("Yearly Pollution Rose Plot by ", selected_pollutant))
      x11()
      e <- pollutionRose(df, pollutant = selected_pollutant, angle = angle_plot,
                         width = width_plot, grid.line = grid_length, type = "season", statistic = "prop.mean",
                         main=paste("Season Pollution Rose Plot by ", selected_pollutant))
      x11()
      f <- pollutionRose(df, pollutant = selected_pollutant, angle = angle_plot,
                         width = width_plot, grid.line = grid_length, type = "weekday", statistic = "prop.mean",
                         main=paste("Weekday Pollution Rose Plot by ", selected_pollutant))
      x11()
      h <- timeVariation(df)
    }
  }
  
}

#----------------------------- sample usage wind rose ---------------------------------
library(openair)
data(mydata)
x=mydata
wind_rose_autoplot(mydata)
wind_rose_autoplot(mydata, selected_pollutant = "no2", timeframeplot = TRUE)

#format = either venn or upset
venn_upset_df_converter <- function(input_individual=c(), 
                              colname_selection=c(), 
                              input_intersection=c(),
                              format="venn"){
  library(VennDiagram)
  library(UpSetR)
  library(utils)
  
  sep=""
  if(format=="venn"){
    sep <- "_"
  }
  else if(format=="upset"){
    sep <- "&"
  }
  individual_category <- length(colname_selection)
  writeLines("To make a proper Venn DF, please input as follows:")
  writeLines("1 Individual Category -> 0 input intersection")
  writeLines("2 Individual Category -> 1 input intersection (1x2 intersection)")
  writeLines("3 Individual Category -> 4 input intersection (3x2 intersection + 1x3 intersection)")
  writeLines("4 Individual Category -> 11 input intersection (6x2 intersection + 4x3 intersection + 1x4 intersection)")
  writeLines("5 Individual Category -> 25 input intersection (11x2 Intersection + 10x3 Intersection + 3x4 Intersection + 4x1 Intersection)")
  if(individual_category==1 && length(input_intersection)==0){
    vec <- input_individual
    names(vec) <- colname_selection
    vec <- as.data.frame(t(vec))
    if(format=="upset"){
      vec_name <- colnames(vec)
      vec <- as.vector(unlist(vec))
      names(vec) <- vec_name
    }
    return(vec)
  }
  else if(individual_category==2 && length(input_intersection)==1){
    vec_intersection <- input_intersection
    names(vec_intersection) <- paste0(colname_selection[1], sep, colname_selection[2])
    vec <- input_individual
    names(vec) <- colname_selection
    vec <- c(vec, vec_intersection)
    vec <- as.data.frame(t(vec))
    if(format=="upset"){
      vec_name <- colnames(vec)
      vec <- as.vector(unlist(vec))
      names(vec) <- vec_name
    }
    return(vec)
  }
  else if(individual_category==3 && length(input_intersection)==4){
    vec_intersection <- input_intersection
    phase1_intersection_name <- c()
    phase2_intersection_name <- c(paste0(colname_selection[1], sep, colname_selection[2],sep,colname_selection[3]))
    
    combination_category <- combn(colname_selection, 2)
    combination_category <- t(combination_category)
    for(b in 1:nrow(combination_category)){
      phase1_intersection_name <- c(phase1_intersection_name, paste0(combination_category[b,], collapse=sep))
    }
    
    intersection_name <- c(phase1_intersection_name, phase2_intersection_name)
    names(vec_intersection) <- intersection_name
    vec <- input_individual
    names(vec) <- colname_selection
    vec <- c(vec, vec_intersection)
    vec <- as.data.frame(t(vec))
    if(format=="upset"){
      vec_name <- colnames(vec)
      vec <- as.vector(unlist(vec))
      names(vec) <- vec_name
    }
    return(vec)
  }
  else if(individual_category==4 && length(input_intersection)==11){
    vec_intersection <- input_intersection
    phase1_intersection_name <- c()
    phase2_intersection_name <- c()
    phase3_intersection_name <- c(paste0(colname_selection[1], sep, colname_selection[2], sep,
                                         colname_selection[3], sep, colname_selection[4]))
    
    combination_category <- combn(colname_selection, 2)
    combination_category <- t(combination_category)
    for(b in 1:nrow(combination_category)){
      phase1_intersection_name <- c(phase1_intersection_name, paste0(combination_category[b,], collapse=sep))
    }
    
    combination_category <- combn(colname_selection, 3)
    combination_category <- t(combination_category)
    for(c in 1:nrow(combination_category)){
      phase2_intersection_name <- c(phase2_intersection_name, paste0(combination_category[c,], collapse=sep))
    }
    
    intersection_name <- c(phase1_intersection_name, phase2_intersection_name, phase3_intersection_name)
    names(vec_intersection) <- intersection_name
    vec <- input_individual
    names(vec) <- colname_selection
    vec <- c(vec, vec_intersection)
    vec <- as.data.frame(t(vec))
    if(format=="upset"){
      vec_name <- colnames(vec)
      vec <- as.vector(unlist(vec))
      names(vec) <- vec_name
    }
    return(vec)
  }
  else if(individual_category==5 && length(input_intersection)==25){
    vec_intersection <- input_intersection
    phase1_intersection_name <- c()
    phase2_intersection_name <- c()
    phase3_intersection_name <- c()
    phase4_intersection_name <- c(paste0(colname_selection[1], sep, colname_selection[2], sep,
                                         colname_selection[3], sep, colname_selection[4], sep,
                                         colname_selection[5]))
    
    combination_category <- combn(colname_selection, 2)
    combination_category <- t(combination_category)
    for(b in 1:nrow(combination_category)){
      phase1_intersection_name <- c(phase1_intersection_name, paste0(combination_category[b,], collapse=sep))
    }
    
    combination_category <- combn(colname_selection, 3)
    combination_category <- t(combination_category)
    for(c in 1:nrow(combination_category)){
      phase2_intersection_name <- c(phase2_intersection_name, paste0(combination_category[c,], collapse=sep))
    }
    
    combination_category <- combn(colname_selection, 4)
    combination_category <- t(combination_category)
    for(d in 1:nrow(combination_category)){
      phase3_intersection_name <- c(phase3_intersection_name, paste0(combination_category[d,], collapse=sep))
    }
    
    intersection_name <- c(phase1_intersection_name, phase2_intersection_name, 
                           phase3_intersection_name, phase4_intersection_name)
    names(vec_intersection) <- intersection_name
    vec <- input_individual
    names(vec) <- colname_selection
    vec <- c(vec, vec_intersection)
    vec <- as.data.frame(t(vec))
    if(format=="upset"){
      vec_name <- colnames(vec)
      vec <- as.vector(unlist(vec))
      names(vec) <- vec_name
    }
    return(vec)
  }
  else{
    message("Not a valid input for Venn DF Converter!")
    stop()
  }
}

#category_colname -> Fill Individual Colname then Intersection Colname 
venn_diagram <- function(df, num_categories=2, 
                         category_colname=c(), category_title=c(), category_color=c()){
  library(VennDiagram)
  grid.newpage()
  
  if(length(category_color)==0){
    writeLines("Adjust Auto Color for Venn Diagram")
    if(num_categories==1){
      category_color <- c("#ffdb59")
    }
    else if(num_categories==2){
      category_color <- c("#ffdb59","#59ffd0")
    }
    else if(num_categories==3){
      category_color <- c("#ff6242","#52fa60","#61d5ff")
    }
    else if(num_categories==4){
      category_color <- c("#ff432e","#4592ff","#57ff5f","#ffff61")
    }
    else if(num_categories==5){
      category_color <- c("#ff432e","#4592ff","#57ff5f","#ffff61","#ff29ed")
    }
  }
  
  if(num_categories==1){
    draw.single.venn(
      df[1,1],                       # Size of the left circle.
      lty = "blank",                       # Hide the border of circles.
      fill = category_colname,             # Color.
      category = category_title,           # Label text.
      cat.pos = c(0, 0),                   # Position of labels.
      scaled=TRUE,                         # Scale the circle size or not. 
      fontfamily = "Arial",                # Font name for numbers.
      cat.fontface="bold",                 # Font style for labels.
      cat.fontfamily ="Arial"              # Font name for labels.
    )
  }
  else if(num_categories==2){
    draw.pairwise.venn(
      df[1,1],                       # Size of the left circle.
      df[1,2],                       # Size of the right circle.
      df[1,3],                       # Size of the overlapping area.
      lty = rep("blank", 2),               # Hide the border of circles.
      fill = category_colname,             # Color.
      category = category_title,           # Label text.
      cat.pos = c(0, 0),                   # Position of labels.
      scaled=TRUE,                         # Scale the circle size or not. 
      fontfamily = "Arial",                # Font name for numbers.
      cat.fontface="bold",                 # Font style for labels.
      cat.fontfamily ="Arial"              # Font name for labels.
    )
  }
  else if(num_categories==3){
    draw.triple.venn(
      area1 = df[1,1],        # Size of the circle1.
      area2 = df[1,2],        # Size of the circle2.
      area3 = df[1,3],        # Size of the circle3.
      n12 = df[1,4],          # Size of the overlapping area of circle 1 and 2.
      n13 = df[1,5],          # Size of the overlapping area of circle 1 and 3.
      n23 = df[1,6],          # Size of the overlapping area of circle 2 and 3.
      n123 = df[1,7],         # Size of the overlapping area of circle 1, 2, 3.
      category = category_title,    # Label text. 
      lty = "blank",                # Hide the border of circles.
      fill = category_color,        # Color
      fontfamily = "Arial",         # Font name for numbers.
      cat.fontface="bold",          # Font style for labels.
      cat.fontfamily ="Arial"       # Font name for labels.
    )
  }
  else if(num_categories==4){
    draw.quad.venn(
      area1 = df[1,1],     # Size of the circle1.
      area2 = df[1,2],     # Size of the circle2.
      area3 = df[1,3],     # Size of the circle3.
      area4 = df[1,4],     # Size of the circle4.
      n12 = df[1,5],       # Size of the overlapping area of circle 1 and 2.
      n13 = df[1,6],       # Size of the overlapping area of circle 1 and 3.
      n14 = df[1,7],       # Size of the overlapping area of circle 1 and 4.
      n23 = df[1,8],       # Size of the overlapping area of circle 2 and 3.
      n24 = df[1,9],       # Size of the overlapping area of circle 2 and 4.
      n34 = df[1,10],      # Size of the overlapping area of circle 3 and 4.
      n123 = df[1,11],     # Size of the overlapping area of circle 1, 2, 3.
      n124 = df[1,12],     # Size of the overlapping area of circle 1, 2, 4.
      n134 = df[1,13],     # Size of the overlapping area of circle 1, 3, 4.
      n234 = df[1,14],     # Size of the overlapping area of circle 2, 3, 4.
      n1234 = df[1,15],    # Size of the overlapping area of circle 1, 2, 3, 4.
      category = category_title, # Label text. 
      lty = "blank",           # Hide the border of circles.
      fill = category_color,   # Color
      fontfamily = "Arial",    # Font name for numbers.
      cat.fontface="bold",     # Font style for labels.
      cat.fontfamily ="Arial"  # Font name for labels.
    )
  }
  else if(num_categories==5){
    draw.quintuple.venn(
      area1 = df[1,1],        # Size of the circle1.
      area2 = df[1,2],        # Size of the circle2.
      area3 = df[1,3],        # Size of the circle3.
      area4 = df[1,4],        # Size of the circle4.
      area5 = df[1,5],        # Size of the circle4.
      n12 = df[1,6],      # Size of the overlapping area of circle 1 and 2.
      n13 = df[1,7],      # Size of the overlapping area of circle 1 and 3.
      n14 = df[1,8],      # Size of the overlapping area of circle 1 and 4.
      n15 = df[1,9],      # Size of the overlapping area of circle 1 and 5.
      n23 = df[1,10],      # Size of the overlapping area of circle 2 and 3.
      n24 = df[1,11],      # Size of the overlapping area of circle 2 and 4.
      n25 = df[1,12],      # Size of the overlapping area of circle 2 and 5.
      n34 = df[1,13],      # Size of the overlapping area of circle 3 and 4.
      n35 = df[1,14],      # Size of the overlapping area of circle 3 and 5.
      n45 = df[1,15],      # Size of the overlapping area of circle 4 and 5.
      n123 = df[1,16], # Size of the overlapping area of circle 1, 2, 3.
      n124 = df[1,17], # Size of the overlapping area of circle 1, 2, 4.
      n125 = df[1,18],      # Size of the overlapping area of circle 1, 2, 5.
      n134 = df[1,19], # Size of the overlapping area of circle 1, 3, 4.
      n135 = df[1,20], # Size of the overlapping area of circle 1, 3, 5.
      n145 = df[1,21], # Size of the overlapping area of circle 1, 4, 5
      n234 = df[1,22], # Size of the overlapping area of circle 2, 3, 4.
      n235 = df[1,23], # Size of the overlapping area of circle 2, 3, 5.
      n245 = df[1,24], # Size of the overlapping area of circle 2, 4, 5.
      n345 = df[1,25], # Size of the overlapping area of circle 3, 4, 5.
      n1234 = df[1,26], # Size of the overlapping area of circle 1, 2, 3, 4.
      n1235 = df[1,27], # Size of the overlapping area of circle 1, 2, 3, 5.
      n1245 = df[1,28], # Size of the overlapping area of circle 1, 2, 4, 5.
      n1345 = df[1,29], # Size of the overlapping area of circle 1, 3, 4, 5.
      n12345 = df[1,30], # Size of the overlapping area of circle 1, 2, 3, 4, 5.
      category = category_title, # Label text. 
      lty = "blank",           # Hide the border of circles.
      fill = category_color,   # Color
      fontfamily = "Arial",    # Font name for numbers.
      cat.fontface="bold",     # Font style for labels.
      cat.fontfamily ="Arial"  # Font name for labels.
    )
  }
  
}

upset_diagram <- function(named_vector, n_intersect=30, mb_ratio=c(0.6, 0.4), 
                          value_sort=TRUE, point_size=2.8, line_size=1, text_size=1.1) {
  # Plot
  library(UpSetR)
  library(stringr)
  longest_intersection_col <- which(nchar(names(named_vector)) == max(nchar(names(named_vector))))
  nsets_num <- (str_count(names(named_vector)[longest_intersection_col], "\\&")) + 1
  print(upset(fromExpression(named_vector), 
        nintersects = n_intersect, 
        nsets = nsets_num, 
        order.by = "freq", 
        decreasing = value_sort, 
        mb.ratio = mb_ratio,
        number.angles = 0, 
        text.scale = text_size, 
        point.size = point_size, 
        line.size = line_size
  ))
}

#----------------------------- sample Venn Diagram & Upset Diagram Plot usage -----------------------------------------------------------
#writeLines("To make a proper Venn DF, please input as follows:")
#writeLines("1 Individual Category -> 0 input intersection")
#writeLines("2 Individual Category -> 1 input intersection (1x2 intersection)")
#writeLines("3 Individual Category -> 4 input intersection (3x2 intersection + 1x3 intersection)")
#writeLines("4 Individual Category -> 11 input intersection (6x2 intersection + 4x3 intersection + 1x4 intersection)")
#writeLines("5 Individual Category -> 25 input intersection (11x2 Intersection + 10x3 Intersection + 3x4Intersection + 4x1 Intersection)")
uph_subject <- venn_upset_df_converter(c(2405,2643,2945,3934),
                                 c("Math","Biology","Physics","Chemistry"),
                                 c(253,255,346,451,422,299,102,98,67,59,12))
uph_subject_2 <- venn_upset_df_converter(c(2405,2643,2945,3934),
                                       c("Math","Biology","Physics","Chemistry"),
                                       c(253,255,346,451,422,299,102,98,67,59,12),
                                       format="upset")
uph_venn <- venn_diagram(uph_subject, 4, colnames(uph_subject), colnames(uph_subject)[1:4])
uph_upset <- upset_diagram(uph_subject_2, n_intersect=30, mb_ratio=c(0.6, 0.4), 
                           value_sort=TRUE, point_size=2.8, line_size=1, text_size=1.1)
disease_type <- venn_upset_df_converter(c(305,1643,945),
                                  c("Malaria","COVID","Fever"),
                                  c(167,89,555,20))
disease_type_2 <- venn_upset_df_converter(c(305,1643,945),
                                        c("Malaria","COVID","Fever"),
                                        c(167,89,555,20), format="upset")
disease_venn <- venn_diagram(disease_type, 3, colnames(disease_type), colnames(disease_type)[1:3])
disease_upset <- upset_diagram(disease_type_2, n_intersect=30, mb_ratio=c(0.6, 0.4), 
                           value_sort=TRUE, point_size=2.8, line_size=1, text_size=1.1)
library(ggraph)
data("flare")

circular_packing_plot <- function(df, value_col="", text_col="", levelname_col="",
                                  interactive=TRUE, npoint=50, descriptive_text = TRUE,
                                  levelname_sep=".", text_size=1, color_theme="viridis"){
  # Generate the layout
  library(ggraph)
  library(igraph)
  library(tidyverse)
  library(viridis)
  
  calculate_max_char_occurences <- function(string_vector, search_for=".", mode="max"){
    library(stringr)
    num_vector <- c()
    if(grepl('[^[:punct:]]', search_for) == FALSE){
      search_for <- paste0("\\",search_for)
    }
    for(a in 1:length(string_vector)){
      num_vector <- c(num_vector, str_count(string_vector[a], search_for))
    }
    if(mode=="max"){
      return(max(num_vector))
    }
    else if(mode=="vector"){
      return(num_vector)
    }
  }
  
  if(levelname_col != ""){
    df$level <- calculate_max_char_occurences(df[[levelname_col]], levelname_sep, mode="vector")
  }
  
  if(descriptive_text){
    if(levelname_col != ""){
      df$descriptive <- paste0("Name = ", df[[text_col]], "\n",
                               "Value = ", df[[value_col]], "\n",
                               "Level = ", df$level)
    }
    else{
      df$descriptive <- paste0("Name = ", df[[text_col]], "\n",
                               "Value = ", df[[value_col]], "\n")
    }
  }
  
  packing <- circleProgressiveLayout(df[[value_col]], sizetype='area')
  df <- cbind(df, packing)
  dat.gg <- circleLayoutVertices(packing, npoints=npoint)
  unique_id <- length(unique(dat.gg$id))
  scale_fill_vec <- c()
  scale_color_black <- rep("black",unique_id)
  
  x11()
  # Make the plot with a few differences compared to the static version:
  p <- NULL
  if(descriptive_text){
    p <- ggplot() + 
      geom_polygon_interactive(data = dat.gg, aes(x, y, group = id, fill=id, tooltip = df$descriptive[id], data_id = id), colour = "#000000", alpha = 1) +
      scale_fill_viridis() +
      geom_text(data = df, aes(x, y, label = eval(parse(text=value_col)), size=text_size, colour="black")) +
      scale_color_manual(values=c("black")) +
      theme_void() + 
      theme(legend.position="none", plot.margin=unit(c(0,0,0,0),"cm") ) + 
      coord_equal()
  }
  else{
    p <- ggplot() + 
      geom_polygon_interactive(data = dat.gg, aes(x, y, group = id, fill=id, tooltip = df[[text_col]][id], data_id = id), colour = "#000000", alpha = 1) +
      scale_fill_viridis() +
      geom_text(data = df, aes(x, y, label = eval(parse(text=value_col)), size=text_size, colour="black")) +
      scale_color_manual(values=c("black")) +
      theme_void() + 
      theme(legend.position="none", plot.margin=unit(c(0,0,0,0),"cm") ) + 
      coord_equal()
  }
  
  if(interactive==TRUE){
    widg <- ggiraph(ggobj = p, width_svg = 7, height_svg = 7) 
    print(widg)
    return(widg)
  }
  else{
    print(p)
    return(p)
  }
}

#----------------------------- sample Circular Plot usage ---------------------------------------------------------
vertices <- flare$vertices
vertices <- vertices[which(vertices$size > 0),]
cp_packing <- circular_packing_plot(vertices, "size", "shortName", npoint=220)
cp_packing <- circular_packing_plot(vertices, "size", "shortName", npoint=220, descriptive_text = FALSE, interactive = FALSE)

hierarchy_circular_packing_plot <- function(edge_df, vertice_df, hierarchy_col, 
                                            exclude_level_circular=-1,
                                            sep_hierarchy=".", level_label=2, 
                                            color_theme="viridis"){
  # Libraries
  library(data.tree)
  library(ggraph)
  library(igraph)
  library(tidyverse)
  library(viridis)
  
  calculate_max_char_occurences <- function(string_vector, search_for=".", mode="max"){
    library(stringr)
    num_vector <- c()
    if(grepl('[^[:punct:]]', search_for) == FALSE){
      search_for <- paste0("\\",search_for)
    }
    for(a in 1:length(string_vector)){
      num_vector <- c(num_vector, str_count(string_vector[a], search_for))
    }
    if(mode=="max"){
      return(max(num_vector))
    }
    else if(mode=="vector"){
      return(num_vector)
    }
  }
  
  max_hierarchy <- calculate_max_char_occurences(vertice_df[[hierarchy_col]], sep_hierarchy, "max")
  vertice_df$level <- calculate_max_char_occurences(vertice_df[[hierarchy_col]], sep_hierarchy, "vector")
  writeLines(paste0("Max Hierarchy: ", max_hierarchy))
  
  # Now we can add label for level1 and 2 only for example:
  vertice_df <- vertice_df %>% 
    mutate(new_label=ifelse(level==level_label, shortName, NA))
  mygraph <- graph_from_data_frame(edges, vertices=vertice_df)
  
  scale_fill_vec <- c()
  scale_color_vec <- c()
  if(exclude_level_circular==-1){
    if(color_theme=="viridis"){
      scale_fill_vec <- viridis::viridis(max_hierarchy + 1)
      scale_color_vec <- rep("black", max_hierarchy + 1)
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="inferno"){
      scale_fill_vec <- viridis::inferno(max_hierarchy + 1)
      scale_color_vec <- rep("black", max_hierarchy + 1)
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="magma"){
      scale_fill_vec <- viridis::magma(max_hierarchy + 1)
      scale_color_vec <- rep("black", max_hierarchy + 1)
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="plasma"){
      scale_fill_vec <- viridis::plasma(max_hierarchy + 1)
      scale_color_vec <- rep("black", max_hierarchy + 1)
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="cividis"){
      scale_fill_vec <- viridis::cividis(max_hierarchy + 1)
      scale_color_vec <- rep("black", max_hierarchy + 1)
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
  }
  else if(exclude_level_circular >= 0){
    scale_fill_vec <- rep("white", exclude_level_circular+1)
    scale_color_vec <- rep("white", exclude_level_circular+1)
    if(color_theme=="viridis"){
      scale_fill_vec <- c(scale_fill_vec, viridis::viridis(max_hierarchy - exclude_level_circular))
      scale_color_vec <- c(scale_color_vec, rep("black", max_hierarchy - exclude_level_circular))
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="inferno"){
      scale_fill_vec <- c(scale_fill_vec, viridis::inferno(max_hierarchy - exclude_level_circular))
      scale_color_vec <- c(scale_color_vec, rep("black", max_hierarchy - exclude_level_circular))
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="magma"){
      scale_fill_vec <- c(scale_fill_vec, viridis::magma(max_hierarchy - exclude_level_circular))
      scale_color_vec <- c(scale_color_vec, rep("black", max_hierarchy - exclude_level_circular))
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="plasma"){
      scale_fill_vec <- c(scale_fill_vec, viridis::plasma(max_hierarchy - exclude_level_circular))
      scale_color_vec <- c(scale_color_vec, rep("black", max_hierarchy - exclude_level_circular))
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
    else if(color_theme=="cividis"){
      scale_fill_vec <- c(scale_fill_vec, viridis::cividis(max_hierarchy - exclude_level_circular))
      scale_color_vec <- c(scale_color_vec, rep("black", max_hierarchy - exclude_level_circular))
      names(scale_fill_vec) <- seq(0:max_hierarchy)-1
      names(scale_color_vec) <- seq(0:max_hierarchy)-1
    }
  }
  writeLines("Fill Vector")
  print(scale_fill_vec)
  writeLines("Color Vector")
  print(scale_color_vec)
  
  # Make the graph
  x11()
  
  graph <- ggraph(mygraph, layout = 'circlepack', weight=size) + 
    geom_node_circle(aes(fill = as.factor(depth), color = as.factor(depth))) +
    scale_fill_manual(values=scale_fill_vec) +
    scale_color_manual(values=scale_color_vec) +
    geom_node_label(aes(label=new_label), size=max_hierarchy) +
    theme_void() + 
    theme(legend.position="FALSE", plot.margin = unit(rep(0,max_hierarchy), "cm"))
  print(graph)
  return(graph)
}

#----------------------------- sample Hierarchy Circular Plot USage -------------------------------------------------------
hierarchy_cp <- hierarchy_circular_packing_plot(flare$edges, flare$vertices, "name", -1, level_label=1, color_theme = "magma")
hierarchy_cp <- hierarchy_circular_packing_plot(flare$edges, flare$vertices, "name", 2, level_label=2, color_theme = "magma")
hierarchy_cp <- hierarchy_circular_packing_plot(flare$edges, flare$vertices, "name", 1, level_label=1, color_theme = "viridis")
hierarchy_cp <- hierarchy_circular_packing_plot(flare$edges, flare$vertices, "name", 2, level_label=2, color_theme = "magma")

#------------------------------------------------------------------------------------------------------------------------
################################# 2) Highcharter Plotting ###############################################################
#------------------------------------------------------------------------------------------------------------------------

shiny_inspector_highcharter <- function(plot_list){
  length_plot = length(plot_list)
  require(rCharts)
  require(highcharter)
  require(shiny)
  require(data.table)
  
  runApp(list(
    ui = bootstrapPage(
      mainPanel(width=12,
                Output("plots")
      )
    ),
    server = function(input, output)
    {
      output$plots <- renderUI({
        do.call(tagList, plot_list)
      })
      
      for (i in 1:length_plot) {
        local({
          my_i <- i
          plotname <- paste("plot", my_i, sep="")
          output[[plotname]] <- renderChart({
            x <- plot_list[i]
            x$chart(backgroundColor = "white")
            x$set(dom = plotname)
            return(x)
          })
        })
      }
    }
  ))
  shinyApp(ui = ui, server = server)
}

#this feature only allows singular categories columns
highcharter_all_plot <- function(df, categories_col, value_col, metrics_plot=c(1,2,3,4), 
                                 title="Highcharter Automatic Plot", nonnegative=TRUE, 
                                 sort_values=FALSE, scroll_activated = FALSE, top_n=-1, bottom_n=-1){
  library(highcharter)
  library(dplyr)
  library(stringr)
  library(purrr)
  
  #Preprocess NA in Categories Column
  #https://stackoverflow.com/questions/11254524/omit-rows-containing-specific-column-of-na
  completeFun <- function(data, desiredCols) {
    completeVec <- complete.cases(data[, desiredCols])
    return(data[completeVec, ])
  }
  df <- completeFun(df, categories_col)
  
  #1) Group by Dataframe first and summarise
  df_backup <- df
  df <- df %>% 
    group_by(eval(parse(text=categories_col))) %>% 
    summarise(total = sum(eval(parse(text=value_col)), na.rm=TRUE), 
              average = round(mean(eval(parse(text=value_col)), na.rm=TRUE),0),
              minimum = min(eval(parse(text=value_col)), na.rm=TRUE),
              maximum = max(eval(parse(text=value_col)), na.rm=TRUE),
              variance = var(eval(parse(text=value_col)), na.rm=TRUE),
              stddev = sd(eval(parse(text=value_col)), na.rm=TRUE),
              count = n()
    )
  df <- data.frame(df)
  colnames(df)[1] <- categories_col
  print(head(df))
  
  
  writeLines("Removed Categories because of NA Variance and zero sum: ")
  print(as.character(df[which(df$total == 0 | is.na(df$variance)),][categories_col]))
  df <- df[which(df$total > 0 & !is.na(df$variance)),]
  writeLines("Clean dataset to plot")
  print(head(df))
  
  n <- length(unique(df[,categories_col]))
  set.seed(123)
  df_plot <- data.frame()
  
  #Bold Color
  colors = c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03","#18fc03","#03fc5e","#03fcc6",
             "#03f0fc","#03adfc","#035afc","#be03fc","#fc03f8","#fc03a1")
  
  #Soft Color
  colors2 = c("#ffc6c2","#ffdaa6","#fff9a6","#ecffa6","#d1ffa6","#a9ffa6","#a6ffc4","#a6ffdb","#a6ffff",
              "#a6deff","#a6caff","#a6a7ff","#b9a6ff","#d7a6ff","#f0a6ff","#fca6ff","#ffa6ea","#ffa6c1",
              "#ffa6b3","#ffa6b5")
  
  
  #2) Initial Data Frame to plot
  df_list = list()
  hcs_list = list()
  metrics_name <- c("total","average","minimum","maximum")
  #metrics_plot <- na.omit(metrics_plot)
  for(loop in 1:length(metrics_plot)){
    writeLines(paste("Computing and Assigning plot by", metrics_name[metrics_plot[loop]]))
    categories <- df[,categories_col]
    value <- df[,metrics_name[metrics_plot[loop]]]
    color = rep(colors, length.out = n)
    segmentColor = rep(colors2, length.out = n)
    hcs <- NULL
    low <- c()
    high <- c()
    confidence_interval <- NULL
    if(nonnegative==TRUE){
      confidence_interval <- function(value, sample_length, sd, interval) {
        error <- qnorm((interval+1)/2)*sd/sqrt(sample_length)
        left <- value-error
        right <- value+error
        if(left < 0){
          right <- right + abs(left)
          left <- 0
        }
        return(c(left, right))
      } 
    }
    else if(nonnegative==FALSE){
      confidence_interval <- function(value, sample_length, sd, interval) {
        error <- qnorm((interval+1)/2)*sd/sqrt(sample_length)
        left <- value-error
        right <- value+error
        return(c(left, right))
      }
    }
    for(f in 1:nrow(df)){
      confint_vector <- confidence_interval(value=value[f], sample_length = nrow(df_backup), 
                                            df[,"stddev"][f], 0.95)
      low <- c(low, round(confint_vector[1],0))
      high <- c(high, round(confint_vector[2],0))
    }
    
    x <- as.numeric(0:(nrow(df)-1))
    y <- value
    z <- round((df$count/sum(df$count)) * 100, 3)
    sumvalue <- rep(sum(value), length(x))
    count <- df$count
    sumcount <- rep(sum(count), length(x))
    value_percentage <- round((y/sumvalue * 100),2)
    proportion_percentage <- round((count/sumcount * 100),2)
    name <- categories
    
    df_plot <- data.frame(x, y, z, name, categories, value, sumvalue, value_percentage,
                          color, segmentColor, low, high, count, sumcount, proportion_percentage)
    df_plot$categories <- as.character(df_plot$categories) 
    df_plot$color <- as.character(df_plot$color)
    df_plot$segmentColor <- as.character(df_plot$segmentColor)
    
    if(sort_values==TRUE){
      df_plot <- df_plot[order(df_plot$y, decreasing = TRUE),]
      df_plot$x <- as.numeric(0:(nrow(df_plot)-1))
    }
    if(top_n != -1){
      df_plot <- head(df_plot, top_n)
    }
    if(bottom_n != -1){
      df_plot <- tail(df_plot, botttom_n)
    }
    
    annotation_grand_info <- ""
    hc_series_name <- ""
    if(metrics_plot[loop]==1){
      hc_series_name <- paste("Total Metrics of", title)
      annotation_grand_info <- paste(", With Grand Total Value =", df_plot$sumvalue[1])
    }
    else if(metrics_plot[loop]==2){
      hc_series_name <- paste("Average Metrics of", title)
      annotation_grand_info <- paste(", With Grand Average Value =", round(mean(df_plot$y),0))
    }
    else if(metrics_plot[loop]==3){
      hc_series_name <- paste("Minimum Metrics of", title)
      annotation_grand_info <- paste(", With Highest Minimum =", max(df_plot$y))
    }
    else if(metrics_plot[loop]==4){
      hc_series_name <- paste("Maximum Metrics of", title)
      annotation_grand_info <- paste(", With Highest Maximum =", max(df_plot$y))
    }
    
    create_hc <- function(t, name_series) {
      series_full_name <- paste(name_series, "with", t)
      dont_rm_high_and_low <- c("arearange", "areasplinerange",
                                "columnrange", "errorbar")
      is_polar <- str_detect(t, "polar")
      t <- str_replace(t, "polar", "")
      if(!t %in% dont_rm_high_and_low){
        df_hc <- df_plot %>% dplyr::select(x, y, z, name, categories, value, 
                                           color, segmentColor, sumvalue)
      } 
      else{
        df_hc <- df_plot %>% dplyr::select(x, y, z, name, categories, value, 
                                           color, segmentColor, low, high, sumvalue)
      }
      
      #https://stackoverflow.com/questions/45322124/highcharter-move-data-label-to-top-of-column
      #https://stackoverflow.com/questions/16706068/how-to-make-highcharts-scrollable-horizontally-when-having-big-range-in-x-axis
      if(nrow(df_plot) <= 30){
        xaxis_fontsize <- 12
        yaxis_fontsize <- 20
        highchart() %>%
          hc_title(text = paste(ifelse(is_polar, "polar ", ""), t),
                   style = list(fontSize = "15px", color="white")) %>%
          hc_subtitle(text = paste0(series_full_name, annotation_grand_info),
                      style = list(fontSize = "10px", color="white")) %>%
          hc_chart(type = t, polar = is_polar) %>% 
          hc_xAxis(categories = df_hc$categories, 
                   labels=list(style = list(color="white", fontSize = xaxis_fontsize, 
                                            rotation = 45, autoRotation = FALSE))) %>% 
          hc_yAxis(labels=list(style = list(color="white", fontSize = yaxis_fontsize))) %>%
          hc_add_series(df_hc, name=series_full_name, showInLegend = TRUE, 
                        dataLabels = list(enabled=TRUE, verticalAlign= 'top')) %>%
          hc_add_theme(hc_theme_monokai()) 
      } 
      else if(nrow(df_plot) > 30 && scroll_activated==TRUE){
        xaxis_fontsize <- 10
        yaxis_fontsize <- 20
        highchart() %>%
          hc_title(text = paste(ifelse(is_polar, "polar ", ""), t),
                   style = list(fontSize = "15px", color="white")) %>%
          hc_subtitle(text = paste0(series_full_name, annotation_grand_info),
                      style = list(fontSize = "10px", color="white")) %>%
          hc_chart(type = t, polar = is_polar) %>% 
          hc_xAxis(categories = df_hc$categories, 
                   labels=list(style = list(color="white", fontSize = xaxis_fontsize, 
                                            rotation = 45, autoRotation = FALSE)),
                   min=0, max=35) %>% 
          hc_yAxis(labels=list(style = list(color="white", fontSize = yaxis_fontsize))) %>%
          hc_add_series(df_hc, name=series_full_name, showInLegend = TRUE, 
                        dataLabels = list(enabled=TRUE, verticalAlign= 'top')) %>%
          hc_scrollbar(enabled=TRUE) %>%
          hc_add_theme(hc_theme_monokai())  
      }
      else if(nrow(df_plot) > 30 && scroll_activated==FALSE){
        xaxis_fontsize <- 10
        yaxis_fontsize <- 20
        if(t == "bar"){
          if(nrow(df_plot) > 25 && nrow(df_plot) <= 30){
            xaxis_fontsize <- 4 
          }
          else if(nrow(df_plot) > 20 && nrow(df_plot) <= 25){
            xaxis_fontsize <- 6 
          }
          else if(nrow(df_plot) > 15 && nrow(df_plot) <= 20){
            xaxis_fontsize <- 8 
          }
          else if(nrow(df_plot) > 10 && nrow(df_plot) <= 15){
            xaxis_fontsize <- 10 
          }
          else if(nrow(df_plot) <= 10){
            xaxis_fontsize <- 12 
          }
        }
        
        highchart() %>%
          hc_title(text = paste(ifelse(is_polar, "polar ", ""), t),
                   style = list(fontSize = "15px", color="white")) %>%
          hc_subtitle(text = paste0(series_full_name, annotation_grand_info),
                      style = list(fontSize = "10px", color="white")) %>%
          hc_chart(type = t, polar = is_polar) %>% 
          hc_xAxis(categories = df_hc$categories, 
                   labels=list(style = list(color="white", fontSize = xaxis_fontsize, 
                                            rotation = 45, autoRotation = FALSE)),
                   min=0, max=30) %>% 
          hc_yAxis(labels=list(style = list(color="white", fontSize = yaxis_fontsize))) %>%
          hc_add_series(df_hc, name=series_full_name, showInLegend = TRUE, 
                        dataLabels = list(enabled=TRUE, verticalAlign= 'top')) %>%
          hc_add_theme(hc_theme_monokai())  
      }
    }
    hcs <- c("line", "spline",  "area", "areaspline",
             "column", "bar", "waterfall" , "funnel", "pyramid",
             "pie" , "treemap", "scatter", "bubble",
             "arearange", "areasplinerange", "columnrange", "errorbar",
             "polygon", "polarline", "polarcolumn", "polarcolumnrange",
             "coloredarea", "coloredline") %>% map(create_hc, hc_series_name)
    
    df_list <- c(df_list, list(df_plot))
    hcs_list <- c(hcs_list, list(hcs))
  }
  return(list(df_list,hcs_list))
}

#this feature allows vectorized categoris columns
highcharter_all_plot_multiple_category <- function(df, categories_col, value_col, metrics_plot=c(1,2,3,4), 
                                                   title="Highcharter Automatic Plot", nonnegative=TRUE, 
                                                   include_na=TRUE, sort_values=TRUE, scroll_activated = TRUE,
                                                   top_n=-1, bottom_n=-1){
  library(highcharter)
  library(dplyr)
  library(stringr)
  library(purrr)
  
  #Preprocess NA in Categories Column
  #https://stackoverflow.com/questions/11254524/omit-rows-containing-specific-column-of-na
  completeFun <- function(data, desiredCols) {
    completeVec <- complete.cases(data[, desiredCols])
    return(data[completeVec, ])
  }
  
  if(include_na == FALSE){
    for(omit_col in 1:length(categories_col)){
      df <- completeFun(df, categories_col)
    }
  }
  
  #1) Group by Dataframe first and summarise
  df_backup <- df
  
  groupcolnames <- df %>% select(categories_col) %>% colnames()
  print(groupcolnames)
  groupcolnameseval <- paste0(groupcolnames, collapse=",")
  groupcolnameseval <- paste0("paste(",groupcolnameseval,",","sep='_')")
  
  print(groupcolnameseval)
  
  group_by_df <- df %>%
    group_by(.dots = df %>% select(categories_col) %>% colnames())
  
  df <- group_by_df %>% 
    summarise(total = sum(eval(parse(text=value_col)), na.rm=TRUE), 
              average = round(mean(eval(parse(text=value_col)), na.rm=TRUE),0),
              minimum = min(eval(parse(text=value_col)), na.rm=TRUE),
              maximum = max(eval(parse(text=value_col)), na.rm=TRUE),
              variance = var(eval(parse(text=value_col)), na.rm=TRUE),
              stddev = sd(eval(parse(text=value_col)), na.rm=TRUE),
              count = n()
    )
  
  df <- data.frame(df)
  
  writeLines("Removing Categories because of NA Variance and zero sum: ")
  df <- df[which(df$total > 0 & !is.na(df$variance)),]
  
  attach(df)
  df_cat_grouped <- df %>% select_if(is.factor) %>%
    mutate(category_grouped = eval(parse(text=groupcolnameseval)))
  df["category_grouped"] <- df_cat_grouped["category_grouped"]
  
  n <- length(unique(df[,"category_grouped"]))
  set.seed(123)
  df_plot <- data.frame()
  
  #Bold Color
  colors = c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03","#18fc03","#03fc5e","#03fcc6",
             "#03f0fc","#03adfc","#035afc","#be03fc","#fc03f8","#fc03a1")
  
  #Soft Color
  colors2 = c("#ffc6c2","#ffdaa6","#fff9a6","#ecffa6","#d1ffa6","#a9ffa6","#a6ffc4","#a6ffdb","#a6ffff",
              "#a6deff","#a6caff","#a6a7ff","#b9a6ff","#d7a6ff","#f0a6ff","#fca6ff","#ffa6ea","#ffa6c1",
              "#ffa6b3","#ffa6b5")
  
  print(head(df))
  
  #2) Initial Data Frame to plot
  df_list = list()
  hcs_list = list()
  metrics_name <- c("total","average","minimum","maximum")
  #metrics_plot <- na.omit(metrics_plot)
  for(loop in 1:length(metrics_plot)){
    writeLines(paste("Computing and Assigning plot by", metrics_name[metrics_plot[loop]]))
    categories <- df[,"category_grouped"]
    value <- df[,metrics_name[metrics_plot[loop]]]
    color = rep(colors, length.out = n)
    segmentColor = rep(colors2, length.out = n)
    hcs <- NULL
    low <- c()
    high <- c()
    confidence_interval <- NULL
    if(nonnegative==TRUE){
      confidence_interval <- function(value, sample_length, sd, interval) {
        error <- qnorm((interval+1)/2)*sd/sqrt(sample_length)
        left <- value-error
        right <- value+error
        if(left < 0){
          right <- right + abs(left)
          left <- 0
        }
        return(c(left, right))
      } 
    }
    else if(nonnegative==FALSE){
      confidence_interval <- function(value, sample_length, sd, interval) {
        error <- qnorm((interval+1)/2)*sd/sqrt(sample_length)
        left <- value-error
        right <- value+error
        return(c(left, right))
      }
    }
    for(f in 1:nrow(df)){
      confint_vector <- confidence_interval(value=value[f], sample_length = nrow(df_backup), 
                                            df[,"stddev"][f], 0.95)
      low <- c(low, round(confint_vector[1],0))
      high <- c(high, round(confint_vector[2],0))
    }
    
    x <- as.numeric(0:(nrow(df)-1))
    y <- value
    z <- round((df$count/sum(df$count)) * 100, 3)
    sumvalue <- rep(sum(value), length(x))
    count <- df$count
    sumcount <- rep(sum(count), length(x))
    value_percentage <- round((y/sumvalue * 100),2)
    proportion_percentage <- round((count/sumcount * 100),2)
    name <- categories
    df_plot <- data.frame(x, y, z, name, categories, value, sumvalue, value_percentage,
                          color, segmentColor, low, high, count, sumcount, proportion_percentage)
    df_plot$categories <- as.character(df_plot$categories) 
    df_plot$color <- as.character(df_plot$color)
    df_plot$segmentColor <- as.character(df_plot$segmentColor)
    
    if(sort_values==TRUE){
      df_plot <- df_plot[order(df_plot$y, decreasing = TRUE),]
      df_plot$x <- as.numeric(0:(nrow(df_plot)-1))
    }
    if(top_n != -1){
      df_plot <- head(df_plot, top_n)
    }
    if(bottom_n != -1){
      df_plot <- tail(df_plot, botttom_n)
    }
    
    hc_series_name <- ""
    if(metrics_plot[loop]==1){
      hc_series_name <- paste("Total Metrics of", title)
      annotation_grand_info <- paste(", With Grand Total Value =", df_plot$sumvalue[1])
    }
    else if(metrics_plot[loop]==2){
      hc_series_name <- paste("Average Metrics of", title)
      annotation_grand_info <- paste(", With Grand Average Value =", round(mean(df_plot$y),0))
    }
    else if(metrics_plot[loop]==3){
      hc_series_name <- paste("Minimum Metrics of", title)
      annotation_grand_info <- paste(", With Highest Minimum =", max(df_plot$y))
    }
    else if(metrics_plot[loop]==4){
      hc_series_name <- paste("Maximum Metrics of", title)
      annotation_grand_info <- paste(", With Highest Maximum =", max(df_plot$y))
    }
    
    create_hc <- function(t, name_series) {
      series_full_name <- paste(name_series, "with", t)
      dont_rm_high_and_low <- c("arearange", "areasplinerange",
                                "columnrange", "errorbar")
      is_polar <- str_detect(t, "polar")
      t <- str_replace(t, "polar", "")
      if(!t %in% dont_rm_high_and_low){
        df_hc <- df_plot %>% dplyr::select(x, y, z, name, categories, value, 
                                           color, segmentColor)
      } 
      else{
        df_hc <- df_plot %>% dplyr::select(x, y, z, name, categories, value, 
                                           color, segmentColor, low, high)
      }
      
      if(nrow(df_plot) <= 30){
        xaxis_fontsize <- 12
        yaxis_fontsize <- 20
        highchart() %>%
          hc_title(text = paste(ifelse(is_polar, "polar ", ""), t),
                   style = list(fontSize = "15px", color="white")) %>%
          hc_subtitle(text = paste0(series_full_name, annotation_grand_info),
                      style = list(fontSize = "10px", color="white")) %>%
          hc_chart(type = t, polar = is_polar) %>% 
          hc_xAxis(categories = df_hc$categories, 
                   labels=list(style = list(color="white", fontSize = xaxis_fontsize, 
                                            rotation = 45, autoRotation = FALSE))) %>% 
          hc_yAxis(labels=list(style = list(color="white", fontSize = yaxis_fontsize))) %>%
          hc_add_series(df_hc, name=series_full_name, showInLegend = TRUE, 
                        dataLabels = list(enabled=TRUE, verticalAlign= 'top')) %>%
          hc_add_theme(hc_theme_monokai()) 
      } 
      else if(nrow(df_plot) > 30 && scroll_activated==TRUE){
        xaxis_fontsize <- 10
        yaxis_fontsize <- 20
        highchart() %>%
          hc_title(text = paste(ifelse(is_polar, "polar ", ""), t),
                   style = list(fontSize = "15px", color="white")) %>%
          hc_subtitle(text = paste0(series_full_name, annotation_grand_info),
                      style = list(fontSize = "10px", color="white")) %>%
          hc_chart(type = t, polar = is_polar) %>% 
          hc_xAxis(categories = df_hc$categories, 
                   labels=list(style = list(color="white", fontSize = xaxis_fontsize, 
                                            rotation = 45, autoRotation = FALSE)),
                   min=0, max=35) %>% 
          hc_yAxis(labels=list(style = list(color="white", fontSize = yaxis_fontsize))) %>%
          hc_add_series(df_hc, name=series_full_name, showInLegend = TRUE, 
                        dataLabels = list(enabled=TRUE, verticalAlign= 'top')) %>%
          hc_scrollbar(enabled=TRUE) %>%
          hc_add_theme(hc_theme_monokai())  
      }
      else if(nrow(df_plot) > 30 && scroll_activated==FALSE){
        xaxis_fontsize <- 10
        yaxis_fontsize <- 20
        if(t == "bar"){
          if(nrow(df_plot) > 25 && nrow(df_plot) <= 30){
            xaxis_fontsize <- 4 
          }
          else if(nrow(df_plot) > 20 && nrow(df_plot) <= 25){
            xaxis_fontsize <- 6 
          }
          else if(nrow(df_plot) > 15 && nrow(df_plot) <= 20){
            xaxis_fontsize <- 8 
          }
          else if(nrow(df_plot) > 10 && nrow(df_plot) <= 15){
            xaxis_fontsize <- 10 
          }
          else if(nrow(df_plot) <= 10){
            xaxis_fontsize <- 12 
          }
        }
        
        highchart() %>%
          hc_title(text = paste(ifelse(is_polar, "polar ", ""), t),
                   style = list(fontSize = "15px", color="white")) %>%
          hc_subtitle(text = paste0(series_full_name, annotation_grand_info),
                      style = list(fontSize = "10px", color="white")) %>%
          hc_chart(type = t, polar = is_polar) %>% 
          hc_xAxis(categories = df_hc$categories, 
                   labels=list(style = list(color="white", fontSize = xaxis_fontsize, 
                                            rotation = 45, autoRotation = FALSE)),
                   min=0, max=30) %>% 
          hc_yAxis(labels=list(style = list(color="white", fontSize = yaxis_fontsize))) %>%
          hc_add_series(df_hc, name=series_full_name, showInLegend = TRUE, 
                        dataLabels = list(enabled=TRUE, verticalAlign= 'top')) %>%
          hc_add_theme(hc_theme_monokai())  
      }
    }
    hcs <- c("line", "spline",  "area", "areaspline",
             "column", "bar", "waterfall" , "funnel", "pyramid",
             "pie" , "treemap", "scatter", "bubble",
             "arearange", "areasplinerange", "columnrange", "errorbar",
             "polygon", "polarline", "polarcolumn", "polarcolumnrange",
             "coloredarea", "coloredline") %>% map(create_hc, hc_series_name)
    
    df_list <- c(df_list, list(df_plot))
    hcs_list <- c(hcs_list, list(hcs))
  }
  return(list(df_list,hcs_list))
}

highcharter_sankey_dependencywheel <- function(df, from_col, to_col, weight_col, 
                                               title="Distribution Wheel", theme="sandsignika",
                                               type="sankey"){
  library(data.table)
  library(sankeywheel)
  sankeywheel(from = df[,which(colnames(df)==from_col)],
              to = df[,which(colnames(df)==to_col)],
              weight = df[,which(colnames(df)==weight_col)],
              type = type, 
              width = "100%",
              theme = theme,
              title = title)
}

highcharter_timeline_variwide <- function(df, date_col, value_col, group_by_col,
                                          include_na=FALSE, datalabel=FALSE, 
                                          color_selection=c()){
  library(highcharter)
  library(dplyr)
  library(stringr)
  library(purrr)
  
  #Preprocess NA in Categories Column
  #https://stackoverflow.com/questions/11254524/omit-rows-containing-specific-column-of-na
  completeFun <- function(data, desiredCols) {
    completeVec <- complete.cases(data[, desiredCols])
    return(data[completeVec, ])
  }
  
  datalabel <<- datalabel
  #Bold Color
  colors <- c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03","#18fc03","#03fc5e","#03fcc6",
              "#03f0fc","#03adfc","#035afc","#be03fc","#fc03f8","#fc03a1")
  
  #Soft Color
  colors2 <- c("#ffc6c2","#ffdaa6","#fff9a6","#ecffa6","#d1ffa6","#a9ffa6","#a6ffc4","#a6ffdb","#a6ffff",
               "#a6deff","#a6caff","#a6a7ff","#b9a6ff","#d7a6ff","#f0a6ff","#fca6ff","#ffa6ea","#ffa6c1",
               "#ffa6b3","#ffa6b5")
  
  if(length(color_selection)==0){
    unique_group <- length(unlist(unique(df[group_by_col])))
    if(unique_group > length(colors)){
      colors <- c(colors, colors2)
      colors <- colors[1:unique_group]
    }
    else{
      colors <- colors[1:unique_group]
    }
  }
  else if(length(color_selection)>0){
    if(any(color_selection > length(colors))){
      colors <- c(colors, colors2)
      colors <- colors[color_selection]
    }
    else{
      colors <- colors[color_selection]
    }
  }
  
  if(include_na == FALSE){
    df <- completeFun(df, group_by_col)
    df <- completeFun(df, date_col)
    df <- completeFun(df, value_col)
  }
  
  #1) Group by Dataframe first and summarise
  df_backup <- df
  
  groupcolnames <- df %>% select(group_by_col) %>% colnames()
  groupcolnameseval <- paste0(groupcolnames, collapse=",")
  groupcolnameseval <- paste0("paste(",groupcolnameseval,",","sep='_')")
  
  group_by_df <- df %>%
    group_by(.dots = df %>% select(date_col, group_by_col) %>% colnames())
  
  df <- group_by_df %>% 
    summarise(total = sum(eval(parse(text=value_col)), na.rm=TRUE), 
              minimum = min(eval(parse(text=value_col)), na.rm=TRUE),
              maximum = max(eval(parse(text=value_col)), na.rm=TRUE),
              count = n()
    )
  
  df <- data.frame(df)
  print(head(df))
  
  #2) Plot Variwide
  date_col <<- date_col
  group_by_col <<- group_by_col
  variwide <- highchart() %>% 
    hc_add_series(df, "area", hcaes(x = datetime_to_timestamp(eval(parse(text=date_col))), 
                                    y = total, 
                                    group = eval(parse(text=group_by_col))),
                  dataLabels = list(enabled=datalabel, verticalAlign= 'top'),
                  color = colors) %>% 
    hc_xAxis(type = "datetime",
             title = "",
             showFirstLabel = TRUE,
             showLastLabel = TRUE,
             enableMouseTracking = TRUE,
             labels=list(style = list(color="white", fontSize = 16))) %>% 
    hc_yAxis(labels=list(style = list(color="white", fontSize = 25))) %>%
    hc_plotOptions(area = list(marker = list(
      enabled = FALSE,
      lineWidth = 0,
      states = list(
        hover = list(enabled = TRUE))))) %>% 
    hc_tooltip(followPointer = TRUE) %>%
    hc_add_theme(hc_theme_monokai()) 
  
  return(list(df,variwide))
}

highcharter_heatmap <- function(df_cor){
  library(highcharter)
  library(dplyr)
  library(stringr)
  library(purrr)
  hchart(round(df_cor,2),"heatmap")
}

highcharter_sidebyside_bar <- function(df, title="", subtitle="", category="", time_colname=c(),
                                       time_title=c(), data_point=1:5){
  
  n_top <- length(time_colname)
  
  df_top <- df[data_point, ]
  df_top <- na.fill(df_top, 0)
  df_top <- data.frame(df_top)
  colnames(df_top) <- colnames(df)
  
  #Bold Color
  colors = c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03","#18fc03","#03fc5e","#03fcc6",
             "#03f0fc","#03adfc","#035afc","#be03fc","#fc03f8","#fc03a1")
  
  #Soft Color
  colors2 = c("#ffc6c2","#ffdaa6","#fff9a6","#ecffa6","#d1ffa6","#a9ffa6","#a6ffc4","#a6ffdb","#a6ffff",
              "#a6deff","#a6caff","#a6a7ff","#b9a6ff","#d7a6ff","#f0a6ff","#fca6ff","#ffa6ea","#ffa6c1",
              "#ffa6b3","#ffa6b5")
  
  #print(df_top)
  #print(class(df_top))
  
  format <- highchart() %>%
    hc_chart(type = "column") %>%
    hc_title(text = title) %>%
    hc_subtitle(text = subtitle) %>%
    hc_xAxis(categories = time_title,
             tickmarkPlacement = "on",
             title = list(enabled = FALSE),
             labels=list(style = list(color="white", fontSize = 25))) %>%
    hc_yAxis(labels=list(style = list(color="white", fontSize = 20))) %>%
    hc_tooltip(pointFormat = "<span style=\"color:{series.color}\">{series.name}</span>:
            ({point.y:,.0f})<br/>", shared = TRUE) %>%
    hc_plotOptions(area = list(
      stacking = "normal",
      lineColor = "#ffffff",
      lineWidth = 1,
      marker = list(
        lineWidth = 1,
        lineColor = "#ffffff"
      )) 
    )%>%
    hc_add_theme(hc_theme_monokai())
  
  color_index <- 1
  for(r in 1:nrow(df_top)){
    if(r%%length(colors) == 0){
      color_index <- color_index + 1
    }
    colorpicker <- colors[color_index%%length(colors)]
    format <- format %>% hc_add_series(name=as.character(unlist(df_top[category]))[r], 
                                       data=as.numeric(as.character(unlist(df_top[r,time_colname]))),
                                       color=colorpicker)
    color_index <- color_index + 1
  }
  
  if(length(data_point) > 30){
    format <- format %>%
      hc_scrollbar(enabled=TRUE) 
  }
  
  print(format)
  return(format)
}

#------------------------------------------------------------------------------------------------------------------------
################################# 3) Statistical Testing ################################################################
#------------------------------------------------------------------------------------------------------------------------

#----------------------------- ANOVA Statistical Measures -----------------------------
#Note: Use Factor of length 3 or more to use as ANOVA Factor Modeling
anova_statistical_modeling <- function(regr_data, regr_model, 
                                       type_log_explain=FALSE, 
                                       type="II"){
  #https://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/
  #https://stats.stackexchange.com/questions/345684/which-one-to-choose-type-i-type-ii-or-type-iii-anova
  library(car)
  model_permitted <- c("lm", "aov", "glm", "multinom", "polr",
                       "mlm", "coxph", "coxme", "lme", "mer", 
                       "merMod", "svyglm", "rlm")
  model_for_type_I <- c("lm","glm")
  
  if(class(regr_model) %in% model_permitted){
    if(type=="I" && class(regr_model) %in% model_for_type_I){
      writeLines("Calculating ANOVA. .")
    }else{
      writeLines("Given Regression Model is limited with ANOVA Type I calculation")
      break
    }
  }else{
    writeLines("Given Regression Model is not supported with ANOVA calculation")
    break
  }
  
  if(type_log_explain){
    writeLines("ANOVA calculates Sum of Squares in main factors and also interaction factor on each factor")
    writeLines("Explanation About ANOVA Types: ")
    writeLines("Defined 2 Factors A and B")
    writeLines("--------------------------------- Type I Explanation ----------------------------------------")
    writeLines("Type I, also called 'sequential' sum of squares:")
    writeLines("This tests the main effect of factor A, 
             followed by the main effect of factor B after the main effect of A, 
             followed by the interaction effect AB after the main effects.")
    writeLines("Because of the sequential nature and the fact that the two main factors are tested in a particular order, 
             this type of sums of squares will give different results for unbalanced data depending on which main effect 
             is considered first.")
    writeLines("For unbalanced data, this approach tests for a difference in the weighted marginal means. 
             In practical terms, this means that the results are dependent on the realized sample sizes, 
             namely the proportions in the particular data set. In other words, it is testing the first factor 
             without controlling for the other factor")
    writeLines("--------------------------------- Type II Explanation ----------------------------------------")
    writeLines("Type II, This type tests for each main effect after the other main effect.")
    writeLines("Note that no significant interaction is assumed 
             (in other words, you should test for interaction first (SS(AB | A, B)) 
             and only if AB is not significant, continue with the analysis for main effects).")
    writeLines("--------------------------------- Type III Explanation ----------------------------------------")
    writeLines("Type III, This type tests for the presence of a main effect after the other main effect and interaction. 
             This approach is therefore valid in the presence of significant interactions.")
    writeLines("However, it is often not interesting to interpret a main effect if interactions are present 
             (generally speaking, if a significant interaction is present, the main effects should not be further analysed).")
    writeLines("NOTE: Again, due to the way in which the SS are calculated when incorporating the interaction effect, 
             for type III you must specify the contrasts option to obtain sensible results")
    writeLines("---------------------------------- Main Outline -----------------------------------")
    writeLines("ANOVA implementation with varying type give same results for balanced data")
    writeLines("For Unbalanced Data, the main difference is when computing the main effects:")
    writeLines("In a Type III model, each cell of the factorial gets the same weight (even if some have smaller sample sizes)")
    writeLines("In a Type II model, each observation gets the same weight, In most situations (but not all!)")
    writeLines("If Interaction effects already given, then all types of anova will give a same results (no need to make decisions)")
  }
  writeLines("H0: There are no difference among group means")
  writeLines("H1: There are difference among group means")
  if(class(regr_model)=="lm"){ #this section use aov
    one.way <- aov(regr_model)
    print(summary(one.way))
    writeLines("Implement Post-Hoc Testing with TukeyHSD (Tukey's Honestly-Significant Difference)")
    writeLines("H0: There is no significant difference of factor A to factor B")
    writeLines("H1: There is significant difference of factor A to factor B")
    print(TukeyHSD(one.way))
    plot(one.way, las = 1)
  }else{ #this section use anova and Anova (from car)
    if(type=="I"){
      anova_result <- anova(regr_model, data=regr_data)
    }else if(type=="II"){
      anova_result <- Anova(regr_model, data=regr_data, type=2)
    }else if(type=="III"){
      anova_result <- Anova(regr_model, data=regr_data, 
                            contrasts=list(topic=contr.sum, sys=contr.sum), type=3)
    }
    print(summary(anova_result))
    return(anova_result) 
  }
}

#multiple dependent variable, or with interaction variables included
#2 type manova implementation is for random_design data and repeated_measure data  
#example random_design data see ?Soils, example repeated_measure data see ?OBirenKaiser
#anova_interaction_formula <- as.formula("~x*y + z")
manova_statitical_modeling <- function(regr_data, regr_model, 
                                       type="random_design", 
                                       metadata_dependent_repeated_df,
                                       anova_interaction_formula){
  #https://www.youtube.com/watch?v=2lrcdToCOSs&ab_channel=StatPharm
  if(type=="random_design"){
    manova_random <- Manova(regr_model)
    print(summary(manova_random))
    print(summary(Anova(regr_model), univariate=TRUE, multivariate=FALSE,
                  p.adjust.method=TRUE))
  }else if(type=="repeated_measure"){
    manova_repeat <- Anova(regr_data, idata=metadata_dependent_repeated_df, 
                           idesign=anova_interaction_formula)
    print(summary(manova_repeat, multivariate=FALSE))
  }
}

#----------------------------- Proportion Testing ----------------------------------------

categorical_table_visualization <- function(my_table){
  library(vcd)
  x11()
  mosaic(my_table, shade=TRUE, legend=TRUE)
  x11()
  assoc(my_table, shade=TRUE)
}

#Using prop.test
binomial_proportion_test <- function(x, y, null_object="Null", prop=0.1, alternative_test="greater"){
  if(alternative_test == "greater"){
    writeLines(paste("H0: Proportion of",null_object,"is equal to",prop))
    writeLines(paste("H1: Proportion of",null_object,"is greater to",prop))
    writeLines("with Confidence Level of 95%")
    print(prop.test(x,y, p=prop, alternative = alternative_test))
    g <- prop.test(x,y, p=prop, alternative = alternative_test)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Proportion of",null_object,"is greater to",prop))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Proportion of",null_object,"is equal to",prop))
    }
  }
  else if(alternative_test == "less"){
    writeLines(paste("H0: Proportion of",null_object,"is equal to",prop))
    writeLines(paste("H1: Proportion of",null_object,"is less to",prop))
    print(prop.test(x,y, p=prop, alternative = alternative_test))
    g <- prop.test(x,y, p=prop, alternative = alternative_test)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Proportion of",null_object,"is less to",prop))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Proportion of",null_object,"is equal to",prop))
    }
  }
  else if(alternative_test == "two.sided"){
    writeLines(paste("H0: Proportion of",null_object,"is equal to",prop))
    writeLines(paste("H1: Proportion of",null_object,"is either greater or lower to",prop))
    print(prop.test(x,y, p=prop, alternative = alternative_test))
    g <- prop.test(x,y, p=prop, alternative = alternative_test)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Proportion of",null_object,"is either greater or lower to",prop))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Proportion of",null_object,"is equal to",prop))
    }
  }
}

#Using chisq.test
multinomial_proportion_test <- function(x){
  writeLines("H0: No relationship exists on the categorical variables in the population; they are independent")
  writeLines("H1: relationship exists on the categorical variables in the population; they are dependent")
  writeLines("Conducting Chi Square Test")
  chi = chisq.test(table(x))
  print(chi)
  if(chi$p.value < 0.05){
    writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
    writeLines("H1: relationship exists on the categorical variables in the population; they are dependent")
  }
  else if(chi$p.value >=0.05){
    writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
    writeLines("H0: No relationship exists on the categorical variables in the population; they are independent")
  }
}

confidence_proportional_test <- function(x_vec, y_vec, conf=0.95){
  freqTable_normal <- table(y_vec, x_vec)
  writeLines("Make a Frequency Table")
  print(freqTable_normal)
  writeLines("Using Proportional Test")
  print(prop.test(freqTable_normal, conf.level = conf))
}

#----------------------------- Mean Testing from Vector ----------------------------------------

#using t.test one sample
univariate_mean_test <- function(x, mean=0, alternative_test="greater"){
  if(alternative_test == "greater"){
    writeLines(paste("H0: Mean of the data is equal to",mean))
    writeLines(paste("H1: Mean of the data is greater to",mean))
    writeLines("with Confidence Level of 95%")
    print(t.test(x, mean=mean, alternative = alternative_test))
    g <- t.test(x, mean=mean, alternative = alternative_test)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Mean of the data is greater to",mean))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: Mean of the data is equal to",mean))
    }
  }
  else if(alternative_test == "less"){
    writeLines(paste("H0: Mean of the data is equal to",mean))
    writeLines(paste("H1: Mean of the data is less to",mean))
    writeLines("with Confidence Level of 95%")
    print(t.test(x, mean=mean, alternative = alternative_test))
    g <- t.test(x, mean=mean, alternative = alternative_test)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Mean of the data is less to",mean))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: Mean of the data is equal to",mean))
    }
  }
  else if(alternative_test == "two.sided"){
    writeLines(paste("H0: Mean of the data is equal to",mean))
    writeLines(paste("H1: Mean of the data is either greater or lower to",mean))
    writeLines("with Confidence Level of 95%")
    print(t.test(x, mean=mean, alternative = alternative_test))
    g <- t.test(x, mean=mean, alternative = alternative_test)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: Mean of the data is either greater or lower to",mean))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: Mean of the data is equal to",mean))
    }
  }
}

#using paired t.test (x-> numeric, y->numeric)
paired_mean_test <- function(x, y, alternative_test="greater"){
  if(alternative_test == "greater"){
    writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    writeLines(paste("H1: the mean of data 2 is greater than data 1"))
    writeLines("with Confidence Level of 95%")
    print(t.test(x, y, alternative = alternative_test, paired = TRUE))
    g <- t.test(x, y, alternative = alternative_test, paired = TRUE)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: the mean of data 2 is greater than data 1"))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    }
  }
  else if(alternative_test == "less"){
    writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    writeLines(paste("H1: the mean of data 2 is less than data 1"))
    writeLines("with Confidence Level of 95%")
    print(t.test(x, y, alternative = alternative_test, paired = TRUE))
    g <- t.test(x, y, alternative = alternative_test, paired = TRUE)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: the mean of data 2 is less than data 1"))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    }
  }
  else if(alternative_test == "two.sided"){
    writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    writeLines(paste("H1: the mean of data 2 is either greater or lower than data 1"))
    writeLines("with Confidence Level of 95%")
    writeLines("with Confidence Level of 95%")
    print(t.test(x, y, alternative = alternative_test, paired = TRUE))
    g <- t.test(x, y, alternative = alternative_test, paired = TRUE)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: the mean of data 2 is either greater or lower than data 1"))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    }
  }
}

# Two Sample t-test Independent (Using Improved Welch t-test) (x-> numeric, y->numeric)
#https://stackoverflow.com/questions/16719669/the-argument-var-equal-true-or-false-in-t-test-function/16719900
independent_mean_test <- function(x, y, alternative_test="greater"){
  if(alternative_test == "greater"){
    writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    writeLines(paste("H1: the mean of data 2 is greater than data 1"))
    writeLines("with Confidence Level of 95%")
    print(t.test(x, y, alternative = alternative_test, var.equal = FALSE))
    g <- t.test(x, y, alternative = alternative_test, var.equal = FALSE)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: the mean of data 2 is greater than data 1"))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    }
  }
  else if(alternative_test == "less"){
    writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    writeLines(paste("H1: the mean of data 2 is less than data 1"))
    writeLines("with Confidence Level of 95%")
    print(t.test(x, y, alternative = alternative_test, var.equal = FALSE))
    g <- t.test(x, y, alternative = alternative_test, var.equal = FALSE)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: the mean of data 2 is less than data 1"))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    }
  }
  else if(alternative_test == "two.sided"){
    writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    writeLines(paste("H1: the mean of data 2 is either greater or lower than data 1"))
    writeLines("with Confidence Level of 95%")
    writeLines("with Confidence Level of 95%")
    print(t.test(x, y, alternative = alternative_test, var.equal = FALSE))
    g <- t.test(x, y, alternative = alternative_test, var.equal = FALSE)
    if(g$p.value < 0.05){
      writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
      writeLines(paste("H1: the mean of data 2 is either greater or lower than data 1"))
    }
    else if(g$p.value >=0.05){
      writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
      writeLines(paste("H0: There is no mean difference between data 1 and data 2"))
    }
  }
}

mean_with_categoric_test <- function(data, x_factor_col, y_numeric_col, 
                                     boxplot_title="Custom Boxplot"){
  library(gplots)
  library(stats)
  library(broom)
  library(rlang)
  
  x_sym <- sym(x_factor_col)
  y_sym <- sym(y_numeric_col)
  my_formula <- as.formula(paste0(y_numeric_col, "~", x_factor_col))
  
  x11()
  boxplot(formula, data=data, 
          main=boxplot_title, 
          col= rainbow(length(unique(data[[x_factor_col]]))))
  
  writeLines("====================== 1) Begin Anova Test =============================")
  writeLines("ANOVA Assumptions in mind: ")
  writeLines("ANOVA can be used for data with a fair Normality Distribution (check using Shapiro Wilk)")
  writeLines("ANOVA can be used for data that has homogeneity of variance (check using Levene & Bartlett Test")
  
  writeLines("======================== ANOVA in Generally =========================")
  lm_model <- lm(my_formula, data = data)
  print(summary(lm_model))
  anova_model <- anova(lm_model)
  print(anova_model)
  
  writeLines("====================== ANOVA in Specifically =======================")
  aov_model <- aov(my_formula, data = data)
  x11()
  par(mfrow=c(2,2))
  plot(purpose_aov)
  tukey_output <- TukeyHSD(aov_model)
  specific_relationship <- tidy(tukey_output)
  bartlett.test(my_formula, data=data)
  
  x11()
  plot(TukeyHSD(model_anova, conf.level = 0.99),las=1, col = "red")
  
  uhat<-resid(model_anova)
  normality <- shapiro.test(uhat)
  writeLines("============================================================================")
  writeLines("========================= Normality Tests ==================================")
  writeLines(paste("H0: The Sample Observations are taken from a Normal population"))
  writeLines(paste("H1: The Sample Observations are taken from a Non Normal population"))
  writeLines("============================================================================")
  print(normality)
  
  if(normality$p.value < 0.05){
    writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
    writeLines(paste("H1: The Sample Observations are taken from a Non Normal population"))
  }
  else if(normality$p.value >=0.05){
    writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
    writeLines(paste("H0: The Sample Observations are taken from a Normal population"))
  } 
  
  writeLines("=========================================================================")
  writeLines("=========== Bartlett and Levene Variances Homogeniety Test ==============")
  writeLines(paste("H0: Homogeneity of variance Found across group"))
  writeLines(paste("H1: Heterogenity of variance Found across group"))
  writeLines("=========================================================================")
  writeLines("")
  
  library(car)
  writeLines("Using Formula")
  print(my_formula)
  btest <- bartlett.test(my_formula, data=data)
  ltest <- leveneTest(my_formula, data=data)
  
  writeLines("================== Bartlett Tests =======================")
  print(btest)
  writeLines("=================== Levene Tests =======================")
  print(ltest)
  
  if(ltest[,3][1] < 0.05){
    writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
    writeLines(paste("H1: Heterogenity of variance across the cross-sectional group"))
  }
  else if(ltest[,3][1] >= 0.05){
    writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
    writeLines(paste("H0: Homogenity of Variance across the cross-sectional group"))
  } 
  
  if(btest$p.value < 0.05 && ltest[,3][1] < 0.05){
    writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
    writeLines(paste("H1: Heterogenity of variance across the cross-sectional group"))
  }
  else if(btest$p.value < 0.05 || ltest[,3][1] < 0.05){
    writeLines("Caution: One of the homogenity test have a small p value!!")
    writeLines("P Value is small, Reject Null Hypothesis, Conclusion is:")
    writeLines(paste("H1: Heterogenity of variance across the cross-sectional group"))
  }
  else if(btest$p.value >= 0.05 && ltest[,3][1] >= 0.05){
    writeLines("P Value is adequate/big enough, Accept Null Hypothesis, Conclusion is:")
    writeLines(paste("H0: Homogenity of Variance across the cross-sectional group"))
  } 
  return(model_anova)
}

nonparametric_group_mean_test <- function(data, value_col=c(), group_col=c()){
  library(coin)
  library(dplyr)
  
  df_statistical_pval <- NULL
  if(length(value_col) != length(group_col)){
    message("Value and Group Length is not match! Please make a correct input!")
    break
  }
  else{
    writeLines("==================================")
    writeLines("H0: All Group Mean is Equal ")
    writeLines("H1: Not All Group Mean is Equal")
    writeLines("==================================")
    for(test in 1:length(value_col)){
      value_vec <- data[[value_col[test]]]
      group_vec <- data[[group_col[test]]]
      writeLines(paste0(test,") Testing ",value_col[test], " ~ ", group_col[test]))
      
      #Two- and K-Sample Location Tests
      oneway_t <- oneway_test(value_vec~group_vec)
      if(length(unique(group_vec)) == 2){
        wilcox_t <- wilcox_test(value_vec~group_vec)
      }else{
        writeLines("Cannot apply Wilcoxon Test!")
        wilcox_t <- NA
      }
      kruskal_t <- kruskal_test(value_vec~group_vec) 
      normal_t <- normal_test(value_vec~group_vec)
      median_t <- median_test(value_vec~group_vec)
      savage_t <- savage_test(value_vec~group_vec)
      
      print(oneway_t)
      print(wilcox_t)
      print(kruskal_t)
      print(normal_t)
      print(median_t)
      print(savage_t)
      
      #Two- and K-Sample Scale Tests
      ansari_t <- ansari_test(value_vec~group_vec)
      taha_t <- taha_test(value_vec~group_vec)
      mood_t <- mood_test(value_vec~group_vec)
      klotz_t <- klotz_test(value_vec~group_vec)
      fligner_t <- fligner_test(value_vec~group_vec)
      conover_t <- conover_test(value_vec~group_vec)
      
      print(ansari_t)
      print(taha_t)
      print(mood_t)
      print(klotz_t)
      print(fligner_t)
      print(conover_t)
      
      if(length(df_statistical_pval)==0){
        df_statistical_pval <- data.frame(numeric_attribute=value_col[test],
                                          group_attribute=group_col[test],
                                          type_test="Sample Location Tests",
                                          test_name="Oneway Test",
                                          test_pvalue=pvalue(oneway_t))
        all_type_test <- c(rep("Sample Location Tests", 5), rep("Sample Scale Tests", 6))
        all_test_name <- c("Wilcoxon Mann Whitney Test", "Kruskal Wallis Test","Normal Test",
                           "Median Test", "Savage Test", "Ansari Bradley Test", "Taha Test",
                           "Mood Test","Klotz Test","Fligner Test","Conover Test")
        if(!is.na(wilcox_t)){
          all_test_pvalue <- c(pvalue(wilcox_t), pvalue(kruskal_t), pvalue(normal_t),
                               pvalue(median_t), pvalue(savage_t), pvalue(ansari_t),
                               pvalue(taha_t), pvalue(mood_t), pvalue(klotz_t), 
                               pvalue(fligner_t), pvalue(conover_t))
          remain_df <- data.frame(numeric_attribute=value_col[test],
                                  group_attribute=group_col[test],
                                  type_test=all_type_test,
                                  test_name=all_test_name,
                                  test_pvalue=all_test_pvalue)
          df_statistical_pval <- rbind(df_statistical_pval, remain_df)
        }else{
          all_test_pvalue <- c(NA, pvalue(kruskal_t), pvalue(normal_t),
                               pvalue(median_t), pvalue(savage_t), pvalue(ansari_t),
                               pvalue(taha_t), pvalue(mood_t), pvalue(klotz_t), 
                               pvalue(fligner_t), pvalue(conover_t))
          remain_df <- data.frame(numeric_attribute=value_col[test],
                                  group_attribute=group_col[test],
                                  type_test=all_type_test,
                                  test_name=all_test_name,
                                  test_pvalue=all_test_pvalue)
          df_statistical_pval <- rbind(df_statistical_pval, remain_df)
        }
      }else{
        all_type_test <- c(rep("Sample Location Tests", 6), rep("Sample Scale Tests", 6))
        all_test_name <- c("Oneway Test","Wilcoxon Mann Whitney Test", "Kruskal Wallis Test","Normal Test",
                           "Median Test", "Savage Test", "Ansari Bradley Test", "Taha Test",
                           "Mood Test","Klotz Test","Fligner Test","Conover Test")
        if(!is.na(wilcox_t)){
          all_test_pvalue <- c(pvalue(oneway_t),pvalue(wilcox_t), pvalue(kruskal_t), pvalue(normal_t),
                               pvalue(median_t), pvalue(savage_t), pvalue(ansari_t),
                               pvalue(taha_t), pvalue(mood_t), pvalue(klotz_t), 
                               pvalue(fligner_t), pvalue(conover_t))
          remain_df <- data.frame(numeric_attribute=value_col[test],
                                  group_attribute=group_col[test],
                                  type_test=all_type_test,
                                  test_name=all_test_name,
                                  test_pvalue=all_test_pvalue)
          df_statistical_pval <- rbind(df_statistical_pval, remain_df)
        }else{
          all_test_pvalue <- c(pvalue(oneway_t), NA, pvalue(kruskal_t), pvalue(normal_t),
                               pvalue(median_t), pvalue(savage_t), pvalue(ansari_t),
                               pvalue(taha_t), pvalue(mood_t), pvalue(klotz_t), 
                               pvalue(fligner_t), pvalue(conover_t))
          remain_df <- data.frame(numeric_attribute=value_col[test],
                                  group_attribute=group_col[test],
                                  type_test=all_type_test,
                                  test_name=all_test_name,
                                  test_pvalue=all_test_pvalue)
          df_statistical_pval <- rbind(df_statistical_pval, remain_df)
        }
      }
    }
    df_statistical_pval$test_pvalue <- round(df_statistical_pval$test_pvalue,5)
    df_statistical_pval$conclusion <- ifelse(df_statistical_pval$test_pvalue <= 0.05, 
                                             "Reject H0: Not All Group Mean is Not Equal",
                                             "Fail to Reject H0: All Group Mean is Equal")
    df_statistical_pval$conclusion_likewise <- ifelse(df_statistical_pval$test_pvalue <= 0.05, 1,
                                                      ifelse(df_statistical_pval$test_pvalue > 0.05, -1, 0))
    sum_likewise <- df_statistical_pval %>%
      group_by(numeric_attribute, group_attribute) %>%
      summarise(conclusion_likewise_total = sum(conclusion_likewise, na.rm=TRUE)) %>% as.data.frame()
    df_statistical_pval <- merge(df_statistical_pval, sum_likewise,
                                 by=c("numeric_attribute", "group_attribute"))
    return(df_statistical_pval)
  }
}

#value_to_test <- c(rep("diag",4),rep("death",4),rep("age",4))
#group_to_test <- c(rep(c("state", "sex", "status", "T.categ"),3))
#test_nonpar <- nonparametric_group_mean_test(Aids2, value_to_test, group_to_test)


#----------------------------- Calculate Confidence Interval from Vector ------------------------
confidence_interval <- function(vector, interval=0.95) {
  vec_sd <- sd(vector)
  n <- length(vector)
  vec_mean <- mean(vector)
  error <- qt((interval + 1)/2, df = n - 1) * vec_sd / sqrt(n)
  writeLines(paste0("Confidence Interval of ",interval*100, " %"))
  result <- c("lower" = vec_mean - error, "upper" = vec_mean + error)
  return(result)
}


#----------------------------- Contingency Table Testing ----------------------------
#input is limited to 2D/3D Contingency Table (3D is with Array)
contingency_table_independence_test <- function(data, factor1_col="", factor2_col="", 
                                                use_resampling=FALSE, use_plot=TRUE){
  library(coin)
  library(vcd)
  if(class(data) == "data.frame"){
    contingency_table <- table(data[[factor1_col]], data[[factor2_col]])
    writeLines("Contingency Table Result: ")
    print(contingency_table)
    table_row <- dim(contingency_table)[1]
    table_col <- dim(contingency_table)[2]
    writeLines("--------------------- 1) Perform Chi-Square Testing --------------------------------")
    writeLines("H0: no relationship exists on the categorical variables in the population; they are independent.")
    writeLines("H1: relationship exists on the categorical variables in the population; they are dependent")
    if(use_resampling){
      ch_test <- chisq.test(contingency_table)
      print(ch_test)
      if(ch_test$p.value > 0.05){
        writeLines("Failed to Reject H0, concludes that there is no relationship exists on contingency table categorical variables")
      }else if(ch_test$p.value <= 0.05){
        writeLines("Conclusion: Reject H0, concludes that there is relationship exists on contingency table categorical variables")
      }
    }else{
      ch_test <- chisq_test(contingency_table)
      print(ch_test)
      if(pvalue(ch_test) > 0.05){
        writeLines("Failed to Reject H0, concludes that there is no relationship exists on contingency table categorical variables")
      }else if(pvalue(ch_test) <= 0.05){
        writeLines("Conclusion: Reject H0, concludes that there is relationship exists on contingency table categorical variables")
      }
    }
    
    if(table_row == 2 && table_col ==2){
      writeLines("--------------------- 2) Perform Fisher-Exact Testing ------------------------------")
      writeLines("H0: two classifications are not different in 2x2 contingency tables in all possible table combinations")
      writeLines("H1: two classifications are different in 2x2 contingency tables in all possible table combinations")
      fs_test <- fisher.test(contingency_table)
      print(fs_test)
      if(fs_test$p.value > 0.05){
        writeLines("Failed to Reject H0, concludes that two classifications are not much different in 2x2 contingency tables")
      }else if(fs_test$p.value < 0.05){
        writeLines("Conclusion: Reject H0, concludes that two classifications are different in 2x2 contingency tables in some way combination")
      } 
    }else{
      fs_test <- "Not Possible to Compute"
    }
    
    if(table_row > 2){
      writeLines("Design a Proportion Table")
      props_table <- prop.table(contingency_table, margin=NULL)
      x11()
      spineplot(contingency_table)
      writeLines("--------------------- 3) Perform Linear to Linear Testing --------------------------------")
      writeLines("Note: Use Linear to Linear if there table more than 2 factors")
      writeLines("H0: Groups in ordered factors of Contingency tables have no differences in mean")
      writeLines("H1: Some Groups in ordered factors of Contingency tables have differences in mean")
      linear_to_linear_test <- lbl_test(contingency_table)
      print(linear_to_linear_test)
      return(list(ch_test, fs_test, linear_to_linear_test))
    }else{
      linear_to_linear_test <- "Not Possible to Compute"
    }
    
    if(use_plot){
      writeLines("Draw Mosaic and Association Plot for 2D Contingency Matrix")
      x11()
      mosaic(data[[factor1_col]] ~ data[[factor2_col]])
      x11()
      assoc(data[[factor1_col]] ~ data[[factor2_col]])
    }
    
    return(list(ch_test, fs_test, linear_to_linear_test))
  }
  else if(class(data) == "array"){
    writeLines("--------------------- Perform Mantel-Haenszel Testing --------------------------------")
    writeLines("Note: Use Mantel-Haenszel if there any separate analysis of contingency tables on different experiment")
    writeLines("H0: the odds ratios within each repetition of contingency tables are equal to 1/no consistent difference")
    writeLines("H1: the odds ratios within each repetition of contingency tables are not equal to 1/exist consistent difference")
    mantel_haenszel_test <- mantelhaen.test(contingency_matrix)
    print(mantel_haenszel_test)
    
    if(use_plot){
      writeLines("Draw Mosaic and Association Plot for 3D Contingency Matrix")
      writeLines("Color Representation: (https://stats.stackexchange.com/questions/147863/how-to-interpret-the-residual-colors-on-a-mosaic-plot)")
      writeLines("Blue for Pearson Residual means there are more observations in that cell than would be expected under the null model (independence)")
      writeLines("Red for Pearson Residual means there are fewer observations than would have been expected")
      x11()
      mosaic(data, shade=TRUE, legend=TRUE)
      x11()
      assoc(data, shade=TRUE)
    }
    
    return(mantel_haenszel_test)
  }
}

#input can be N Array of contingency table
contingency_table_by_logmodel_test <- function(data, logmodel_formula="", 
                                               simulate_interaction=FALSE, 
                                               interaction_effect_setup="partial",
                                               interaction_effect_size="n-1",
                                               interaction_effect_partial_size=0.66){
  library(combinat)
  library(MASS)
  var_raw <- unlist(strsplit(logmodel_formula))
  var_raw[1] <- substr(var_raw[1],2,nchar(var_raw[1]))
  new_logmodel_formula <- logmodel_formula
  logmodel_formula <- as.formula(logmodel_formula)
  writeLines("----------------- Perform Loglinear Model with Pearson & Likelihood ChiSquare Testing ---------------")
  writeLines("H0: no relationship exists on the categorical variables in the population; they are independent.")
  writeLines("H1: relationship exists on the categorical variables in the population; they are dependent")
  log_table <- xtabs(~state+sex+status, data=Aids2)
  writeLines('Extra notes source: https://support.minitab.com/en-us/minitab/19/help-and-how-to/statistics/tables/how-to/chi-square-test-for-association/interpret-the-results/all-statistics/#pearson-chi-square-and-likelihood-ratio-chi-square')
  writeLines("Pearson Chi-Square Method: involves the squared difference between the observed and the expected frequencies.")
  writeLines("Likelihood Chi-Square Method: based on the ratio of the observed to the expected frequencies.")
  if(simulate_interaction){
    last_size <- as.numeric(substr(interaction_effect_size,2,nchar(interaction_effect_size)))
    var_length <- length(var_raw)
    simulate_size <- var_length + last_size
    if(simulate_size > var_length){
      message("Formula is Not Valid!, Variable for Simulation length is higher than Formula provided")
      break
    }else if(simulate_size <= 1){
      writeLines("Not Possible to make Interaction Effects!")
    }
    else{
      all_possible_df <- NULL
      possible_combn_var <- list()
      for(k in 2:simulate_size){
        combn_df <- data.frame(t(combn(var_raw,k)))
        possible_combn_var <- c(possible_combn_var, list(combn_df))
      }
      t<-0
      var_index <- var_length
      while(var_index > 1){
        var_index <- (var_length - t)
        possible_df <- possible_combn_var[[var_index]]
        if(t==0){
          all_possible_df <- rbind(all_possible_df, possible_df)
        }else if(t > 0){
          seq_missing <- seq(var_index, var_length, 1)
          var_missing <- paste0("X",seq_missing[2:length(seq_missing)])
          for(add in 1:length(var_missing)){
            possible_df[[var_missing]] <- NA
          }
          all_possible_df <- rbind(all_possible_df, possible_df)
        }
        t <- t+1
      }
      if(interaction_effect_setup=="partial"){
        n_sample <- round(nrow(all_possible_df) * interaction_effect_partial_size, 0)
        used_combn <- all_possible_df[sample(1:nrow(all_possible_df), n_sample),]
        rownames(used_combn) <- NULL
        for(j in 1:nrow(used_combn)){
          interaction_effect <- paste0(as.character(unlist(used_combn[j,])),collapse ="*")
          new_logmodel_formula <- paste0(new_logmodel_formula,"+",interaction_effect)
        }
      }else if(interaction_effect_setup=="whole"){
        used_combn <- all_possible_df
        for(j in 1:nrow(used_combn)){
          interaction_effect <- paste0(as.character(unlist(used_combn[j,])),collapse ="*")
          new_logmodel_formula <- paste0(new_logmodel_formula,"+",interaction_effect)
        }
      }
      new_logmodel_formula <- as.formula(new_logmodel_formula)
      logmodel_test <- loglm(new_logmodel_formula, data=data)
      print(logmodel_test)
      return(logmodel_test)
    }
  }else{
    logmodel_test <- loglm(logmodel_formula, data=data)
    print(logmodel_test)
    return(logmodel_test)
  }
}

#----------------------------- Symmetrical Testing Block Design/Vector ----------------------------------
symmetrical_block_design_testing <- function(data, value_col="", group_col="", block_col=""){
  library(coin)
  block_formula <- as.formula(paste0(value_col, " ~ ", group_col, "|",block_col))
  writeLines("H0: Block Design mean over groups are all the same")
  writeLines("H1: There contains difference mean in Block Design over groups.")
  writeLines("====================== 1) Friedman Test ==============================")
  fr_test <- friedman_test(block_formula, data=data)
  print(fr_test)
  if(pvalue(fr_test) > 0.05){
    writeLines('Conclusion: Failed to Reject H0, Block Design over groups are all the same')
  }else if(pvalue(fr_test) <= 0.05){
    writeLines('Conclusion: Reject H0, There is a difference Block Design over groups.')
  }
  writeLines("====================== 2) Sign Test ==============================")
  sg_test <- sign_test(block_formula, data=data)
  print(sg_test)
  if(pvalue(sg_test) > 0.05){
    writeLines('Conclusion: Failed to Reject H0, Block Design over groups are all the same')
  }else if(pvalue(sg_test) <= 0.05){
    writeLines('Conclusion: Reject H0, There is a difference Block Design over groups.')
  }
  writeLines("====================== 3) Quade Test ==============================")
  qd_test <- quade_test(block_formula, data=data)
  print(qd_test)
  if(pvalue(qd_test) > 0.05){
    writeLines('Conclusion: Failed to Reject H0, Block Design over groups are all the same')
  }else if(pvalue(qd_test) <= 0.05){
    writeLines('Conclusion: Reject H0, There is a difference Block Design over groups.')
  }
}

symmetrical_vector_testing <- function(y1, y2, alternative="two.sided"){
  if(alternative == "two.sided"){
    writeLines("H0: Group 1 have symmetrical value distribution to Group 2")
    writeLines("H1: Group 1 is not exactly symmetrical to Group 2") 
  }else if(alternative == "less"){
    writeLines("H0: Group 1 have symmetrical value distribution to Group 2")
    writeLines("H1: Group 1 is symmetrical to Group 2 with lower mean")
  }else if(alternative == "greater"){
    writeLines("H0: Group 1 have symmetrical value distribution to Group 2")
    writeLines("H1: Group 1 is symmetrical to Group 2 with greater mean")
  }
  writeLines("==================== 1 Sign Test ==============================")
  st <- sign_test(y1 ~ y2, distribution = "exact", alternative = alternative)
  if(pvalue(st) < 0.05){
    if(alternative == "two.sided"){
      writeLines("Conclusion, Reject H0: Group 1 is not exactly symmetrical to Group 2") 
    }else if(alternative == "less"){
      writeLines("Conclusion, Reject H0: Group 1 is symmetrical to Group 2 with lower mean") 
    }else if(alternative == "greater"){
      writeLines("Conclusion, Reject H0: Group 1 is symmetrical to Group 2 with greter mean") 
    }
  }else if(pvalue(st) >= 0.05){
    writeLines("Conclusion, Fail to Reject H0: Group 1 have symmetrical value distribution to Group 2")
  }
  writeLines("==================== 2 Wilcon Sign Test ==============================")
  wt <- wilcoxsign_test(y1 ~ y2, distribution = "exact", alternative = "greater")
  if(pvalue(wt) < 0.05){
    if(alternative == "two.sided"){
      writeLines("Conclusion, Reject H0: Group 1 is not exactly symmetrical to Group 2") 
    }else if(alternative == "less"){
      writeLines("Conclusion, Reject H0: Group 1 is symmetrical to Group 2 with lower mean") 
    }else if(alternative == "greater"){
      writeLines("Conclusion, Reject H0: Group 1 is symmetrical to Group 2 with greter mean") 
    }
  }else if(pvalue(wt) >= 0.05){
    writeLines("Conclusion, Fail to Reject H0: Group 1 have symmetrical value distribution to Group 2")
  }
  return(list(st, wt))
}

#----------------------------- Generally Maximal Selected Statistics ----------------------------
maximally_selected_statistic <- function(data, factor_numerical_col="", value_col="", apply_cutpoint=TRUE){
  library(coin)
  #detailed references: https://www.stochastik.uni-freiburg.de/emeriti/lerche/inhalte/2001-summary-maximally-selected
  writeLines("H0: No effect detected from features on the distribution of the response (Independent)")
  writeLines("H1: Theres effect detected from features on the distribution of the response (Dependent)")
  maxstat_formula <- as.formula(paste0(factor_numerical_col, " ~ ", value_col))
  max_result <- maxstat_test(maxstat_formula, data = treepipit) 
  print(max_result)
  if(pvalue(max_result) < 0.05){
    writeLines("Conclusion: Reject HO: Theres effect detected from features on the distribution of the response (Dependent)")
    if(apply_cutpoint){
      writeLines("Optimal Distribution by Cutpoint")
      cutpoint_val <- max_result@estimates$estimate$cutpoint
      data <- data[data[[value_col]] <= cutpoint_val,]
      return(list(max_result, data))
    }
    else{
      return(max_result)
    }
  }else if(pvalue(max_result) >= 0.05){
    writeLines("Conclusion: Fail to Reject HO: No effect detected from features on the distribution of the response (Independent)")
    return(max_result)
  }
}

res <- maximally_selected_statistic(treepipit, "counts", "coverstorey")
resres <- maximally_selected_statistic(res[[2]], "counts", "coverstorey")

#----------------------------- Sphericity Test for Repeated Experiment -----------------------
sphericity_test <- function(data_in_groups, id_col="", sphericity_correction="auto"){
  #https://www.datanovia.com/en/lessons/mauchlys-test-of-sphericity-in-r/
  library(tidyverse)
  library(rstatix)
  
  data_without_id <- data_in_groups[,-which(colnames(data_in_groups) == id_col)]
  column_lengths <- ncol(data_without_id)
  column_names <- colnames(data_without_id)
  g <- data.frame(t(combn(colnames(data_without_id))))
  group_differences <- data.frame()
  writeLines("1. Compute Group Differences")
  for(m in 1:nrow(g)){
    diffname <- paste0(as.character(g[m,]),collapse="-")
    group_differences[[diffname]] <- data_without_id[[g[m,1]]] - data_without_id[[g[m,2]]]
  }
  writeLines("2. Compute Variances of Group Differences (See wether any of the groups have an uncommon variance than others)")
  var_group <- group_differences %>% map(var)
  print(var_group)
  writeLines("3. Convert Data into long format")
  id_sym <- sym(id_col)
  long_format <- data_in_groups %>%
    gather(key = "group", value = "score", -{{id_sym}}) %>%
    convert_as_factor(id, group)
  writeLines("4. Begin Mauchly test of Sphericity from ANOVA test")
  writeLines("H0: Variances of Group Differences are approximately equal for each other")
  writeLines("H1: Variances of Group Differences are not equal for each other")
  res <- anova_test(data = data_in_groups, dv = score, wid = id, within = group)
  print(res)
  
  writeLines("============================ Sphericity Correction Options ===============================")
  writeLines("GG: applies Greenhouse-Geisser correction to all within-subjects factors even if the assumption of sphericity is met (i.e., Mauchly's test is not significant, p > 0.05).")
  writeLines("HF: applies Hyunh-Feldt correction to all within-subjects factors even if the assumption of sphericity is met,")
  writeLines('none: returns the standard ANOVA table without any correction')
  writeLines("auto: apply automatically GG correction to only within-subjects factors violating the sphericity assumption (i.e., Mauchly's test p-value is significant, p <= 0.05).")
  writeLines("==========================================================================================")
  
  writeLines(paste0("Use Correction = ", sphericity_correction))
  res_correction <- get_anova_table(res, correction = c("auto"))
  return(list(res, res_correction))
}


#----------------------------- Binomial Testing Success/Failure Event ------------------
binomial_testing <- function(n_success, n_experiment, 
                             expected_p_success=0.7, alternative="two.sided"){
  if(alternative=="two.sided"){
    writeLines(paste0("H0: True Probability Success is equal to ",expected_p_success))
    writeLines(paste0("H1: True Probability Success is not equal to ",expected_p_success))
  }else if(alternative=="greater"){
    writeLines(paste0("H0: True Probability Success is equal to ",expected_p_success))
    writeLines(paste0("H1: True Probability Success is greater than ",expected_p_success))
  }else if(alternative=="less"){
    writeLines(paste0("H0: True Probability Success is equal to ",expected_p_success))
    writeLines(paste0("H1: True Probability Success is less than ",expected_p_success))
  }
  binom_test <- binom.test(x=n_success, n=n_experiment, 
                           p = expected_p_success, alternative=alternative)
  print(binom_test)
  if(binom_test$p.value > 0.05){
    writeLines(paste0("Conclusion: Fail to Reject H0: True Probability Success is equal to ",expected_p_success))
  }else if(binom_test$p.value <= 0.05){
    if(alternative=="two.sided"){
      writeLines(paste0("Conclusion: Reject H0: True Probability Success is not equal to ",expected_p_success))
    }else if(alternative=="greater"){
      writeLines(paste0("Conclusion: Reject H0: True Probability Success is greater than ",expected_p_success))
    }else if(alternative=="less"){
      writeLines(paste0("Conclusion: Reject H0: True Probability Success is less than ",expected_p_success))
    }
  }
  return(binom_test)
}

#----------------------------- Power Analysis Testing (Compute N, Effect Size, Power in Desired Length)------------------------
power_test <- function(guide_note=FALSE, test_type="t_test", 
                       effect_size=NULL, power=NULL, n_obs=NULL, power=NULL, 
                       anova_k_group=NULL, dual_proportion=FALSE, n1=NULL, n2=NULL, 
                       sig.error=0.05, hypo_alternative="two.sided", chisq_df=NULL,
                       linear_df_numerator=NULL, linear_df_denominator=NULL){
  library(pwr)
  if(guide_note){
    writeLines("========================= Power Test is about a statistical testing ==============================")
    writeLines("which shows Probability of Making Type 1 Error Hypothesis or Not Making Type 2 Error Hypothesis")
    writeLines("Type 1 Error and Type 2 Error Hypothesis is basically a Result Conclusion that differs to the Accepted Conclusion")
    writeLines("Suppose we want to test that in a proportion")
    writeLines("H0: p = 0.4")
    writeLines("H1: p > 0.4")
    writeLines("Type 1 Error is probability of Rejecting H0 | H0 is False")
    writeLines("In this case of hypothesis, Type 1 Error will be like proportion size is at most 40% but we conclude that proportion size is larger than 0.4")
    writeLines("which in short, Result Conclusion is H0 but Accepted Conclusion is H1")
    writeLines("Type 2 Error is probability of Not Rejecting H0 | H0 is False")
    writeLines("In this case of hypothesis, Type 2 Error will be like proportion size is actually over 40% but we conclude that proportion size is at most 0.4")
    writeLines("which in short, Result Conclusion is H1 but Accepted Conclusion is H0")
    writeLines("Parameter Correlation for Power: ")
    writeLines("alpha, n (up) Power (up) P(Type II Error) (down)")
    writeLines("alpha, n (down) Power (down) P(Type II Error) (up)")
    writeLines("For example, if experiment E has a statistical power of 0.7, and experiment F has a statistical power of 0.95")
    writeLines("then there is a stronger probability that experiment E had a type II error than experiment F.")
    writeLines("This reduces experiment E's sensitivity to detect significant effects")
    writeLines("However, experiment E is consequently more reliable than experiment F due to its lower probability of a type I error")
    writeLines("in Simple terms, the more the power the more likely probability of false positive rate (Type 1 Error) occurs than false negative rate")
    writeLines("in Contrary, the less the power the more likely probability of false negative rate (Type 2 Error) occurs than false positive rate") 
    writeLines("===================================================================================================")
  }
  
  if(test_type=="t.test"){
    null_count <- 0
    input_par_miss <- c()
    if(is.null(effect_size)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Effect Size")
    }
    if(is.null(n_obs)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "N Observation")
    } 
    if(is.null(power)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Power")
    }
    if(dual_proportion && is.null(n1)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "n1 For First Population")
    }
    if(dual_proportion && is.null(n2)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "n2 For First Population")
    }
    
    if(null_count >= 2){
      message(paste0("Missing Input either ",input_par_miss," is Missing!!"))
      break
    }
    
    if(effect_size >= 0.2 && effect_size <= 0.5){
      writeLines("By Cohen Suggestion Effect Size (d) is relatively small")
    }else if(effect_size > 0.5 && effect_size <= 0.8){
      writeLines("By Cohen Suggestion Effect Size (d) is relatively medium")
    }else if(effect_size > 0.8){
      writeLines("By Cohen Suggestion Effect Size (d) is relatively large")
    }
    if(dual_proportion){
      result <- pwr.t2n.test(d=effect_size, n1=n1, n2=n2, sig.level=sig.error, 
                             power=power, alternative=hypo_alternative)
    }else{
      result <- pwr.t.test(d=effect_size, n=n_obs, sig.level=sig.error, type="one.sample", 
                           power=power, alternative=hypo_alternative)
    }
  }
  else if(test_type == "proportion.test"){
    null_count <- 0
    input_par_miss <- c()
    if(is.null(effect_size)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Effect Size")
    }
    if(is.null(n_obs)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "N Observation")
    } 
    if(is.null(power)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Power")
    }
    if(dual_proportion && is.null(n1)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "n1 For First Population")
    }
    if(dual_proportion && is.null(n2)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "n2 For First Population")
    }
    
    if(null_count >= 2){
      message(paste0("Missing Input either ",input_par_miss," is Missing!!"))
      break
    }
    
    if(effect_size >= 0.2 && effect_size <= 0.5){
      writeLines("By Cohen Suggestion Effect Size (h) is relatively small")
    }else if(effect_size > 0.5 && effect_size <= 0.8){
      writeLines("By Cohen Suggestion Effect Size (h) is relatively medium")
    }else if(effect_size > 0.8){
      writeLines("By Cohen Suggestion Effect Size (h) is relatively large")
    }
    if(dual_proportion){
      result <- pwr.2p2n.test(h=effect_size, n1=n1, power=power, n2=n2,
                              sig.level=sig.error,alternative=hypo_alternative)
    }else{
      result <- pwr.2p.test(h=effect_size, n=n_obs, power=power,
                            sig.level=sig.error, alternative=hypo_alternative)
    }
  }
  else if(test_type == "chi.square.test"){
    null_count <- 0
    input_par_miss <- c()
    if(is.null(effect_size)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Effect Size")
    }
    if(is.null(n_obs)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "N Observation")
    } 
    if(is.null(power)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Power")
    }
    
    if(null_count >= 2){
      message(paste0("Missing Input either ",input_par_miss," is Missing!!"))
      break
    }
    if(is.null(chisq_df)){
      message("Missing Degree of Freedom for Chi Square test! cannot proceed!")
      break
    }
    
    
    if(effect_size >= 0.1 && effect_size <= 0.25){
      writeLines("By Cohen Suggestion Effect Size (w) is relatively small")
    }else if(effect_size > 0.25 && effect_size <= 0.4){
      writeLines("By Cohen Suggestion Effect Size (w) is relatively medium")
    }else if(effect_size > 0.4){
      writeLines("By Cohen Suggestion Effect Size (w) is relatively large")
    }
    result <- pwr.chisq.test(w=effect_size,df=chisq_df,
                             power=power,N=n_obs,sig.level=sig.error)
  }
  else if(test_type == "anova.test"){
    null_count <- 0
    input_par_miss <- c()
    if(is.null(effect_size)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Effect Size")
    }
    if(is.null(n_obs)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "N Observation")
    } 
    if(is.null(power)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Power")
    }
    
    if(null_count >= 2){
      message(paste0("Missing Input either ",input_par_miss," is Missing!!"))
      break
    }
    if(is.null(anova_k_group)){
      message("Missing K (Group) for ANOVA test! cannot proceed!")
      break
    }
    
    if(effect_size >= 0.1 && effect_size <= 0.25){
      writeLines("By Cohen Suggestion Effect Size (f) is relatively small")
    }else if(effect_size > 0.25 && effect_size <= 0.4){
      writeLines("By Cohen Suggestion Effect Size (f) is relatively medium")
    }else if(effect_size > 0.4){
      writeLines("By Cohen Suggestion Effect Size (f) is relatively large")
    }
    result <- pwr.anova.test(f=effect_size,n=n_obs,
                             k=anova_k_group,power=power,sig.level=sig.error)
  }
  else if(test_type == "linear.model"){
    null_count <- 0
    input_par_miss <- c()
    if(is.null(effect_size)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Effect Size")
    }
    if(is.null(linear_df_numerator)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "df Numerator")
    } 
    if(is.null(linear_df_denominator)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "df Denominator")
    } 
    if(is.null(power)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Power")
    }
    
    if(null_count >= 2){
      message(paste0("Missing Input either ",input_par_miss," is Missing!!"))
      break
    }
    if(is.null(linear_df_numerator)){
      #simulate numerator=1 to check denominator,
      #if this result a higher denominator than input, then the test is not possible
      simulate <- pwr.f2.test(u=1, v=NULL,
                              f2=effect_size, power=power, sig.level=sig.error)
      if(simulate$v > linear_df_denominator){
        message("DF Denominator is too low, the Linear Model power test cannot proceed!")
        break 
      }
    }
    
    if(effect_size >= 0.02 && effect_size <= 0.15){
      writeLines("By Cohen Suggestion Effect Size (f2) is relatively small")
    }else if(effect_size > 0.15 && effect_size <= 0.35){
      writeLines("By Cohen Suggestion Effect Size (f2) is relatively medium")
    }else if(effect_size > 0.35){
      writeLines("By Cohen Suggestion Effect Size (f2) is relatively large")
    }
    result <- pwr.f2.test(u=linear_df_numerator,
                          v=linear_df_denominator,
                          f2=effect_size,power=power,sig.level=sig.error)
  }
  else if(test_type == "correlation.test"){
    null_count <- 0
    input_par_miss <- c()
    if(is.null(effect_size)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Effect Size")
    }
    if(is.null(n_obs)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "N Observation")
    } 
    if(is.null(power)){
      null_count <- null_count + 1
      input_par_miss <- c(input_par_miss, "Power")
    }
    
    if(null_count >= 2){
      message(paste0("Missing Input either ",input_par_miss," is Missing!!"))
      break
    }
    
    if(effect_size >= 0.1 && effect_size <= 0.3){
      writeLines("By Cohen Suggestion Effect Size (r) is relatively small")
    }else if(effect_size > 0.3 && effect_size <= 0.5){
      writeLines("By Cohen Suggestion Effect Size (r) is relatively medium")
    }else if(effect_size > 0.5){
      writeLines("By Cohen Suggestion Effect Size (r) is relatively large")
    }
    result <- pwr.r.test(r=effect_size, n=n_obs, power=power,
                         sig.level=sig.error,alternative=hypo_alternative)
  }
  print(result)
  return(result)
}


#------------------------------------------------------------------------------------------------------------------------
##################### 4) Icon/Emoji Plotting (FontAwesome/Parliament/waffle) ############################################
#------------------------------------------------------------------------------------------------------------------------

load_fontawesome_slideshow <- function(folder_path="C:/Users/User/Desktop/Projects External/Picto Graph/FontAwesome/",
                                       fa_type="FA_All"){
  folder_path <- paste0(folder_path, fa_type, "/")
  print(folder_path)
  library(shiny)
  library(slickR)
  
  ui <- fluidPage(
    mainPanel(
      slickROutput("slickr", width="1300px", height="600px")
    )
  )
  
  server <- function(input, output) {
    output$slickr <- renderSlickR({
      imgs <- list.files(folder_path, 
                         pattern=".JPG", full.names = TRUE)
      slickR(imgs)
    })
  }
  # Run the application 
  shinyApp(ui = ui, server = server)
}
load_fontawesome_iconname <- function(filter_not_available=TRUE){
  fa_solid <- read.csv("C:/Users/User/Desktop/Projects External/Picto Graph/FontAwesome/FA_Solid.csv")
  fa_regular <- read.csv("C:/Users/User/Desktop/Projects External/Picto Graph/FontAwesome/FA_Regular.csv")
  fa_brand <- read.csv("C:/Users/User/Desktop/Projects External/Picto Graph/FontAwesome/FA_Brand.csv")
  df_fa <- data.frame(rbind(fa_solid, fa_regular, fa_brand))
  #print(head(df_fa))
  df_fa <- df_fa[which(duplicated.data.frame(df_fa)==FALSE),]
  colnames(df_fa)[2] <- "Last_Encoding_FA"
  
  #Unicode from FontAwesome Function
  df_fa$fontawesome_unicode_r_available <- NULL
  for(x in 1:nrow(df_fa)){
    df_fa$fontawesome_unicode_r_available[x] <- fontawesome(paste0('fa-',df_fa$Icons[x]))
  }
  #Unicode Generate by Transforming to Hexadecimal and then to Unicode
  df_fa$fontawesome_unicode <- NULL
  for(x in 1:nrow(df_fa)){
    df_fa$fontawesome_unicode[x] <- intToUtf8(paste0("0x",df_fa$Last_Encoding_FA[x]))
  }
  if(filter_not_available){
    df_fa <- df_fa[which(is.na(df_fa$fontawesome_unicode_r_available)==FALSE),]
    row.names(df_fa) <- NULL
  }
  return(df_fa)
}

#Check Maximum Variance of Unicode for 1 Emoji
#max(str_count(emoji_df$Emoji.Unicode, " "))
load_emoji_unicodename <- function(return_all_unicode=FALSE){
  library(stringr)
  library(dplyr)
  emoji_df <- read.csv("C:/Users/User/Desktop/Projects External/Picto Graph/Emoji/emoji_icon.csv")
  emoji_splitted <- data.frame(str_split_fixed(emoji_df$Emoji.Unicode, " ", 
                                               max(str_count(emoji_df$Emoji.Unicode, " "))))
  colnames(emoji_splitted)[1:max(str_count(emoji_df$Emoji.Unicode, " "))] <- paste0("Unicode", seq(1,max(str_count(emoji_df$Emoji.Unicode, " "))))
  emoji_splitted$"CLDR.Short.Name" <- emoji_df$"CLDR.Short.Name"
  #print(head(emoji_splitted))
  if(return_all_unicode){
    for(g in 1:max(str_count(emoji_df$Emoji.Unicode, " "))){
      emoji_splitted[,g] <- gsub("U\\+","0x",emoji_splitted[,g])
      for(h in 1:nrow(emoji_splitted)){
        emoji_splitted[h,g] <- intToUtf8(emoji_splitted[h,g])
      }
    }
    return(emoji_splitted) 
  }
  else{
    emoji_splitted <- emoji_splitted %>% select("Unicode1", "CLDR.Short.Name")
    colnames(emoji_splitted)[1] <- "Unicode"
    emoji_splitted$Unicode <- gsub("U\\+","0x",emoji_splitted$Unicode)
    for(h in 1:nrow(emoji_splitted)){
      emoji_splitted$Unicode[h] <- intToUtf8(emoji_splitted$Unicode[h])
    }
    return(emoji_splitted)
  }
}

FA_FULL <- load_fontawesome_iconname(FALSE)
FA <- load_fontawesome_iconname()
EMOJI <- load_emoji_unicodename()

test_fontawesome_emoticon_waffle_random_plot <- function(n=100, unicode_label, 
                                                         size_icon=10, type="FA"){
  library(ggwaffle)
  library(emojifont)
  library(showtext)
  library(fontawesome)
  library(dplyr)
  random <- data.frame(x = runif(n), y=rnorm(n), 
                       group=rpois(n,1))
  random$group <- as.character(LETTERS[random$group + 1])
  waffle <- waffle_iron(random, aes_d(group = group)) %>%
    mutate(label = unicode_label)
  x11()
  if(type=="FA"){
    print(ggplot(waffle, aes(x, y, colour = group)) + 
            geom_text(aes(label=label), family='fontawesome-webfont', size=size_icon))
  }
  else if(type=="EMOJI"){
    print(ggplot(waffle, aes(x, y, colour = group)) + 
            geom_text(aes(label=label), family='EmojiOne', size=size_icon))
  }
  return(list(random, waffle))
}
test_fontawesome_emoticon_waffle_group_plot <- function(n=100, unicode_vector, 
                                                        size_icon=10, type="FA", label="Random"){
  library(ggwaffle)
  library(emojifont)
  library(showtext)
  library(fontawesome)
  library(dplyr)
  random <- data.frame(x = runif(n), y=rnorm(n), 
                       group=rpois(n,1))
  random$group <- as.character(LETTERS[random$group + 1])
  waffle <- waffle_iron(random, aes_d(group = group)) %>%
    mutate(label = "x")
  
  if(label=="Random")
  {
    label_sampling <- sample(length(unicode_vector),length(unique(random$group)))
    print(label_sampling)
    for(group_labeling in 1:length(unique(waffle$group))){
      #print(head(waffle[which(waffle$group==unique(waffle$group)[group_labeling]),]))
      waffle[which(waffle$group==unique(waffle$group)[group_labeling]),]$label <- unicode_vector[label_sampling[group_labeling]] 
    } 
  }
  else if(label=="Fixed"){
    if(length(unicode_vector) > length(unique(waffle$group))){
      unicode_vector <- unicode_vector[1:length(unique(waffle$group))]
    }
    for(group_labeling in 1:length(unique(waffle$group))){
      waffle[which(waffle$group==unique(waffle$group)[group_labeling]),]$label <- unicode_vector[group_labeling]
    } 
  }
  x11()
  if(type=="FA"){
    print(ggplot(waffle, aes(x, y, colour = group)) + 
            geom_text(aes(label=label), family='fontawesome-webfont', size=size_icon))
  }
  else if(type=="EMOJI"){
    print(ggplot(waffle, aes(x, y, colour = group)) + 
            geom_text(aes(label=label), family='EmojiOne', size=size_icon))
  }
  return(list(random, waffle))
}
parliament_auto_plot <- function(df, seat_col, group_col, 
                                 parliament_rows = 10, seat_size = 1, 
                                 highlight_formula="", highlight_color="purple", 
                                 highlight_size=4, auto_adjust_size=FALSE, 
                                 parliament_proportion=0.01){
  x11()
  df_parliament <- NULL
  seat_size <- seat_size
  
  df_group <- df %>% 
    group_by(eval(parse(text=group_col))) %>% 
    summarise(total_seat = sum(eval(parse(text=seat_col)), na.rm=TRUE))
  colnames(df_group) <- c(group_col, seat_col)
  df_group$colour <- sample(colors(), nrow(df_group), replace=TRUE)
  df_group <- data.frame(df_group)
  print(df_group)
  
  if(auto_adjust_size==TRUE){
    #Take 1% off nrow data as parliament rows
    parliament_rows <- round(sum(as.numeric(unlist(df_group[seat_col]))) * parliament_proportion) 
    writeLines(paste0("Using Parliament Rows = ", parliament_rows))
    minimum_seat <- min(as.numeric(unlist(df_group[seat_col])))
    
    #Take Minimum Seats/ Parliament Rows as number of sizes
    #if minimum seats < parliament rows, seat = minimum seat
    if(minimum_seat < parliament_rows){
      seat_size <- minimum_seat
    }
    else if(minimum_seat >= parliament_rows) {
      seat_size <- floor(minimum_seat / parliament_rows)
    }
    writeLines(paste0("Using Seat Size = ", seat_size))
    
    df_parliament <- parliament_data(election_data = df_group, 
                                     parl_rows = parliament_rows,
                                     type = 'semicircle',
                                     party_seats = as.numeric(unlist(df_group[seat_col])))
  }
  else{
    df_parliament <- parliament_data(election_data = df_group, 
                                     parl_rows = parliament_rows,
                                     type = 'semicircle',
                                     party_seats = as.numeric(unlist(df_group[seat_col]))) 
  }
  
  writeLines("Top 10 Rows Parliament")
  print(head(df_parliament,10))
  writeLines("Bottom 10 Rows Parliament")
  print(tail(df_parliament,10))
  writeLines("Summary Parliament")
  print(summary(df_parliament))
  
  if(highlight_formula != ""){
    parliament_plot <- ggplot(df_parliament, aes(x, y, colour = eval(parse(text=group_col)))) +
      geom_parliament_seats(size = seat_size) +
      geom_highlight_government(eval(parse(text=highlight_formula)), 
                                colour = highlight_color, size = highlight_size) +
      geom_parliament_bar(colour = colour, party = eval(parse(text=group_col))) +
      labs(colour="Party") +  
      theme_ggparliament(legend = TRUE) +
      scale_colour_manual(values = df_parliament$colour, 
                          limits = as.character(unlist(df_parliament[group_col])))
    
    print(parliament_plot)  
    return(list(df_parliament, parliament_plot))
  }
  else{
    parliament_plot <- ggplot(df_parliament, aes(x, y, colour = eval(parse(text=group_col)))) +
      geom_parliament_seats(size = seat_size) +
      geom_parliament_bar(colour = colour, party = eval(parse(text=group_col))) +
      labs(colour="Party") +  
      theme_ggparliament(legend = TRUE) +
      scale_colour_manual(values = df_parliament$colour, 
                          limits = as.character(unlist(df_parliament[group_col])))
    print(parliament_plot)
    return(list(df_group, df_parliament, parliament_plot))
  }
}
parliament_and_waffle_plot <- function(df, seat_col, group_col, 
                                       parliament_rows = 10, seat_size = 1, picto_size=10,
                                       auto_adjust_size=FALSE, simplify_values=1,
                                       parliament_proportion=0.01, unicode_vector, 
                                       unicode_type="FA", unicode_sampling="Random"){
  library(ggwaffle)
  library(emojifont)
  library(showtext)
  library(fontawesome)
  library(dplyr)
  
  df_parliament <- NULL
  seat_size <- seat_size
  
  df_group <- df %>% 
    group_by(eval(parse(text=group_col))) %>% 
    summarise(total_seat = sum(eval(parse(text=seat_col)), na.rm=TRUE))
  colnames(df_group) <- c(group_col, seat_col)
  df_group$colour <- sample(colors(), nrow(df_group), replace=TRUE)
  df_group <- data.frame(df_group)
  print(df_group)
  
  if(auto_adjust_size==TRUE){
    #Take 1% off nrow data as parliament rows
    parliament_rows <- round(sum(as.numeric(unlist(df_group[seat_col]))) * parliament_proportion) 
    writeLines(paste0("Using Parliament Rows = ", parliament_rows))
    minimum_seat <- min(as.numeric(unlist(df_group[seat_col])))
    
    #Take Minimum Seats/ Parliament Rows as number of sizes
    #if minimum seats < parliament rows, seat = minimum seat
    if(minimum_seat < parliament_rows){
      seat_size <- minimum_seat
    }
    else if(minimum_seat >= parliament_rows) {
      seat_size <- floor(minimum_seat / parliament_rows)
    }
    writeLines(paste0("Using Seat Size = ", seat_size))
    
    df_parliament <- parliament_data(election_data = df_group, 
                                     parl_rows = parliament_rows,
                                     type = 'semicircle',
                                     party_seats = round(as.numeric(unlist(df_group[seat_col])) * simplify_values))
  }
  else{
    df_parliament <- parliament_data(election_data = df_group, 
                                     parl_rows = parliament_rows,
                                     type = 'semicircle',
                                     party_seats = round(as.numeric(unlist(df_group[seat_col])) * simplify_values))
  }
  print(head(df_parliament))
  
  fontawesome_emoticon_waffle_group_plot <- function(df_parliament, group_col, unicode_vector, 
                                                     size_icon=10, type, label){
    df_parliament$group <- as.character(unlist(df_parliament[group_col]))
    waffle <- waffle_iron(df_parliament, aes_d(group = group)) %>%
      mutate(label = "x")
    
    #print(waffle)
    #writeLines("List of Unicode Vector")
    #print(unicode_vector)
    
    if(label=="Random")
    {
      label_sampling <- sample(length(unicode_vector),length(unique(waffle$group)))
      print(label_sampling)
      for(group_labeling in 1:length(unique(waffle$group))){
        #print(head(waffle[which(waffle$group==unique(waffle$group)[group_labeling]),]))
        waffle[which(waffle$group==unique(waffle$group)[group_labeling]),]$label <- unicode_vector[label_sampling[group_labeling]] 
      } 
    }
    else if(label=="Fixed"){
      if(length(unicode_vector) > length(unique(waffle$group))){
        unicode_vector <- unicode_vector[1:length(unique(waffle$group))]
      }
      for(group_labeling in 1:length(unique(waffle$group))){
        waffle[which(waffle$group==unique(waffle$group)[group_labeling]),]$label <- unicode_vector[group_labeling]
      } 
    }
    x11()
    if(type=="FA"){
      print(ggplot(waffle, aes(x, y, colour = group)) + 
              geom_text(aes(label=label), family='fontawesome-webfont', size=size_icon))
    }
    else if(type=="EMOJI"){
      print(ggplot(waffle, aes(x, y, colour = group)) + 
              geom_text(aes(label=label), family='EmojiOne', size=size_icon))
    }
    return(waffle)
  }
  
  waffle_pictograph <- fontawesome_emoticon_waffle_group_plot(df_parliament, group_col, unicode_vector,
                                                              picto_size, unicode_type, unicode_sampling)
  
  x11()
  writeLines("Top 10 Rows Parliament")
  print(head(df_parliament,10))
  writeLines("Bottom 10 Rows Parliament")
  print(tail(df_parliament,10))
  writeLines("Summary Parliament")
  print(summary(df_parliament))
  
  parliament_plot <- ggplot(df_parliament, aes(x, y, colour = eval(parse(text=group_col)))) +
    geom_parliament_seats(size = seat_size) +
    geom_parliament_bar(colour = colour, party = eval(parse(text=group_col))) +
    labs(colour="Party") +  
    theme_ggparliament(legend = TRUE) +
    scale_colour_manual(values = df_parliament$colour, 
                        limits = as.character(unlist(df_parliament[group_col])))
  print(parliament_plot)
  return(list(df_parliament, parliament_plot, waffle_pictograph))
  
}
a <- parliament_and_waffle_plot(germany, "seats", "party_short", auto_adjust_size=TRUE, picto_size=7,
                                parliament_proportion=0.01, simplify_values=0.5, unicode_vector=FA$fontawesome_unicode, 
                                unicode_type="FA", unicode_sampling="Random")
a[[3]] %>% group_by(group) %>% summarize(group_total = n())

b <- parliament_and_waffle_plot(australia, "seats", "party_short", auto_adjust_size=TRUE, picto_size=7,
                                parliament_proportion=0.01, simplify_values=1, unicode_vector=FA$fontawesome_unicode, 
                                unicode_type="FA", unicode_sampling="Random")
b[[3]] %>% group_by(group) %>% summarize(group_total = n())

#------------------------------------------------------------------------------------------------------------------------
####################################### 5) Quality Control Analysis #####################################################
#------------------------------------------------------------------------------------------------------------------------

quality_control_analysis <- function(data, value_col, numerical_group_col="",
                                     include_nonconforming=FALSE,
                                     size_nonconforming_col="",
                                     logical_nonconforming_col="",
                                     new_data=data.frame(), 
                                     std_value_warning = 0,
                                     qcc_group_train_prop=0.7,
                                     xbar_target=-1,
                                     ewma_lambda=0.5,
                                     ewma_nsigmas=5){
  newdata_length <- nrow(new_data)
  if(numerical_group_col==""){
    writeLines("Make an XBar one type Quality Control Analysis")
    if(newdata_length > 0){
      quality_control <- qcc(data[[value_col]], type="xbar.one", 
                             newdata = new_data[[value_col]])
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
    else if(newdata_length == 0){
      quality_control <- qcc(data[[value_col]], type="xbar.one")
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
  }
  else if(numerical_group_col!=""){
    grouped_qcc <- qcc.groups(data[[value_col]], data[[numerical_group_col]])
    rows_grouped_qcc <- nrow(grouped_qcc)
    if(newdata_length > 0){
      new_data_grouped_qcc <- qcc.groups(new_data[[value_col]], new_data[[numerical_group_col]])
      new_data_rows_grouped_qcc <- nrow(new_data_grouped_qcc) 
    }
    writeLines("========================================= chart type description ==============================================")
    writeLines("1) 'xbar'	mean --> means of a continuous process variable")
    writeLines("2) 'R'	range	--> ranges of a continuous process variable")
    writeLines("3) 'S'	standard --> deviation	standard deviations of a continuous variable")
    writeLines("4) 'p'	proportion	--> proportion of nonconforming units")
    writeLines("5) 'np'	count	--> number of nonconforming units")
    writeLines("6) 'c'	count	--> nonconformities per unit (Calculate Unit Damage)")
    writeLines("7) 'u'	count	--> average nonconformities per unit (Calculate Average of Unit Damage)")
    writeLines("8) 'p.std'	count	--> standardized p chart")
    writeLines("9) 'cusum' count --> Cumulative Sum Chart")
    writeLines("10) 'EWMA' count --> draw an Exponential Weighted Moving Average Chart")
    writeLines("11) 'Process Capability' -->  process capability indices chart")
    writeLines("12) Pareto --> make a pareto chart (cumulative sum of group probabilities)")
    writeLines("13) OCC --> make an Operating Characteristic Curves to provide information probability of not detecting a shift (std.dev) in the process")
    writeLines("===============================================================================================================")
    writeLines("")
    writeLines("======================= 1) Make xbar Quality Control Chart ============================")
    xbar_chart <- NULL
    x11()
    if(newdata_length > 0){
      quality_control <- qcc(grouped_qcc, type="xbar", 
                             newdata = new_data_grouped_qcc)
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
    else if(newdata_length == 0){
      quality_control <- qcc(grouped_qcc, type="xbar")
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
    xbar_chart <- quality_control
    print(summary(xbar_chart))
    writeLines("---------------- Make xbar Operating Characteristic Curves -------------------")
    x11()
    beta = oc.curves.xbar(xbar_chart)
    print(round(beta, digits=4))
    
    writeLines("========================= 2) Make R Quality Control Chart =======================")
    r_chart <- NULL
    x11()
    if(newdata_length > 0){
      quality_control <- qcc(grouped_qcc, type="R", 
                             newdata = new_data_grouped_qcc)
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
    else if(newdata_length == 0){
      quality_control <- qcc(grouped_qcc, type="R")
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
    r_chart <- quality_control
    print(summary(r_chart))
    writeLines("---------------- Make R Operating Characteristic Curves -------------------")
    x11()
    beta = oc.curves(r_chart)
    print(round(beta, digits=4))
    
    writeLines("====================== 3) Make S Quality Control Chart ====================================")
    s_chart <- NULL
    x11()
    if(newdata_length > 0){
      quality_control <- qcc(grouped_qcc, type="S", 
                             newdata = new_data_grouped_qcc)
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
    else if(newdata_length == 0){
      quality_control <- qcc(grouped_qcc, type="S")
      if(std_value_warning > 0){
        writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
        warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                  quality_control$sizes, std_value_warning)
        plot(quality_control, restore.par = FALSE)
        abline(h = warn.limits, lty = 3, col = "chocolate")
      }
    }
    s_chart <- quality_control
    print(summary(s_chart))
    writeLines("---------------- Make S Operating Characteristic Curves -------------------")
    x11()
    beta = oc.curves(s_chart)
    print(round(beta, digits=4))
    
    if(logical_nonconforming_col!=""){
      stats.p.std = function(data, sizes)
      {
        data = as.vector(data)
        sizes = as.vector(sizes)
        pbar = sum(data)/sum(sizes)
        z = (data/sizes - pbar)/sqrt(pbar*(1-pbar)/sizes)
        list(statistics = z, center = 0)
      }
      
      sd.p.std = function(data, sizes, ...) { return(1) }
      
      limits.p.std = function(center, std.dev, sizes, conf)
      {
        if(conf >= 1) 
        { lcl = -conf
        ucl = +conf 
        }
        else
        { if(conf > 0 & conf < 1)
        { nsigmas = qnorm(1 - (1 - conf)/2)
        lcl = -nsigmas
        ucl = +nsigmas }
          else stop("invalid 'conf' argument.") 
        }
        limits = matrix(c(lcl, ucl), ncol = 2)
        rownames(limits) = rep("", length = nrow(limits))
        colnames(limits) = c("LCL", "UCL")
        return(limits)
      }
      
      writeLines("=========================== 4) Make p Quality Control chart =================================")
      p_chart <- NULL
      x11()
      if(newdata_length > 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes =data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="p",
                               newdata = data[[value_col]][which(data[[logical_nonconforming_col]]==FALSE)],
                               newsizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==FALSE)])
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      else if(newdata_length == 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes =data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="p")
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      p_chart <- quality_control
      print(summary(p_chart))
      writeLines("---------------- Make P Operating Characteristic Curves -------------------")
      x11()
      beta = oc.curves(p_chart)
      print(round(beta, digits=4))
      
      writeLines("============================= 5) Make np Quality Control chart =============================")
      np_chart <- NULL
      x11()
      if(newdata_length > 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="np",
                               newdata = data[[value_col]][which(data[[logical_nonconforming_col]]==FALSE)],
                               newsizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==FALSE)])
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      else if(newdata_length == 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes =data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="np")
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      np_chart <- quality_control
      print(summary(np_chart))
      writeLines("---------------- Make NP Operating Characteristic Curves -------------------")
      x11()
      beta = oc.curves(np_chart)
      print(round(beta, digits=4))
      
      writeLines("========================= 6) Make c Quality Control chart =================================")
      c_chart <- NULL
      x11()
      if(newdata_length > 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="c",
                               newdata = data[[value_col]][which(data[[logical_nonconforming_col]]==FALSE)],
                               newsizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==FALSE)])
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(q1, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      else if(newdata_length == 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes =data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="c")
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(q1, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      c_chart <- quality_control
      print(summary(c_chart))
      writeLines("---------------- Make C Operating Characteristic Curves -------------------")
      x11()
      beta = oc.curves(c_chart)
      print(round(beta, digits=4))
      
      writeLines("============================ 7) Make U Quality Control chart ===============================")
      u_chart <- NULL
      x11()
      if(newdata_length > 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="u",
                               newdata = data[[value_col]][which(data[[logical_nonconforming_col]]==FALSE)],
                               newsizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==FALSE)])
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      else if(newdata_length == 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes =data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="u")
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      u_chart <- quality_control
      print(summary(u_chart))
      writeLines("---------------- Make U Operating Characteristic Curves -------------------")
      x11()
      beta = oc.curves(u_chart)
      print(round(beta, digits=4))
      
      writeLines("===================== 8) Make Standardized P Quality Control chart ====================") 
      p_std_chart <- NULL
      x11()
      if(newdata_length > 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="p_std",
                               newdata = data[[value_col]][which(data[[logical_nonconforming_col]]==FALSE)],
                               newsizes = data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==FALSE)])
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      else if(newdata_length == 0){
        quality_control <- qcc(data[[value_col]][which(data[[logical_nonconforming_col]]==TRUE)], 
                               sizes =data[[size_nonconforming_col]][which(data[[logical_nonconforming_col]]==TRUE)],
                               type="p_std")
        if(std_value_warning > 0){
          writeLines(paste0("Draw Chocolate Lines Warning with std value of = ",std_value_warning))
          warn.limits = limits.xbar(quality_control$center, quality_control$std.dev, 
                                    quality_control$sizes, std_value_warning)
          plot(quality_control, restore.par = FALSE)
          abline(h = warn.limits, lty = 3, col = "chocolate")
        }
      }
      p_std_chart <- quality_control
      print(summary(p_std_chart))
      writeLines("---------------- Make P Std Operating Characteristic Curves -------------------")
      x11()
      beta = oc.curves(p_std_chart)
      print(round(beta, digits=4))
    }
    writeLines("========================= 9) Make Cusum Quality Control chart =========================")
    x11()
    #data[[value_col]]
    cusum_chart = cusum(grouped_qcc)
    print(summary(cusum_chart))
    writeLines("==================  10) Make EWMA Quality Control chart ================================")
    writeLines("============EWMA Parameter tips===============")
    writeLines("Higher the Lambda, the Higher and Smoother the USL and LSL for Universal Group")
    writeLines("Higher the nsigmas, the Higher the USL and LSL Ranges for Universal Group")
    writeLines("if Lambda is low, and nsigmas are high, control chart will make a narrow and growing USL and LSL that starts from beginning")
    writeLines("if Lambda is high, and nsigmas are low, control chart will make a flat USL and LSL but small ranges of USL and LSL")
    x11()
    ewma_chart = ewma(grouped_qcc, lambda=ewma_lambda, nsigmas=ewma_nsigmas)
    print(summary(ewma_chart))
    writeLines("=================== 11) Make Process capability analysis Control Chart ================")
    process_capability_chart <- NULL
    x11()
    if(xbar_target > 0){
      process_capability_chart <- process.capability(xbar_chart, 
                                                     spec.limits=c(xbar_chart$limits[1],xbar_chart$limits[2]), 
                                                     target=xbar_target)
    }
    else{
      process_capability_chart <- process.capability(xbar_chart, 
                                                     spec.limits=c(xbar_chart$limits[1],xbar_chart$limits[2]))
    }
    writeLines("======================== 12) Make Pareto Diagram Chart ===============================")
    x11()
    pareto_xbar = as.numeric(xbar_chart$data)
    names(pareto_xbar) = paste0("Group_",seq(1:length(pareto_xbar)))
    pareto_chart <- pareto.chart(pareto_xbar, ylab = "Frequency")
    
    if(include_nonconforming==TRUE){
      return(list(xbar_chart, r_chart, s_chart, p_chart, np_chart, c_chart, u_chart, 
                  p_std_chart, cusum_chart, ewma_chart, process_capability_chart, pareto_chart))
    }
    else if(include_nonconforming==FALSE){
      return(list(xbar_chart, r_chart, s_chart, cusum_chart, 
                  ewma_chart, process_capability_chart, pareto_chart))
    }
  }
}

library(qcc)
data(pistonrings)
qcc_pistonrings <- quality_control_analysis(pistonrings, "diameter", "sample")
qcc_pistonrings <- quality_control_analysis(pistonrings, "diameter", "sample", std_value_warning = 3)
qcc_pistonrings <- quality_control_analysis(pistonrings[1:150,], "diameter", "sample", 
                                            std_value_warning = 3,
                                            new_data = pistonrings[151:200,])

#------------------------------------------------------------------------------------------------------------------------
####################################### 6) Benford Analysis #############################################################
#------------------------------------------------------------------------------------------------------------------------

benford_analysis <- function(df, value_col, digits_pattern=3, discrete_rounding=2, number_sign="both",
                             search_value_pattern=c(), value_type="discrete", top_show=20){
  library(benford.analysis)
  x11()
  if(value_type=="discrete"){
    cp <- benford(df[[value_col]], digits_pattern, sign=number_sign,
                  discrete = TRUE, round=discrete_rounding) #generates benford object
  }
  else if(value_type=="continous"){
    cp <- benford(df[[value_col]], digits_pattern, sign=number_sign, 
                  discrete = FALSE) #generates benford object
  }
  writeLines("================================= 1) Benford Analysis Results ========================================")
  print(cp)  
  plot(cp) #plots
  head(suspectsTable(cp),top_show)
  writeLines("============ 2) Conducting MAD and Chi Square Statistic for the Benford Analysis Results =============")
  print(paste0("MAD (Mean Absolute Deviation) score = ", MAD(cp))) #gets the Mean Absolute Deviation
  writeLines("Hypothesis of Chi Square testing: (FSD -> First Significant Digit)")
  writeLines("H0 (null hypothesis): The population's FSD distribution conforms to Benford's Law")
  writeLines("H1 (alternate hypothesis): The popluation's FSD distribution is different from Benford's Law")
  print(chisq(cp)) #gets the Chi-squared test
  writeLines("============ 3) Get Most Duplicated Value pattern from Benford Analysis ==============================")
  duplicate_table <- duplicatesTable(cp)
  print(duplicatesTable(cp)) #prints the duplicates by decreasing order
  if(length(search_value_pattern) > 0){
    writeLines("============== 4) Find Data with desired Numerical Pattern ======================================")
    digits_search <- getDigits(cp, df, digits=search_value_pattern)
    print(digits_search)
    return(list(cp, digits_search))
  }
  return(cp)
}

library(benford.analysis)
data("corporate.payment")
ben_analysis <- benford_analysis(corporate.payment, "Amount")

#------------------------------------------------------------------------------------------------------------------------
####################################### 7) Association Rule Mining ######################################################
#------------------------------------------------------------------------------------------------------------------------

df_to_transaction <- function(df, filter_n_factor = 50, convert=TRUE,
                              exclude_column = c()){
  col_filter_vector <- c()
  for(x in 1:length(colnames(df))){
    if(class(df[[colnames(df)[x]]]) == "factor"){
      if(length(levels(df[[colnames(df)[x]]])) > filter_n_factor){
        col_filter_vector <- c(col_filter_vector, x)
      }
      else if(length(levels(df[[colnames(df)[x]]])) == 0 || 
              length(levels(df[[colnames(df)[x]]])) == 1){
        col_filter_vector <- c(col_filter_vector, x)
      }
    }
    else{
      col_filter_vector <- c(col_filter_vector, x)
    }
  }
  if(length(exclude_column) > 0){
    for(y in 1:length(exclude_column)){
      index_col <- which(colnames(df)==exclude_column[y])
      col_filter_vector <- c(col_filter_vector, index_col)
    } 
  }
  df <- df[, -col_filter_vector]
  if(convert){
    df <- as(df, "transactions") 
  }
  return(df)
}

#Method available is apriori and eclat
association_rule <- function(transaction, method="apriori", support_lvl=0.001, confidence_lvl=0.1, 
                             sequence_min=1, sequence_max=2, top_n=20, paracoord_enabled=FALSE){
  library(arules)
  library(arulesViz)
  library(grid) #For Gridding model into one plot
  library(ggplot2) #For Plotting GGPlot Diagram
  library(gridBase) #For Gridding model into one plot
  
  if(method=="apriori"){
    writeLines("Item Info in Transaction")
    print(head(transaction@itemInfo, n=10))
    tr <- itemFrequency(transaction, type="absolute")
    tr <- sort(tr, decreasing=TRUE)
    name <- names(tr)
    freq <- data.frame(tr)[[1]]
    
    min_support <- min(freq)/sum(freq)
    max_support <- max(freq)/sum(freq)
    
    writeLines(paste("Minimum Support found on Transaction:", min_support))
    writeLines(paste("Maximum Support found on Transaction:", max_support))
    
    groceries_df <- data.frame(name, freq)
    
    writeLines("1) Top 10 Transactions ")
    trtop10 <- sort(tr, decreasing=TRUE)[1:10]
    name <- names(trtop10)
    freq <- data.frame(trtop10)[[1]]
    groceries_df_top <- data.frame(name, freq)
    print(groceries_df_top)
    
    writeLines("2) Bottom 10 Transactions ")
    trbottom10 <- sort(tr)[1:10]
    name <- names(trbottom10)
    freq <- data.frame(trbottom10)[[1]]
    groceries_df_bottom <- data.frame(name, freq)
    print(groceries_df_bottom)
    
    colors = c("#ffc6c2","#ffdaa6","#fff9a6","#ecffa6","#d1ffa6","#a9ffa6","#a6ffc4","#a6ffdb","#a6ffff",
               "#a6deff","#a6caff","#a6a7ff","#b9a6ff","#d7a6ff","#f0a6ff","#fca6ff","#ffa6ea","#ffa6c1",
               "#ffa6b3","#ffa6b5")
    colors_bold = c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03",
                    "#18fc03","#03fc5e","#03fcc6","#03f0fc","#03adfc","#035afc",
                    "#be03fc","#fc03f8","#fc03a1")
    
    x11()
    writeLines("3) Plotting with Apriori Item Frequency ")
    arules::itemFrequencyPlot(transaction, topN=top_n,
                              col=colors,
                              main='Relative Item Frequency Plot',
                              type="relative",ylab="Item Frequency (Relative)")
    
    writeLines("4) Apriori Rules Validation ")
    rules <- apriori(transaction, parameter=list(supp = support_lvl, 
                                                 confidence= confidence_lvl, 
                                                 minlen=sequence_min, maxlen=sequence_max, 
                                                 target="rules"))
    rules_backup <- rules
    writeLines("5) Rules Plotting ")
    x11()
    plot(rules, title="Whole Rules Plot")
    x11()
    plot(rules[1:top_n], method = "graph", control = list(type = "items"), 
         title=paste("Top",top_n,"Rules with Graph Plot"))
    
    if(paracoord_enabled){
      x11()
      plot(rules[1:top_n], method = "paracoord", control = list(reorder = TRUE), 
           title=paste("Top",top_n,"Rules with Paracoord Plot")) 
    }
    
    writeLines("A) Inspect Rules by High Support")
    print(inspect(head(rules, by="support", n=top_n) , rulesep="=>"))
    writeLines("B) Inspect Rules by High Lift")
    print(inspect(head(rules, by="lift", n=top_n) , rulesep="=>"))
    writeLines("C) Inspect Rules by High Confidence")
    print(inspect(head(rules, by="confidence", n=top_n) , rulesep="=>"))
    
    #Konversi ke DF
    rules <- unclass(head(rules, by="lift", n=length(rules)))
    x <- attr(rules, "quality")
    lhs <- as(as(attr(rules, "lhs"), "transactions"), "data.frame")$items
    rhs <- as(as(attr(rules, "rhs"), "transactions"), "data.frame")$items
    lhs <- as.character(lhs)
    rhs <- as.character(rhs)
    rules <- paste(lhs,rhs,sep=" => ")
    rules <- as.data.frame(rules)
    combination_item <- cbind(rules,x)
    return(list(rules_backup, combination_item))
  }
  else if(method=="eclat"){
    writeLines("Item Info in Transaction")
    print(head(transaction@itemInfo, n=12))
    tr <- itemFrequency(transaction, type="absolute")
    tr <- sort(tr, decreasing=TRUE)
    name <- names(tr)
    freq <- data.frame(tr)[[1]]
    
    min_support <- min(freq)/sum(freq)
    max_support <- max(freq)/sum(freq)
    writeLines(paste("Minimum Support found on Transaction:", min_support))
    writeLines(paste("Maximum Support found on Transaction:", max_support))
    
    groceries_df <- data.frame(name, freq)
    
    writeLines("1) Top 10 Transactions ")
    trtop10 <- sort(tr, decreasing=TRUE)[1:10]
    name <- names(trtop10)
    freq <- data.frame(trtop10)[[1]]
    groceries_df_top <- data.frame(name, freq)
    
    writeLines("2) Bottom 10 Transactions ")
    trbottom10 <- sort(tr)[1:10]
    name <- names(trbottom10)
    freq <- data.frame(trbottom10)[[1]]
    groceries_df_bottom <- data.frame(name, freq)
    
    colors = c("#ffc6c2","#ffdaa6","#fff9a6","#ecffa6","#d1ffa6","#a9ffa6","#a6ffc4","#a6ffdb","#a6ffff",
               "#a6deff","#a6caff","#a6a7ff","#b9a6ff","#d7a6ff","#f0a6ff","#fca6ff","#ffa6ea","#ffa6c1",
               "#ffa6b3","#ffa6b5")
    
    x11()
    writeLines("3) Eclat Rules Validation ")
    rules_eclat = eclat(data = transaction, parameter = list(support = support_lvl, 
                                                             minlen = sequence_min, 
                                                             maxlen=sequence_max))
    rules_backup <- rules_eclat
    x11()
    plot(rules_eclat)
    x11()
    plot(rules_eclat[1:top_n], method = "graph", control = list(type = "items"))
    x11()
    plot(rules_eclat[1:top_n], method = "paracoord", control = list(reorder = TRUE))
    
    eclat_inspection <- inspect(head(rules_eclat, by="support", n=top_n) , rulesep="=>")
    
    #Konversi ke DF
    rules_eclat <- unclass(head(rules_eclat, by="support", n=length(rules_eclat)))
    x <- attr(rules_eclat, "quality")
    items <- as(as(attr(rules_eclat, "items"), "transactions"), "data.frame")$items
    items <- as.character(items)
    combination_item <- cbind(items,x)
    return(list(rules_backup, combination_item))
  }
}

#------------------------------------------------------------------------------------------------------------------------
####################################### 8) Cluster Analysis #############################################################
#------------------------------------------------------------------------------------------------------------------------

types_of_clustering <- function(){
  writeLines("=========================== Source: https://en.wikipedia.org/wiki/Cluster_analysis =======================================")
  writeLines("1) Connectivity-based clustering (hierarchical clustering) -> Dendogram, Agnes (Agglomerative Nesting), Diana (Divisive Analysis)")
  writeLines("2) Centroid-based clustering -> KMeans, KMedoids, KMedian (Searching based on the nearest k centroid)")
  writeLines("3) Distribution-based clustering -> EM algorithm (Expectation Maximization) Clusters be defined as objects belonging most likely to the same distribution")
  writeLines("4) Density-based clustering -> DBSCAN algorithm (only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius)")
  writeLines("5) Grid Based Clustering -> STING and CLIQUE algorithm (used for a multi-dimensional data set)")
  writeLines("==========================================================================================================================")
}

library(datasets)
library(mlbench)
data('npk')
data('infert')
data('Sonar')
data('Shuttle')

#--------------------------------- 1) Connectivity-based Clustering (hierarchical clustering) ------------------------------------
#try to improve source: http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning

optimal_k_hierarchial_cluster <- function(df){
  library(NbClust)
  table_recommendation_list <- list()
  writeLines("1) Optimal Cluster Suggestion from NbClust with Single Linkage")
  x11()
  nb_clust_recommendation <- NbClust(df, method="single")
  table_recommendation_list <- c(table_recommendation_list, list(table(nb_clust_recommendation$Best.nc[1,])))
  print(nb_clust_recommendation)
  writeLines("2) Optimal Cluster Suggestion from NbClust with Complete Linkage")
  x11()
  nb_clust_recommendation <- NbClust(df, method="complete")
  table_recommendation_list <- c(table_recommendation_list, list(table(nb_clust_recommendation$Best.nc[1,])))
  print(nb_clust_recommendation)
  writeLines("3) Optimal Cluster Suggestion from NbClust with Average Linkage")
  x11()
  nb_clust_recommendation <- NbClust(df, method="average")
  table_recommendation_list <- c(table_recommendation_list, list(table(nb_clust_recommendation$Best.nc[1,])))
  print(nb_clust_recommendation)
  writeLines("4) Optimal Cluster Suggestion from NbClust with Ward.D Linkage")
  x11()
  nb_clust_recommendation <- NbClust(df, method="ward.D")
  table_recommendation_list <- c(table_recommendation_list, list(table(nb_clust_recommendation$Best.nc[1,])))
  print(nb_clust_recommendation)
  writeLines("5) Optimal Cluster Suggestion from NbClust with Ward.D2 Linkage")
  x11()
  nb_clust_recommendation <- NbClust(df, method="ward.D2")
  table_recommendation_list <- c(table_recommendation_list, list(table(nb_clust_recommendation$Best.nc[1,])))
  print(nb_clust_recommendation)
  writeLines("6) Optimal Cluster Suggestion from NbClust with Centroid Linkage")
  x11()
  nb_clust_recommendation <- NbClust(df, method="centroid")
  table_recommendation_list <- c(table_recommendation_list, list(table(nb_clust_recommendation$Best.nc[1,])))
  print(nb_clust_recommendation)
  writeLines("7) Optimal Cluster Suggestion from NbClust with Mcquitty Linkage")
  x11()
  nb_clust_recommendation <- NbClust(df, method="mcquitty")
  table_recommendation_list <- c(table_recommendation_list, list(table(nb_clust_recommendation$Best.nc[1,])))
  print(nb_clust_recommendation)
  
  writeLines("1) Cluster Proposal from NbClust with Single Linkage Method")
  print(table_recommendation_list[[1]])
  writeLines(paste0("Cluster Conclusion : ", as.numeric(which(table_recommendation_list[[1]] == max(table_recommendation_list[[1]])))))
  writeLines("2) Cluster Proposal from NbClust with Single Complete Method")
  print(table_recommendation_list[[2]])
  writeLines(paste0("Cluster Conclusion : ", as.numeric(which(table_recommendation_list[[2]] == max(table_recommendation_list[[2]])))))
  writeLines("3) Cluster Proposal from NbClust with Single Average Method")
  print(table_recommendation_list[[3]])
  writeLines(paste0("Cluster Conclusion : ", as.numeric(which(table_recommendation_list[[3]] == max(table_recommendation_list[[3]])))))
  writeLines("4) Cluster Proposal from NbClust with Single Ward D Method")
  print(table_recommendation_list[[4]])
  writeLines(paste0("Cluster Conclusion : ", as.numeric(which(table_recommendation_list[[4]] == max(table_recommendation_list[[4]])))))
  writeLines("5) Cluster Proposal from NbClust with Single Ward D2 Method")
  print(table_recommendation_list[[5]])
  writeLines(paste0("Cluster Conclusion : ", as.numeric(which(table_recommendation_list[[5]] == max(table_recommendation_list[[5]])))))
  writeLines("6) Cluster Proposal from NbClust with Single Centroid Method")
  print(table_recommendation_list[[6]])
  writeLines(paste0("Cluster Conclusion : ", as.numeric(which(table_recommendation_list[[6]] == max(table_recommendation_list[[6]])))))
  writeLines("7) Cluster Proposal from NbClust with Single Mcqquitty Method")
  print(table_recommendation_list[[7]])
  writeLines(paste0("Cluster Conclusion : ", as.numeric(which(table_recommendation_list[[7]] == max(table_recommendation_list[[7]])))))
  return(table_recommendation_list)
}
infert_op_k <- optimal_k_hierarchial_cluster(infert[,-1])

#Available Distance Methods:
#1) euclidean 2) maximum 3) manhattan 4) canberra 5) binary 6) minkowski
#Available Linkage Connectivity:
#1) Single 2) Complete 3) Average 4) Centroid 5) Ward.d 6) Ward.d2 7) Mcquitty
hierarchial_agnes_diana_clustering <- function(df, distance_method="euclidean", 
                                               method_linkage="complete", k=3){
  library(cluster)    # clustering algorithms
  library(factoextra) # clustering algorithms & visualization
  library(gridExtra)  # Grid Plot in one view
  library(corrplot)
  
  writeLines("1) Hierarchial Clustering needs to compute the distances of the variable to variable first")
  writeLines("Using Dissimilarity Matrix")
  # Dissimilarity matrix
  d <- dist(df, method = distance_method)
  
  # 1) Hierarchical clustering using Single Linkage
  hc_single <- hclust(d, method = "single")
  cophen_hc_single <- cophenetic(hc_single)
  correlation_single <- cor(cophen_hc_single, d)
  writeLines(paste0("Correlation Value of Hierarchical clustering Single Linkage with Distance = ", correlation_single))
  x11()
  plot(hc_single, title="Hierarchical clustering Correlation with Single Linkage")
  
  # 2) Hierarchical clustering using Complete Linkage
  hc_complete <- hclust(d, method = "complete")
  cophen_hc_complete <- cophenetic(hc_complete)
  correlation_complete <- cor(cophen_hc_complete, d)
  writeLines(paste0("Correlation Value of Hierarchical clustering Complete Linkage with Distance = ", correlation_complete))
  x11()
  plot(hc_complete, title="Hierarchical clustering Correlation with Complete Linkage")
  
  # 3) Hierarchical clustering using Average Linkage
  hc_average <- hclust(d, method = "average")
  cophen_hc_average <- cophenetic(hc_average)
  correlation_average <- cor(cophen_hc_average, d)
  writeLines(paste0("Correlation Value of Hierarchical clustering Average Linkage with Distance = ", correlation_average))
  x11()
  plot(hc_average, title="Hierarchical clustering Correlation with Average Linkage")
  
  # 4) Hierarchical clustering using Centroid Linkage
  hc_centroid <- hclust(d, method = "centroid")
  cophen_hc_centroid <- cophenetic(hc_centroid)
  correlation_centroid <- cor(cophen_hc_centroid, d)
  writeLines(paste0("Correlation Value of Hierarchical clustering Centroid Linkage with Distance = ", correlation_centroid))
  x11()
  plot(hc_centroid, title="Hierarchical clustering Correlation with Centroid Linkage")
  
  # 5) Hierarchical clustering using Ward D Linkage
  hc_ward1 <- hclust(d, method = "ward.D")
  cophen_hc_ward1 <- cophenetic(hc_ward1)
  correlation_hcward1 <- cor(cophen_hc_ward1, d)
  writeLines(paste0("Correlation Value of Hierarchical clustering Ward D Linkage with Distance = ", correlation_hcward1))
  x11()
  plot(hc_ward1, title="Hierarchical clustering Correlation with Ward D Linkage")
  
  # 6) Hierarchical clustering using Ward D2 Linkage
  hc_ward2 <- hclust(d, method = "ward.D2")
  cophen_hc_ward2 <- cophenetic(hc_ward2)
  correlation_hcward2 <- cor(cophen_hc_ward2, d)
  writeLines(paste0("Correlation Value of Hierarchical clustering Ward D2 Linkage with Distance = ", correlation_hcward2))
  x11()
  plot(hc_ward2, title="Hierarchical clustering Correlation with Ward D2 Linkage")
  
  # 7) Hierarchical clustering using Mcquitty Linkage
  hc_mcquitty <- hclust(d, method = "mcquitty")
  cophen_hc_mcquitty <- cophenetic(hc_mcquitty)
  correlation_mcquitty <- cor(cophen_hc_mcquitty, d)
  writeLines(paste0("Correlation Value of Hierarchical clustering Mcquitty Linkage with Distance = ", correlation_mcquitty))
  x11()
  plot(hc_mcquitty, title="Hierarchical clustering Correlation with Mcquitty Linkage")
  
  # Plot the obtained dendrogram
  x11()
  hc_single_plot = fviz_dend(hc_single, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Hierarchy Cluster Method Single')
  hc_complete_plot = fviz_dend(hc_complete, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Hierarchy Cluster Method Complete')
  hc_average_plot = fviz_dend(hc_average, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Hierarchy Cluster Method Average')
  hc_centroid_plot = fviz_dend(hc_centroid, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Hierarchy Cluster Method Centroid')
  hc_ward1_plot = fviz_dend(hc_ward1, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Hierarchy Cluster Method Ward D')
  hc_ward2_plot = fviz_dend(hc_ward2, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Hierarchy Cluster Method Ward D2')
  hc_mcquitty_plot = fviz_dend(hc_mcquitty, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Hierarchy Cluster Method McQuitty')
  grid.arrange(hc_single_plot, hc_complete_plot, hc_average_plot, 
               hc_centroid_plot, hc_ward1_plot, hc_ward2_plot, hc_mcquitty_plot,
               nrow=2, ncol=4)
  
  writeLines("2) Compute with agnes (Agglomerative Nesting)")
  writeLines("with the agnes function you can also get the agglomerative coefficient")
  writeLines("which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).")
  
  agnes_average <- agnes(df, method="average")
  agnes_weighted <- agnes(df, method="weighted")
  agnes_single <- agnes(df, method="single")
  agnes_complete <- agnes(df, method = "complete")
  agnes_ward <- agnes(df, method = "ward")
  print(paste0("Agglomerative Coefficient with Average method: ", round(agnes_average$ac,3)))
  print(paste0("Agglomerative Coefficient with Weighted Average method: ", round(agnes_weighted$ac,3)))
  print(paste0("Agglomerative Coefficient with Single method: ", round(agnes_single$ac,3)))
  print(paste0("Agglomerative Coefficient with Complete method: ", round(agnes_complete$ac,3)))
  print(paste0("Agglomerative Coefficient with Ward method: ", round(agnes_ward$ac,3)))
  
  x11()
  agnes_single_plot = fviz_dend(agnes_single, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Agnes Method Single')
  agnes_complete_plot = fviz_dend(agnes_complete, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Agnes Method Complete')
  agnes_average_plot = fviz_dend(agnes_average, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Agnes Method Average')
  agnes_weighted_plot = fviz_dend(agnes_weighted, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Agnes Method Weighted')
  agnes_ward_plot = fviz_dend(agnes_ward, cex = 0.5, k=k, color_labels_by_k = TRUE, type="rectangle") + ggtitle('Agnes Method Ward')
  grid.arrange(agnes_single_plot, agnes_complete_plot, agnes_average_plot, agnes_weighted_plot, agnes_ward_plot, nrow=2, ncol=3)
  
  writeLines("3) Compute with diana (Divisive Analysis)")
  x11()
  diana <- diana(df, metric='euclidean')
  diana_plot <- fviz_dend(diana, cex = 0.5, k=k, color_labels_by_k = TRUE) + ggtitle('Divisive Analysis Hieararchy Cluster')
  print(diana_plot)
  return(list(hc_single, hc_complete, hc_average, hc_centroid, hc_ward1, hc_ward2, hc_mcquitty,
              agnes_single, agnes_average, agnes_weighted, agnes_complete, agnes_ward, diana))
}

#Dendogram improvement using circlize and ddextend packages
#p value method dist = "correlation", "uncentered", "abscor"
custom_dendogram <- function(df, dendogram_title="Custom Dendogram", 
                             weight_col="", limit_observation=0, 
                             label_size=2, k_group=4,
                             label_color=c("green", "blue", "red", "yellow"),
                             node_color=c("#f691ff"), 
                             leaves_color=c("#70fffa","#ff5c26"),
                             branch_color=c("green", "blue", "red", "yellow"),
                             node_pch=19, node_size=2,
                             change_label_vector=c(), 
                             hclust_method="complete", 
                             draw_k_rect=0, abline_pointer=0,
                             circular_plot=TRUE, circular_method="circlize",
                             leaves_pch=18, leaves_size=2, p_value_calculate=TRUE,
                             p_value_method_dist = "correlation", p_value_bootstrap=50){
  library(dendextend)
  library(circlize)
  library(pvclust)
  library(dplyr)
  t <- colors()[-which(colors() %like% "grey")]
  t <- t[-which(t %like% "gray")]
  t <- t[-which(t %like% "white")]
  t <- t[-which(t %like% "light")]
  t <- t[-which(t %like% "thistle")]
  if(label_color == "" || branch_color == ""){
    label_color <- sample(t, k_group)
    branch_color <- label_color
    writeLines("Using Custom Color")
    print(label_color)
  }
  
  node_length <- nrow(df)
  dend_plot <- NULL
  
  x11()
  if(limit_observation > 0){
    if(draw_k_rect!=0){
      dend_plot <- df[1:limit_observation,] %>% 
        scale %>% 
        dist %>%  
        hclust(method = hclust_method) %>%  
        as.dendrogram %>% 
        set("labels_col", value = label_color, k=k_group) %>% 
        set("labels_cex", label_size) %>% # Label Size
        set("nodes_pch", node_pch) %>%  # node point type
        set("nodes_cex", node_size) %>%  # node point size
        set("nodes_col", node_color) %>%  # node color
        set("leaves_pch", leaves_pch) %>%  # node point type
        set("leaves_cex", leaves_size) %>%  # node point size
        set("leaves_col", leaves_color) %>% #node point color# node point color
        set("branches_k_color", value = branch_color, k=k_group)
      
      dend_plot %>%
        rect.dendrogram(k=k_group, border = 8, lty = 5, lwd = 2) %>%
        plot(main = dendogram_title)
      abline(h = abline_pointer, lty = 2)
      
    }
    else{
      dend_plot <- df[1:limit_observation,] %>% 
        scale %>% 
        dist %>%  
        hclust(method = hclust_method) %>%  
        as.dendrogram %>% 
        set("labels_col", value = label_color, k=k_group) %>% 
        set("labels_cex", label_size) %>% # Label Size
        set("nodes_pch", node_pch) %>%  # node point type
        set("nodes_cex", node_size) %>%  # node point size
        set("nodes_col", node_color) %>%  # node color
        set("leaves_pch", leaves_pch) %>%  # node point type
        set("leaves_cex", leaves_size) %>%  # node point size
        set("leaves_col", leaves_color) %>%  # node point color
        set("branches_k_color", value = branch_color, k=k_group) 
      
      dend_plot %>% plot(main = dendogram_title)
      abline(h = abline_pointer, lty = 2)
    }
  }
  else{
    if(draw_k_rect!=0){
      dend_plot <- df %>% 
        scale %>% 
        dist %>%  
        hclust(method = hclust_method) %>%  
        as.dendrogram %>% 
        set("labels_col", value = label_color, k=k_group) %>% 
        set("labels_cex", label_size) %>% # Label Size
        set("nodes_pch", node_pch) %>%  # node point type
        set("nodes_cex", node_size) %>%  # node point size
        set("nodes_col", node_color) %>%  # node color
        set("leaves_pch", leaves_pch) %>%  # node point type
        set("leaves_cex", leaves_size) %>%  # node point size
        set("leaves_col", leaves_color) %>% #node point color# node point color
        set("branches_k_color", value = branch_color, k=k_group) 
      
      dend_plot %>%
        rect.dendrogram(k=k_group, border = 8, lty = 5, lwd = 2) %>%
        plot(main = dendogram_title)
      abline(h = abline_pointer, lty = 2)
    }
    else{
      dend_plot <- df %>% 
        scale %>% 
        dist %>%  
        hclust(method = hclust_method) %>%  
        as.dendrogram %>% 
        set("labels_col", value = label_color, k=k_group) %>% 
        set("labels_cex", label_size) %>% # Label Size
        set("nodes_pch", node_pch) %>%  # node point type
        set("nodes_cex", node_size) %>%  # node point size
        set("nodes_col", node_color) %>%  # node color
        set("leaves_pch", leaves_pch) %>%  # node point type
        set("leaves_cex", leaves_size) %>%  # node point size
        set("leaves_col", leaves_color) %>%  #node point color# node point color
        set("branches_k_color", value = branch_color, k=k_group) 
      
      dend_plot %>% plot(main = dendogram_title)
      abline(h = abline_pointer, lty = 2)
    }
  }
  
  if(p_value_calculate){
    result <- pvclust(df, method.dist=p_value_method_dist, 
                      method.hclust=hclust_method, nboot=p_value_bootstrap)
    # with pvrect
    x11()
    plot(result)
    pvrect(result)
  }
  
  min_max_norm <- function(x) {
    return((x - min(x)) / (max(x) - min(x)))
  }
  
  label_dendogram <- labels(dend_plot)
  weight_value <- c()
  writeLines('Label Dendogram')
  print(head(label_dendogram))
  if(weight_col != ""){
    writeLines(paste0("Using Weight column: ", weight_col))
    weight_value <- df[which(rownames(df) %in% label_dendogram), which(colnames(df) %in% weight_col)]
    weight_value <- min_max_norm(weight_value)
    writeLines("Converted Weight Values: ")
    print(head(weight_value))
  }
  
  if(circular_plot){
    x11()
    if(circular_method=="circlize"){
      writeLines("Make Plain Convert of Circlize Dendogram")
      circlize <- circlize_dendrogram(dend_plot)
      if(weight_col != ""){
        x11()
        writeLines("Make Summary of weight columns: ")
        print(summary(df[[weight_col]]))
        writeLines("Make Histogram for each node with rectangle")
        circos.initialize("circos_dendogram", xlim = c(0, node_length))
        circos.track(ylim = c(0, 1), panel.fun = function(x, y) {
          circos.rect(1:node_length-0.8, rep(0, node_length), 1:node_length-0.2, weight_value, 
                      col = rand_color(node_length), border = NA)
        }, bg.border = NA)
        
        writeLines("Make Label Node for each Circlos")
        circos.track(ylim = c(0, 1), panel.fun = function(x, y) {
          circos.text(1:node_length-0.5, rep(0, node_length), label_dendogram, 
                      col = labels_colors(dend), facing = "clockwise", 
                      niceFacing = TRUE, adj = c(0, 0.5))
        }, bg.border = NA, track.height = 0.1)
        
        writeLines("Make the main plot inside label and histogram plot") 
        max_height = attr(dend_plot, "height")
        circos.track(ylim = c(0, max_height), panel.fun = function(x, y) {
          circos.dendrogram(dend_plot, max_height = max_height)
        }, track.height = 0.5, bg.border = NA)
      }
      else{
        x11()
        writeLines("Make Label Node for each Circlos")
        circos.initialize("circos_dendogram", xlim = c(0, node_length))
        circos.track(ylim = c(0, 1), panel.fun = function(x, y) {
          circos.text(1:node_length-0.5, rep(0, node_length), label_dendogram, 
                      col = labels_colors(dend_plot), facing = "clockwise", 
                      niceFacing = TRUE, adj = c(0, 0.5))
        }, bg.border = NA, track.height = 0.1)
        
        writeLines("Make the main plot inside label and histogram plot") 
        max_height = attr(dend_plot, "height")
        circos.track(ylim = c(0, max_height), panel.fun = function(x, y) {
          circos.dendrogram(dend_plot, max_height = max_height)
        }, track.height = 0.5, bg.border = NA)
      }
    }
    else if(circular_method == "ggplot"){
      library(ggplot2)
      ggd <- as.ggdend(dend_plot)
      x11()
      gg_dendogram <- ggplot(ggd, labels = FALSE) + 
        scale_y_reverse(expand = c(0.2, 0)) +
        coord_polar(theta="x")
      print(gg_dendogram)
      return(list(dend, dend_plot, gg_dendogram))
    }
  }
  print(dend_plot)
  return(dend_plot)
}

#custom_dendogram_source_code <- custom_dendogram(source_code_trxdata, dendogram_title="Source Code Dendogram",
#                                                 k_group=16, label_color="",branch_color="")
#custom_dendogram_merchant_tag <- custom_dendogram(merchant_tag_trxdata, dendogram_title="Merchant Tag Dendogram",
#                                                  k_group=16, label_color="",branch_color="", label_size=0.5)

#------------------------------ Example usage of Connectivity Based Clustering -----------------------------------
infert_agnes_diana <- hierarchial_agnes_diana_clustering(infert[,-1], k=2)

#------------------------------ Example of Custom Dendogram using ddextend and circlize packages -------------------------
USArrests_dendogram <- custom_dendogram(USArrests, weight_col="Rape", circular_plot = FALSE)
USArrests_dendogram_circle_ggplot <- custom_dendogram(USArrests, weight_col="Rape", circular_plot = TRUE, circular_method = "ggplot")
USArrests_dendogram_circle_circlize <- custom_dendogram(USArrests, weight_col="Rape", 
                                                        circular_plot = TRUE, circular_method = "circlize")

#--------------------------------- 2) Centroid-based Clustering -------------------------------------------------------------------

#available method
#1) k-means
#2) k-medians
#3) k-medoids
#For a Fastest Analysis Choose deep_optimal = FALSE, and check_cluster_tendency = FALSE
#Check Cluster Tendency is very slow at operational, reuqires O(2) Computation Time
#deep_optimal == TRUE, Make All Analysis from Elbow Method, Average Silhouette, Gap Statistic, and NB Clust Recommendation
#deep_optimal == FALSE, Make NB Clust Recommendation Only 
optimal_k_centroid_clustering <- function(df, method="k-means", kmin=2, kmax=10, 
                                 bootstrap_iter=50, check_cluster_tendency=FALSE,
                                 deep_optimal = FALSE){
  
  x11()
  start_time <- Sys.time()
  library(stats)
  library(tidyverse) # data manipulation
  library(cluster) # clustering algorithms
  library(factoextra) # clustering algorithms & visualization
  library(gridExtra) # Grid Plot in one view
  library(Gmedian) #For K-median using MacQueen Algorithms
  library(skmeans) #For K-median
  library(pryr) # Helping save a plot to an object
  library(cluster) #For k-medoids using PAM 
  library(grid) #For Gridding model into one plot
  library(ggplot2) #For Plotting GGPlot Diagram
  library(gridBase) #For Gridding model into one plot
  library(mclust)
  library(NbClust)
  
  if(check_cluster_tendency == TRUE)
  {
    start_cluster_tendency <- Sys.time()
    writeLines("Measuring Cluster Tendency with Hopkins Statistic")
    writeLines("Reject H0 if Hopkins Statistic Over 0.5")
    writeLines("H0: structures in the data set are not random and real structures in the data is clusterable")
    writeLines("H1: structures in the data set are random and real structures in the data is not clusterable")
    tendency_statistic <- get_clust_tendency(df, graph=FALSE, n= round(0.9 * nrow(df)), 
                                             seed=123)
    print(tendency_statistic) 
    end_cluster_tendency <- Sys.time()
    writeLines(paste0("Time Execution Checking Cluster Tendency for Dataframe with Rows = ",nrow(df), "and Columns = ",ncol(df)))
    print(end_cluster_tendency - start_cluster_tendency)
  }
  if(method=="k-means"){
    if(deep_optimal == TRUE){
      set.seed(123)
      writeLines("1) Optimal Cluster by Elbow Method")
      fviz_nbclust(df, kmeans, method = "wss")
      op1 = fviz_nbclust(df, kmeans, method = "wss", 
                         k.max = kmax)
      
      writeLines("2) Optimal Cluster by Silhouette")
      fviz_nbclust(df, kmeans, method = "silhouette")
      op2= fviz_nbclust(df, kmeans, method = "silhouette", 
                        k.max = kmax)
      
      writeLines("3) Optimal Cluster by Gap Statistic")
      #K.max -> Max Cluster to test, B -> Sample Bootstrap
      gap_stat <- clusGap(df, FUN = kmeans,
                          K.max = kmax, B = bootstrap_iter)
      
      writeLines("4) Optimal Cluster Suggestion from NbClust using K-Means")
      nb_clust_recommendation <- NbClust(df, method="kmeans", min.nc=kmin, max.nc=kmax)
      print(nb_clust_recommendation)
      
      # Print the result
      print(gap_stat, method = "firstmax")
      fviz_gap_stat(gap_stat)
      op3 = fviz_gap_stat(gap_stat)
      
      op1$labels$title <- "Optimal Clusters by Elbow Method"
      op2$labels$title <- "Optimal Clusters by Average Silhouette" 
      op3$labels$title <- "Optimal Clusters by Gap Statistic" 
      print(grid.arrange(op1, op2, op3, ncol = 3))
      end_time <- Sys.time()
      writeLines(paste0("Time Execution for Dataframe with Rows = ",nrow(df), "and Columns = ",ncol(df)))
      print(end_time - start_time)
      return(list(op1, op2, op3, nb_clust_recommendation))
    }
    else if(deep_optimal == FALSE){
      writeLines("Optimal Cluster Suggestion from NbClust using K-Means")
      nb_clust_recommendation <- NbClust(df, method="kmeans", min.nc=kmin, max.nc=kmax)
      print(nb_clust_recommendation)
      end_time <- Sys.time()
      writeLines(paste0("Time Execution for Dataframe with Rows = ",nrow(df), "and Columns = ",ncol(df)))
      print(end_time - start_time)
      return(nb_clust_recommendation)
    }
    
  }
  else if(method=="k-medians"){
    if(deep_optimal==TRUE){
      set.seed(123)
      writeLines("1) Optimal Cluster by Average Silhouette")
      
      # function to compute average silhouette for k clusters
      avg_sil <- function(k) {
        km.res <- skmeans(df, k)
        ss <- silhouette(km.res$cluster, dist(df))
        mean(ss[, 3])
      }
      
      k.values <- kmin:kmax
      
      plot.new()
      grid.newpage()
      pushViewport(viewport(layout = grid.layout(1, 2)))
      
      # extract avg silhouette for 2-15 clusters
      avg_sil_values <- map_dbl(k.values, avg_sil)
      
      pushViewport(viewport(layout.pos.col = 1))
      par(fig = gridFIG(), new = TRUE)
      plot(k.values, avg_sil_values,
           type = "b", pch = 19, frame = FALSE, 
           xlab = "Number of clusters K",
           ylab = "Average Silhouettes",
           main = "Optimal Cluster by Average Silhouette")
      popViewport()
      
      writeLines("2) Optimal Cluster by Gap Statistic")
      gap_stat <- clusGap(df, FUN = skmeans,
                          K.max = kmax, B = bootstrap_iter)
      # Print the result
      print(gap_stat, method = "firstmax")
      op2 = fviz_gap_stat(gap_stat)
      
      pushViewport(viewport(layout.pos.col = 2))
      op2$labels$title <- "Optimal Cluster by Gap Statistic"
      print(op2, newpage = FALSE)
      popViewport()
      
      writeLines("3) Optimal Cluster Suggestion from NbClust using K-Median")
      nb_clust_recommendation <- NbClust(df, method="median", min.nc=kmin, max.nc=kmax)
      print(nb_clust_recommendation)
      end_time <- Sys.time()
      writeLines(paste0("Time Execution for Dataframe with Rows = ",nrow(df), "and Columns = ",ncol(df)))
      print(end_time - start_time)
      return(list(op1, op2, nb_clust_recommendation))
    }
    else if(deep_optimal==FALSE){
      writeLines("Optimal Cluster Suggestion from NbClust using K-Median")
      nb_clust_recommendation <- NbClust(df, method="median", min.nc=kmin, max.nc=kmax)
      print(nb_clust_recommendation)
      end_time <- Sys.time()
      writeLines(paste0("Time Execution for Dataframe with Rows = ",nrow(df), "and Columns = ",ncol(df)))
      print(end_time - start_time)
      return(nb_clust_recommendation)
    }
  }
  else if(method=="k-medoids"){
    set.seed(123)
    writeLines("1) Optimal Cluster by Average Silhouette")
    fviz_nbclust(df, pam, method = "silhouette")
    op1= fviz_nbclust(df, pam, method = "silhouette")
    
    writeLines("2) Optimal Cluster by Gap Statistic")
    gap_stat <- clusGap(df, FUN = pam,
                        K.max = kmax, B = bootstrap_iter)
    # Print the result
    print(gap_stat, method = "firstmax")
    fviz_gap_stat(gap_stat)
    op2 = fviz_gap_stat(gap_stat)
    
    op1$labels$title <- "Optimal Clusters by Average Silhouette"
    op2$labels$title <- "Optimal Clusters by Gap Statistic" 
    print(grid.arrange(op1, op2, ncol = 2))
    end_time <- Sys.time()
    writeLines(paste0("Time Execution for Dataframe with Rows = ",nrow(df), "and Columns = ",ncol(df)))
    print(end_time - start_time)
    return(list(op1, op2))
  }
}

#------------------------------ Example usage of optimal cluster searching -------------------------------
infert_op_cent_clust_kmeans <- optimal_k_centroid_clustering(infert[,-1], "k-means")
infert_op_cent_clust_kmedians <- optimal_k_centroid_clustering(infert[,-1], "k-medians")
infert_op_cent_clust_kmedoids <- optimal_k_centroid_clustering(infert[,-1], "k-medoids")

k_clustering_plot <- function(df, method="k-means", k=3, label=""){
  library(ggpubr)
  library(ggrepel)
  library(factoextra)
  library(skmeans)
  library(cluster)
  
  plot_cluster = function(df, clust, individual_label="", method="k-means"){
    if(ncol(df) > 2){
      axes=c(1,2)
      pca <- stats::prcomp(df, scale = FALSE, center = FALSE)
      ind <- facto_summarize(pca, element = "ind", result = "coord", axes = c(1,2))
      ind$cluster <- as.factor(clust$cluster)
      eig <- get_eigenvalue(pca)[axes, 2]
      xlab = paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
      ylab = paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
      if(individual_label!=""){
        plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                  show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                  ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                  pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                  xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label), segment.color = clust$cluster, segment.colour = clust$cluster)
        print(plot)
        return(plot)
      }
      else{
        plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                  show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                  ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                  pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                  xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)), segment.color = clust$cluster, segment.colour = clust$cluster)
        print(plot)
        return(plot)
      }
    }
    else if (ncol(df) == 2){
      ind <- as.data.frame(df)
      ind$cluster <- as.factor(clust$cluster)
      xlab <- colnames(ind)[1]
      ylab <- colnames(ind)[2]
      if(individual_label!=""){
        plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                  show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                  ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                  pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                  xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label), segment.color = clust$cluster, segment.colour = clust$cluster)
        print(plot)
        return(plot)
      }
      else{
        plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                  show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                  ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                  pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                  xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)), segment.color = clust$cluster, segment.colour = clust$cluster)
        print(plot)
        return(plot)
      }
    }
    else{
      print("You Cannot Plot Cluster with Univariate Data!")
    }
  }
  
  x11()
  if(method=="k-means"){
    cluster <- kmeans(df, k)
    if(label!=""){
      plot_cluster(df, cluster, label, method="k-means")
    }
    else{
      plot_cluster(df, cluster, method="k-means")
    }
  }
  else if(method=="k-medians"){
    cluster <- skmeans(as.matrix(df), k)
    if(label!=""){
      plot_cluster(df, cluster, label, method="k-medians")
    }
    else{
      plot_cluster(df, cluster, method="k-medians")
    }
  }
  else if(method=="k-medoids"){
    cluster <- pam(df, k)
    if(label!=""){
      plot_cluster(df, cluster, label, method="k-medoids")
    }
    else{
      plot_cluster(df, cluster, method="k-medoids")
    }
  }
  return(cluster)
}

#------------------------------ Example usage of centroid cluster -------------------------------
infert_cluster_kmeans <- k_clustering_plot(infert[,-1], "k-means", 2)
infert_cluster_kmedians <- k_clustering_plot(infert[,-1], "k-medians", 2)
infert_cluster_kmedoids <- k_clustering_plot(infert[,-1], "k-medoids", 2)

#--------------------------------- 3) Distribution-based Clustering -------------------------------------------------------------
#improvements: https://bradleyboehmke.github.io/HOML/model-clustering.
EM_clustering <- function(df, uncertainty_filter_prob=0.25){
  library(mclust)
  library(dplyr)
  writeLines("The EM algorithm consists of 3 major steps:")
  writeLines("1) Initialization")
  writeLines("2) Expectation (E-step)")
  writeLines("3) Maximization (M-step)")
  
  writeLines("This Clustering method assumes that data is normally distributed multivariate")
  writeLines("Optimal Covariance Structures is selected either from max BIC or ICL")
  writeLines("Smaller Size Clusters = more Compact Cluster, Largest Size Clusters = less Compact Clusters")
  writeLines("==========================Covariance Matrix Structures:=============================")
  writeLines("E -> Univariate Volume=Equal")
  writeLines("V -> Univariate Volume=Variable")
  writeLines("EII -> Spherical Volume=Equal, Shape=Equal")
  writeLines("VII -> Spherical Volume=Variable, Shape=Equal")
  writeLines("EEI -> Diagonal Volume=Equal, Shape=Equal, Orientation=Coordinate Axes")
  writeLines("VEI -> Diagonal Volume=Variable, Shape=Equal, Orientation=Coordinate Axes")
  writeLines("EVI -> Diagonal Volume=Equal, Shape=Variable, Orientation=Coordinate Axes")
  writeLines("VVI -> Diagonal Volume=Variable, Shape=Variable, Orientation=Coordinate Axes")
  writeLines("EEE -> Ellipsoidal/General Volume=Equal, Shape=Equal, Orientation=Equal")
  writeLines("EEV -> Ellipsoidal/General Volume=Equal, Shape=Equal, Orientation=Variable")
  writeLines("VEV -> Ellipsoidal/General Volume=Variable, Shape=Equal, Orientation=Variable")
  writeLines("VVV -> Ellipsoidal/General Volume=Variable, Shape=Variable, Orientation=Variable")
  writeLines("====================================================================================")
  writeLines("========= These constraints can be one or more of the following: ===================")
  writeLines("1) Volume: each cluster has approximately the same number of observations;")
  writeLines("2) Shape: each cluster has approximately the same variance so that the distribution is spherical;")
  writeLines("3) Orientation: each cluster is forced to be axis-aligned.")
  writeLines("====================================================================================")
  
  #EM Algorithm to fit finite Gaussian Mixture Model
  em_clust = Mclust(df)
  x11()
  plot(em_clust, legendArgs = list(x = "bottomleft"), what="BIC")  
  x11()
  plot(em_clust, legendArgs = list(x = "topright"), what="classification")
  x11()
  plot(em_clust, legendArgs = list(x = "topright"), what="uncertainty")
  x11()
  plot(em_clust, legendArgs = list(x = "topright"), what="density")
  
  #EM Algorithm to fit finite Gaussian Mixture Model with BIC Criterion Parameter
  em_bic_clust = mclustBIC(df)
  summary(em_bic_clust)
  x11()
  plot(em_bic_clust, legendArgs = list(x = "bottomleft"), 
       title="EM Algorithm to fit finite Gaussian Mixture Model with BIC Criterion Parameter")   
  
  #EM Algorithm to fit finite Gaussian Mixture Model with ICL Criterion Parameter
  em_icl_clust = mclustICL(df)
  summary(em_icl_clust)
  x11()
  plot(em_icl_clust, legendArgs = list(x = "bottomleft"),
       title="EM Algorithm to fit finite Gaussian Mixture Model with ICL Criterion Parameter")  
  
  probability_cluster_sum <- colSums(em_clust$z)
  
  writeLines('Display Probability Distribution of Cluster')
  probabilities <- em_clust$z 
  colnames(probabilities) <- paste0('C', 1:max(em_clust$classification))
  
  probabilities <- probabilities %>%
    as.data.frame() %>%
    mutate(id = row_number()) %>%
    tidyr::gather(cluster, probability, -id)
  
  x11()
  prob_plot <- ggplot(probabilities, aes(probability)) +
    geom_histogram() +
    facet_wrap(~ cluster, nrow = 2)
  print(prob_plot)
  
  writeLines("======================= Conclusion from EM Cluster Analysis ====================================================")
  writeLines(paste0("1) Optimal Covariance Matrix Structure is: ", em_clust$modelName))
  writeLines(paste0("2) Total Cluster: ", max(em_clust$classification)))
  writeLines(paste0("3) Cluster member with the most Uncertainty (Weak Membership): ", which(probability_cluster_sum == max(probability_cluster_sum))))
  writeLines(paste0("4) Cluster member with the less Uncertainty (Strong Membership): ", which(probability_cluster_sum == min(probability_cluster_sum))))
  return(em_clust)
}

#------------------------------ Example usage of EM cluster/ Distribution Based Cluster -------------------------------
infert_em_cluster <- EM_clustering(infert[,-1])

#--------------------------------- 4) Density-based Clustering ------------------------------------------------------------------
optimal_epsilon_dbscan <- function(df, paint_line=-1, nk=10){
  library(dbscan)
  writeLines("Method for determining the optimal eps value")
  writeLines("How to see it: See a point where it acts a checkpoint before a sharp change of distance occurs")
  x11()
  print(dbscan::kNNdistplot(df, k = nk))
  if(paint_line!=-1){
    abline(h = paint_line, lty = 2) 
  }
}

dbscan_cluster <- function(df, epsilon, nk=10){
  library(fpc)
  library(dbscan)
  library("factoextra")
  
  # Compute DBSCAN using fpc package
  set.seed(123)
  db <- fpc::dbscan(df, eps = epsilon, MinPts = nk)
  db2 <- dbscan::dbscan(df, epsilon, nk)
  
  library("factoextra")
  fviz_cluster(db, data = df, stand = FALSE,
               ellipse = FALSE, show.clust.cent = FALSE,
               geom = "point",palette = "jco", ggtheme = theme_classic())
  
  writeLines("Summary from DBSCAN from FPC Packages")
  print(db)
  writeLines("Summary from DBSCAN from DBSCAN Packages")
  print(db2)
  
  writeLines("Is Both Version From FPC and DBScan Package have a same value of seed in calculation?")
  print(table(db$cluster) == table(db2$cluster))
  
  return(list(db, db2))
}

#------------------------------ Example Usage of Density Based Clustering ----------------------------------------
infert_op_k_dbscan <- optimal_epsilon_dbscan(infert[,-1], 11) #epsilon value of 11
infert_dbscan <- dbscan_cluster(infert[,-1], 11)

#--------------------------------- 5) Grid Based Clustering ---------------------------------------------------------------------

#this type of clustering is specifically made for data with big features (multidimensional data)
total_object_subspace_clustered <- function(obj, df_ref){
  xi <- 0
  for(j in 1:length(obj)){
    xi <- xi + length(obj[[j]]$objects)
  }
  if(xi != nrow(df_ref)){
    writeLines(paste0("Row Clustered by Sub Spaces: ", xi))
    writeLines(paste0("Total Row in DF: ",nrow(df_ref)))
    not_clustered <- nrow(df_ref) - xi
    writeLines(paste0("Row Not Clustered: ", not_clustered))
    writeLines(paste0("Percentages Not Clustered: ", round(not_clustered/nrow(df_ref) * 100, 4), "%"))
  }
}

#interval_cluster = integer,
#nk -> KNN number

#------------------------------ Subspace cluster plot cannot work properly for now --------------------------------------------
subspace_cluster <- function(df, interval_cluster=10, density_threshold=0.01,
                             poisson_threshold=1, nk=10, average_dimension_number=3,
                             epsilon_DBSCAN=1){
  library(subspace)
  writeLines("===================================================================================================================")
  writeLines("======================== Using Subspace Method for Clustering, known 5 Algorithms is ==============================")
  writeLines("===================================================================================================================")
  writeLines("")
  writeLines("CLIQUE Algorithm is a grid based clustering that finds")
  writeLines("clusters by first dividing each dimension into xi equal-width intervals")
  writeLines("and saving those intervals where the density is greater than tau as clusters")
  writeLines("")
  writeLines("FIRES Algorithm follows a three phase frameworks")
  writeLines("1) base-clusters are generated using a clustering-algorithm on each dimension in isolation")
  writeLines("2) Then these base-clusters are merged in a second phase to find multidimensional cluster-approximations")
  writeLines("3) These approximations are then refined in the third phase")
  writeLines("")
  writeLines("P3c Algorithm use statistical distributions for the task of finding clusters, that works in these steps:")
  writeLines("1) To this end each dimension is first split into 1+log_2(nrow(data)) bins and the chi-square test is used to compute the probability that the sizes of these bins are uniformly distributed")
  writeLines("2) f this probability is bigger than 1-ChiSquareAlpha, nothing happens. Otherwise the largest bins will be removed until this is the case")
  writeLines("3) The bins that were removed in this way are then used to find clusters. To this end, bins that are adjacent are merged.")
  writeLines("Then clusters are formed by taking a bin from one dimension and determining the probability of sharing as many points as it does with each bin from another dimension")
  writeLines("4) The bin is then intersected with the bin from another dimension where this probability is the lowest")
  writeLines("given that it is at least lower than 1.00E-Poisson Threshold and this is repeated until no such bin is found.")
  writeLines("")
  writeLines("ProClus Algorithm works in a manner similar to K-Medoids")
  writeLines("")
  writeLines("SubClu Algorithm follows a bottom-up framework")
  writeLines("1) in which one-dimensional clusters are generated with DBSCAN and then each cluster is expanded one dimension at a time into a dimension that is known to have a cluster that only differs in one dimension from this cluster")
  writeLines("2) This expansion is done using DBSCAN with the same parameters that were used for the original DBSCAN that produced the clusters.")
  
  a <- CLIQUE(df,xi=round(nrow(df) * interval_cluster), tau=density_threshold)
  print(a)
  b <- FIRES(df)
  print(b)
  c <- P3C(df,PoissonThreshold=poisson_threshold)
  print(c)
  d <- ProClus(df, k=nk, d=average_dimension_number)
  print(d)
  e <- SubClu(df, epsilon=epsilon_DBSCAN, minSupport=nk)
  print(e)
  return(list(a,b,c,d,e))
}

library(mlbench)
data("Sonar")
sonar_op_epsilon <- optimal_epsilon_dbscan(Sonar[,-61]) 
sonar_op_epsilon <- optimal_epsilon_dbscan(Sonar[,-61], paint_line=1.39) #known optimal epsilon value of 1.39
sonar_subspace_cluster <- subspace_cluster(Sonar[,-61], epsilon_DBSCAN = 1.39, interval_cluster=5,
                                            density_threshold = 0.001,
                                            average_dimension_number = 7, 
                                            poisson_threshold=3)

nhanes_epsilon <- optimal_epsilon_dbscan(nhanes_clean) 
nhanes_epsilon <- optimal_epsilon_dbscan(nhanes_clean, paint_line=5600) #known optimal epsilon value of 5600
nhanes_subspace_cluster <- subspace_cluster(nhanes, epsilon_DBSCAN = 5600, interval_cluster=5,
                                           density_threshold = 0.01,
                                           average_dimension_number = 4, 
                                           poisson_threshold=3)

#-------------------------------------------------------------------------------------------------------------------------
########################################### 9) Bayesian Model Analysis ###################################################
#-------------------------------------------------------------------------------------------------------------------------
row_wise_pattern_count <- function(df, col_search, pattern_search=c(), 
                                   pattern_target=c(), mode="count"){
  row_vector <- df[, which(colnames(df)==col_search)]
  count_pattern <- 0
  for(x in 1:length(row_vector)){
    if(row_vector[x] %in% pattern_search && row_vector[x+1] %in% pattern_target){
      count_pattern <- count_pattern + 1
    }
  }
  if(mode=="count"){
    return(count_pattern) 
  }
  else if(mode=="prob"){
    return(count_pattern / (length(row_vector) - 1))
  }
}

#1) Basic Naive Bayes Model
#2) Bernoulli Naive Bayes Model
#3) Multinomial Naive Bayes Model
#4) Non Parametric Naive Bayes Model
#=========== 1) Available Kernel ===============:
  #gaussian
  #rectangular
  #triangular
  #epanechnikov
  #biweight
  #cosine
  #optcosine
#=========== 2) Available Bandwidth Method===============:
  #nrd0 
  #SJ
#5) Poisson Naive Bayes Model
#6) Gaussian Naive Bayes Model
#use priority = none, to fit ordinary/default nonparametric Naive Bayes 
#use priority = kernel, to fit nonparametric Naive Bayes with Kernel Parameter on focus
#use priority = value, to fit nonparametric Naive Bayes with Bandwidth Value Parameter on focus
#use priority = method, to fit nonparametric Naive Bayes with Bandwidth Method Parameter on focus
#type=4 nonparametric naive bayes

naive_bayes_model <- function(df, formula, type=1, train_proportion=1, 
                              laplace_value=1, nonparametric_kernel = "gaussian", 
                              nonparametric_bandwidth_value=1, 
                              nonparametric_bandwidth_method="nrd0",
                              nonparametric_parameter_priority="none"){
  writeLines("Reminder! Naive Bayes Model need Numeric Input of Features to Continue!")
  dependent_var <- strsplit(formula, " ")[[1]][1]
  matrix_independent <- as.matrix(df[,which(colnames(df)!=dependent_var)])
  matrix_dependent <- as.matrix(df[,which(colnames(df)==dependent_var)])
  formula <- as.formula(formula)
  train_data <- NULL
  test_data <- NULL
  train_matrix_independent <- NULL
  test_matrix_independent <- NULL
  train_matrix_dependent <- NULL
  test_matrix_dependent <- NULL
  
  if(train_proportion < 1){
    ## set the seed to make your partition reproducible
    set.seed(123)
    smp_size <- floor(train_proportion * nrow(df))
    train_ind <- sample(seq_len(nrow(df)), size = smp_size)
    train_data <- df[train_ind, ]
    test_data <- df[-train_ind, ]
    train_matrix_independent <- as.matrix(train_data[,which(colnames(train_data)!=dependent_var)])
    test_matrix_independent <- as.matrix(test_data[,which(colnames(test_data)!=dependent_var)])
    train_matrix_dependent <- as.matrix(train_data[,which(colnames(train_data)==dependent_var)])
    test_matrix_dependent <- as.matrix(test_data[,which(colnames(test_data)==dependent_var)])
  }
  
  if(type==1){ # Naive Bayes Conditional Posterior Probabilities
    Naive_Bayes_Model=naiveBayes(formula, data=df)
    print(Naive_Bayes_Model)
    if(train_proportion == 1){
      Naive_Bayes_Model=naiveBayes(formula, data=df)
      print(Naive_Bayes_Model)
      NB_Predictions = predict(Naive_Bayes_Model, Titanic_dataset)
      print(table(NB_Predictions,Titanic_dataset$Survived))
      
      #Create Naive Bayes Comparison using MLR Packages with Classification Task and Naive Bayes Learner
      task = makeClassifTask(data = df, target = dependent_var)
      selected_model = makeLearner("classif.naiveBayes")
      NB_mlr = mlr::train(selected_model, task)
      print(NB_mlr$learner.model)
      
      #Predict on the dataset without passing the target feature
      predictions_mlr = as.data.frame(predict(NB_mlr, newdata = df[,which(colnames(df) != dependent_var)]))
      
      ##Confusion matrix to check accuracy
      print(table(predictions_mlr[,1], df[dependent_var]))
    }
    else if(train_proportion < 1){
      Naive_Bayes_Model=naiveBayes(formula, data=train_data)
      print(Naive_Bayes_Model)
      NB_Predictions = predict(Naive_Bayes_Model, test_data)
      print(table(NB_Predictions, test_data[dependent_var]))
      
      #Create Naive Bayes Comparison using MLR Packages with Classification Task and Naive Bayes Learner
      task = makeClassifTask(data = train_data, target = dependent_var)
      selected_model = makeLearner("classif.naiveBayes")
      NB_mlr = mlr::train(selected_model, task)
      print(NB_mlr$learner.model)
      
      #Predict on the dataset without passing the target feature
      predictions_mlr = as.data.frame(predict(NB_mlr, newdata = test_data[,which(colnames(test_data) != dependent_var)]))
      
      ##Confusion matrix to check accuracy
      print(table(predictions_mlr[,1], test_data[dependent_var]))
    }
    return(Naive_Bayes_Model)
  }
  else if(type==2){ # Bernoulli Naive Bayes (Multivariate with Binary Values and Predictor)
    #http://www.learnbymarketing.com/tutorials/naive-bayes-in-r/
    #Laplace Smoothing value = The bigger the laplace smoothing value, the more you are making the models the same
    
    if(train_proportion == 1){
      bnb <- bernoulli_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                   laplace = laplace_value)
      print(summary(bnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(bnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(bnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(bnb))
      return(bnb) 
    }
    else if(train_proportion < 1){
      bnb <- bernoulli_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                   laplace = laplace_value)
      print(summary(bnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(bnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(bnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(bnb))
      return(bnb)
    }
  }
  else if(type==3){
    ### Train the Multinomial Naive Bayes
    if(train_proportion == 1){
      mnb <- multinomial_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                     laplace = laplace_value)
      print(summary(mnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(mnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(mnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Multinomial Naive Bayes Coefficients")
      print(coef(mnb))
      return(mnb) 
    }
    else if(train_proportion < 1){
      mnb <- multinomial_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                     laplace = laplace_value)
      print(summary(mnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(mnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(mnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Multinomial Naive Bayes Coefficients")
      print(coef(mnb))
      return(mnb)
    }
    
  }
  else if(type==4){
    if(train_proportion == 1){
      if(nonparametric_parameter_priority=="none"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent)
        print(summary(nnb))
        writeLines("Prediction By Posterior Probabilities")
        print(head(predict(nnb, newdata = matrix_independent, type = "prob")))
        print(tables(nnb, which = 1))
      }
      else if(nonparametric_parameter_priority=="kernel"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                         kernel=nonparametric_kernel)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="method"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                         bw = nonparametric_bandwidth_method)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="value"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                         adjust = nonparametric_bandwidth_value)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      return(nnb) 
    }
    else if(train_proportion < 1){
      if(nonparametric_parameter_priority=="none"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent)
        print(summary(nnb))
        writeLines("Prediction By Posterior Probabilities")
        print(head(predict(nnb, newdata = test_matrix_independent, type = "prob")))
        print(tables(nnb, which = 1))
      }
      else if(nonparametric_parameter_priority=="kernel"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                         kernel=nonparametric_kernel)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="method"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                         bw = nonparametric_bandwidth_method)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="value"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                         adjust = nonparametric_bandwidth_value)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      return(nnb) 
    }
  }
  else if(type==5){
    if(train_proportion == 1){
      pnb <- poisson_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                 laplace = laplace_value)
      print(summary(pnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(pnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(pnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(pnb))
      return(pnb) 
    }
    else if(train_proportion < 1){
      pnb <- poisson_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                 laplace = laplace_value)
      print(summary(pnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(pnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(pnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(pnb))
      return(pnb)
    }
  }
  else if(type==6){
    if(train_proportion == 1){
      gnb <- gaussian_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                  laplace = laplace_value)
      print(summary(gnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(gnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(gnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(gnb))
      return(gnb) 
    }
    else if(train_proportion < 1){
      gnb <- gaussian_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                  laplace = laplace_value)
      print(summary(gnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(gnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(gnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(gnb))
      return(gnb)
    }
  } 
}

aode_model <- function(df, dependent_col, train_proportion=1){
  library(arules)
  library(gmodels)
  library(AnDE)
  
  train_data <- NULL
  test_data <- NULL
  num_discretize = (length(colnames(df)) + 1)
  aode_df <- data.frame(lapply(df[,which(colnames(df)!=dependent_col)], discretize, breaks=5),
                        target=df[dependent_col])
  
  if(train_proportion < 1){
    ## set the seed to make your partition reproducible
    set.seed(123)
    smp_size <- floor(train_proportion * nrow(df))
    train_ind <- sample(seq_len(nrow(df)), size = smp_size)
    train_data <- aode_df[train_ind, ]
    test_data <- aode_df[-train_ind, ]
  }
  
  if(train_proportion == 1){
    AODE_Model = aode(aode_df)
    predict_aode = predict(AODE_Model, aode_df)
    print(CrossTable(as.numeric(aode_df[dependent_col]), predict_aode))
  }
  else if(train_proportion < 1){
    AODE_Model = aode(train_data)
    predict_aode = predict(AODE_Model, test_data)
    print(CrossTable(as.numeric(test_data[dependent_col]), predict_aode))
  }
  return(AODE_Model)
}

#Use Bayesian Belief Network to classify Probability of Multivariate Factor Variable
bayesian_network_prelearning <- function(df, highlight_variable){
  library(bnlearn)
  tabu_bn_learning <- tabu(df)
  hc_bn_learning <- hc(df)
  
  writeLines("Model Structure with Hill Climbing Learning Algorithms")
  plot(hc_bn_learning, highlight = c(highlight_variable, mb(hc_bn_learning, highlight_variable)))
  writeLines("Model Structure with Tabu Learning Algorithms")
  plot(tabu_bn_learning, highlight = c(highlight_variable, mb(tabu_bn_learning, highlight_variable)))
  return(list(hc_bn_learning, tabu_bn_learning))
}

bayesian_belief_network <- function(df, highlight_variable, 
                                    remove_unwanted_relations=c(),
                                    use_learning="hc"){
  library(bnlearn)
  tabu_bn_learning <- tabu(df)
  hc_bn_learning <- hc(df)
  
  writeLines("Model Structure with Hill Climbing Learning Algorithms")
  plot(hc_bn_learning, highlight = c(highlight_variable, mb(hc_bn_learning, highlight_variable)))
  writeLines("Model Structure with Tabu Learning Algorithms")
  plot(tabu_bn_learning, highlight = c(highlight_variable, mb(tabu_bn_learning, highlight_variable)))
  
  if(use_learning == "hc")
  {
    #Remove unwanted relationship if any exists
    if(!is.null(remove_unwanted_relations)){
      for(j in 1:length(remove_unwanted_relations)){
        hc_bn_learning$arcs <- hc_bn_learning$arcs[-remove_unwanted_relations[j],]
      }
    } 
    fittedbn <- bn.fit(hc_bn_learning, data = df)
    print(fittedbn)
  }
  else if(use_learning == "tabu"){
    #Remove unwanted relationship if any exists
    if(!is.null(remove_unwanted_relations)){
      for(j in 1:length(remove_unwanted_relations)){
        tabu_bn_learning$arcs <- tabu_bn_learning$arcs[-remove_unwanted_relations[j],]
      }
    }
    fittedbn <- bn.fit(tabu_bn_learning, data = df)
    print(fittedbn)
  }
  return(fittedbn)
}

#-------------------------------------------------------------------------------------------------------------------------
########################################## 10) Decision Tree Models ######################################################
#-------------------------------------------------------------------------------------------------------------------------

class_tree_info <- function(){
  writeLines("1) Classification Tree Linearly with rpart")
  writeLines("2) Classification Tree Binary approach with Iterative Dichotomizer (ID3)")
  writeLines("3) Classification Tree with C4.5 and C5.0 Algorithm")
  writeLines("4) Classification Tree with CHAID (Chi-squared Automatic Interaction Detection) Algorithm")
  writeLines("5) Classification Tree with Single Split Tree of Decision Stump (Base Model for Another Model)")
  writeLines("6) Classification Tree with M5 Algorithm from RWeka")
  writeLines("7) Classification Tree with LMT (Logistic Model Trees) Algorithm from RWeka")
  writeLines("8) Classification Tree with Conditional Classification")
  writeLines("9) Classification tree with Projection Pursuit")
}

classification_tree <- function(formula, df, method=1, dependent_col="", 
                                chaid_minsplit_n="", chaid_minprob="", chaid_maxtreeheight="",
                                chaid_alpha2="", chaid_alpha3="", chaid_alpha4=""){
  
  library(mlr)
  library(rpart)
  library(rpart.plot)
  library(datasets)
  library(data.tree)
  library(DiagrammeR)
  library(RWeka)
  library(caret)
  library(partykit)
  library(C50)
  library(printr)
  library(CHAID)
  library(kableExtra)
  library(rsample) # for dataset and splitting also loads broom and tidyr
  library(dplyr)
  library(ggplot2)
  library(purrr)
  library(pROC)
  library(party)
  library(PPtree)
  library(PPtreeViz)
  library(mlbench)
  formula <- as.formula(formula)
  
  if(method==1){
    writeLines("========================= 1) Classification Tree Linearly with rpart =================================")
    writeLines("Use Best CP Parameter for rpart")
    writeLines("Step-1: Begin with a small cp.")
    set.seed(123)
    tree <- rpart(formula, data = df, control = rpart.control(cp = 0.0001))
    
    writeLines("Step-2: Pick the tree size that minimizes misclassification rate (i.e. prediction error).")
    writeLines("Prediction error rate in training data = Root node error * rel error * 100%")
    writeLines("Prediction error rate in cross-validation = Root node error * xerror * 100%")
    writeLines("Hence we want the cp value (with a simpler tree) that minimizes the xerror.")
    printcp(tree)
    bestcp <- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
    
    writeLines("Step3: Prune the tree using the best cp.")
    tree.pruned <- prune(tree, cp = bestcp)
    x11()
    plot(tree.pruned)
    text(tree.pruned, cex = 0.8, use.n = TRUE, xpd = TRUE)
    x11()
    prp(tree.pruned, faclen = 0, cex = 0.8, extra = 1, box.palette="auto")
    
    writeLines("Step4: Make Confusion Matrix")
    # confusion matrix (training data)
    conf.matrix <- table(df[,dependent_col], predict(tree.pruned,type="class"))
    rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
    colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
    print(conf.matrix)
    return(list(tree.pruned, conf.matrix))
  }
  else if(method==2){
    writeLines("2) Classification Tree Binary approach with Iterative Dichotomizer (ID3)")
    writeLines("Reference from: https://www.r-bloggers.com/id3-classification-using-data-tree/")
    writeLines("Note: ID3 is Sensitive to NAN Values with Data, make sure to get rid of NA Values first before modeling")
    
    IsPure <- function(data) {
      length(unique(data[,ncol(data)])) == 1
    }
    
    Entropy <- function( vls ) {
      res <- vls/sum(vls) * log2(vls/sum(vls))
      res[vls == 0] <- 0
      -sum(res)
    }
    
    writeLines("Entropy Example Reminder")
    writeLines("If a dataset is completely pure, then it has entropy 0:")
    writeLines("Entropy = 0 -> Classification is Homogenous/Pure")
    writeLines("Entropy = 1 -> Classification is Heterogenous/Unique")
    writeLines("Small Example:")
    Entropy(c(10, 0))
    Entropy(c(0, 10))
    Entropy(c(5, 5))
    entropy_curve <- function(edible) Entropy(c(edible, 100 - edible))
    entropy_curve <- Vectorize(entropy_curve)
    curve(entropy_curve, from = 0, to = 100, xname = 'edible')
    
    InformationGain <- function( tble ) {
      tble <- as.data.frame.matrix(tble)
      entropyBefore <- Entropy(colSums(tble))
      s <- rowSums(tble)
      entropyAfter <- sum (s / sum(s) * apply(tble, MARGIN = 1, FUN = Entropy ))
      informationGain <- entropyBefore - entropyAfter
      return (informationGain)
    }
    
    TrainID3 <- function(node, data) {
      node$obsCount <- nrow(data)
      #if the data-set is pure (e.g. all toxic), then
      if (IsPure(data)) 
      {
        #construct a leaf having the name of the pure feature (e.g. 'toxic')
        child <- node$AddChild(unique(data[,ncol(data)]))
        node$feature <- tail(names(data), 1)
        child$obsCount <- nrow(data)
        child$feature <- ''
      } 
      else 
      {
        #chose the feature with the highest information gain (e.g. 'color')
        ig <- sapply(colnames(data)[-ncol(data)], function(x) InformationGain(table(data[,x], data[,ncol(data)])))
        feature <- names(ig)[ig == max(ig)][1]
        node$feature <- feature
        #take the subset of the data-set having that feature value
        childObs <- split(data[,!(names(data) %in% feature)], data[,feature], drop = TRUE)
        
        for(i in 1:length(childObs)) 
        {
          #construct a child having the name of that feature value (e.g. 'red')
          child <- node$AddChild(names(childObs)[i])
          #call the algorithm recursively on the child and the subset      
          TrainID3(child, childObs[[i]])
        }
        
      }
    }
    
    PredictID3 <- function(tree, features) {
      if (tree$children[[1]]$isLeaf) return (tree$children[[1]]$name)
      child <- tree$children[[features[[tree$feature]]]]
      return(PredictID3(child, features))
    }
    
    writeLines("Check feature combination with the most high information gain")
    dependent_col = colnames(df[,colnames(df)!=dependent_col])
    information_gain_column <- c()
    information_gain_vector <- c()
    for(g in 1:length(dependent_col)){
      information_gain_vector <- c(information_gain_vector, InformationGain(table(df[,c(dependent_col[g], dependent_col)])))
      information_gain_column <- c(information_gain_column, dependent_col[g])
    }
    sorted_information_vector = sort(information_gain_vector, decreasing = TRUE)
    index_sorted = c()
    for(h in 1:length(dependent_col)){
      index_sorted <- c(index_sorted, which(information_gain_vector == sorted_information_vector[h]))
    }
    sorted_information_column <- information_gain_column[index_sorted]
    writeLines("Classification Ordering based by Information Gain")
    string_append <- ""
    for(i in 1:length(sorted_information_column)){
      if(i!=length(sorted_information_column)){
        string_append <- paste0(sorted_information_column[i], " > ")
      }
      else{
        string_append <- paste0(sorted_information_column[i])
      }
    }
    writeLines(string_append)
    ID3_tree <- Node$new("Breast_Cancer")
    TrainID3(br_cancer_tree, BC)
    print(ID3_tree, "feature", "obsCount")
    
    writeLines("10 Random Prediction with given Random Parameters")
    for(u in range(1:10)){
      message = "with parameter of "
      combine_eval = "c("
      for(x in range(1:length(dependent_col))){
        col_data_unique = as.numeric(unique(df[,dependent_col[x]]))
        random_par = runif(1, min(col_data_unique), max(data_unique))
        if(x != length(dependent_col)){
          message = paste0(message, dependent_col[x], " = ", random_par, ",")
          combine_eval = c(combine_eval, paste0(dependent_col[x], " = ", random_par, ",")) 
        }
        else{
          message = paste0(message, dependent_col[x], " = ", random_par, ")")
          combine_eval = c(combine_eval, paste0(dependent_col[x], " = ", random_par, ")")) 
        }
      }
      writeLines(message)
      writeLines("Get Classification Predict Results of: ")
      PredictID3(ID3_tree, eval(parse(text=combine_eval)))
    }
    return(ID3_tree)
  }
  else if(method==3){
    writeLines("3) Classification Tree with C4.5 and C5.0 Algorithm")
    writeLines("Reference from https://rpubs.com/kjmazidi/195428")
    writeLines("Model C4.5 and C5.0 Using RWeka, Caret, Partykit")
    writeLines("Warning! Model C5.0 Need a Clean Categoric Data free from irrelevant Symbols, otherwise modeling will not work")
    set.seed(1958)  # set a seed to get replicable results
    
    train <- createFolds(df[,dependent_col], k=10)
    C45Fit <- caret::train(formula, method="J48", data=df,
                           tuneLength = 5,
                           trControl = trainControl(
                             method="cv", indexOut=train))
    writeLines("======================== C4.5 Model Summary==========================")
    x11()
    print(C45Fit)
    print(C45Fit$finalModel)
    plot(C45Fit$finalModel)
    
    writeLines("======================== C4.5 Predictions ==========================")
    resultsc45 <- predict(object=C45Fit, newdata=df, type="raw")
    conf.matrixC45 <- table(df[,dependent_col], resultsc45)
    rownames(conf.matrixC45) <- paste("Actual", rownames(conf.matrixC45), sep = ":")
    colnames(conf.matrixC45) <- paste("Pred", colnames(conf.matrixC45), sep = ":")
    print(conf.matrixC45)
    
    C50Fit <- C5.0(formula, data=df)
    writeLines("======================== C5.0 Model Summary==========================")
    x11()
    print(C50Fit)
    plot(C50Fit)
    
    writeLines("======================== C5.0 Predictions==========================")
    resultsc50 <- predict(object=C50Fit, newdata=df, type="class")
    conf.matrixC50 <- table(df[,dependent_col], resultsc50)
    rownames(conf.matrixC50) <- paste("Actual", rownames(conf.matrixC50), sep = ":")
    colnames(conf.matrixC50) <- paste("Pred", colnames(conf.matrixC50), sep = ":")
    print(conf.matrixC50)
    return(list(C45Fit, C50Fit, conf.matrixC45, conf.matrixC50))
  }
  else if(method==4){
    writeLines("4) Classification Tree with CHAID (Chi-squared Automatic Interaction Detection) Algorithm")
    writeLines("What does CHAID do? Straight from the help pages")
    writeLines("Select the predictor that has the smallest adjusted p-value (i.e., most significant).") 
    writeLines("If this adjusted p-value is less than or equal to a user-specified alpha-level alpha4, ")
    writeLines("split the node using this predictor.")
    writeLines("Else, do not split and the node is considered as a terminal node.")
    writeLines("So it will take our 18 predictors and test each one against our outcome variable - attrition.")
    writeLines("The one with the lowest p value (a proxy for is most predictive) will anchor our decision tree.") 
    writeLines("It will then repeat this process of splitting until more splits fail to yield significant results")
    
    ctrl <- NULL
    if(chaid_minsplit_n != "" || chaid_minprob != "" || chaid_maxtreeheight != "" ||
       chaid_alpha2!="" || chaid_alpha3 != "" || chaid_alpha4 != ""){
      string_chaid_control = "chaid_control("
      if(chaid_minsplit_n != ""){
        string_chaid_control = paste0(string_chaid_control, "minsplit = ",chaid_minsplit_n)
      }
      if(chaid_minprob != ""){
        string_chaid_control = paste0(string_chaid_control, ",minprob = ",chaid_minprob)
      }
      if(chaid_maxtreeheight != ""){
        string_chaid_control = paste0(string_chaid_control, ",maxheight = ",chaid_maxtreeheight)
      }
      if(chaid_alpha2 != ""){
        string_chaid_control = paste0(string_chaid_control, ",alpha2 = ",chaid_alpha2)
      }
      if(chaid_alpha3 != ""){
        string_chaid_control = paste0(string_chaid_control, ",alpha3 = ",chaid_alpha3)
      }
      if(chaid_alpha4 != ""){
        string_chaid_control = paste0(string_chaid_control, ",alpha4 = ",chaid_alpha4)
      }
      string_chaid_control = paste0(string_chaid_control, ")")
      ctrl <- eval(parse(text=string_chaid_control))
      chaid_model <- chaid(formula, data = df, control = ctrl)
      x11()
      print(chaid_model)
      plot(
        chaid_model,
        main = paste("With", string_chaid_control),
        gp = gpar(
          fontsize=7,
          col = "black",
          lty = "solid",
          lwd = 2  )
      )
      conf.matrix <- table(df[,dependent_col], as.character(fitted(chaid_model)$"(response)"))
      rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
      colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
      print(conf.matrix)
      return(list(chaid_model, conf.matrix))
    }
    else{
      chaid_model <- chaid(formula, data = df)
      x11()
      print(chaid_model)
      plot(
        chaid_model,
        main = "Default CHAID Model",
        gp = gpar(
          fontsize=7,
          col = "black",
          lty = "solid",
          lwd = 2  )
      )
      conf.matrix <- table(df[,dependent_col], as.character(fitted(chaid_model)$"(response)"))
      rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
      colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
      print(conf.matrix)
      return(list(chaid_model, conf.matrix))
    }
  }
  else if(method==5){
    writeLines("5) Classification Tree with Single Split Tree of Decision Stump (Base Model for Another Model)")
    sample <- sample(1:nrow(df), round(70*nrow(df)/100,0))
    train <- df[sample, ]
    test <- df[-sample, ]
    
    writeLines("As the area under an ROC curve is a measure of the usefulness of a test in general,")
    writeLines("where a greater area means a more useful test")
    writeLines("the areas under ROC curves are used to compare the usefulness of tests.")
    writeLines("ROC curves are frequently used to show the connection")
    writeLines("between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests")
    
    ### CONSTRUCT A WEAK CLASSIFIER OF DECISION STUMP ###
    stump <- DecisionStump(formula, data = train)
    print(stump)
    roc(as.factor(test[,dependent_col]), 
        predict(stump, newdata = test, type = "probability")[, 2])
    return(stump)
  }
  else if(method==6){
    writeLines("6) Classification Tree with M5 and LMT (Logistic Model Trees) Algorithm from RWeka")
    
    writeLines("Model M5 Example -> Use RWeka,")
    writeLines("Note: Classification M5 Model nead all numeric/integer")
    writeLines("including Independent and Dependent Variabel")
    writeLines("Model Logistic -> Use RWeka")
    
    levels_info <- lapply(df, levels)
    
    df_numeric <- df %>%
      mutate_if(is.factor, as.numeric)
    
    cut_numeric_to_factor <- function(df="", vector=c(), specific=FALSE, column, range_label=c(), label=c(), n_factor=5){
      if(df != ""){
        if(specific==FALSE){
          df <- df %>%
            mutate_if(is.numeric, funs(cut_number(., n=n_factor)))
        }
        else if(specific==TRUE){
          df[,column] <- cut(
            df[,column],
            breaks = range_label,
            labels = label
          )
        }
        return(df) 
      }
      else if(is.null(vector) == FALSE){
        if(specific==FALSE){
          vector <- vector %>%
            mutate_if(is.numeric, funs(cut_number(., n=n_factor)))
        }
        else if(specific==TRUE){
          vector <- cut(
            vector,
            breaks = range_label,
            labels = label
          )
        }
        return(vector) 
      }
    }
    
    writeLines("M5P Interpretation is in Numerical Code, converted from the Original Categoric Variable, below is Interpretation of Code")
    print(levels_info)
    M5Fit <- M5P(formula, data = df_numeric)
    x11()
    print(M5Fit)
    plot(M5Fit)
    predict_M5 <- predict(M5Fit)
    label_rounding <- 0
    unique_label_num <- NULL
    while(TRUE){
      unique_label_num <- unique(round(unique(predict_M5),label_rounding))
      if(length(unique_label_num) >= length(unique(df[,dependent_col]))){
        break
      }
      else{
        label_rounding <- label_rounding + 1
      }
    }
    unique_label_num <- sort(unique_label_num)
    if(label_rounding == 0){
      unique_label_num <- c(0, unique_label_num)
      print(unique_label_num)
    }
    else{
      unique_label_num <- c(unique_label_num[1]-1*(10**-label_rounding), unique_label_num+1*(10**-label_rounding))
      print(unique_label_num) 
    }
    transform_predict = cut_numeric_to_factor(vector=predict_M5, specific=TRUE, range_label = unique_label_num, 
                                              label=levels(df[,dependent_col]))
    print(length(df[,dependent_col]))
    print(length(transform_predict))
    conf.matrix <- table(df[,dependent_col], transform_predict)
    rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
    colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
    print(conf.matrix)
    return(list(M5Fit, levels_info, conf.matrix))
  }
  else if(method==7){
    writeLines("7) Classification Tree with LMT (Logistic/Binary Model Trees) Algorithm from RWeka")
    logfit <- LMT(formula., data = df)
    x11()
    print(logfit)
    plot(logfit) #Model Tree doesn't seem building a tree
    conf.matrix <- table(df[,dependent_col], predict(logfit))
    rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
    colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
    print(conf.matrix)
    return(list(logfit, conf.matrix))
  }
  else if(method==8){
    writeLines("8) Classification Tree with Conditional Classification")
    cond_tree <- party::ctree(formula, data=df)
    x11()
    print(cond_tree)
    plot(cond_tree)
    conf.matrix <- table(df[,dependent_col], predict(cond_tree))
    rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
    colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
    print(conf.matrix)
    return(list(cond_tree, conf.matrix))
  }
  else if(method==9){
    writeLines("9) Classification tree with Projection Pursuit")
    writeLines("Projection Pursuit needs Predictor Input of numeric not Categoric!")
    writeLines("PP Tree Analysis Documentation:")
    writeLines("https://pdfs.semanticscholar.org/fb8e/b9cf58286bef0f40eb527ac1e31eae9487e3.pdf?_ga=2.198190831.1502602227.1588660179-552138508.1588660179")
    
    
    levels_info <- lapply(df, levels)
    
    df_numeric <- df %>%
      mutate_if(is.factor, as.numeric)
    
    print("Unique Code of Categoric")
    print(levels_info)
    
    x11()
    writeLines("Using Linear Discriminant Analysis Method")
    pp_tree_lda <- PP.Tree("LDA", df_numeric[,dependent_col], 
                           df_numeric[,colnames(df_numeric) != dependent_col])
    print(pp_tree_lda)
    pp_tree_lda_viz <- PPTreeclass(formula, data=df_numeric, method="LDA")
    plot(pp_tree_lda_viz)
    predict_pp_lda <- PP.classify(df_numeric[,colnames(df_numeric) != dependent_col], 
                                  df_numeric[,dependent_col], pp_tree_lda, Rule=1)
    predict_lda_result <- as.factor(sort(predict_pp_lda$predict.class))
    levels(predict_lda_result) <- unique(df_numeric[,dependent_col])
    writeLines("Prediction Accuracy with LDA PP Tree")
    conf.matrixLDA <- table(df_numeric[,dependent_col], predict_lda_result)
    rownames(conf.matrixLDA) <- paste("Actual", rownames(conf.matrixLDA), sep = ":")
    colnames(conf.matrixLDA) <- paste("Pred", colnames(conf.matrixLDA), sep = ":")
    print(conf.matrixLDA)
    
    writeLines("Using Penalized Discriminant Analysis")
    writeLines("Calculating Optimal Lambda")
    
    x11()
    pp_tree_pda <- PP.Tree("PDA", df_numeric[,dependent_col], 
                           df_numeric[,colnames(df_numeric) != dependent_col],
                           lambda = 0.1)
    print(pp_tree_pda)
    pp_tree_pda_viz <- PPTreeclass(formula, data=df_numeric, method="PDA")
    plot(pp_tree_pda_viz)
    predict_pp_pda <- PP.classify(df_numeric[,colnames(df_numeric) != dependent_col], 
                                  df_numeric[,dependent_col], pp_tree_pda, Rule=1)
    predict_pda_result <- as.factor(sort(predict_pp_pda$predict.class))
    levels(predict_pda_result) <- unique(df_numeric[,dependent_col])
    writeLines("Prediction Accuracy with PDA PP Tree")
    conf.matrixPDA <- table(df_numeric[,dependent_col], predict_pda_result)
    rownames(conf.matrixPDA) <- paste("Actual", rownames(conf.matrixPDA), sep = ":")
    colnames(conf.matrixPDA) <- paste("Pred", colnames(conf.matrixPDA), sep = ":")
    print(conf.matrixPDA)
    
    writeLines("Using Lp Analysis")
    x11()
    pp_tree_lp <- PP.Tree("Lp", df_numeric[,dependent_col], 
                          df_numeric[,colnames(df_numeric) != dependent_col], r=0.1)
    print(pp_tree_lp)
    pp_tree_lr_viz<- PPTreeclass(formula, data=df_numeric, method="Lr")
    plot(pp_tree_lr_viz)
    predict_pp_lda <- PP.classify(df_numeric[,colnames(df_numeric) != dependent_col], 
                                  df_numeric[,dependent_col], pp_tree_lp, Rule=1)
    predict_lp_result <- as.factor(predict_pp_lda$predict.class)
    levels(predict_lp_result) <- unique(df_numeric[,dependent_col])
    writeLines("Prediction Accuracy with Lp PP Tree")
    conf.matrixLP <- table(df_numeric[,dependent_col], predict_lp_result)
    rownames(conf.matrixLP) <- paste("Actual", rownames(conf.matrixLP), sep = ":")
    colnames(conf.matrixLP) <- paste("Pred", colnames(conf.matrixLP), sep = ":")
    print(conf.matrixLP)
    
    return(list(pp_tree_lda, pp_tree_pda, pp_tree_lp, conf.matrixLDA, conf.matrixPDA, conf.matrixLP))
  }
}



#-------------------------------------------------------------------------------------------------------------------------
######### 11) Factor Analysis with EFA (Exploratory Factor Analysis) and SEM (Structural Equation Modeling) ##############
#-------------------------------------------------------------------------------------------------------------------------

factor_analysis_supporter_library <- function(){
  library(parameters)
  library(dplyr)
  library(psych)
  library(psycho)
  library(see)
  library(nFactors)
  library(GPArotation)
  library(sem)
  library(seminr)
  library(lavaan)
  library(performance)
  library(semPlot)
  library(OpenMx)
}

factor_structure <- function(data){
  data <- na.omit(data)  
  fact_structure <- check_factorstructure(data)
  print(fact_structure)
  return(fact_structure)
}

EFA_Analysis_recommendation <- function(data, threshold=0.95, show_n_best_manifest=5, 
                                        filter_factor_loading=0.5){
  writeLines("####################### 1) Explore Best Component #############################")
  pca_data <- princomp(data)
  summary(pca_data)
  x11()
  plot(pca_data)
  proportion_variance <- round(pca_data$sdev**2 / sum(pca_data$sdev**2),2)
  length_recommended <- length(which(proportion_variance >= threshold))
  if(length_recommended < 2) length_recommended <- 2
  writeLines(paste0("Best Recommended Component to use: ", length_recommended))
  writeLines("Standard Deviation of Component")
  print(pca_data$sdev)
  writeLines("Variance Proportion")
  print(proportion_variance)
  writeLines("Cumulative Variance")
  print(cumsum(proportion_variance))
  
  writeLines("####################### 2) EFA Analysis #############################")
  efa_varimax <- factanal(data, factors = length_recommended, rotation="varimax")
  efa_loading_value <- data.frame(efa_varimax$loadings[1:length(colnames(data)),])
  efa_loading_value$rowname <- row.names(efa_loading_value)
  for(g in 1:length_recommended){
    factor_name <- paste0("Factor",g)
    string_eval <- paste0("efa_loading_value$",factor_name)
    string_eval2 <- paste0("factor_value$",factor_name)
    factor_value <- efa_loading_value[order(eval(parse(text=string_eval)), decreasing = TRUE), 
                                      which(colnames(efa_loading_value) %in% c(factor_name,"rowname"))]
    writeLines(paste(factor_name), "Best Manifest Variables")
    factor_value <- factor_value[which(eval(parse(text=string_eval2)) > filter_factor_loading),]
    print(head(factor_value, show_n_best_manifest))
  }
}

SEM_compare_hierarchial_model <- function(formula_child_hierarchy, formula_parent_hierarchy){
  library(lavaan)
  library(semPlot)
  writeLines("Examine the fit indices for the Child Hierarchy")
  childfit <- fitmeasures(formula_child_hierarchy, c("rmsea", "srmr"))
  writeLines("Examine the fit indices for the Parent Hierarchy")
  parentfit <- fitmeasures(formula_parent_hierarchy, c("rmsea", "srmr"))
  print(childfit)
  print(parentfit)
  return(list(childfit, childfit2))
}

#layout available = tree, circle, spring, tree2, circle2
SEM_building_model <- function(formula, data, modification=FALSE, 
                               sem_layout="tree", sem_rotation="north", 
                               sem_color="purple", sem_font_text = 1, 
                               sem_value="standardized", check_variance=FALSE){
  
  library(lavaan)
  library(semPlot)
  writeLines("============================= SEM Building Model Tips ==============================")
  writeLines("=~ Quote is to define a relationship of Latent Variables with Manifest Variables")
  writeLines("~~ Quote is to define a relationship of Coveriance / Correlation")
  writeLines("~ Quote is to define a Direct Prediction (Linear Regression)")
  writeLines("One Factor Model is Useful to prevent Error caused by Heywood Cases, but Accuracy may be lower")
  writeLines("Two/Multiple Factor Model is Useful to make Multiple/Hierarchial Latent")
  writeLines("and this approach can improve Model Accuracy, but this approach are more likely to get Error of Heywood Cases")
  writeLines("Data cannot be Modeled into SEM if Degree of Freedom = 0")
  writeLines("Fittest SEM Model Goal Analysis")
  writeLines("1) High Value of CFI (Comparative Fit Index) Preferable > 0.9")
  writeLines("2) High Value of TLI (Tucker Lewis Index) Preferable > 0.9")
  writeLines("3) Low Value of RMSEA (Root Mean Square Approximation) Preferable > 0.9")
  writeLines("4) Low Value of SRMR (Standardized Root Mean Square Residual) Preferable > 0.9")
  writeLines("====================================================================================")
  
  writeLines("Creating Latent Variable based by Formula")
  cfa_model <- cfa(formula, data=data)
  
  if(modification==TRUE){
    modify_list <- modificationindices(cfa_model, sort=TRUE)
    print(modify_list)
    best_formula <- paste0(modify_list[1,1], modify_list[1,2], modify_list[1,3])
    writeLines(paste("Adding Path Best Modification: ", best_formula))
    added_formula <- paste0(formula, "\n", best_formula)
    cfa_model_modified <- cfa(added_formula, data=data)
    summary_model <- summary(cfa_model_modified, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)
    
    score_model = 0 
    goodness_parameter <- summary_model$FIT[c(9,10,17,18,19,21)]
    if(as.numeric(goodness_parameter[1]) > 0.9){
      score_model = score_model + 1
    }
    if(as.numeric(goodness_parameter[2]) > 0.9){
      score_model = score_model + 1
    }
    if(as.numeric(goodness_parameter[3]) < 0.1){
      score_model = score_model + 1
    }
    if(as.numeric(goodness_parameter[4]) < 0.1){
      score_model = score_model + 1
    }
    if(as.numeric(goodness_parameter[5]) < 0.1){
      score_model = score_model + 1
    }
    if(as.numeric(goodness_parameter[6]) < 0.1){
      score_model = score_model + 1
    }
    if(score_model ==6){
      writeLines("Conclusion: Model has an Excellent Fit Indices!")
    }
    else if(score_model >3 && score_model<=5){
      writeLines("Conclusion: Model has an Adequate Fit Indices")
    }
    else if(score_model <=3){
      message("Conclusion: Model has a Poor Fit Indices!")
    }
    
    writeLines("Checking Old Model ~ New Model")
    writeLines("1) ANOVA Approach (Useful for Comparing Similiar Model, (Model 2 is a new version of Model 1))")
    anova_result <- anova(cfa_model, cfa_model_modified)
    print(anova_result)
    writeLines("2) Compare Fit Indices Approach (Useful for Models that different with another one)")
    model1measure <- fitmeasures(cfa_model, c("aic","ecvi"))
    model2measure <- fitmeasures(cfa_model_modified, c("aic","ecvi"))
    print(model1measure)
    print(model2measure)
    
    sem_rotate = 0
    if(sem_rotation=="north"){sem_rotate <- 1}
    else if(sem_rotation=="east"){sem_rotate <- 2}
    else if(sem_rotation=="south"){sem_rotate <- 3}
    else if(sem_rotation=="west"){sem_rotate <- 4}
    
    sem_value_display <- ""
    if(sem_value=="standardized"){sem_value_display <- "std"}
    else if(sem_value=="unstandardized"){sem_value_display <- "par"}
    
    
    if(check_variance==TRUE)
    {
      writeLines("Checking Model Variance with Original Variance")
      for(b in 1:length(colnames(data))){
        writeLines(paste0("Variance of ", colnames(data)[b]))
        print(var(data[,b]))
      }
    }
    
    writeLines("Building Plot of SEM Model")
    semPaths(object = cfa_model_modified,
             layout = sem_layout,
             rotation = sem_rotate,
             whatLabels = sem_value_display,
             edge.label.cex = sem_font_text,
             what = sem_value_display,
             edge.color=sem_color)
    return(list(cfa_model, cfa_model_modified))
    
  }
  
  summary_model <- summary(cfa_model, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)
  score_model = 0 
  goodness_parameter <- summary_model$FIT[c(9,10,17,18,19,21)]
  if(as.numeric(goodness_parameter[1]) > 0.9){
    score_model = score_model + 1
  }
  if(as.numeric(goodness_parameter[2]) > 0.9){
    score_model = score_model + 1
  }
  if(as.numeric(goodness_parameter[3]) < 0.1){
    score_model = score_model + 1
  }
  if(as.numeric(goodness_parameter[4]) < 0.1){
    score_model = score_model + 1
  }
  if(as.numeric(goodness_parameter[5]) < 0.1){
    score_model = score_model + 1
  }
  if(as.numeric(goodness_parameter[6]) < 0.1){
    score_model = score_model + 1
  }
  if(score_model ==6){
    writeLines("Model has an Excellent Fit Indices!")
  }
  else if(score_model >=3 && score_model<=5){
    writeLines("Model has an Adequate Fit Indices")
  }
  else if(score_model <3){
    message("Model has a Poor Fit Indices!")
  }
  
  if(check_variance==TRUE)
  {
    writeLines("Checking Model Variance with Original Variance")
    for(b in 1:length(colnames(data))){
      writeLines(paste0("Variance of ", colnames(data)[b]))
      print(var(data[,b]))
    }
  }
  
  sem_rotate = 0
  if(sem_rotation=="north"){sem_rotate <- 1}
  else if(sem_rotation=="east"){sem_rotate <- 2}
  else if(sem_rotation=="south"){sem_rotate <- 3}
  else if(sem_rotation=="west"){sem_rotate <- 4}
  
  sem_value_display <- ""
  if(sem_value=="standardized"){sem_value_display <- "std"}
  else if(sem_value=="unstandardized"){sem_value_display <- "par"}
  
  writeLines("Building Plot of SEM Model")
  writeLines("")
  x11()
  # Update the default picture
  semPaths(object = cfa_model,
           layout = sem_layout,
           rotation = sem_rotate,
           whatLabels = sem_value_display,
           edge.label.cex = sem_font_text,
           what = sem_value_display,
           edge.color=sem_color)
  return(cfa_model)
}

library(parameters)
library(dplyr)
library(psych)
library(psycho)
library(see)
library(nFactors)
library(GPArotation)
library(sem)
library(seminr)
library(lavaan)
library(performance)
library(semPlot)
library(OpenMx)

# Load the data
data <- psych::bfi[, 1:25]  # Select only the 25 first columns corresponding to the items
data <- na.omit(data)  # remove missing values

# Check factor structure
check_factorstructure(data)

#Example usage 1) Default Model
BFI_SEM <- SEM_building_model('Neuroticism =~ N1+N2+N3+N4+N5
Conscientiousness =~ C1+C2+C3+C4+C5
Extraversion =~ E1+E2+E3+E4+E5
Agreeableness =~ A1+A2+A3+A4+A5
Openness =~ O1+O2+O3+O4+O5', data=data, sem_layout="circle", sem_color="red")

#Example usage 2) Modified Model
BFI_SEM <- SEM_building_model('Neuroticism =~ N1+N2+N3+N4+N5
Conscientiousness =~ C1+C2+C3+C4+C5
Extraversion =~ E1+E2+E3+E4+E5
Agreeableness =~ A1+A2+A3+A4+A5
Openness =~ O1+O2+O3+O4+O5', data=data, sem_layout="circle", sem_color="blue", 
                              modification=TRUE)

EFA_Analysis_recommendation(data, filter_factor_loading = 0)

#Example usage 3) Model by EFA Information
BFI_SEM <- SEM_building_model('Neuroticism =~ N1+N2+N3+N4+N5
Conscientiousness =~ C1+C2+C3
Extraversion =~ E3+E4+E5
Agreeableness =~ A2+A3+A4+A5
Openness =~ O1+O3+O5', data=data, sem_layout="circle", sem_color="yellow")

#Example usage 4) Model by EFA and then modified
BFI_SEM <- SEM_building_model('Neuroticism =~ N1+N2+N3+N4+N5
Conscientiousness =~ C1+C2+C3
Extraversion =~ E3+E4+E5
Agreeableness =~ A2+A3+A4+A5
Openness =~ O1+O3+O5', data=data, sem_layout="circle", sem_color="green", 
                              modification=TRUE)

EFA_Analysis_recommendation(data, filter_factor_loading = 0.5)

#Example usage 5) Model by EFA Information with filtered factor loading > 0.5
BFI_SEM <- SEM_building_model('Neuroticism =~ N1+N2+N3+N4+N5
Conscientiousness =~ C1+C2+C3
Extraversion =~ E4
Agreeableness =~ A2+A3+A5
Openness =~ O1+O3', data=data, sem_layout="circle", sem_color="orange")

#Example usage 6) Model by EFA and then modified with filtered factor loading > 0.5
BFI_SEM <- SEM_building_model('Neuroticism =~ N1+N2+N3+N4+N5
Conscientiousness =~ C1+C2+C3
Extraversion =~ E4
Agreeableness =~ A2+A3+A5
Openness =~ O1+O3', data=data, sem_layout="circle", sem_color="navy",
                              modification = TRUE)



#-------------------------------------------------------------------------------------------------------------------------
########################################## 12) Time Series Analysis ######################################################
#-------------------------------------------------------------------------------------------------------------------------

load_time_series_library <- function(){
  library(tseries)
  library(forecast)
  library(prophet)
  library(quantmod)
}

ts_stationary <- function(x, stationary_method = "Both"){
  if(stationary_method=="Both"){
    writeLines("H0: Process is Non Stationary in mean")
    writeLines("H1: Process is Stationary in mean")
    diff <- 1
    while(1){
      print(adf.test(x))
      adf = adf.test(x)$p.value
      if(adf < 0.05){
        writeLines(paste("Process is already Statonary in mean, with Pvalue", adf))
        break
      }
      else if (adf >= 0.05){
        writeLines(paste("Process is Non Stationary in mean, Differences the Time Series, with Pvalue", adf))
        x = diff(x, differences=diff)
        x11()
        plot(x, main=paste("Time Series Differences Lag = ",diff))
        diff <- diff + 1
      }
    }
    writeLines("Continue to Stationary Variance test")
    writeLines("H0: Process is Non Stationary in Variance")
    writeLines("H1: Process is Stationary in Variance")
    lambda <- BoxCox.lambda(na.contiguous(x))
    print(paste("Lambda = ", lambda))
    if(lambda != 1){
      writeLines(paste("Process is Not Stationary in variance (Transforming BoxCox), lambda = ", lambda))
      x11()
      par(mfrow=c(1,2))
      plot(x, main = "Before Transform BoxCox")
      x <- BoxCox(x,lambda)
      plot(x, main = "After Transform BoxCox")
      writeLines("Finished Transforming")
      return(list(transformed = x, differences = diff, lambda = lambda))
    }
    else if(lambda == 1){
      writeLines(paste("Process is Statonary in variance (Continue to Checking Stationary in mean), lambda = ", lambda))
      x11()
      plot(x, main="Transformed Time Series")
      return(list(transformed = x, differences = diff, lambda = lambda))
    }
  } 
  else if(stationary_method=="Mean"){
    writeLines("H0: Process is Non Stationary in mean")
    writeLines("H1: Process is Stationary in mean")
    diff <- 1
    while(1){
      print(adf.test(x))
      adf = adf.test(x)$p.value
      if(adf < 0.05){
        writeLines(paste("Process is already Statonary in mean, with Pvalue", adf))
        return(list(transformed = x, differences = diff))
      }
      else if (adf >= 0.05){
        writeLines(paste("Process is Non Stationary in mean, Differences the Time Series, with Pvalue", adf))
        x = diff(x, differences=diff)
        x11()
        plot(x, main=paste("Time Series Differences Lag = ",diff))
        diff <- diff + 1
      }
    }
  }
  else if(stationary_method=="Variance"){
    writeLines("H0: Process is Non Stationary in Variance")
    writeLines("H1: Process is Stationary in Variance")
    lambda <- BoxCox.lambda(na.contiguous(x))
    print(paste("Lambda = ", lambda))
    if(lambda != 1){
      writeLines(paste("Process is Not Stationary in variance (Transforming BoxCox), lambda = ", lambda))
      x11()
      par(mfrow=c(1,2))
      plot(x, main = "Before Transform BoxCox")
      x <- BoxCox(x,lambda)
      plot(x, main = "After Transform BoxCox")
      writeLines("Finished Transforming")
      return(list(transformed = x, lambda = lambda))
    }
    else if(lambda == 1){
      writeLines(paste("Process is Statonary in variance (Continue to Checking Stationary in mean), lambda = ", lambda))
      x11()
      plot(x, main="Transformed Time Series")
      return(list(transformed = x, lambda = lambda))
    }
  }
}

find.freq <- function(x){
  n <- length(x)
  spec <- spec.ar(c(na.contiguous(x)),plot=FALSE)
  if(max(spec$spec)>10) # Arbitrary threshold chosen by trial and error.
  {
    period <- round(1/spec$freq[which.max(spec$spec)])
    if(period==Inf) # Find next local maximum
    {
      j <- which(diff(spec$spec)>0)
      if(length(j)>0)
      {
        nextmax <- j[1] + which.max(spec$spec[j[1]:500])
        if(nextmax <= length(spec$freq))
          period <- round(1/spec$freq[nextmax])
        else
          period <- 1
      }
      else
        period <- 1
    }
  }
  else
    period <- 1
  return(period)
}

decomp <- function(x,transform=TRUE){
  require(forecast)
  # Transform series
  if(transform & min(x,na.rm=TRUE) >= 0)
  {
    lambda <- BoxCox.lambda(na.contiguous(x))
    x <- BoxCox(x,lambda)
  }
  else
  {
    lambda <- NULL
    transform <- FALSE
  }
  # Seasonal data
  if(frequency(x)>1)
  {
    writeLines("===========Decompose a Seasonal Data============")
    x.stl <- stl(x,s.window="periodic",na.action=na.contiguous)
    trend <- x.stl$time.series[,2]
    season <- x.stl$time.series[,1]
    remainder <- x - trend - season
    components.ts = decompose(x)
    plot(components.ts)
  }
  else #Nonseasonal data
  {
    writeLines("===========Decompose a NonSeasonal Data============")
    require(mgcv)
    tt <- 1:length(x)
    trend <- rep(NA,length(x))
    trend[!is.na(x)] <- fitted(gam(x ~ s(tt)))
    season <- NULL
    remainder <- x - trend
    components.ts = decompose(x)
    plot(components.ts)
  }
  return(list(x=x,trend=trend,season=season,remainder=remainder,
              transform=transform,lambda=lambda))
}

#http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.
detrended_projectory <- function(x){
  library(GeneCycle)
  x11()
  par(mfrow=c(1,3))
  plot(x, type="l", main = "Actual Time Series")
  trend <- lm(x ~ index(x), data = x)
  abline(trend, col="red")
  detrended.trajectory <- trend$residuals
  plot(detrended.trajectory, type="l", main="Detrended time series")
  f.data <- GeneCycle::periodogram(detrended.trajectory)
  harmonics <- 1:20 
  plot(f.data$freq[harmonics]*length(detrended.trajectory), 
       f.data$spec[harmonics]/sum(f.data$spec), main = "Cycle Signal Plot",
       xlab="Harmonics (Hz)", ylab="Amplitute Density", type="h")
}


#----------------------------------- Stocks Time Series Function ------------------------------------------
load_stock_symbols <- function(){
  symbols <- stockSymbols()
  symbols <- symbols[,1]
}

#https://stackoverflow.com/questions/24219694/get-symbols-quantmod-ohlc-currency-data
stock_symbol_to_xts <- function(symbol, start_date, end_date){
  library(TTR)
  library(quantmod)
  getSymbols(symbol,from=start_date, to=end_date)
  getdata_string = paste0("OHLCV(",symbol,")")
  return(eval(parse(text=getdata_string)))
}

stock_chart_trading_rules <- function(stock_symbol, add_feature=c(), date_range="2019-01-01::2019-02-01", info_rules=FALSE){
  library(TTR)
  library(quantmod)
  library(xts)
  library(rvest)
  library(tidyverse)
  library(stringr)
  library(forcats)
  library(lubridate)
  library(plotly)
  library(dplyr)
  library(PerformanceAnalytics)
  
  if(info_rules){
    writeLines("1) Add Welles Wilder's Directional Movement Indicator")
    writeLines("2) Add Average True Range")
    writeLines("3) Add Bollinger Bands")
    writeLines("4) Add Commodity Channel Index")
    writeLines("5) Add Chaiken Money Flow")
    writeLines("6) Add Chande Momentum Oscillator")
    writeLines("7) Add Double Exponential Moving Average")
    writeLines("8) Add Detrended Price Oscillator")
    writeLines("9) Add Exponential Moving Average")
    writeLines("10) Add Price Envelope")
    writeLines("11) Add Exponential Volume Weigthed Moving Average")
    writeLines("12) Add Options and Futures Expiration")
    writeLines("13) Add Moving Average Convergence Divergence")
    writeLines("14) Add Momentum")
    writeLines("15) Add Rate of Change")
    writeLines("16) Add Relative Strength Indicator")
    writeLines("17) Add Parabolic Stop and Reverse")
    writeLines("18) Add Simple Moving Average")
    writeLines("19) Add Stocastic Momentum Index")
    writeLines("20) Add Triple Smoothed Exponential Oscillator")
    writeLines("21) Add Volume")
    writeLines("22) Add Weighted Moving Average")
    writeLines("23) Add Williams %R")
    writeLines("24) Add ZLEMA")
    writeLines("25) Add Money Flow Index")
  }

  fullstartdate = paste0(strsplit(date_range, "::")[[1]][1], "-01")
  fullenddate = paste0(strsplit(date_range, "::")[[1]][2], "-01")
  print(paste("Stock Symbols = ", stock_symbol))
  print(paste("Start Date = ", fullstartdate))
  print(paste("End Date = ", fullstartdate))
  getSymbols(stock_symbol, from = fullstartdate, to = fullenddate)
  
  feature_string <- ""
  if(1 %in% add_feature){
    writeLines("Add Welles Wilder's Directional Movement Indicator")
    feature_string <- paste0(feature_string,"addADX();")
  }
  if(2 %in% add_feature){
    writeLines("Add Average True Range")
    feature_string <- paste0(feature_string,"addATR();")
  }
  if(3 %in% add_feature){
    writeLines("Add Bollinger Bands")
    feature_string <- paste0(feature_string,"addBBands();")
  }
  if(4 %in% add_feature){
    writeLines("Add Commodity Channel Index")
    feature_string <- paste0(feature_string,"addCCI();")
  }
  if(5 %in% add_feature){
    writeLines("Add Chaiken Money Flow")
    feature_string <- paste0(feature_string,"addCMF();")
  }
  if(6 %in% add_feature){
    writeLines("Add Chande Momentum Oscillator")
    feature_string <- paste0(feature_string,"addCMO();")
  }
  if(7 %in% add_feature){
    writeLines("Add Double Exponential Moving Average")
    feature_string <- paste0(feature_string,"addDEMA();")
  }
  if(8 %in% add_feature){
    writeLines("Add Detrended Price Oscillator")
    feature_string <- paste0(feature_string,"addDPO();")
  }
  if(9 %in% add_feature){
    writeLines("Add Exponential Moving Average")
    feature_string <- paste0(feature_string,"addEMA();")
  }
  if(10 %in% add_feature){
    writeLines("Add Price Envelope")
    feature_string <- paste0(feature_string,"addEnvelope();")
  }
  if(11 %in% add_feature){
    writeLines("Add Exponential Volume Weigthed Moving Average")
    feature_string <- paste0(feature_string,"addEVWMA();")
  }
  if(12 %in% add_feature){
    writeLines("Add Options and Futures Expiration")
    feature_string <- paste0(feature_string,"addExpiry();")
  }
  if(13 %in% add_feature){
    writeLines("Add Moving Average Convergence Divergence")
    feature_string <- paste0(feature_string,"addMACD();")
  }
  if(14 %in% add_feature){
    writeLines("Add Momentum")
    feature_string <- paste0(feature_string,"addMomentum();")
  }
  if(15 %in% add_feature){
    writeLines("Add Rate of Change")
    feature_string <- paste0(feature_string,"addROC();")
  }
  if(16 %in% add_feature){
    writeLines("Add Relative Strength Indicator")
    feature_string <- paste0(feature_string,"addRSI();")
  }
  if(17 %in% add_feature){
    writeLines("Add Parabolic Stop and Reverse")
    feature_string <- paste0(feature_string,"addSAR();")
  }
  if(18 %in% add_feature){
    writeLines("Add Simple Moving Average")
    feature_string <- paste0(feature_string,"addSMA();")
  }
  if(19 %in% add_feature){
    writeLines("Add Stocastic Momentum Index")
    feature_string <- paste0(feature_string,"addSMI();")
  }
  if(20 %in% add_feature){
    writeLines("Add Triple Smoothed Exponential Oscillator")
    feature_string <- paste0(feature_string,"addTRIX();")
  }
  if(21 %in% add_feature){
    writeLines("Add Volume")
    feature_string <- paste0(feature_string,"addVo();")
  }
  if(22 %in% add_feature){
    writeLines("Add Weighted Moving Average")
    feature_string <- paste0(feature_string,"addWMA();")
  }
  if(23 %in% add_feature){
    writeLines("Add Williams %R")
    feature_string <- paste0(feature_string,"addWPR();")
  }
  if(24 %in% add_feature){
    writeLines("Add ZLEMA")
    feature_string <- paste0(feature_string,"addZLEMA();")
  }
  if(25 %in% add_feature){
    writeLines("Add ZLEMA")
    feature_string <- paste0(feature_string,"addMFI();")
  }
  x11()
  chart_series_eval <- paste0(stock_symbol,"%>%Ad()%>%chartSeries()")
  chart_plot_eval <- paste0(stock_symbol,"%>%chartSeries(TA='",feature_string,"',subset='",date_range,"')")
  eval(parse(text=chart_series_eval))
  eval(parse(text=chart_plot_eval))
}

#----------------------------------- Time Series Tools & Forecasting ----------------------------------------
difference <- function(vec, saved_initial_vec_name=""){
  if(any(is.na(vec))){
    writeLines("Remove every NA values in time series first")
    var_name <- "clear_time_series"
    vec <- na.omit(vec)
    assign(var_name, vec, envir = .GlobalEnv)
  }
  if(any(class(vec) %like% "ts")){
    saved_index <- paste0("index_",saved_initial_vec_name)
    assign(saved_initial_vec_name, vec[1], envir = .GlobalEnv)
    assign(saved_index, index(vec[1]), envir = .GlobalEnv) 
    x11()
    par(mfrow=c(1,2))
    print(plot(vec, main="Before Differencing"))
    vec <- diff(vec)
  }else{
    assign(saved_initial_vec_name, vec[1], envir = .GlobalEnv)
    vec <- diff(vec)
  }
  if(any(is.na(vec))){
    writeLines("Remove NA values after differencing")
    vec <- na.omit(vec)
    print(plot(vec, main="After Differencing"))
  }
  return(vec)
}

undifference <- function(vec, initial_value=0, index_value=""){
  undifference_vec <- c(initial_value)
  for(v in 1:length(vec)){
    last_value <- undifference_vec[length(undifference_vec)]
    new_value <- last_value + as.numeric(vec[v])
    undifference_vec <- c(undifference_vec, new_value)
  }
  if(any(class(vec) %like% "ts")){
    index_new_vec <- c(index_value, index(vec))
    print(index_new_vec)
    new_xts <- xts(undifference_vec, order.by=index_new_vec)
    return(new_xts)
  }
  else{
    return(undifference_xts) 
  }
}

power_transformation_lambda <- function(vec){
  library(tseries)
  library(forecast)
  library(prophet)
  library(quantmod)
  old_lambda <- BoxCox.lambda(na.contiguous(vec))
  writeLines(paste("Current lambda before Transformed = ", old_lambda))
  transformed <- BoxCox(vec,old_lambda)
  new_lambda <- BoxCox.lambda(na.contiguous(transformed))
  writeLines(paste("lambda after Transformed = ", new_lambda))
  x11()
  par(mfrow=c(1,2))
  print(plot(vec, main = "Before Transform BoxCox"))
  print(plot(transformed, main = "After Transform BoxCox with Lambda"))
  return(transformed)
}

dickey_fuller_extended_test <- function(time_series_set){
  #https://stats.stackexchange.com/questions/24072/interpreting-rs-ur-df-dickey-fuller-unit-root-test-results
  library(urca)
  writeLines("Using URCA Packages there is 3 types of Dickey Fuller test implementation which can be choose: ")
  writeLines("Note: (a0 is a sub zero, drift constant term, a2 is trend term)")
  writeLines("1) Type=none, testing that (Delta)yt= (Gamma)yt???1+et ")
  writeLines("H0: Gamma=0 (Contains Unit Root / Not Stationary)")
  writeLines("H1: Gamma!=0 (No Unit Root, Time Series is considered Random Walk / Stationary)")
  none_result <- summary(ur.df(time_series_set, type="none", lags=1, selectlags = c("Fixed", "AIC", "BIC")))
  print(none_result)
  none_test_stat <- none_result@teststat
  none_critical_val <- none_result@cval
  writeLines("=========================== Conclusion by type None ==============================")
  if(none_test_stat < none_critical_val[3]){
    writeLines("Tau Test Statistic is valid for 10 Percent Critical Values!")
    writeLines("Reject Null With 90% Confidence, Conclude that Gamma!=0")
    if(none_test_stat < none_critical_val[2]){
      writeLines("Tau Test Statistic is valid for 5 Percent Critical Values!")
      writeLines("Reject Null With 95% Confidence, Conclude that Gamma!=0")
      if(none_test_stat < none_critical_val[1]){
        writeLines("Tau Test Statistic is valid for 1 Percent Critical Values!")
        writeLines("Reject Null With 99% Confidence, Conclude that Gamma!=0")
      }
    }
  }else if(none_test_stat >= none_critical_val[3]){
    writeLines("Tau Test Statistic is not valid!")
    writeLines("Fail to Reject Null, Conclude that Gamma=0 (Not Statistically Significant)")
  }
  writeLines("===================================================================================")
  writeLines("2) Type=drift, testing that (Delta)yt= a0+ (gamma)yt???1+et")
  writeLines("Tau Value Hypothesis")
  writeLines("H0: Gamma=0 (Contains Unit Root/Not Stationary)")
  writeLines("H1: Gamma!=0 (No Unit Root, Time Series is considered Random Walk/Stationary)")
  writeLines("Phi1 Value Hypothesis")
  writeLines("H0: a0=Gamma=0 (Concludes that there is no drift term)")
  writeLines("H1: one or both a0 and gamma is != 0")
  drift_result <- summary(ur.df(time_series_set, type="drift", lags=1, selectlags = c("Fixed", "AIC", "BIC")))
  print(drift_result)
  drift_tau_result <- drift_result@teststat[1]
  drift_phi_result <- drift_result@teststat[2]
  drift_tau_critical <- as.numeric(drift_result@cval[1,])
  drift_phi_critical <- as.numeric(drift_result@cval[2,])
  writeLines("=========================== Conclusion by type Drift ==============================")
  if(drift_tau_result < drift_tau_critical[3]){
    writeLines("Tau Test Statistic is valid for 10 Percent Critical Values!")
    writeLines("Reject Null With 90% Confidence, Conclude that Gamma!=0")
    if(drift_tau_result < drift_tau_critical[2]){
      writeLines("Tau Test Statistic is valid for 5 Percent Critical Values!")
      writeLines("Reject Null With 95% Confidence, Conclude that Gamma!=0")
      if(drift_tau_result < drift_tau_critical[1]){
        writeLines("Tau Test Statistic is valid for 1 Percent Critical Values!")
        writeLines("Reject Null With 99% Confidence, Conclude that Gamma!=0")
      }
    }
  }else if(drift_tau_result >= drift_tau_critical[3]){
    writeLines("Tau Test Statistic is not valid!")
    writeLines("Fail to Reject Null, Conclude that Gamma=0 (Not Statistically Significant)")
  }
  if(drift_phi_result > drift_phi_critical[3]){
    writeLines("Phi Test Statistic is valid for 10 Percent Critical Values!")
    writeLines("Reject Null With 90% Confidence, Conclude that one or both a0 and gamma is != 0")
    if(drift_phi_result > drift_phi_critical[2]){
      writeLines("Phi Test Statistic is valid for 5 Percent Critical Values!")
      writeLines("Reject Null With 95% Confidence, Conclude that one or both a0 and gamma is != 0")
      if(drift_phi_result > drift_phi_critical[1]){
        writeLines("Phi Test Statistic is valid for 1 Percent Critical Values!")
        writeLines("Reject Null With 99% Confidence, Conclude that one or both a0 and gamma is != 0")
      }
    }
  }else if(drift_phi_result <= drift_phi_critical[3]){
    writeLines("Phi Test Statistic is not valid!")
    writeLines("Fail to Reject Null, Conclude that There is no Drift term in the time series and also Gamma is not statistically significant")
  }
  writeLines("===================================================================================")
  writeLines("3) Type=trend, testing that (Delta)yt=a0+(Gamma)yt???1+a2t+et")
  writeLines("Tau Value Hypothesis")
  writeLines("H0: Gamma=0 (Contains Unit Root/Not Stationary)")
  writeLines("H1: Gamma!=0 (No Unit Root, Time Series is considered Random Walk/Stationary)")
  writeLines("Phi3 Value Hypothesis")
  writeLines("H0: a2=Gamma=0 (Concludes that there is no trend term")
  writeLines("H1: one or both a2 and gamma is != 0")
  writeLines("Phi2 Value Hypothesis")
  writeLines("H0: a2=a0=Gamma=0 (Concludes that there is no drift and trend term)")
  writeLines("H1: one or both or all three, a0 a2 and gamma is != 0")
  trend_result <- summary(ur.df(time_series_set, type="trend", lags=1, selectlags = c("Fixed", "AIC", "BIC")))
  print(trend_result)
  trend_tau_result <- trend_result@teststat[1]
  trend_phi2_result <- trend_result@teststat[2]
  trend_phi3_result <- trend_result@teststat[3]
  trend_tau_critical <- as.numeric(trend_result@cval[1,])
  trend_phi2_critical <- as.numeric(trend_result@cval[2,])
  trend_phi3_critical <- as.numeric(trend_result@cval[3,])
  writeLines("=========================== Conclusion by type Trend ==============================")
  if(trend_tau_result < trend_tau_critical[3]){
    writeLines("Tau Test Statistic is valid for 10 Percent Critical Values!")
    writeLines("Reject Null With 90% Confidence, Conclude that Gamma!=0")
    if(trend_tau_result < trend_tau_critical[2]){
      writeLines("Tau Test Statistic is valid for 5 Percent Critical Values!")
      writeLines("Reject Null With 95% Confidence, Conclude that Gamma!=0")
      if(trend_tau_result < trend_tau_critical[1]){
        writeLines("Tau Test Statistic is valid for 1 Percent Critical Values!")
        writeLines("Reject Null With 99% Confidence, Conclude that Gamma!=0")
      }
    }
  }else if(trend_tau_result >= trend_tau_critical[3]){
    writeLines("Tau Test Statistic is not valid!")
    writeLines("Fail to Reject Null, Conclude that Gamma=0 (Not Statistically Significant)")
  }
  if(trend_phi2_result > trend_phi2_critical[3]){
    writeLines("Phi Test Statistic is valid for 10 Percent Critical Values!")
    writeLines("Reject Null With 90% Confidence, Conclude that one or both or all three, a0 a2 and gamma is != 0")
    if(trend_phi2_result > trend_phi2_critical[2]){
      writeLines("Phi Test Statistic is valid for 5 Percent Critical Values!")
      writeLines("Reject Null With 95% Confidence, Conclude that one or both or all three, a0 a2 and gamma is != 0")
      if(trend_phi2_result > trend_phi2_critical[1]){
        writeLines("Phi Test Statistic is valid for 1 Percent Critical Values!")
        writeLines("Reject Null With 99% Confidence, Conclude that one or both or all three, a0 a2 and gamma is != 0")
      }
    }
  }else if(trend_phi2_result <= trend_phi2_critical[3]){
    writeLines("Phi Test Statistic is not valid!")
    writeLines("Fail to Reject Null, Conclude that There is no Drift and Trend term in the time series and also Gamma is not statistically significant")
  }
  if(trend_phi3_result > trend_phi3_critical[3]){
    writeLines("Phi3 Test Statistic is valid for 10 Percent Critical Values!")
    writeLines("Reject Null With 90% Confidence, Conclude that one or both a2 and gamma is != 0")
    if(trend_phi3_result > trend_phi3_critical[2]){
      writeLines("Phi3 Test Statistic is valid for 5 Percent Critical Values!")
      writeLines("Reject Null With 95% Confidence, Conclude one or both a2 and gamma is != 0")
      if(trend_phi3_result > trend_phi3_critical[1]){
        writeLines("Phi3 Test Statistic is valid for 1 Percent Critical Values!")
        writeLines("Reject Null With 100% Confidence, Conclude one or both a2 and gamma is != 0")
      }
    }
  }else if(trend_phi3_result <= trend_phi3_critical[3]){
    writeLines("Phi3 Test Statistic is not valid!")
    writeLines("Fail to Reject Null, Conclude that There is no Trend term in the time series and also Gamma is not statistically significant")
  }
  writeLines("===================================================================================")
  return(list(none_result, drift_result, trend_result))
}

kpss_test <- function(time_series_set){
  writeLines("H0: Time Series is Stationary around a deterministic trend")
  writeLines("H1: Time Series is not Stationary")
  g <- kpss.test(time_series_set)
  print(g)
  if(g$p.value >= 0.05){
    writeLines('Accept H0, This Concludes that Time Series is Stationary')
  }else if(g$p.value < 0.05){
    writeLines('Reject H0, This Concludes that Time Series is Not Stationary')
  }
}

ts_stationary_check <- function(ts, stationary_method = "Both"){
  if(stationary_method=="Both"){
    writeLines("H0: Process is Non Stationary in mean")
    writeLines("H1: Process is Stationary in mean")
    print(adf.test(ts))
    adf = adf.test(ts)$p.value
    if(adf < 0.05){
      writeLines(paste("Process is already Statonary in mean, with Pvalue", adf))
      break
    }
    else if (adf >= 0.05){
      writeLines(paste("Process is Non Stationary in mean, Differences the Time Series, with Pvalue", adf))
    }
    writeLines("Continue to Stationary Variance test")
    writeLines("H0: Process is Non Stationary in Variance")
    writeLines("H1: Process is Stationary in Variance")
    lambda <- BoxCox.lambda(na.contiguous(ts))
    print(paste("Lambda = ", lambda))
    if(lambda != 1){
      writeLines(paste("Process is Not Stationary in variance (Transforming BotsCots), lambda = ", lambda))
    }
    else if(lambda == 1){
      writeLines(paste("Process is Statonary in variance (Continue to Checking Stationary in mean), lambda = ", lambda))
    }
  } 
  else if(stationary_method=="Mean"){
    writeLines("H0: Process is Non Stationary in mean")
    writeLines("H1: Process is Stationary in mean")
    print(adf.test(ts))
    adf = adf.test(ts)$p.value
    if(adf < 0.05){
      writeLines(paste("Process is already Statonary in mean, with Pvalue", adf))
    }
    else if (adf >= 0.05){
      writeLines(paste("Process is Non Stationary in mean, Differences the Time Series, with Pvalue", adf))
    }
    
  }
  else if(stationary_method=="Variance"){
    writeLines("H0: Process is Non Stationary in Variance")
    writeLines("H1: Process is Stationary in Variance")
    lambda <- BoxCox.lambda(na.contiguous(ts))
    print(paste("Lambda = ", lambda))
    if(lambda != 1){
      writeLines(paste("Process is Not Stationary in variance (Transforming BotsCots), lambda = ", lambda))
    }
    else if(lambda == 1){
      writeLines(paste("Process is Statonary in variance (Continue to Checking Stationary in mean), lambda = ", lambda))
    }
  }
}

ts_find_frequency <- function(ts){
  n <- length(ts)
  spec <- spec.ar(c(na.contiguous(ts)),plot=FALSE)
  if(mats(spec$spec)>10) # Arbitrary threshold chosen by trial and error.
  {
    period <- round(1/spec$freq[which.mats(spec$spec)])
    if(period==Inf) # Find netst local matsimum
    {
      j <- which(diff(spec$spec)>0)
      if(length(j)>0)
      {
        netstmats <- j[1] + which.mats(spec$spec[j[1]:500])
        if(netstmats <= length(spec$freq))
          period <- round(1/spec$freq[netstmats])
        else
          period <- 1
      }
      else
        period <- 1
    }
  }
  else
    period <- 1
  return(period)
}

ts_decomposition <- function(ts,transform=TRUE){
  require(forecast)
  # Transform series
  if(transform & min(ts,na.rm=TRUE) >= 0)
  {
    lambda <- BotsCots.lambda(na.contiguous(ts))
    ts <- BotsCots(ts,lambda)
  }
  else
  {
    lambda <- NULL
    transform <- FALSE
  }
  # Seasonal data
  if(frequency(ts)>1)
  {
    writeLines("===========Decompose a Seasonal Data============")
    ts.stl <- stl(ts,s.window="periodic",na.action=na.contiguous)
    trend <- ts.stl$time.series[,2]
    season <- ts.stl$time.series[,1]
    remainder <- ts - trend - season
    components.ts = decompose(ts)
    plot(components.ts)
  }
  else #Nonseasonal data
  {
    writeLines("===========Decompose a NonSeasonal Data============")
    require(mgcv)
    tt <- 1:length(ts)
    trend <- rep(NA,length(ts))
    trend[!is.na(ts)] <- fitted(gam(ts ~ s(tt)))
    season <- NULL
    remainder <- ts - trend
    components.ts = decompose(ts)
    plot(components.ts)
  }
  return(list(ts=ts,trend=trend,season=season,remainder=remainder,
              transform=transform,lambda=lambda))
}

#http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.
#This Feature shows Seasonal Time Series that is purely not partial dependently to trend attribute
detrended_projectory <- function(x){
  library(GeneCycle)
  x11()
  par(mfrow=c(1,3))
  plot(x, type="l", main = "Actual Time Series")
  trend <- lm(x ~ index(x), data = x)
  abline(trend, col="red")
  detrended.trajectory <- trend$residuals
  plot(detrended.trajectory, type="l", main="Detrended time series")
  f.data <- GeneCycle::periodogram(detrended.trajectory)
  harmonics <- 1:20 
  plot(f.data$freq[harmonics]*length(detrended.trajectory), 
       f.data$spec[harmonics]/sum(f.data$spec), main = "Cycle Signal Plot",
       xlab="Harmonics (Hz)", ylab="Amplitute Density", type="h")
}

forecast_tools <- function(x, n_forecast=5, chosen_prophet_freq='day', metric_choosen=c(8)){
  library(forecast)
  library(prophet)
  library(tseries)
  library(quantmod)
  library(ggplot2)
  library(Metrics)
  x11()
  par(mfrow=c(2,1))
  df_x <- NULL
  
  if(class(x) == "ts"){
    start_date <- as.character(start(x))
    end_date <- as.character(end(x))
    if(start_date[2] < 10){
      start_date[2] <- paste0("0",start_date[2])
    }
    if(end_date[2] < 10){
      end_date[2] <- paste0("0",end_date[2])
    }
    start_date <- as.Date(paste0(paste0(start_date, collapse="-"),"-01"))
    end_date <- as.Date(paste0(paste0(end_date, collapse="-"),"-01"))
    seq_date <- seq(start_date, end_date, by="month")
    df_x = data.frame(ds=seq_date, y=as.numeric(x))
  }else if(class(x) %in% c("xts")){
    df_x = data.frame(ds=index(x), y=as.numeric(x)) 
  }
  
  writeLines("Forecast Feature Include of:")
  writeLines("1) ARIMA (Auto Regressive Integrated Moving Average) Modeling ")
  writeLines("2) ETS (Exponential Smooth State Space) Modeling ")
  writeLines("3) Prophet Modeling ")
  writeLines("4) Simple Exponential Smoothing Modeling ")
  writeLines("5) Holt Winters Modeling ")
  writeLines("6) BATS (Exponential Smooth State Space with Box-Cox transformation) Modeling ")
  writeLines("7) TBATS (ARMA Error + Trend + Seasonal + Box-Cox for Heterogeneity + Trigonometric Seasonality) Modeling ")
  writeLines("8) Naive Model (Random Walk)")
  
  writeLines('Choosen Available Metrics to Error Measurements: ')
  writeLines("1) MAE (Mean Absolute Error)")
  writeLines("2) MAPE (Mean Absolute Percentage Error)")
  writeLines("3) MASE (Mean Absolute Scaled Error)")
  writeLines("4) MDAE (Median Absolute Error)")
  writeLines("5) MSE (Mean Square Error)")
  writeLines("6) MSLE (Mean Square Log Error)")
  writeLines("7) RAE (Relative Absolute Error)")
  writeLines("8) RMSE (Root Mean Square Error)")
  writeLines("9) RMSLE (Root Mean Square Log Error)")
  writeLines("10) RRSE (Root Relative Squared Error)")
  writeLines("11) RSE (Relative Squared Error)")
  writeLines("12) SMAPE (Symmetric Mean Absolute Percentage Error)")
  writeLines("13) SSE (Sum of Square Error)")
  
  ################################### Forecasting with ARIMA Model ####################################
  fit_ARIMA <- auto.arima(x, trace=TRUE) 
  print(summary(fit_ARIMA))
  writeLines("\n\n Generating ARIMA Model Summary \n\n")
  arma_parameter = fit_ARIMA$arma
  ar_ns <- arma_parameter[1]
  ma_ns <- arma_parameter[2]
  ar_s <- arma_parameter[3]
  ma_s <- arma_parameter[4]
  period <- arma_parameter[5]
  diff_ns <- arma_parameter[6]
  diff_s <- arma_parameter[7]
  
  futureARIMA <- forecast::forecast(fit_ARIMA, h=n_forecast)
  if(ar_s == 0 && ma_s == 0 && diff_s == 0)
    plot(futureARIMA , sub=paste0("Plot Forecast ARIMA(",ar_ns,",",ma_ns,",",diff_ns,")"))
  else{
    plot(futureARIMA , sub=paste0("Plot Forecast ARIMA(",ar_ns,",",ma_ns,",",diff_ns,")(",ar_s,",",ma_s,",",diff_s,")[",period,"]"))
  }
  
  observed <- as.numeric(x)
  predicted <- as.numeric(fitted(fit_ARIMA))
  df_comparison = cbind(observed, predicted)
  
  mae_value = mae(observed, predicted)
  mape_value = mape(observed, predicted)
  mase_value = mase(observed, predicted)
  mdae_value = mdae(observed, predicted)
  mse_value = mse(observed, predicted)
  msle_value = msle(observed, predicted)
  rae_value = rae(observed, predicted)
  rmse_value = rmse(observed, predicted)
  rmsle_value = rmsle(observed, predicted)
  rrse_value = rrse(observed, predicted)
  rse_value = rse(observed, predicted)
  smape_value = smape(observed, predicted)
  sse_value = sse(observed, predicted)
  metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                   rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
  metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                   "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
  metric_info_string <- ""
  for(met in 1:length(metric_choosen)){
    if(met == length(metric_choosen)){
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
    }else{
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
    }
  }
  labelx = df_x[,1]
  plot(labelx, observed, col='blue', main=paste0("ARIMA Comparison Model between Actual and Fitted Values"), 
       sub=metric_info_string)
  points(labelx, predicted,col='red')
  lines(labelx, observed, lwd=2, col="blue")
  lines(labelx, predicted, lwd=2, col="red")
  legend("bottomleft", legend=c("Observed", "Predicted"), 
         col=c("blue","red"), lty=1:2, cex=0.8) 
  
  arima_df_comparison <- df_comparison
  
  ################### Forecasting with ETS (Exponential Smooth State Space) Model ##############################
  x11()
  par(mfrow=c(2,1))
  fit_ETS <- ets(x)
  writeLines("\n\n Generating ETS Model Summary \n\n")
  print(summary(fit_ETS))
  futureETS <- forecast::forecast(fit_ETS, h=n_forecast)
  plot(futureETS, sub="ETS Forecast Model")
  
  observed <- as.numeric(x)
  predicted <- as.numeric(fitted(fit_ETS))
  df_comparison = cbind(observed, predicted)
  
  mae_value = mae(observed, predicted)
  mape_value = mape(observed, predicted)
  mase_value = mase(observed, predicted)
  mdae_value = mdae(observed, predicted)
  mse_value = mse(observed, predicted)
  msle_value = msle(observed, predicted)
  rae_value = rae(observed, predicted)
  rmse_value = rmse(observed, predicted)
  rmsle_value = rmsle(observed, predicted)
  rrse_value = rrse(observed, predicted)
  rse_value = rse(observed, predicted)
  smape_value = smape(observed, predicted)
  sse_value = sse(observed, predicted)
  metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                   rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
  metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                   "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
  metric_info_string <- ""
  for(met in 1:length(metric_choosen)){
    if(met == length(metric_choosen)){
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
    }else{
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
    }
  }
  
  labelx = df_x[,1]
  plot(labelx, observed, col='blue', main=paste0("ETS Comparison Model between Actual and Fitted Values"), 
       sub=metric_info_string)
  points(labelx, predicted,col='red')
  lines(labelx, observed, lwd=2, col="blue")
  lines(labelx, predicted, lwd=2, col="red")
  legend("bottomleft", legend=c("Observed", "Predicted"), 
         col=c("blue","red"), lty=1:2, cex=0.8) 
  
  ets_df_comparison <- df_comparison
  
  ################################ Forecasting with Prophet Model ##############################
  x11()
  fit_prophet <- prophet(df_x)
  writeLines("\n\n Generating Prophet Model Summary \n\n")
  print(summary(fit_prophet))
  generate_future <- make_future_dataframe(fit_prophet, periods = n_forecast, 
                                           freq=chosen_prophet_freq)
  futureProphet <- predict(fit_prophet, generate_future)
  print(plot(fit_prophet, futureProphet) + ggtitle("Prophet Forecast Model"))
  x11()
  prophet_plot_components(fit_prophet, futureProphet) 
  x11()
  observed <- as.numeric(x)
  predicted <- futureProphet$yhat[1:(length(futureProphet$yhat) - n_forecast)]
  df_comparison = cbind(observed, predicted)
  
  mae_value = mae(observed, predicted)
  mape_value = mape(observed, predicted)
  mase_value = mase(observed, predicted)
  mdae_value = mdae(observed, predicted)
  mse_value = mse(observed, predicted)
  msle_value = msle(observed, predicted)
  rae_value = rae(observed, predicted)
  rmse_value = rmse(observed, predicted)
  rmsle_value = rmsle(observed, predicted)
  rrse_value = rrse(observed, predicted)
  rse_value = rse(observed, predicted)
  smape_value = smape(observed, predicted)
  sse_value = sse(observed, predicted)
  metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                   rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
  metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                   "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
  metric_info_string <- ""
  for(met in 1:length(metric_choosen)){
    if(met == length(metric_choosen)){
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
    }else{
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
    }
  }
  
  labelx = df_x[,1]
  plot(labelx, observed, col='blue', main=paste0("Prophet Comparison Model between Actual and Fitted Values"), 
       sub=metric_info_string)
  points(labelx, predicted,col='red')
  lines(labelx, observed, lwd=2, col="blue")
  lines(labelx, predicted, lwd=2, col="red")
  legend("bottomleft", legend=c("Observed", "Predicted"), 
         col=c("blue","red"), lty=1:2, cex=0.8) 
  
  prophet_df_comparison <- df_comparison
  
  ###################### Forecast with Simple Exponential Smoothing Model ######################
  x11()
  par(mfrow=c(2,1))
  fit_ses <- ses(x, h = n_forecast)
  writeLines("\n\n Generating SES Model Summary \n\n")
  print(summary(fit_ses))
  plot(forecast::forecast(fit_ses, h=n_forecast), sub="SES Forecast Model")
  
  observed <- as.numeric(x)
  predicted <- as.numeric(fitted(fit_ses))
  df_comparison = cbind(observed, predicted)
  
  mae_value = mae(observed, predicted)
  mape_value = mape(observed, predicted)
  mase_value = mase(observed, predicted)
  mdae_value = mdae(observed, predicted)
  mse_value = mse(observed, predicted)
  msle_value = msle(observed, predicted)
  rae_value = rae(observed, predicted)
  rmse_value = rmse(observed, predicted)
  rmsle_value = rmsle(observed, predicted)
  rrse_value = rrse(observed, predicted)
  rse_value = rse(observed, predicted)
  smape_value = smape(observed, predicted)
  sse_value = sse(observed, predicted)
  metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                   rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
  metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                   "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
  metric_info_string <- ""
  for(met in 1:length(metric_choosen)){
    if(met == length(metric_choosen)){
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
    }else{
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
    }
  }
  
  labelx = df_x[,1]
  plot(labelx, observed, col='blue', main=paste0("SES (Simple Exponential Smoothing) Comparison Model between Actual and Fitted Values"), 
       sub=metric_info_string)
  points(labelx, predicted,col='red')
  lines(labelx, observed, lwd=2, col="blue")
  lines(labelx, predicted, lwd=2, col="red")
  legend("bottomleft", legend=c("Observed", "Predicted"), 
         col=c("blue","red"), lty=1:2, cex=0.8) 
  
  ses_df_comparison <- df_comparison
  
  ########################## Forecasting with Holt Winters Model ###############################
  x11()
  par(mfrow=c(2,1))
  fit_HW <- HoltWinters(x,gamma=FALSE)
  writeLines("\n\n Generating Holt Winters Model Summary \n\n")
  print(summary(fit_HW))
  plot(forecast::forecast(fit_HW, h=n_forecast), sub="Holt Winters Forecast Model")
  
  #Minimal Start Period for HoltWinters is 2, so fitted values is not a full ts
  observed <- as.numeric(x[3:length(x)])
  predicted <- as.numeric(fitted(fit_HW)[,1])
  df_comparison = cbind(observed, predicted)
  
  mae_value = mae(observed, predicted)
  mape_value = mape(observed, predicted)
  mase_value = mase(observed, predicted)
  mdae_value = mdae(observed, predicted)
  mse_value = mse(observed, predicted)
  msle_value = msle(observed, predicted)
  rae_value = rae(observed, predicted)
  rmse_value = rmse(observed, predicted)
  rmsle_value = rmsle(observed, predicted)
  rrse_value = rrse(observed, predicted)
  rse_value = rse(observed, predicted)
  smape_value = smape(observed, predicted)
  sse_value = sse(observed, predicted)
  metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                   rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
  metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                   "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
  metric_info_string <- ""
  for(met in 1:length(metric_choosen)){
    if(met == length(metric_choosen)){
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
    }else{
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
    }
  }
  
  labelx = df_x[,1][3:nrow(df_x)]
  plot(labelx, observed, col='blue', main=paste0("Holt Winters Comparison Model between Actual and Fitted Values"), 
       sub=metric_info_string)
  points(labelx, predicted,col='red')
  lines(labelx, observed, lwd=2, col="blue")
  lines(labelx, predicted, lwd=2, col="red")
  legend("bottomleft", legend=c("Observed", "Predicted"), 
         col=c("blue","red"), lty=1:2, cex=0.8) 
  
  holtwinters_df_comparison <- df_comparison
  fit_HW_gamma <- NULL
  holwinters_gamma_df_comparison <- NULL
  
  ########################## Forecasting with Holt Winters + Gamma Model ###############################
  if(length(x) > 12){
    x11()
    par(mfrow=c(2,1))
    fit_HW_gamma <- HoltWinters(x)
    writeLines("\n\n Generating Holt Winters with Gamma as Seasonal Parameter Model Summary \n\n")
    print(summary(fit_HW_gamma))
    plot(forecast::forecast(fit_HW_gamma, h=n_forecast), sub="Holt Winters Forecast Model")
    
    observed <- as.numeric(x[13:length(x)])
    predicted <- as.numeric(fitted(fit_HW_gamma)[,1])
    ylim_min <- min(c(observed, predicted))
    ylim_max <- max(c(observed, predicted))
    predict_length <- length(predicted)
    print(length(observed))
    print(length(predicted))
    df_comparison = cbind(observed, predicted)
    
    mae_value = mae(observed, predicted)
    mape_value = mape(observed, predicted)
    mase_value = mase(observed, predicted)
    mdae_value = mdae(observed, predicted)
    mse_value = mse(observed, predicted)
    msle_value = msle(observed, predicted)
    rae_value = rae(observed, predicted)
    rmse_value = rmse(observed, predicted)
    rmsle_value = rmsle(observed, predicted)
    rrse_value = rrse(observed, predicted)
    rse_value = rse(observed, predicted)
    smape_value = smape(observed, predicted)
    sse_value = sse(observed, predicted)
    metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                     rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
    metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                     "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
    metric_info_string <- ""
    for(met in 1:length(metric_choosen)){
      if(met == length(metric_choosen)){
        metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
      }else{
        metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
      }
    }
    
    labelx = df_x[,1][13:nrow(df_x)]
    plot(labelx, observed, col='blue', main=paste0("Holt Winters Seasonal Comparison Model between Actual and Fitted Values"), 
         sub=metric_info_string, 
         ylim=c((ylim_min - 0.1 * ylim_min),(ylim_max + 0.1 * ylim_max)))
    points(labelx, predicted,col='red')
    lines(labelx, observed, lwd=2, col="blue")
    lines(labelx, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8) 
    
    holtwinters_gamma_df_comparison <- df_comparison
  }
  else{
    writeLines("Not Possible to Make HoltWinters Seasonal with Current Datasets!")
  }
  
  bats_df_comparison <- NULL
  tbats_df_comparison <- NULL
  
  if(class(x)[1] == "ts"){
    #### Forecasting with BATS (Exponential Smooth State Space with Box-Cox transformation ) ######
    x11()
    par(mfrow=c(2,1))
    fit_BATS <- bats(x)
    writeLines("\n\n Generating BATS Model Summary \n\n")
    print(summary(fit_BATS))
    plot(forecast::forecast(fit_BATS, h=n_forecast), sub="BATS Forecast Model")
    
    observed <- as.numeric(x)
    predicted <- as.numeric(fitted(fit_BATS))
    df_comparison = cbind(observed, predicted)
    
    mae_value = mae(observed, predicted)
    mape_value = mape(observed, predicted)
    mase_value = mase(observed, predicted)
    mdae_value = mdae(observed, predicted)
    mse_value = mse(observed, predicted)
    msle_value = msle(observed, predicted)
    rae_value = rae(observed, predicted)
    rmse_value = rmse(observed, predicted)
    rmsle_value = rmsle(observed, predicted)
    rrse_value = rrse(observed, predicted)
    rse_value = rse(observed, predicted)
    smape_value = smape(observed, predicted)
    sse_value = sse(observed, predicted)
    metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                     rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
    metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                     "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
    metric_info_string <- ""
    for(met in 1:length(metric_choosen)){
      if(met == length(metric_choosen)){
        metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
      }else{
        metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
      }
    }
    
    labelx = df_x[,1]
    plot(labelx, observed, col='blue', main=paste0("BATS (State Space Model + Box-Cox) Comparison Model between Actual and Fitted Values"), 
         sub=metric_info_string)
    points(labelx, predicted,col='red')
    lines(labelx, observed, lwd=2, col="blue")
    lines(labelx, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8) 
    
    bats_df_comparison <- df_comparison
    
    #### Forecast with TBATS (ARMA Error + Trend + Seasonal + Box-Cox for Heterogeneity + Trigonometric Seasonality) Model ########
    x11()
    par(mfrow=c(2,1))
    fit_TBATS <- tbats(x)
    writeLines("\n\n Generating TBATS Model Summary \n\n")
    print(summary(fit_TBATS))
    plot(forecast::forecast(fit_TBATS, h=n_forecast), sub="TBATS Forecast Model")
    
    observed <- as.numeric(x)
    predicted <- as.numeric(fitted(fit_TBATS))
    df_comparison = cbind(observed, predicted)
    
    mae_value = mae(observed, predicted)
    mape_value = mape(observed, predicted)
    mase_value = mase(observed, predicted)
    mdae_value = mdae(observed, predicted)
    mse_value = mse(observed, predicted)
    msle_value = msle(observed, predicted)
    rae_value = rae(observed, predicted)
    rmse_value = rmse(observed, predicted)
    rmsle_value = rmsle(observed, predicted)
    rrse_value = rrse(observed, predicted)
    rse_value = rse(observed, predicted)
    smape_value = smape(observed, predicted)
    sse_value = sse(observed, predicted)
    metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                     rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
    metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                     "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
    metric_info_string <- ""
    for(met in 1:length(metric_choosen)){
      if(met == length(metric_choosen)){
        metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
      }else{
        metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
      }
    }
    
    labelx = df_x[,1]
    plot(labelx, observed, col='blue', main=paste0("TBATS (ARMA Error + Trend + State Space Model + Box-Cox) Comparison Model between Actual and Fitted Values"), 
         sub=metric_info_string)
    points(labelx, predicted,col='red')
    lines(labelx, observed, lwd=2, col="blue")
    lines(labelx, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8) 
    
    tbats_df_comparison <- df_comparison
  }
  
  ##################### Forecast with Naive Model (Random Walk) ###########################
  x11()
  par(mfrow=c(2,1))
  fit_Naive <- naive(x, h=n_forecast)
  writeLines("\n\n Generating Naive Model Summary \n\n")
  print(summary(fit_Naive))
  plot(forecast::forecast(fit_Naive, h=n_forecast), sub="Naive Forecast Model")
  
  observed <- as.numeric(x)
  predicted <- as.numeric(fitted(fit_Naive))
  predicted[is.na(predicted)] <- 0
  df_comparison = cbind(observed, predicted)
  
  mae_value = mae(observed, predicted)
  mape_value = mape(observed, predicted)
  mase_value = mase(observed, predicted)
  mdae_value = mdae(observed, predicted)
  mse_value = mse(observed, predicted)
  msle_value = msle(observed, predicted)
  rae_value = rae(observed, predicted)
  rmse_value = rmse(observed, predicted)
  rmsle_value = rmsle(observed, predicted)
  rrse_value = rrse(observed, predicted)
  rse_value = rse(observed, predicted)
  smape_value = smape(observed, predicted)
  sse_value = sse(observed, predicted)
  metric_calc <- c(mae_value, mape_value, mase_value, mdae_value, mse_value, msle_value,
                   rae_value, rmse_value, rmsle_value, rrse_value, rse_value, smape_value, sse_value)
  metric_name <- c("MAE ","MAPE", "MASE", "MDAE", "MSE", "MSLE",
                   "RAE", "RMSE", "RMSLE", "RRSE", "RSE", "SMAPE", "SSE")
  metric_info_string <- ""
  for(met in 1:length(metric_choosen)){
    if(met == length(metric_choosen)){
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4))
    }else{
      metric_info_string <- paste0(metric_info_string, metric_name[metric_choosen[met]]," Value = ",round(metric_calc[metric_choosen[met]],4),", ")
    }
  }
  
  labelx = df_x[,1]
  plot(labelx, observed, col='blue', main=paste0("Naive Model Comparison Model between Actual and Fitted Values"), 
       sub=metric_info_string)
  points(labelx, predicted,col='red')
  lines(labelx, observed, lwd=2, col="blue")
  lines(labelx, predicted, lwd=2, col="red")
  legend("bottomleft", legend=c("Observed", "Predicted"), 
         col=c("blue","red"), lty=1:2, cex=0.8) 
  
  naive_df_comparison <- df_comparison
  
  if(class(x)[1] == "ts"){
    return(list(fit_ARIMA, fit_ETS, list(fit_prophet, futureProphet), fit_ses, fit_HW, fit_HW_gamma,
                fit_BATS, fit_TBATS, fit_Naive, arima_df_comparison, ets_df_comparison,
                prophet_df_comparison, ses_df_comparison, holtwinters_df_comparison, 
                holtwinters_gamma_df_comparison, bats_df_comparison, tbats_df_comparison, naive_df_comparison))
  }else{
    return(list(fit_ARIMA, fit_ETS, list(fit_prophet, futureProphet), fit_ses, fit_HW, fit_HW_gamma,
                fit_Naive,arima_df_comparison, ets_df_comparison,
                prophet_df_comparison, ses_df_comparison, 
                holtwinters_df_comparison, holtwinters_gamma_df_comparison, naive_df_comparison)) 
  }
}

#Transform time series with Fast Fourier Transform and plot Harmonics, trajectory is the same as time series as fourier definition
fourier_transform <- function(trajectory){
  library(GeneCycle)
  writeLines("The Fourier Transform sees every trajectory (aka time signal, aka signal) as a set of circular motions.")
  writeLines("The trajectory is processed thru a set of filters:")
  writeLines("1) each filter gives us a cycle and the remainder of the trajectory")
  writeLines("2) filters are independent, each one catches a different part of the trajectory")
  writeLines("3) there are enough filters to catch all of the trajectory, ie, the last filter leaves no trajectory remainder")
  writeLines("So, the FT algorithm receives a trajectory, apply its filters to find the appropriate cycles, and outputs the full set of cyclic components. There are two algorithms:")
  writeLines("the Discrete Fourier Transform (DFT) which requires O(n2) operations (for n samples)")
  writeLines("the Fast Fourier Transform (FFT) which requires O(n.log(n)) operations")
  
  plot.fourier <- function(fourier.series, f.0, ts) {
    w <- 2*pi*f.0
    trajectory <- sapply(ts, function(t) fourier.series(t,w))
    x11()
    plot(ts, trajectory, type="l", xlab="time", ylab="f(t)"); abline(h=0,lty=3)
  }
  get.trajectory <- function(X.k,ts,acq.freq) {
    N   <- length(ts)
    i   <- complex(real = 0, imaginary = 1)
    x.n <- rep(0,N)           # create vector to keep the trajectory
    ks  <- 0:(length(X.k)-1)
    for(n in 0:(N-1)) {       # compute each time point x_n based on freqs X.k
      x.n[n+1] <- sum(X.k * exp(i*2*pi*ks*n/N)) / N
    }
    x.n * acq.freq 
  }
  plot.frequency.spectrum <- function(X.k, xlimits=c(0,length(X.k))) {
    plot.data  <- cbind(0:(length(X.k)-1), Mod(X.k))
    # TODO: why this scaling is necessary?
    plot.data[2:length(X.k),2] <- 2*plot.data[2:length(X.k),2] 
    x11()
    plot(plot.data, t="h", lwd=2, main="", 
         xlab="Frequency (Hz)", ylab="Strength", 
         xlim=xlimits, ylim=c(0,max(Mod(plot.data[,2]))))
  }
  get_spectrum <- function(X.k, xlimits=c(0,length(X.k))){
    plot.data  <- cbind(0:(length(X.k)-1), Mod(X.k))
    # TODO: why this scaling is necessary?
    plot.data[2:length(X.k),2] <- 2*plot.data[2:length(X.k),2] 
    return(plot.data)
  }
  plot.harmonic <- function(Xk, i, ts, acq.freq, color="red") {
    Xk.h <- rep(0,length(Xk))
    Xk.h[i+1] <- Xk[i+1] # i-th harmonic
    harmonic.trajectory <- get.trajectory(Xk.h, ts, acq.freq=acq.freq)
    points(ts, harmonic.trajectory, type="l", col=color)
  }
  
  x11()
  writeLines("Plot Actual Trajectory")
  plot(trajectory, type="l")
  
  x11()
  writeLines("Plot Detrended Trajectory (Trajectory without Trend)")
  time_series_quantile <- seq(0,1,length.out = length(trajectory))
  trajectory_num <- as.numeric(trajectory)
  
  trend <- lm(trajectory_num ~ time_series_quantile)
  detrended_trajectory <- trend$residuals
  plot(detrended_trajectory, type="l")
  
  writeLines("------------------------------------------------------")
  writeLines("1) Plot Hidden Harmony in a Trajectory with Genecycle")
  writeLines("------------------------------------------------------")
  
  writeLines("Harmony represents Cycle effects in time series movement")
  periodogram_data <- GeneCycle::periodogram(detrended_trajectory)
  print(periodogram_data)
  harmonics_freq <- 1:(length(detrended_trajectory)/2)
  
  writeLines("Make Harmonic Plot version 1")
  x11()
  plot(periodogram_data$freq[harmonics_freq]*length(detrended_trajectory), 
       periodogram_data$spec[harmonics_freq]/sum(periodogram_data$spec), 
       xlab="Harmonics (Hz)", ylab="Amplitute Density", type="h")
  
  writeLines("---------------------------------------------------------------")
  writeLines("2) Apply a Fast Fourier Transformation to Detrended Trajectory")
  writeLines("---------------------------------------------------------------")
  
  fastfourier <- fft(detrended_trajectory)
  writeLines("Make Harmonic Plot version 2")
  plot.frequency.spectrum(fastfourier)
  spectrum <- get_spectrum(fastfourier)
  colnames(spectrum) <- c("ts","mod_ts")
  spectrum <- as.data.frame(spectrum)
  print(spectrum)
  
  writeLines("---------------------------------------------------------")
  writeLines("3) Plot Top 3 Most fitted Harmony to Detrended Projectory")
  writeLines("---------------------------------------------------------")
  
  x11()
  max_value_trajectory <- max(detrended_trajectory) * 1.2
  min_value_trajectory <- 0
  if(min(detrended_trajectory) >= 0){
    min_value_trajectory <- min(detrended_trajectory) * 0.8
  }
  else if(min(detrended_trajectory) < 0){
    min_value_trajectory <- min(detrended_trajectory) * 1.2
  }
  
  most_fitted_harmony <- spectrum[order(spectrum$mod_ts, decreasing = TRUE),]
  most_fitted_vec <- most_fitted_harmony$ts[1:3]
  plot(time_series_quantile, detrended_trajectory, type="l",
       ylim=c(min_value_trajectory,max_value_trajectory),lwd=2)
  
  writeLines(paste0("Most Fitted Harmony with Red Line, Harmonic = ",most_fitted_vec[1]))
  plot.harmonic(fastfourier, most_fitted_vec[1],ts,length(detrended_trajectory),"red")
  writeLines(paste0("Most Fitted Harmony with Green Line, Harmonic = ",most_fitted_vec[2]))
  plot.harmonic(fastfourier ,most_fitted_vec[2],ts,length(detrended_trajectory),"green")
  writeLines(paste0("Most Fitted Harmony with Blue Line, Harmonic = ",most_fitted_vec[3]))
  plot.harmonic(fastfourier ,most_fitted_vec[3],ts,length(detrended_trajectory),"blue")
  
  return(list(detrended_trajectory, fastfourier, spectrum))
}

#for Fourier Chart Feature to work need to activate CMD Manually
#1) Make sure chrome version is the same as chrome driver
#2) cmd -> cd C:\Users\User\Desktop\Data Science Journey\Data Science with R
#3) java -Dwebdriver.chrome.verboseLogging=true -Dwebdriver.chrome.driver="/Users/User/Desktop/Data Science Journey/Data Science with R/chromedriver_86.exe" -jar selenium-server-standalone-3.8.1.jar -port 4444
fourier_chart_ <- function(file_path="C:/Users/User/Desktop/Data Science Journey/Data Science with R/Fourier_Chart/fourier.", 
                               cycle_string="", time_string="", use="cycle"){
  library(RSelenium)
  library(R.utils)
  library(wdman)
  writeLines("Using Selenium to fill out Cycle and Time Input! ")
  writeLines("Each cycle has a strength, a delay and a speed. How can we represent them?")
  writeLines("1) The Strength is represented by the circle size, which is controlled by z")
  writeLines("2) The Delay, or starting point, is given by an initial value of d")
  writeLines("3) The Speed will be represented by the rate of change of d over time")
  
  writeLines("Connecting to running Selenium Server with Remote Driver. . .")
  remDr <- remoteDriver(
    remoteServerAddr = "localhost",
    port = 4444L,
    browserName = "chrome"
  )
  writeLines('Open a connection')
  remDr$open()
  writeLines('Connection Status')
  remDr$getStatus()
  
  writeLines("Navigate to desired HTML File. . .")
  remDr$navigate(file_path)
  if(use=="cycle"){
    webElem <- remDr$findElement(using = "xpath", "//input[@name = 'data-cycles']")
    webElem$clearElement()
    webElem$sendKeysToElement(list(cycle_string))
  }
  else if(use=="time"){
    webElem <- remDr$findElement(using = "xpath", "//input[@name = 'data-time']")
    webElem$clearElement()
    webElem$sendKeysToElement(list(time_string))
  }
}

#Call a Fourier Chart HTML with a desired cycle input
fourier_chart_(time_string="1 2 3 4")
fourier_chart_(cycle_string="5 5 4")

MARSS_matrix_usage_note <- function(){
  writeLines("====================== Based by Matrix Usage ======================")
  writeLines("B = Matrix of Unique Individual Features")
  writeLines("u = Matrix of Miu/Mean for Calculating xt")
  writeLines("Q = Matrix of Feature Variance/Process Errors")
  writeLines("Z = Matrix of Observation")
  writeLines("A = Matrix of Miu/Mean for Calculating yt")
  writeLines("R = Matrix of Observation Variance")
  writeLines("D = Matrix of Covariate for Observation with (dt and nx1 observation vector)")
  writeLines("d = Vector of Covariate for Observation")
  writeLines("C = Matrix of Covariate for Features with (ct an mx1 feature vector)")
  writeLines("c = Vector of Covariate for Features")
  writeLines("wt = Residual Matrix for xt")
  writeLines("vt = Residual Matrix for yt")
} 

#Stochastic Level Model tend to take a lot of times
MARSS_univariate_ts <- function(time_series, algorithm_method="kem"){
  library(MARSS)
  writeLines("MARSS Model is a State Space Model that Use EM Algorithm to construct time series model")
  writeLines("MARSS Model follow this formula for x and y")
  writeLines("xt = B*xt-1 + u + wt where wt ~ N(0,Q)")
  writeLines("yt = Z*xt + a + vt where vt ~ N(0,R)")
  writeLines("x0 = miu")
  writeLines("Matrix B,Z,u,a,Q,R is an estimated input")
  writeLines("which explains that B,u,Q Matrices are property for xt")
  writeLines("which explains also Z,a,R Matrices are property for yt")
  writeLines("Algorithm Included for MARSS is kem (EM Expectation Maximization) and BFGS")
  
  writeLines("----------------------- 1) Produces Flat Model MARSS -------------------------")
  flat_mod <- list(B = matrix(1), U = matrix(0), Q = matrix(0), 
                   Z = matrix(1), A = matrix(0), R = matrix("r"), x0 = matrix("mu"), 
                   tinitx = 0)
  MARSS1 <- MARSS(time_series, model = flat_mod, method = algorithm_method)
  print(MARSS1)
  MARSS1_pred_result <- fitted(MARSS1)
  MARSS1_pred_result <- cbind(MARSS1_pred_result, 
                              LL = rep(MARSS1$logLik, nrow(MARSS1_pred_result)),
                              AICc = rep(MARSS1$AICc, nrow(MARSS1_pred_result)),
                              category = rep("Flat Level Model", nrow(MARSS1_pred_result)))
  writeLines("Flat Model Paramaters and AICc")
  print(c(coef(MARSS1, type = "vector"), LL = MARSS1$logLik, AICc = MARSS1$AICc))
  
  writeLines("-------------------- 2) Produces Linear Trend Model MARSS -----------------------")
  linear_trend_mod <- list(B = matrix(1), U = matrix("u"), Q = matrix(0), 
                           Z = matrix(1), A = matrix(0), R = matrix("r"), x0 = matrix("mu"), 
                           tinitx = 0)
  MARSS2 <- MARSS(time_series, model = linear_trend_mod, method = algorithm_method)
  MARSS2_pred_result <- fitted(MARSS2)
  MARSS2_pred_result <- cbind(MARSS2_pred_result, 
                              LL = rep(MARSS2$logLik, nrow(MARSS2_pred_result)),
                              AICc = rep(MARSS2$AICc, nrow(MARSS2_pred_result)),
                              category = rep("Linear Trend Level Model", nrow(MARSS2_pred_result)))
  writeLines("Linear Trend Model Paramaters and AICc")
  print(c(coef(MARSS2, type = "vector"), LL = MARSS2$logLik, AICc = MARSS2$AICc))
  
  writeLines("-------------------- 3) Produces Stochastic Level Model MARSS ---------------------")
  stochastic_mod <- list(B = matrix(1), U = matrix(0), Q = matrix("q"), 
                         Z = matrix(1), A = matrix(0), R = matrix("r"), x0 = matrix("mu"), 
                         tinitx = 0)
  MARSS3 <- MARSS(time_series, model = stochastic_mod, method = algorithm_method)
  MARSS3_pred_result <- fitted(MARSS3)
  MARSS3_pred_result <- cbind(MARSS3_pred_result, 
                              LL = rep(MARSS3$logLik, nrow(MARSS3_pred_result)),
                              AICc = rep(MARSS3$AICc, nrow(MARSS3_pred_result)),
                              category = rep("Stochastic Level Model", nrow(MARSS3_pred_result)))
  writeLines("Stochastic Model Paramaters and AICc")
  print(c(coef(MARSS3, type = "vector"), LL = MARSS3$logLik, AICc = MARSS3$AICc))
  
  writeLines("-------------------- 4) Produces Stochastic with Trend Model MARSS ------------------")
  stochastic_drift_mod <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                               Z = matrix(1), A = matrix(0), R = matrix("r"), x0 = matrix("mu"), 
                               tinitx = 0)
  MARSS4 <- MARSS(time_series, model = stochastic_drift_mod, method = algorithm_method)
  MARSS4_pred_result <- fitted(MARSS4)
  MARSS4_pred_result <- cbind(MARSS4_pred_result, 
                              LL = rep(MARSS4$logLik, nrow(MARSS4_pred_result)),
                              AICc = rep(MARSS4$AICc, nrow(MARSS4_pred_result)),
                              category = rep("Stochastic with Trend Level Model", nrow(MARSS4_pred_result)))
  writeLines("Stochastic with Drift Model Paramaters and AICc")
  print(c(coef(MARSS4, type = "vector"), LL = MARSS4$logLik, AICc = MARSS4$AICc))
  
  MARSS_pred <- rbind(MARSS1_pred_result, MARSS2_pred_result, MARSS3_pred_result, MARSS4_pred_result)
  collected_aic <- c(MARSS1$AICc, MARSS2$AICc, MARSS3$AICc, MARSS4$AICc)
  deltaAIC <- collected_aic - min(collected_aic)
  likelihood <- exp(-0.5 * deltaAIC)
  aicweight <- likelihood/sum(likelihood)
  
  aic.table <- data.frame(AICc = collected_aic, delta_AIC = deltaAIC, 
                          relLikelihood = likelihood, weight = aicweight)
  rownames(aic.table) <- c("Flat level", "Linear trend", 
                           "Stoc level", "Stoc level w drift")
  print(round(aic.table, digits = 3))
  
  writeLines("Inspect Residuals from Model")
  x11()
  par(mfrow = c(2, 2), mar = c(2, 2, 4, 2))
  writeLines("Residuals From Flat Model")
  resids1 <- residuals(MARSS1)
  print(resids1)
  acf(resids1$.resids, main = "Flat level v(t)", na.action = na.pass)
  writeLines("Residuals From Linear Trend Model")
  resids2 <- residuals(MARSS2)
  print(resids2)
  acf(resids2$.resids, main = "Linear trend v(t)", na.action = na.pass)
  writeLines("Residuals From Stochastic Model")
  resids3 <- residuals(MARSS3)
  print(resids3)
  acf(resids3$.resids, main = "Stoc level v(t)", na.action = na.pass)
  writeLines("Residuals From Flat Model")
  resids4 <- residuals(MARSS4)
  print(resids4)
  acf(resids4$.resids, main = "Stoc level v(t) with Drift", na.action = na.pass)
  return(list(MARSS1, MARSS2, MARSS3, MARSS4, MARSS_pred, 
              aic.table, resids1, resids2, resids3, resids4))
}

MARSS_multivariate_ts <- function(time_series_matrix, covariate_include=FALSE,
                                  covariate_matrix, season_include=FALSE, 
                                  factor_hypothesis=list(), variance_hypothesis=0.1, 
                                  seasonal_periods=12, seasonal_month_start=1, 
                                  seasonal_polynomial_order=3, algorithm_method="kem"){
  writeLines("Time Series Matrix input used is assumed already transposed to time based")
  rows <- nrow(time_series_matrix)
  if(covariate_include){
    writeLines("MARSS Multivariate Time Series with Covariates do following calculation for X and Y")
    writeLines("Xt = BtXt-1 + ut + Ctct + Wt where wt ~ MVN(0,Qt)")
    writeLines("Yt = ZtXt + at + Dtdt + vt where vt ~ MVN(0,Rt)")
    writeLines("ct is the p  1 vector of covariates (e.g., temperature, rainfall) which affect the states")
    writeLines("dt is a  q  1 vector of covariates (potentially the same as ct), which affect the observations.")
    writeLines("Ct is an m  p matrix of coefficients relating the effects of ct to the  m  1 state vector xt")
    writeLines("Dt is an n  q matrix of coefficients relating the effects of  dt to the  n  1 observation vector  yt")
    writeLines("----------------------- 1) Observation Error-only Model ----------------------------------")
    Q <- U <- x0 <- "zero"
    B <- Z <- "identity"
    d <- covariate_matrix
    A <- "zero"
    D <- "unconstrained"
    y <- dat  # to show relationship between dat & the equation
    model.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, D = D, 
                       d = d, x0 = x0)
    MARSS_cov1 <- MARSS(y, model = model.list, method = algorithm_method)
    print(c(coef(MARSS_cov1, type = "vector"), LL = MARSS_cov1$logLik, AICc = MARSS_cov1$AICc))
    MARSS1_pred_result <- fitted(MARSS_cov1)
    MARSS1_pred_result <- cbind(MARSS1_pred_result, 
                                LL = rep(MARSS_cov1$logLik, nrow(MARSS1_pred_result)),
                                AICc = rep(MARSS_cov1$AICc, nrow(MARSS1_pred_result)),
                                category = rep("Observation Error Model", nrow(MARSS1_pred_result)))
    
    writeLines("----------------------- 2) Process Error-only Model --------------------------------------")
    R <- A <- U <- "zero"
    B <- Z <- "identity"
    c <- covariate_matrix
    Q <- "equalvarcov"
    C <- "unconstrained"
    model.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
                       C = C, c = c)
    MARSS_cov2 <- MARSS(dat, model = model.list, method = algorithm_method)
    MARSS2_pred_result <- fitted(MARSS_cov2)
    MARSS2_pred_result <- cbind(MARSS2_pred_result, 
                                LL = rep(MARSS_cov2$logLik, nrow(MARSS2_pred_result)),
                                AICc = rep(MARSS_cov2$AICc, nrow(MARSS2_pred_result)),
                                category = rep("Process Error Model", nrow(MARSS2_pred_result)))
    
    #Matrix B variation instead of identical
    model.list$B <- "diagonal and unequal"
    MARSS_cov2b <- MARSS(dat, model = model.list, method = algorithm_method)
    MARSS2_variant2_pred_result <- fitted(MARSS_cov2b)
    MARSS2_variant2_pred_result <- cbind(MARSS2_variant2_pred_result, 
                                         LL = rep(MARSS_cov2b$logLik, nrow(MARSS2_variant2_pred_result)),
                                         AICc = rep(MARSS_cov2b$AICc, nrow(MARSS2_variant2_pred_result)),
                                         category = rep("Process Error Model with Matrix B variation", nrow(MARSS2_variant2_pred_result)))
    
    #Adding Initial State of x0 to Model
    x0 <- time_series_matrix[, 1, drop = FALSE]
    model.list$tinitx <- 1
    model.list$x0 <- x0
    MARSS_cov2c <- MARSS(dat, model = model.list, method = algorithm_method)
    MARSS2_variant3_pred_result <- fitted(MARSS_cov2c)
    MARSS2_variant3_pred_result <- cbind(MARSS2_variant3_pred_result, 
                                         LL = rep(MARSS_cov2c$logLik, nrow(MARSS2_variant3_pred_result)),
                                         AICc = rep(MARSS_cov2c$AICc, nrow(MARSS2_variant3_pred_result)),
                                         category = rep("Process Error Model with Adding State of x0", nrow(MARSS2_variant3_pred_result)))
    
    writeLines("--------- 3) Both Observation and Process Error Model with Covariates only in Features ---------------")
    D <- d <- A <- U <- "zero"
    Z <- "identity"
    B <- "diagonal and unequal"
    Q <- "equalvarcov"
    C <- "unconstrained"
    c <- covariate_matrix
    R <- diag(variance_hypothesis, 2)
    x0 <- "unequal"
    tinitx <- 1
    model.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
                       D = D, d = d, C = C, c = c, x0 = x0, tinitx = tinitx)
    MARSS_cov3 <- MARSS(dat, model = model.list, method = algorithm_method)
    MARSS3_pred_result <- cbind(MARSS3_pred_result, 
                                LL = rep(MARSS_cov3$logLik, nrow(MARSS3_pred_result)),
                                AICc = rep(MARSS_cov3$AICc, nrow(MARSS3_pred_result)),
                                category = rep("Both Observation and Process Error Model with Features Covariates", nrow(MARSS3_pred_result)))
    
    
    writeLines("-------- 4) Both Observation and Process Error with Covariate only in Observation ------------------")
    C <- c <- A <- U <- "zero"
    Z <- "identity"
    B <- "diagonal and unequal"
    Q <- "equalvarcov"
    D <- "unconstrained"
    d <- covariate_matrix
    R <- diag(variance_hypothesis, 2)
    x0 <- "unequal"
    tinitx <- 1
    model.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
                       D = D, d = d, C = C, c = c, x0 = x0, tinitx = tinitx)
    MARSS_cov4 <- MARSS(dat, model = model.list, method = algorithm_method)
    MARSS4_pred_result <- cbind(MARSS4_pred_result, 
                                LL = rep(MARSS_cov4$logLik, nrow(MARSS4_pred_result)),
                                AICc = rep(MARSS_cov4$AICc, nrow(MARSS4_pred_result)),
                                category = rep("Both Observation and Process Error Model with Observation Covariates", nrow(MARSS4_pred_result)))
    
    MARSS_pred <- rbind(MARSS1_pred_result, MARSS2_pred_result, 
                        MARSS2_variant2_pred_result,
                        MARSS2_variant3_pred_result,
                        MARSS3_pred_result,
                        MARSS4_pred_result)
    collected_aic <- c(MARSS_cov1$AICc, MARSS_cov2$AICc, MARSS_cov2b$AICc, MARSS_cov2c$AICc,
                       MARSS_cov3$AICc, MARSS_cov4$AICc)
    deltaAIC <- collected_aic - min(collected_aic)
    likelihood <- exp(-0.5 * deltaAIC)
    aicweight <- likelihood/sum(likelihood)
    aic.table <- data.frame(AICc = collected_aic, delta_AIC = deltaAIC, 
                            relLikelihood = likelihood, weight = aicweight)
    rownames(aic.table) <- unique(MARSS_pred$category)
    writeLines("================ Model Performance Summary ==============")
    print(round(aic.table, digits = 3))
    return(list(MARSS_cov1, MARSS_cov2, MARSS_cov2b, MARSS_cov2c,
                MARSS_cov3, MARSS_cov4, MARSS_pred, aic_table))
  }
  else if(season_include){
    TT <- dim(time_series_matrix)[2] #Periods Exist in Time Series Matrix
    B <- "diagonal and unequal"
    Q <- "diagonal and unequal"
    U <- "zero"
    Z <- "identity"
    A <- "zero"
    R <- "diagonal and equal"
    D <- "zero"
    d <- "zero"
    
    writeLines("-------------------- 1) Seasonal Effects as Fixed Factor ------------------------")
    period <- seasonal_periods #season period
    per.1st <- seasonal_month_start #first season
    c.in <- diag(period)
    for (i in 2:(ceiling(TT/period))) {
      c.in <- cbind(c.in, diag(period))
    }
    c.in <- c.in[, (1:TT) + (per.1st - 1)]
    rownames(c.in) <- month.abb
    
    model.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
                       C = C, c = c.in, D = D, d = d)
    MARSS_Seasonal1 <- MARSS(time_series_matrix, model = model.list, 
                             control = list(maxit = 1500), method = algorithm_method)
    
    seasonal1 <- coef(MARSS_Seasonal1, type = "matrix")$C
    rownames(seasonal1) <- rownames(time_series_matrix)
    colnames(seasonal1) <- month.abb
    print(seasonal1)
    
    MARSS1_pred_result <- fitted(MARSS_Seasonal1)
    MARSS1_pred_result <- cbind(MARSS1_pred_result, 
                                LL = rep(MARSS_Seasonal1$logLik, nrow(MARSS1_pred_result)),
                                AICc = rep(MARSS_Seasonal1$AICc, nrow(MARSS1_pred_result)),
                                category = rep("Fixed Seasonal Model", nrow(MARSS1_pred_result)))
    
    writeLines("-------------------- 2) Seasonal Effects as Polynomial ------------------------")
    period <- seasonal_periods
    per.1st <- seasonal_month_start
    poly.order <- seasonal_polynomial_order
    month.cov <- matrix(1, 1, period)
    for(i in 1:poly.order){
      month.cov = rbind(month.cov, (1:12)^i)
    }
    c.m.poly <- matrix(month.cov, poly.order + 1, TT + period, byrow = FALSE)
    c.m.poly <- c.m.poly[, (1:TT) + (per.1st - 1)]
    
    model.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
                       C = C, c = c.m.poly, D = D, d = d)
    MARSS_Seasonal2 <- MARSS(time_series_matrix, model = model.list, 
                             control = list(maxit = 1500), method = algorithm_method)
    
    C.2 = coef(MARSS_Seasonal2, type = "matrix")$C
    seasonal2 = C.2 %*% month.cov
    rownames(seasonal2) <- rownames(time_series_matrix)
    colnames(seasonal2) <- month.abb
    print(seasonal2)
    
    MARSS2_pred_result <- fitted(MARSS_Seasonal2)
    MARSS2_pred_result <- cbind(MARSS2_pred_result, 
                                LL = rep(MARSS_Seasonal2$logLik, nrow(MARSS2_pred_result)),
                                AICc = rep(MARSS_Seasonal2$AICc, nrow(MARSS2_pred_result)),
                                category = rep("Polynomial Seasonal Model", nrow(MARSS2_pred_result)))
    
    writeLines("-------------------- 3) Seasonal Effects as Fourier Effects ------------------------")
    cos.t <- cos(2 * pi * seq(TT)/period)
    sin.t <- sin(2 * pi * seq(TT)/period)
    c.Four <- rbind(cos.t, sin.t)
    
    model.list <- list(B = B, U = U, Q = Q, Z = Z, A = A, R = R, 
                       C = C, c = c.Four, D = D, d = d)
    MARSS_Seasonal3 <- MARSS(time_series_matrix, model = model.list, 
                             control = list(maxit = 1500), method = algorithm_method)
    
    C.3 <- coef(MARSS_Seasonal3, type = "matrix")$C
    seasonal3 <- C.3 %*% c.Four[, 1:period]
    rownames(seasonal3) <- rownames(time_series_matrix)
    colnames(seasonal3) <- month.abb
    print(seasonal3)
    
    MARSS3_pred_result <- fitted(MARSS_Seasonal3)
    MARSS3_pred_result <- cbind(MARSS3_pred_result, 
                                LL = rep(MARSS_Seasonal3$logLik, nrow(MARSS3_pred_result)),
                                AICc = rep(MARSS_Seasonal3$AICc, nrow(MARSS3_pred_result)),
                                category = rep("Fourier Effect in Seasonal Model", nrow(MARSS3_pred_result)))
    MARSS_pred <- rbind(MARSS1_pred_result, MARSS2_pred_result, MARSS3_pred_result)
    
    return(list(MARSS_Seasonal1, MARSS_Seasonal2, MARSS_Seasonal3, 
                MARSS_pred, seasonal1, seasonal2, seasonal3))
    
  }
  else{
    writeLines("----------------------- 1) Single Well Mixed Population ----------------------------------")
    mixed_pop <- list(B = matrix(1), U = matrix("u"), Q = matrix("q"), 
                      Z = matrix(1, nrow(time_series_matrix), 1), A = "scaling", R = "diagonal and unequal", 
                      x0 = matrix("mu"), tinitx = 0)
    MARSS_multi1 <- MARSS(time_series_matrix, model = mixed_pop, method = algorithm_method)
    print(c(coef(MARSS_multi1, type = "vector"), LL = MARSS_multi1$logLik, AICc = MARSS_multi1$AICc))
    MARSS1_pred_result <- fitted(MARSS_multi1)
    MARSS1_pred_result <- cbind(MARSS1_pred_result, 
                                LL = rep(MARSS_multi1$logLik, nrow(MARSS1_pred_result)),
                                AICc = rep(MARSS_multi1$AICc, nrow(MARSS1_pred_result)),
                                category = rep("Single Well Mixed Population Model", nrow(MARSS1_pred_result)))
    
    writeLines("--------- 2) N Sub Population with temporary uncorrelated Errors -------------------------")
    n_pop_no_corr <- list(B = "identity", U = "equal", Q = "diagonal and equal", 
                          Z = "identity", A = "scaling", R = "diagonal and unequal", 
                          x0 = "unequal", tinitx = 0)
    MARSS_multi2 <- MARSS(time_series_matrix, model = n_pop_no_corr, method = algorithm_method)
    print(c(coef(MARSS_multi2, type = "vector"), LL = MARSS_multi2$logLik, AICc = MARSS_multi2$AICc))
    MARSS2_pred_result <- fitted(MARSS_multi2)
    MARSS2_pred_result <- cbind(MARSS2_pred_result, 
                                LL = rep(MARSS_multi2$logLik, nrow(MARSS2_pred_result)),
                                AICc = rep(MARSS_multi2$AICc, nrow(MARSS2_pred_result)),
                                category = rep("N Population with temporary Uncorrelated Error Model", nrow(MARSS2_pred_result)))
    
    writeLines("--------- 3) N Sub Population with temporary correlated Errors ---------------------------")
    n_pop_corr <- n_pop_no_corr
    n_pop_corr$Q <- "equalvarcov"
    MARSS_multi3 <- MARSS(time_series_matrix, model = n_pop_corr, method = algorithm_method)
    print(c(coef(MARSS_multi3, type = "vector"), LL = MARSS_multi3$logLik, AICc = MARSS_multi3$AICc))
    MARSS3_pred_result <- fitted(MARSS_multi3)
    MARSS3_pred_result <- cbind(MARSS3_pred_result, 
                                LL = rep(MARSS_multi3$logLik, nrow(MARSS3_pred_result)),
                                AICc = rep(MARSS_multi3$AICc, nrow(MARSS3_pred_result)),
                                category = rep("N Population with temporary Correlated Error Model", nrow(MARSS3_pred_result)))
    
    
    if(length(factor_hypothesis) > 0){
      writeLines("--------- 4) N Sub Population given Hypothesis of Factors --------------------------------")
      n_pop_hypothesis <- list(B = "identity", U = "unequal", Q = "equalvarcov",
                               Z = "placeholder", A = "scaling", R = "diagonal and equal",
                               x0 = "unequal", tinitx = 0) 
      
      out.tab <- NULL
      predict.tab <- NULL
      MARSS_fits <- list()
      for (i in 1:length(factor_hypothesis)) {
        n_pop_hypothesis$Z <- factor_hypothesis[[i]]
        fit <- MARSS::MARSS(time_series_matrix, model = n_pop_hypothesis, silent = TRUE, 
                            control = list(maxit = 1000), method = algorithm_method)
        predicting <- fitted(fit)
        out <- data.frame(H = names(factor_hypothesis)[i], logLik = fit$logLik, 
                          AICc = fit$AICc, num.param = fit$num.params, 
                          m = length(unique(Z.models[[i]])), 
                          num.iter = fit$numIter, converged = !fit$convergence)
        predicting$LL <- rep(out$LogLik, nrow(predicting))
        predicting$AICc <- rep(out$AICc, nrow(predicting))
        predicting$category <- rep(paste0("N Population with", out$H, "Hypothesis"),nrow(predicting))
        predict.tab <- rbind(predict.tab, predicting)
        MARSS_fits <- c(MARSS_fits, list(fit))
      }
      MARSS4_pred_result <- predicting
      
      MARSS_pred <- rbind(MARSS1_pred_result, MARSS2_pred_result, 
                          MARSS3_pred_result, MARSS4_pred_result)
      collected_aic <- c(MARSS_multi1$AICc, MARSS_multi1$AICc, 
                         MARSS_multi3$AICc, MARSS_multi4$AICc)
      for(item in 1:length(MARSS_fits)){
        collected_aic <- c(collected_aic, MARSS_fits[[item]])
      }
      deltaAIC <- collected_aic - min(collected_aic)
      likelihood <- exp(-0.5 * deltaAIC)
      aicweight <- likelihood/sum(likelihood)
      aic.table <- data.frame(AICc = collected_aic, delta_AIC = deltaAIC, 
                              relLikelihood = likelihood, weight = aicweight)
      rownames(aic.table) <- unique(MARSS_pred$category)
      writeLines("================ Model Performance Summary ==============")
      print(round(aic.table, digits = 3))
      return(list(MARSS1_multi1, MARSS1_multi2, MARSS1_multi3, MARSS_fits, 
                  MARSS_pred, aic_table))
    }
    else{
      MARSS_pred <- rbind(MARSS1_pred_result, MARSS2_pred_result, MARSS3_pred_result)
      collected_aic <- c(MARSS_multi1$AICc, MARSS_multi2$AICc, MARSS_multi3$AICc)
      deltaAIC <- collected_aic - min(collected_aic)
      likelihood <- exp(-0.5 * deltaAIC)
      aicweight <- likelihood/sum(likelihood)
      aic.table <- data.frame(AICc = collected_aic, delta_AIC = deltaAIC, 
                              relLikelihood = likelihood, weight = aicweight)
      rownames(aic.table) <- unique(MARSS_pred$category)
      writeLines("================ Model Performance Summary ==============")
      print(round(aic.table, digits = 3))
      return(list(MARSS1_multi1, MARSS1_multi2, MARSS1_multi3, 
                  MARSS_pred, aic_table))
    }
  }
}

library(forecast)
#univariate test data
data(treering)
em_test <- MARSS_univariate_ts(treering)
bfgs_test <- MARSS_univariate_ts(treering, algorithm_method = "BFGS")

#multivariate test data no covariate
data(harborSealWA, package = "MARSS")
data_multivar_ts <- MARSS::harborSealWA
years <- data_multivar_ts[, "Year"]
data_multivar_ts <- data_multivar_ts[, !(colnames(data_multivar_ts) %in% c("Year", "HC"))]
data_multivar_ts <- t(data_multivar_ts)

em_test_multivariate <- MARSS_multivariate_ts(data_multivar_ts)
bfgs_test_multivariate <- MARSS_multivariate_ts(data_multivar_ts, algorithm_method="BFGS")

#multivariate test data with covariate
data(lakeWAplankton, package = "MARSS")
data_multivar_ts2 <- lakeWAplanktonTrans
years <- fulldat[, "Year"] >= 1965 & fulldat[, "Year"] < 1975
data_multivar_ts2_test1 <- t(data_multivar_ts2[years, c("Greens", "Bluegreens")])
covariates_multivar_test1 <- t(data_multivar_ts2[years, c("Temp", "TP")])

em_test_multivariate2 <- MARSS_multivariate_ts(data_multivar_ts, covariate_include = TRUE, 
                                               covariate_matrix = covariates_multivar_test1)
bfgs_test_multivariate2 <- MARSS_multivariate_ts(data_multivar_ts, covariate_include = TRUE, 
                                                 covariate_matrix = covariates_multivar_test1,
                                                 algorithm_method="BFGS")

#-------------------------------------------------------------------------------------------------------------------------
########################################## 13) Data Panel Analysis #######################################################
#-------------------------------------------------------------------------------------------------------------------------

panel_data_tips <- function(){
  writeLines("===============================================================================================================")
  writeLines("=========================== Source: https://en.wikipedia.org/wiki/Panel_analysis ==============================")
  writeLines("===================================== There are 4 Type of Panel Analysis ======================================")
  writeLines("1) Independently pooled panels")
  writeLines("Key assumption: There are no unique attributes of individuals within the measurement set, and no universal effects across time.)")
  writeLines("2) Fixed Effect Models") 
  writeLines("Key assumption: There are unique attributes of individuals that do not vary across time.")
  writeLines("These attributes may or may not be correlated with the individual dependent variables.") 
  writeLines("To test whether fixed effects, rather than random effects, is needed, the (Durbin-Wu-)Hausman test can be used.)")
  writeLines("3) Random effect models")
  writeLines("Key assumption: There are unique, time constant attributes of individuals that are not correlated with the individual regressors.") 
  writeLines("Pooled OLS can be used to derive unbiased and consistent estimates of parameters even when time constant attributes are present, but random effects will be more efficient.")
  writeLines("4) Dynamic panel models")
  writeLines("In contrast to the standard panel data model, a dynamic panel model also includes lagged values of the dependent variable as regressors.")
  writeLines("For example, including one lag of the dependent variable generates:")
  writeLines("===============================================================================================================")
  writeLines("===============================================================================================================")
}

#coef(Cigar2.KSS)$Var.shares.of.loadings.param
panel_data_explore <- function(Panel, panel_time_var, panel_category_var,
                               var_dependent, var_independent){
  library(foreign)
  Panel <<- Panel
  attach(Panel)
  eval_1 <- paste0(var_dependent," ~ ", panel_time_var,"|",panel_category_var)
  eval_2 <- paste0(var_dependent," ~ ", panel_time_var)
  eval_3 <- ""
  eval_4 <- ""
  eval_5 <- ""
  if(length(var_independent) > 1){
    for(j in 1:length(var_independent))
    {
      if(j==1){
        eval_3 <- paste0(var_dependent," ~ ", var_independent[j])
        eval_4 <- paste0(var_dependent," ~ ", var_independent[j])
        eval_6 <- paste0(var_dependent," ~ ", var_independent[j])
      }
      else if(j > 1 && j < length(var_independent)){
        eval_3 <- paste0(eval_3,"+",var_independent[j])
        eval_4 <- paste0(eval_4,"+",var_independent[j])
        eval_6 <- paste0(eval_6,"+",var_independent[j])
      }
      else if(j == length(var_independent)){
        eval_3 <- paste0(eval_3,"+",var_independent[j])
        eval_4 <- paste0(eval_4,"+", var_independent[j], "+ factor(",panel_category_var,") - 1")
        eval_6 <- paste0(eval_6,"+", var_independent[j], "+ factor(",panel_time_var,")")
      }
    }
  }
  else if(length(var_independent) == 1){
    eval_3 <- paste0(var_dependent," ~ ", var_independent)
    eval_4 <- paste0(var_dependent," ~ ", var_independent, "+ factor(",panel_category_var,") - 1")
    eval_6 <- paste0(var_dependent," ~ ", var_independent, "+ factor(",panel_time_var,")")
  }
  eval_5 <- paste0("yhat ~ ", var_independent, "|",panel_category_var)
  
  writeLines("formula creation")
  print(paste("Eval 1: ",eval_1))
  print(paste("Eval 2: ",eval_2))
  print(paste("Eval 3: ",eval_3))
  print(paste("Eval 4: ",eval_4))
  print(paste("Eval 5: ",eval_5))
  print(paste("Eval 6: ",eval_6))
  
  eval_1 <<- eval_1
  eval_2 <<- eval_2
  eval_3 <<- eval_3
  eval_4 <<- eval_4
  eval_5 <<- eval_5
  eval_6 <<- eval_6
  panel_time_var <<- panel_time_var
  panel_category_var <<- panel_category_var
  
  x11()
  par(mfrow=c(1,2))
  coplot(as.formula(eval_1), type="l", data=Panel) # Lines
  coplot(as.formula(eval_1), type="b", data=Panel) # Points and lines
  
  x11()
  par(mfrow=c(1,1))
  library(car)
  scatterplot(as.formula(eval_1), boxplots=FALSE, smooth=TRUE, reg.line=FALSE, data=Panel)
  
  x11()
  par(mfrow=c(1,1))
  library(gplots)
  plotmeans(as.formula(eval_2), main="Heterogeineity across years", data=Panel)
  
  writeLines("\n============================== 1) Define an OLS Model =================================")
  ols <- lm(as.formula(eval_3), data=Panel)
  print(summary(ols))
  
  writeLines("====================== 2) Define a Dummy Fixed Effect Model =============================")
  fixed.dum <- lm(as.formula(eval_4), data=Panel)
  print(summary(fixed.dum))
  
  yhat <<- fixed.dum$fitted
  library(car)
  if(length(var_independent) > 1){
    for(q in 1:length(var_independent))
    {
      eval_loop <- paste0("yhat ~ ", var_independent[q], "|",panel_category_var)
      eval_lm <- paste0(var_dependent," ~ ", var_independent[j])
      x11()
      scatterplot(as.formula(eval_loop), boxplots=FALSE, xlab=var_independent[q], ylab="yhat",smooth=FALSE)
      abline(lm(as.formula(eval_lm)),lwd=3, col="red")
    }
  }
  else if(length(var_independent) == 1){
    x11()
    scatterplot(as.formula(eval_5), boxplots=FALSE, xlab=var_independent, ylab="yhat",smooth=FALSE)
    abline(lm(as.formula(eval_3)),lwd=3, col="red")
  }
  
  library(apsrtable)
  print(apsrtable(ols, fixed.dum, model.names = c("OLS", "OLS_DUM"))) # Displays a table in Latex form
  
  writeLines("\n==================== 3) Define a Fixed Effect Data Panel Model =========================")
  library(plm)
  fixed <- plm(as.formula(eval_3), data=Panel, 
               index=c(panel_category_var, panel_time_var), model="within")
  print(summary(fixed))
  
  writeLines("Fixed Effects on the Model")
  print(fixef(fixed))
  writeLines("F test for Fixed Effects")
  writeLines("H0: OLS/Ordinary Least Square is Statistically Significant better Model")
  writeLines("H1: Fixed Effect Model is Statistically Significant better Model")
  test1 <- pFtest(fixed, ols)
  print(pFtest(fixed, ols))
  
  writeLines("========================= 4) Define a Random Effect Data Panel Model ========================")
  writeLines("(Using Random Intercept, Partial Pooling Model)")
  random <- plm(as.formula(eval_3), data=Panel, 
                index=c(panel_category_var, panel_time_var), model="random")
  print(summary(random))
  
  writeLines("\n================== 5) Run Hausman test to decide which is the better Model? Fixed or Random? ==================")
  writeLines("H0: Random Effect Model is Statistically Significant better Model")
  writeLines("H1: Fixed Effect Model is Statistically Significant better Model")
  test2 <- phtest(fixed, random)
  print(phtest(fixed, random))
  
  writeLines("===================== 6) Testing for time-fixed effects ===================================")
  fixed.time <- plm(as.formula(eval_6), data=Panel, 
                    index=c(panel_category_var, panel_time_var), model="within")
  print(summary(fixed.time))
  
  writeLines("\nRun F test for Time-Fixed Effects Model between Fixed Effects Model")
  writeLines("H0: Time-Fixed Effects Model is not significantly a Better Model to use")
  writeLines("H1: Time-Fixed Effects Model is significantly a Better Model to use")
  test3 <- pFtest(fixed.time, fixed)
  test4 <- plmtest(fixed, c("time"), type=("bp"))
  
  writeLines("============ Method A) with pFtest (Test Individual Effects) =================")
  print(pFtest(fixed.time, fixed))
  writeLines("============ Method B) with plmtest using Breusch Pagan (Test Panel Effects) =================")
  print(plmtest(fixed, c("time"), type=("bp")))
  
  writeLines("===================== 7) Testing for Random effects: Breusch-Pagan Lagrange multiplier (LM) ========================")
  writeLines("H0: Variance Across Entities = 0/ No Significant Differences across Category")
  writeLines("H1: Variance Across Entities != 0/ There is Significant Differences across Category")
  pool <- plm(as.formula(eval_3), data=Panel, 
              index=c(panel_category_var, panel_time_var), model="pooling")
  summary(pool)
  
  test5 <- plmtest(pool, type=c("bp"))
  print(plmtest(pool, type=c("bp")))
  
  writeLines("====================== 8) Testing for cross-sectional dependence/contemporaneous correlation: =======================")
  writeLines("using Breusch-Pagan LM test of independence and Pasaran CD test")
  writeLines("B-P/LM and Pasaran CD tests of independence May Conclude Either:")
  writeLines("H0: Residuals across entities are not correlated")
  writeLines("H1: Residuals across entities are correlated")
  
  test6 <- pcdtest(fixed, test = c("lm"))
  test7 <- pcdtest(fixed, test = c("cd"))
  print(pcdtest(fixed, test = c("lm")))
  print(pcdtest(fixed, test = c("cd")))
  
  writeLines("===================== 9) Testing for serial correlation ==========================================")
  writeLines("Serial correlation tests apply to macro panels with long time series.")
  writeLines("Not a problem in micro panels (with very few years)")
  writeLines("H0: There is no serial correlation in the Panel")
  writeLines("H1: There is serial correlation in the Panel")
  
  test8 <- pbgtest(fixed)
  print(pbgtest(fixed))
  
  writeLines("============================= 10) ADF Test (Augmented Dickey Fuller) =============================")
  writeLines("H0: There is Unit Root Test in Time Series")
  writeLines("H1: There is No Unit Root Test in Time Series")
  
  lag_unit <- length(unique(Panel$Date))
  writeLines(paste("Using Lag = ", lag_unit))
  Panel.set <- pdata.frame(Panel, index = c(panel_category_var, panel_time_var))
  library(tseries)
  
  test9 <- adf.test(as.numeric(unlist(Panel.set[var_dependent])), k=lag_unit)
  print(test9)
  
  writeLines("============================ 11) BP Test (Heterocedastity test) ==================================")
  writeLines("H0: Homocedastity presents in the data")
  writeLines("H1: Heterocedastity presents in the data")
  library(lmtest)
  test10 <- bptest(as.formula(eval_4), data = Panel, studentize=F)
  print(bptest(as.formula(eval_4), data = Panel, studentize=F))
  
  writeLines("Controlling Heterocedastity with Robust Covariance Matrix into Random Effect Model")
  writeLines("Info for Types of Robust Covariance: ")
  writeLines("HC0 - heteroskedasticity consistent. The default.")
  writeLines("HC1,HC2, HC3 - Recommended for small samples.")
  writeLines("HC3 gives less weight to influential observations.")
  writeLines("HC4 - small samples with influential observations")
  writeLines("HAC - heteroskedasticity and autocorrelation consistent (type ?vcovHAC for more details)")
  
  print(t(sapply(c("HC0", "HC1", "HC2", "HC3", "HC4"), function(x) sqrt(diag(vcovHC(random, type = x))))))
  
  writeLines("Controlling Heterocedastity with Robust Covariance Matrix into Fixed Effect Model")
  print(t(sapply(c("HC0", "HC1", "HC2", "HC3", "HC4"), function(x) sqrt(diag(vcovHC(random, type = x))))))
  detach(Panel)
  rm(list=c("eval_1","eval_2","eval_3","eval_4","eval_5","eval_6",
            "panel_category_var","panel_time_var"))
  return(list(ols, fixed, random, fixed.time, test1, test2, test3, test4, test5, 
              test6, test7, test8, test9, test10))
}

#========================== Heterogenous Data Panel Analysis Using phtt library ===================================
#additive.effects = c("none", "individual", "time", "twoways")
heterogenous_panel <- function(numeric_df, response_var, consult.dim=FALSE,
                               additive_effect = "none", spec_alpha=0.05){
  library(phtt)
  stringeval = paste0("KSS(",response_var," ~ ")
  df_independent <- colnames(numeric_df)[-which(colnames(numeric_df)==response_var)]
  for(x in 1:length(df_independent))
  {
    if (x==length(df_independent)){
      stringeval = paste0(stringeval, df_independent[x])
    }  
    else if (x<length(df_independent)){
      stringeval = paste0(stringeval, df_independent[x], " + ")
    }
  }
  if(consult.dim==TRUE){
    stringeval = paste0(stringeval, ", consult.dim = TRUE, additive.effects =",additive_effect,")")
  }
  else if(consult.dim==FALSE){
    stringeval = paste0(stringeval, ", additive.effects = ",additive_effect,")")
  }
  heterogenous_stochastic <- eval(parse(text=stringeval))
  print(summary(heterogenous_stochastic))
  plot(summary(heterogenous_stochastic))
  writeLines("Check Model Specification")
  checkSpecif(heterogenous_stochastic, spec_alpha)
  return(heterogenous_stochastic)
}

vector_optimal_dim <- function(data_vector, criteria_vector, standardize=FALSE){
  library(phtt)
  OptDim.obj <- OptDim(Obj = data_vector, 
                       criteria = criteria_vector, 
                       standardize = TRUE)
  plot(OptDim.obj)
}

heterogenous_panel_stochastic_factor <- function(numeric_df, response_var, criteria_dim, spec_alpha=0.05){
  library(phtt)
  stringeval = paste0("Eup(",response_var," ~ -1 + ")
  df_independent <- colnames(numeric_df)[-which(colnames(numeric_df)==response_var)]
  for(x in 1:length(df_independent))
  {
    if (x==length(df_independent)){
      stringeval = paste0(stringeval, df_independent[x], ",")
    }  
    else if (x<length(df_independent)){
      stringeval = paste0(stringeval, df_independent[x], " + ")
    }
  }
  stringeval <- paste0(stringeval, "dim.criterion = ",criteria_dim,")")
  for(g in 1:length(colnames(numeric_df))){
    numeric_df[,g] <- diff(numeric_df[,g])
  }
  formula_model <- as.formula(paste0(response_var))
  heterogenous_stochastic <- eval(parse(text=stringeval))
  print(summary(heterogenous_stochastic))
  plot(summary(heterogenous_stochastic))
  writeLines("Check Model Specification")
  checkSpecif(heterogenous_stochastic, spec_alpha)
  return(heterogenous_stochastic)
}

#-------------------------------------------------------------------------------------------------------------------------
################################## 14) Dimensionality Reduction Analysis #################################################
#-------------------------------------------------------------------------------------------------------------------------

dimensionality_reduction_load_package <- function(){
  library(mlbench)
  library(factoextra)
  library(dplyr)
  library(Rdimtools)
  library(ggrepel)
  library(Rtsne)
  library(devtools)
  library(vegan)
  library(grid) #For Gridding model into one plot
  library(ggplot2) #For Plotting GGPlot Diagram
  library(gridBase) #For Gridding model into one plot
  library(gridExtra) # Grid Plot in one view
  library(dimRed)
  library(loe)
  library(RANN)
  library(gmodels)
  library(robustDA)
  library(RDRToolbox)
}

library(mlbench)
library(M3C)
data(mydata)
data(bostonhousing)

mydata["labels"] <- rownames(mydata)
for(x in 1:nrow(mydata)){
  mydata[["group_labels"]][x] <- substr(mydata[["labels"]][x], 
                                        nchar(mydata[["labels"]][x]), 
                                        nchar(mydata[["labels"]][x]))
  if(grepl("^[A-Za-z]+$", mydata[["group_labels"]][x], perl = T)){
    mydata[["universal_group"]][x] <- "alphabetic_end"
  }
  else{
    mydata[["universal_group"]][x] <- "numerical_end"
  }
}

library(dplyr)
library(vegan)
#http://www.imsbio.co.jp/RGM/R_dataset_list?package=vegan&init=true
data("BCI")
data("BCI.env")
data("dune.taxon")
data("dune.env")
data("dune.phylodis")
data("varespec")
data("mite")
data("mite.env")
data("mite.pcnm")
data("mite.xy")
data("pyrifos")
data("sipoo")

BCI1 = BCI
BCI2 = BCI.env
BCI2 <- BCI2 %>%
  mutate_if(is.factor, as.numeric)

plot_cluster = function(df, clust, individual_label=row.names(df), main_label = "Cluster Plot"){
  if(ncol(df) > 2){
    axes=c(1,2)
    pca <- stats::prcomp(df, scale = FALSE, center = FALSE)
    ind <- facto_summarize(pca, element = "ind", result = "coord", axes = c(1,2))
    ind$cluster <- as.factor(clust$cluster)
    eig <- get_eigenvalue(pca)[axes, 2]
    xlab = paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
    ylab = paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
    plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                      show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                      ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                      pointsize = 1.5, labelsize = 12, main = main_label, 
                      xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + 
      geom_text_repel(aes(label=individual_label), segment.color = clust$cluster, segment.colour = clust$cluster)
    print(plot)
  }
  else if (ncol(df) == 2){
    ind <- as.data.frame(df)
    ind$cluster <- as.factor(clust$cluster)
    xlab <- colnames(ind)[1]
    ylab <- colnames(ind)[2]
    plot <- ggpubr::ggscatter(ind, xlab, ylab, color = "cluster",
                      show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                      ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                      pointsize = 1.5, labelsize = 12, main = main_label, 
                      xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + 
      geom_text_repel(aes(label=individual_label), segment.color = clust$cluster, segment.colour = clust$cluster)
    print(plot)
  }
  else{
    print("You Cannot Plot Cluster with Univariate Data!")
  }
}

dim_method_guide <- function(){
  writeLines("Data Dimensional Reduction Available Methods by: ")
  writeLines("1) Feature Selection -> Best implemented to unstandardized raw data")
  writeLines("2) Stepwise Model Selection -> Best implemented to both & standardized data with no missing values")
  writeLines("3) Component Based -> Best Implemented to Standardized Data to make components which describe data in another way")
  writeLines("4) Nonlinear Techniques Mapping -> Best Implemented to Data that represents an Image pattern, also good for lot of cases")
  writeLines("4b) But may be time consuming when using Distance Matrix, This can compose Data to interpret Local and Global Structures to Image Pattern")
}

distance_matrix_test_size <- function(data, return_size=TRUE){
  writeLines(paste0("The Row Sizes of Dataframe with All numerical is ", nrow(data)))
  pos_size <- nrow(data) * (nrow(data) - 1) /2 
  writeLines(paste0('So The Possible Sizes of Distance Matrix Created by this dataframe is equal to ',
                    nrow(data), "x", (nrow(data) - 1), "/2", " = ",pos_size))
  writeLines('Attention! Large Number of Distance Matrix will resulting crash to Dimensional Reduction Technique which uses Distance Matrix as its source computation')
  if(return_size){
    x <- nrow(data)
    return(x*(x-1)/2)
  }
}

dim_reduction_feature_selection <- function(df,distance_matrix_length_threshold=200000,
                                            missing_value_ratio=TRUE,
                                            missing_value_pct_threshold=20,
                                            low_variance_filter=FALSE,
                                            variance_threshold=0.1,
                                            high_correlation_filter=FALSE,
                                            correlation_threshold=0.7,
                                            display_high_correlation=TRUE){
  filtered_df <- df
  if(missing_value_ratio){
    inspect_NA <- function(df, check_str_summary=TRUE, return_colname=FALSE,
                           return_colnames_threshold=1){
      library(naniar)
      na_table <- data.frame(miss_var_summary(df))
      na_table <- na_table[na_table$n_miss >= return_colnames_threshold,]
      colname_with_miss_value <- as.character(na_table$variable)
      if(check_str_summary){
        if(nrow(na_table) == 0){
          writeLines("There is No Missing Values found in this dataset!")
          na_table <- 0
        }
        else{
          print(na_table)
          writeLines('Checking Structures of Dataframe that have Missing Values')
          print(str(df[,which(colnames(df) %in% colname_with_miss_value)]))
          print(summary(df[,which(colnames(df) %in% colname_with_miss_value)])) 
        }
      }
      if(return_colname){
        return(colname_with_miss_value)
      }else{
        return(na_table) 
      }
    }
    df_na_tables <- inspect_NA(df)
    if(df_na_tables == 0){
      writeLines("Feature cant be selected through missing values! There was no missing values in dataset found")
    }else{
      variable_selected <- df_na_tables[which(df_na_tables$pct_miss <= missing_value_pct_threshold),]$variable
      if(length(variable_selected) > 0){
        filtered_df <- filtered_df[,which(colnames(filtered_df) %in% variable_selected)]
      }else{
        writeLines("Feature cant be selected through missing values! Since all variable seems to have missing values over Threshold")
      }
    }
  }
  if(low_variance_filter){
    variances_var <- lapply(filtered_df, FUN = function(x) var(x, na.rm=TRUE))
    variance_df <- data.frame(variable=names(variances_var), value=as.numeric(variances_var))
    variable_selected <- variances_df[which(variances_df$value <= variance_threshold),]$variable
    if(length(variable_selected) > 0){
      filtered_df <- df[,-which(colnames(df) %in% variable_selected)] 
    }
    else{
      writeLines("Feature cant be selected through Low Variances since all Variables suspected has low variances in threshold!")
    }
  }
  if(high_correlation_filter){
    v <- lapply(filtered_df, class)
    validate_class <- which(v %in% c("numeric","integer"))
    temp_df <- filtered_df[,validate_class]
    cor_df <- cor(temp_df)
    diag_index <- seq(1, by=length(colnames(temp_df)) + 1, length.out=length(colnames(temp_df)))
    cor_pos_threshold_idx <- which(cor_df >= correlation_threshold)
    cor_neg_threshold_idx <- which(cor_df <= correlation_threshold)
    valid_cor <- c(cor_pos_threshold_idx, cor_neg_threshold_idx)
    valid_cor <- valid_cor[-which(valid_cor %in% diag_index)]
    if(length(valid_cor) > 0){
      rowname_vector <- c()
      colname_vector <- c()
      cor_value_vector <- c()
      for(a in 1:length(valid_cor)){
        index_mat <- arrayInd(valid_cor[a], dim(cor_df))
        rowname_vector <- c(rowname_vector, rownames(cor_df)[index_mat[,1]])
        colname_vector <- c(colname_vector, colnames(cor_df)[index_mat[,2]])
        cor_value_vector <- c(cor_value_vector, cor_df[valid_cor])
      }
      if(display_high_correlation){
        writeLines("Variable Correlation Over Threshold")
        cor_results <- data.frame(var1 = rowname_vector, var2=colname_vector, cor=cor_value_vector)
        print(cor_results)
      }
      writeLines("Deselecting Correlation Over Threshold")
      unique_deselect_cols <- unique(c(rowname_vector, colname_vector))
      filtered_df <- filtered_df[,-which(colnames(filtered_df) %in% unique_deselect_cols)]
    }
    else{
      writeLines("There are no Pairs of Variable resulting High Correlation over Threshold!")
    }
  }
  return(filtered_df)
}

dim_reduction_stepwise <- function(df, stepwise_dependent="",
                                   stepwise_independent=c(""),
                                   stepwise_method="forward",
                                   stepwise_int_method="poisson",
                                   stepwise_factor_method="logit",
                                   stepwise_mprobit_draw=200, 
                                   stepwise_mprobit_burn=100, 
                                   stepwise_mprobit_thin=25){
  stepwise_modeling <- function(df, dependent_col="", independent_col="",
                                stepwise_method="forward", 
                                integer_response_regr_method="poisson",
                                factor_response_regr_method="logit",
                                multi_probit_draw=200, multi_probit_burn=100, 
                                multi_probit_thin=25){
    model_type <- c()
    model_formula <- c()
    model_aic <- c()
    response_type <- class(df[[dependent_col]])
    my_formula_initial <- as.formula(paste0(dependent_col, " ~ 1"))
    my_formula <- as.formula(paste0(dependent_col, " ~ ",paste0(independent_col, collapse=" + ")))
    n_sample <- nrow(df)
    real_n <- nrow(df)
    subdf <- df
    if(response_type=="numeric"){
      if(stepwise_method=="forward"){
        model_initial <- lm(my_formula_initial, data=df)
        model_full <- lm(my_formula, data=df)
        model_lm <- step(model_initial,
                         scope = list(upper=model_full),
                         direction="forward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "lm_forward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
      else if(stepwise_method=="backward"){
        model_initial <- lm(my_formula_initial, data=df)
        model_full <- lm(my_formula, data=df)
        model_lm <- step(model_full,
                         scope = list(upper=model_initial),
                         direction="backward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "lm_backward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
    }
    else if(response_type=="integer"){
      if(stepwise_method=="forward"){
        if(integer_response_regr_method == "poisson"){
          model_initial <- glm(my_formula_initial, data=df, family="poisson")
          model_full <- glm(my_formula, data=df, family="poisson")
          model_lm <- step(model_initial,
                           scope = list(upper=model_full),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "poisson_forward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(integer_response_regr_method == "negative_binomial"){
          model_initial <- glm.nb(my_formula_initial, data=df)
          model_full <- glm.nb(my_formula, data=df)
          model_lm <- step(model_initial,
                           scope = list(upper=model_full),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "negative_binomial_forward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(integer_response_regr_method == "quasi_poisson"){
          model_initial <- glm(my_formula_initial, data=df, family="quasipoisson")
          model_full <- glm(my_formula, data=df, family="quasipoisson")
          model_lm <- step(model_initial,
                           scope = list(upper=model_full),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "quasi_poisson_forward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
      }
      else if(stepwise_method=="backward"){
        if(integer_response_regr_method == "poisson"){
          model_initial <- glm(my_formula_initial, data=df, family="poisson")
          model_full <- glm(my_formula, data=df, family="poisson")
          model_lm <- step(model_full,
                           scope = list(upper=model_initial),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "poisson_backward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(integer_response_regr_method == "negative_binomial"){
          model_initial <- glm.nb(my_formula_initial, data=df)
          model_full <- glm.nb(my_formula, data=df)
          model_lm <- step(model_full,
                           scope = list(upper=model_initial),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "negative_binomial_backward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(integer_response_regr_method == "quasi_poisson"){
          model_initial <- glm(my_formula_initial, data=df, family="quasipoisson")
          model_full <- glm(my_formula, data=df, family="quasipoisson")
          model_lm <- step(model_full,
                           scope = list(upper=model_initial),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "quasi_poisson_backward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
      }
    }
    else if(response_type=="factor"){
      if(stepwise_method=="forward"){
        if(factor_response_regr_method=="logit"){
          if(length(unique(df[[dependent_col]])) == 2){
            model_initial <- glm(my_formula_initial, data=df, family = binomial(link="logit"))
            model_full <- glm(my_formula, data=df, family = binomial(link="logit"))
            model_lm <- step(model_initial,
                             scope = list(upper=model_full),
                             direction="forward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "logit_foward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
          else if(length(unique(df[[dependent_col]])) > 2){
            model_initial <- multinom(formula=my_formula_initial, data = df)
            model_full <- multinom(formula=my_formula, data = df)
            model_lm <- step(model_initial,
                             scope = list(upper=model_full),
                             direction="forward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "multilogit_forward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
        }
        else if(factor_response_regr_method=="probit"){
          if(length(unique(df[[dependent_col]])) == 2){
            model_initial <- glm(my_formula_initial, data=df, family = binomial(link="probit"))
            model_full <- glm(my_formula, data=df, family = binomial(link="probit"))
            model_lm <- step(model_initial,
                             scope = list(upper=model_full),
                             direction="forward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "probit_foward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
          else if(length(unique(df[[dependent_col]])) > 2){
            model_initial <- mnp(formula = my_formula_initial, 
                                 data = df, n.draws = multi_probit_draw, 
                                 burnin = multi_probit_burn,
                                 thin = multi_probit_thin, verbose = TRUE)
            model_full <- mnp(formula = my_formula, 
                              data = df, n.draws = multi_probit_draw, 
                              burnin = multi_probit_burn,
                              thin = multi_probit_thin, verbose = TRUE)
            model_lm <- step(model_initial,
                             scope = list(upper=model_full),
                             direction="backward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "multiprobit_forward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
        }
      }
      else if(stepwise_method=="backward"){
        if(factor_response_regr_method=="logit"){
          if(length(unique(df[[dependent_col]])) == 2){
            model_initial <- glm(my_formula_initial, data=df, family = binomial(link="logit"))
            model_full <- glm(my_formula, data=df, family = binomial(link="logit"))
            model_lm <- step(model_full,
                             scope = list(upper=model_initial),
                             direction="backward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "logit_backward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
          else if(length(unique(df[[dependent_col]])) > 2){
            model_initial <- multinom(formula=my_formula_initial, data = df)
            model_full <- multinom(formula=my_formula, data = df)
            model_lm <- step(model_full,
                             scope = list(upper=model_initial),
                             direction="backward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "multilogit_backward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
        }
        else if(factor_response_regr_method=="probit"){
          if(length(unique(df[[dependent_col]])) == 2){
            model_initial <- glm(my_formula_initial, data=df, family = binomial(link="probit"))
            model_full <- glm(my_formula, data=df, family = binomial(link="probit"))
            model_lm <- step(model_full,
                             scope = list(upper=model_initial),
                             direction="backward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "probit_backward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
          else if(length(unique(df[[dependent_col]])) > 2){
            model_initial <- mnp(formula = my_formula_initial, 
                                 data = df, n.draws = multi_probit_draw, 
                                 burnin = multi_probit_burn,
                                 thin = multi_probit_thin, verbose = TRUE)
            model_full <- mnp(formula = my_formula, 
                              data = df, n.draws = multi_probit_draw, 
                              burnin = multi_probit_burn,
                              thin = multi_probit_thin, verbose = TRUE)
            model_lm <- step(model_full,
                             scope = list(upper=model_initial),
                             direction="backward",
                             test="Chisq",
                             data=df)
            model_type <- c(model_type, "multiprobit_backward_stepwise")
            model_formula <- c(model_formula, as.character(model_lm$call)[2])
            model_aic <- c(model_aic, model_lm$aic)
            return(list(model_lm, model_type, model_formula, model_aic))
          }
        }
      }
    }
    
    sampling_result <- data.frame(model_type, model_formula, model_aic, n_model, model_rep)
    return(sampling_result)
  }
  models <- stepwise_modeling(df, dependent_col = stepwise_dependent, 
                              independent_col = stepwise_independent,
                              stepwise_method = stepwise_method,
                              integer_response_regr_method = stepwise_int_method,
                              factor_response_regr_method = stepwise_factor_method,
                              multi_probit_draw = stepwise_mprobit_draw,
                              multi_probit_burn = stepwise_mprobit_burn,
                              multi_probit_thin = stepwise_mprobit_thin)
  result_formula <- models[[3]]
  independent_columns <- unlist(strsplit(unlist(strsplit(result_formula, " ~ "))[2]," \\+ "))
  filtered_df <- filtered_df[,colnames(filtered_df) %in% independent_columns]
  return(filtered_df)
}

dim_reduction_component_based <- function(df, class_name="", 
                                          component_method="PCA",
                                          component_dependent="",
                                          n_component=2){
  if(component_method=="FA"){
    #use factor analysis if there most variables correlated variables
    factor_analysis <- function(df, component=2, rotation="varimax"){
      model_fac <- factanal(df, factors=component, rotation=rotation)
      print(model_fac)
      writeLines("========================== Factor Statistical Testing =====================================================")
      writeLines(paste0("H0: The ",component," factors Component in Model is sufficient to capture the full dimensionality of the data set"))
      writeLines(paste0("H1: The ",component," factors Component in Model is not sufficient to capture the full dimensionality of the data set"))
      if(as.numeric(model_fac$PVAL) <= 0.05){
        writeLines(paste0("Result is Reject H0: Model is not Sufficient to capture the full dimensionality of the data set with ",component," factors"))
      }else if(as.numeric(model_fac$PVAL) > 0.05){
        writeLines(paste0("Result is Fail to Reject H0: Model is Sufficient to capture the full dimensionality of the data set with ",component," factors"))
      }
      writeLines("============================================================================================================")
      writeLines("Communality Scores (Variability explained by the factors explained be a linear combination of the factors)")
      comm_score <- apply(model_fac$loadings^2,1,sum)
      print(comm_score)
      writeLines("Uniqueness Scores (Variability, which can not be explained by a linear combination of the factors)")
      unique_score <- 1 - apply(model_fac$loadings^2,1,sum)
      print(unique_score)
      writeLines("Goodness fit of Model (Residual Matrix")
      Lambda <- model_fac$loadings
      Psi <- diag(model_fac$uniquenesses)
      S <- model_fac$correlation
      Sigma <- Lambda %*% t(Lambda) + Psi
      res_matrix <- round(S - Sigma, 6)
      print(res_matrix)
      
      x11()
      plot(model_fac$loadings[,1], 
           model_fac$loadings[,2],
           xlab = "Factor 1", 
           ylab = "Factor 2", 
           ylim = c(-1,1),
           xlim = c(-1,1),
           main = paste0("Factor Analysis with Rotation ",rotation))
      abline(h = 0, v = 0)
      text(model_fac$loadings[,1]-0.08, 
           model_fac$loadings[,2]+0.08,
           colnames(df),
           col="blue")
      abline(h = 0, v = 0)
      
      return(list(model_fac, comm_score, unique_score, res_matrix))
    }
    component <- factor_analysis(df, component=n_component)
    return(component)
  }
  else if(component_method=="PCA"){
    #use principal component analysis if most variables are uncorrelated
    PCA <- function(df, group_col="", center_value=TRUE, scale_value=TRUE, 
                    make_ellipse=TRUE, make_circle=TRUE, legend_pos="topright",
                    unique_label=rownames(df), 
                    gradient_cols=c("#00AFBB", "#E7B800", "#FC4E07"),
                    individual_col = "#c2ae00",
                    variable_col = "#0505a8",
                    variable_scale=1, individual_scale=1,
                    activate_ggplot=TRUE,
                    activate_fviz=TRUE){
      
      writeLines("=================== Making Principal Component Analysis =======================")
      library(factoextra)
      ggbiplot <- function(pcobj, choices = 1:2, scale = 1, pc.biplot = TRUE, 
                           obs.scale = 1 - scale, var.scale = scale, 
                           groups = NULL, ellipse = FALSE, ellipse.prob = 0.68, 
                           labels = NULL, labels.size = 3, alpha = 1, 
                           var.axes = TRUE, 
                           circle = FALSE, circle.prob = 0.69, 
                           varname.size = 3, varname.adjust = 1.5, 
                           varname.abbrev = FALSE, ...){
        library(ggplot2)
        library(plyr)
        library(scales)
        library(grid)
        
        stopifnot(length(choices) == 2)
        
        # Recover the SVD
        if(inherits(pcobj, 'prcomp')){
          nobs.factor <- sqrt(nrow(pcobj$x) - 1)
          d <- pcobj$sdev
          u <- sweep(pcobj$x, 2, 1 / (d * nobs.factor), FUN = '*')
          v <- pcobj$rotation
        } else if(inherits(pcobj, 'princomp')) {
          nobs.factor <- sqrt(pcobj$n.obs)
          d <- pcobj$sdev
          u <- sweep(pcobj$scores, 2, 1 / (d * nobs.factor), FUN = '*')
          v <- pcobj$loadings
        } else if(inherits(pcobj, 'PCA')) {
          nobs.factor <- sqrt(nrow(pcobj$call$X))
          d <- unlist(sqrt(pcobj$eig)[1])
          u <- sweep(pcobj$ind$coord, 2, 1 / (d * nobs.factor), FUN = '*')
          v <- sweep(pcobj$var$coord,2,sqrt(pcobj$eig[1:ncol(pcobj$var$coord),1]),FUN="/")
        } else if(inherits(pcobj, "lda")) {
          nobs.factor <- sqrt(pcobj$N)
          d <- pcobj$svd
          u <- predict(pcobj)$x/nobs.factor
          v <- pcobj$scaling
          d.total <- sum(d^2)
        } else {
          stop('Expected a object of class prcomp, princomp, PCA, or lda')
        }
        
        # Scores
        choices <- pmin(choices, ncol(u))
        df.u <- as.data.frame(sweep(u[,choices], 2, d[choices]^obs.scale, FUN='*'))
        
        # Directions
        v <- sweep(v, 2, d^var.scale, FUN='*')
        df.v <- as.data.frame(v[, choices])
        
        names(df.u) <- c('xvar', 'yvar')
        names(df.v) <- names(df.u)
        
        if(pc.biplot) {
          df.u <- df.u * nobs.factor
        }
        
        # Scale the radius of the correlation circle so that it corresponds to 
        # a data ellipse for the standardized PC scores
        r <- sqrt(qchisq(circle.prob, df = 2)) * prod(colMeans(df.u^2))^(1/4)
        
        # Scale directions
        v.scale <- rowSums(v^2)
        df.v <- r * df.v / sqrt(max(v.scale))
        
        # Change the labels for the axes
        if(obs.scale == 0) {
          u.axis.labs <- paste('standardized PC', choices, sep='')
        } else {
          u.axis.labs <- paste('PC', choices, sep='')
        }
        
        # Append the proportion of explained variance to the axis labels
        u.axis.labs <- paste(u.axis.labs, 
                             sprintf('(%0.1f%% explained var.)', 
                                     100 * pcobj$sdev[choices]^2/sum(pcobj$sdev^2)))
        
        # Score Labels
        if(!is.null(labels)) {
          df.u$labels <- labels
        }
        
        # Grouping variable
        if(!is.null(groups)) {
          df.u$groups <- groups
        }
        
        # Variable Names
        if(varname.abbrev) {
          df.v$varname <- abbreviate(rownames(v))
        } else {
          df.v$varname <- rownames(v)
        }
        
        # Variables for text label placement
        df.v$angle <- with(df.v, (180/pi) * atan(yvar / xvar))
        df.v$hjust = with(df.v, (1 - varname.adjust * sign(xvar)) / 2)
        
        # Base plot
        g <- ggplot(data = df.u, aes(x = xvar, y = yvar)) + 
          xlab(u.axis.labs[1]) + ylab(u.axis.labs[2]) + coord_equal()
        
        if(var.axes) {
          # Draw circle
          if(circle) 
          {
            theta <- c(seq(-pi, pi, length = 50), seq(pi, -pi, length = 50))
            circle <- data.frame(xvar = r * cos(theta), yvar = r * sin(theta))
            g <- g + geom_path(data = circle, color = muted('white'), 
                               size = 1/2, alpha = 1/3)
          }
          
          # Draw directions
          g <- g +
            geom_segment(data = df.v,
                         aes(x = 0, y = 0, xend = xvar, yend = yvar),
                         arrow = arrow(length = unit(1/2, 'picas')), 
                         color = muted('red'))
        }
        
        # Draw either labels or points
        if(!is.null(df.u$labels)) {
          if(!is.null(df.u$groups)) {
            g <- g + geom_text(aes(label = labels, color = groups), 
                               size = labels.size)
          } else {
            g <- g + geom_text(aes(label = labels), size = labels.size)      
          }
        } else {
          if(!is.null(df.u$groups)) {
            g <- g + geom_point(aes(color = groups), alpha = alpha)
          } else {
            g <- g + geom_point(alpha = alpha)      
          }
        }
        
        # Overlay a concentration ellipse if there are groups
        if(!is.null(df.u$groups) && ellipse) {
          theta <- c(seq(-pi, pi, length = 50), seq(pi, -pi, length = 50))
          circle <- cbind(cos(theta), sin(theta))
          
          ell <- ddply(df.u, 'groups', function(x) {
            if(nrow(x) <= 2) {
              return(NULL)
            }
            sigma <- var(cbind(x$xvar, x$yvar))
            mu <- c(mean(x$xvar), mean(x$yvar))
            ed <- sqrt(qchisq(ellipse.prob, df = 2))
            data.frame(sweep(circle %*% chol(sigma) * ed, 2, mu, FUN = '+'), 
                       groups = x$groups[1])
          })
          names(ell)[1:2] <- c('xvar', 'yvar')
          g <- g + geom_path(data = ell, aes(color = groups, group = groups))
        }
        
        # Label the variable axes
        if(var.axes) {
          g <- g + 
            geom_text(data = df.v, 
                      aes(label = varname, x = xvar, y = yvar, 
                          angle = angle, hjust = hjust), 
                      color = 'darkred', size = varname.size)
        }
        return(g)
      }
      
      ggscreeplot <- function(pcobj, type = c('pev', 'cev')) {
        type <- match.arg(type)
        d <- pcobj$sdev^2
        yvar <- switch(type, 
                       pev = d / sum(d), 
                       cev = cumsum(d) / sum(d))
        
        yvar.lab <- switch(type,
                           pev = 'proportion of explained variance',
                           cev = 'cumulative proportion of explained variance')
        
        df <- data.frame(PC = 1:length(d), yvar = yvar)
        
        ggplot(data = df, aes(x = PC, y = yvar)) + 
          xlab('principal component number') + ylab(yvar.lab) +
          geom_point() + geom_path()
      }
      
      pca_group <- df[[group_col]]
      df <- df[,-which(colnames(df)==group_col)]
      #print(head(df))
      
      #fraud_PCA[[1]]$x[,1:2]
      pca_object <- prcomp(df, center = center_value, scale. = scale_value)
      pca_eigenvalues <- get_eig(pca_object)
      print(pca_eigenvalues)
      xlab_string <- paste0("PCA 1 with % Variance (",round(pca_eigenvalues$variance.percent[1],1),")")
      ylab_string <- paste0("PCA 2 with % Variance (",round(pca_eigenvalues$variance.percent[2],1),")")
      
      writeLines('Plotting 2 First Components')
      x11()
      plot(x=pca_object$x[,1], y=pca_object$x[,2], col=pca_group, main = "PCA Components Projection",
           xlab=xlab_string, ylab=ylab_string)
      legend(x=legend_pos, legend = unique(pca_group), col=unique(pca_group), pch=1)
      
      if(activate_ggplot){
        writeLines('Plotting Components with Biplot and Screeplot')
        x11()
        plot <- ggbiplot(pca_object, ellipse=make_ellipse, circle=make_circle, 
                         labels=unique_label, groups=pca_group, 
                         obs.scale = individual_scale, 
                         var.scale = variable_scale)
        print(plot)
        x11()
        plot <- ggscreeplot(pca_object)
        print(plot)
      }
      
      if(activate_fviz){
        writeLines("Make an Eigenvalues/Variances of Principal Dimension")
        x11()
        plot <- fviz_eig(pca_object)
        print(plot)
        
        writeLines("Make Graph for Individuals as Focus")
        x11()
        plot <- fviz_pca_ind(pca_object,
                             col.ind = individual_col,
                             gradient.cols = gradient_cols,
                             repel = TRUE)
        print(plot)
        writeLines("Make Graph for Variable/Feature as Focus")
        x11()
        plot <- fviz_pca_var(pca_object,
                             col.var = variable_col,
                             gradient.cols = gradient_cols,
                             repel = TRUE)
        print(plot)
        writeLines("Make Graph of Variable for Both Individual and Variable with Biplot")
        x11()
        plot <- fviz_pca_biplot(pca_object, repel = TRUE,
                                col.var = variable_col,
                                col.ind = individual_col, 
                                addEllipses = make_ellipse,
                                gradient.cols = gradient_cols)
        print(plot)
      }
      return(list(pca_object, pca_eigenvalues))
    }
    component <- PCA(df, group_col=component_dependent, activate_ggplot = FALSE, activate_fviz = FALSE,
                     legend_pos = "bottomright")
    return(component)
  }
  else if(component_method=="ICA"){
    #use independent component analysis to kinds of cases 
    ICA <- function(data_matrix, data_label,
                    n_component=3, func_ICA="logcosh",
                    legend_pos="topright", guide=TRUE){
      library(fastICA)
      data_matrix <- as.matrix(data_matrix)
      if(guide){
        writeLines("=================== Making Independent Component Analysis =======================")
        writeLines("Paramater Availables:")
        writeLines("if alg.typ == parallel the components are extracted simultaneously (the default)")
        writeLines("if alg.typ == deflation the components are extracted one at a time.")
        writeLines("Properties of ICA")
        writeLines("X -> pre-processed data matrix")
        writeLines("K -> pre-whitening matrix that projects data onto the first n.comp principal components.")
        writeLines("W -> estimated un-mixing matrix (see definition in details)")
        writeLines("A -> estimated mixing matrix")
        writeLines("S -> estimated source matrix") 
        writeLines("Tips: Use ICA to decompose Mixing Systems to an Independent Components which is not dependent to one and another Components")
        writeLines("Assumption: Components decomposed will have non Gaussian Distributions")
      }
      
      ica_result <- fastICA(data_matrix, n_component, 
                            alg.typ = "parallel", fun = func_ICA, alpha = 1,
                            method = "R", row.norm = FALSE, maxit = 200,
                            tol = 0.0001, verbose = TRUE)
      print(ica_result)
      
      x11()
      plot(fraud_ICA$S, col=data_label, main = "ICA Components Projection")
      legend(x=legend_pos, legend = unique(data_label), col=unique(data_label), pch=1)
      return(ica_result)
    }
    component <- ICA(df, df[[component_dependent]], n_component=n_component)
    return(component)
  }
  else if(component_method=="DA"){
    discriminant_analysis <- function(data, formula, type_analysis=1,
                                      mixture_cluster_component=3, k_group=4,
                                      mclust_model_pattern="EEI",
                                      draw_partition_matrix = FALSE){
      library(MASS)
      library(mda)
      library(robustDA)
      library(graphics)
      library(car)
      library(rattle)
      library(klaR)
      library(MASS)
      library(gmodels)
      target_var <- unlist(strsplit(formula, " ~ "))[1]
      print(target_var)
      formula <- as.formula(formula)
      
      plot_factor = function(df, factor_var, factor_name, 
                             individual_label=row.names(df), 
                             main_label = "Cluster Plot"){
        if(ncol(df) > 2){
          axes=c(1,2)
          pca <- stats::prcomp(df, scale = FALSE, center = FALSE)
          ind <- facto_summarize(pca, element = "ind", result = "coord", axes = c(1,2))
          ind$cluster <- factor_var
          colnames(ind)[length(colnames(ind))] <- factor_name
          print(head(ind))
          eig <- get_eigenvalue(pca)[axes, 2]
          xlab = paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
          ylab = paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
          plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = factor_name,
                                    show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                    ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                    pointsize = 1.5, labelsize = 12, main = main_label, 
                                    xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label), segment.color = factor_var, segment.colour = factor_var)
          return(plot)
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          ind$cluster <- factor_var
          colnames(ind)[length(colnames(ind))] <- factor_name
          print(head(ind))
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          plot <- ggpubr::ggscatter(ind, xlab, ylab, color = factor_name,
                                    show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                    ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                    pointsize = 1.5, labelsize = 12, main = main_label, 
                                    xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label), segment.color = factor_var, segment.colour = factor_var)
          return(plot)
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }
      
      discriminant_analysis_error_rate <- function (grouping, x, y, method = "lda", prec = 100, xlab = NULL, 
                                                    ylab = NULL, col.correct = "black", col.wrong = "red", 
                                                    col.mean = "black", col.contour = "darkgrey", 
                                                    gs = as.character(grouping), pch.mean = 19, cex.mean = 1.3, 
                                                    print.err = 0.7, legend.err = FALSE, legend.bg = "white", 
                                                    imageplot = TRUE, image.colors = cm.colors(nc), plot.control = list(), 
                                                    ...){
        success <- switch(method, rpart = requireNamespace("rpart"), 
                          naiveBayes = requireNamespace("e1071"))
        if (!is.null(success) && !success) 
        {
          message("For method 'rpart' the 'rpart' package is required, for method 'naiveBayes' the package 'e1071'.")
          return(NULL)
        }
        z <- switch(method, lda = lda(grouping ~ x + y, ...), 
                    qda = qda(grouping ~ x + y, ...), svmlight = svmlight(grouping ~ x + y, ...), 
                    rda = rda(grouping ~ x + y, data = cbind.data.frame(grouping = grouping, x = x, y = y), ...), 
                    sknn = sknn(grouping ~ x + y, ...), 
                    rpart = rpart::rpart(grouping ~ x + y, ...), 
                    naiveBayes = e1071::naiveBayes(grouping ~ x + y, data = cbind.data.frame(grouping = grouping, x = x, y = y), ...), stop("method not yet supported"))
        xg <- seq(min(x), max(x), length = prec)
        yg <- seq(min(y), max(y), length = prec)
        grd <- expand.grid(x = xg, y = yg)
        temp <- switch(method, lda = predict(z, grd, ...)$post, 
                       qda = predict(z, grd, ...)$post, svmlight = e.scal(predict(z, grd, ...)$post)$sv, 
                       rda = predict(z, grd, posterior = TRUE, aslist = TRUE)$post, 
                       rpart = predict(z, grd, ...), sknn = predict(z, grd, ...)$post, 
                       naiveBayes = predict(z, grd, type = "raw", ...), stop("method not yet supported"))
        khead <- switch(method, lda = predict(z, data.frame(cbind(x, y)), ...)$class, 
                        qda = predict(z, data.frame(cbind(x, y)), ...)$class, 
                        svmlight = predict(z, data.frame(cbind(x,y)), ...)$class, 
                        rda = predict(z, data.frame(cbind(x, y)), posterior = TRUE, aslist = TRUE)$class, 
                        rpart = predict(z, data.frame(cbind(x, y)), type = "class", ...), 
                        sknn = predict(z, data.frame(cbind(x, y)), ...)$class, 
                        naiveBayes = predict(z, data.frame(cbind(x, y)), ...), 
                        stop("method not yet supported"))
        colorw <- grouping != khead
        err <- round(mean(colorw), 3)
        return(err)
      }
      
      partimat_break <- function(df, coldependent, by=4, method_partimat="lda"){
        df_column_independent = colnames(df)
        df_column = df_column_independent[which(df_column_independent != coldependent)]
        pos_combi = expand.grid(df_column, df_column)
        pos_combi = pos_combi[which(pos_combi$Var1 != pos_combi$Var2),]
        row.names(pos_combi) <- NULL
        #https://stackoverflow.com/questions/9028369/removing-duplicate-combinations-irrespective-of-order
        pos_combi_real <- t(apply(pos_combi, 1, sort))
        pos_combi_real <- pos_combi_real[!duplicated(pos_combi_real),]
        pos_combi_real <- as.data.frame(pos_combi_real)
        pos_combi_real$formula <- paste0(coldependent, " ~ ", pos_combi_real$V1, " + ", pos_combi_real$V2)
        pos_combi_real$error_rate <- NULL
        for(error_iter in 1:nrow(pos_combi_real)){
          v1 = pos_combi_real$V1[error_iter]
          v2 = pos_combi_real$V2[error_iter]
          x1 <- df[,which(df_column_independent==v1)]
          y1 <- df[,which(df_column_independent==v2)]
          pos_combi_real$error_rate[error_iter] <- discriminant_analysis_error_rate(grouping = df[,coldependent], x=x1, y=y1,
                                                                                    method=method_partimat, xlab=v1, ylab=v2)
        }
        iteration_stop = nrow(pos_combi_real)
        iteration_partimat = round(nrow(pos_combi_real) / by, 0)
        iter = 0
        #print(head(df))
        #print(head(pos_combi_real))
        
        for(possible in 1:iteration_partimat){
          x11()
          par(mfrow=c(2,2))
          v1_1st_explain = pos_combi_real$V1[1 + 4*(possible-1)]
          v2_1st_explain = pos_combi_real$V2[1 + 4*(possible-1)]
          v1_2nd_explain = pos_combi_real$V1[2 + 4*(possible-1)]
          v2_2nd_explain = pos_combi_real$V2[2 + 4*(possible-1)]
          v1_3rd_explain = pos_combi_real$V1[3 + 4*(possible-1)]
          v2_3rd_explain = pos_combi_real$V2[3 + 4*(possible-1)]
          v1_4th_explain = pos_combi_real$V1[4 + 4*(possible-1)]
          v2_4th_explain = pos_combi_real$V2[4 + 4*(possible-1)]
          
          x1 <- df[,which(df_column_independent==v1_1st_explain)]
          y1 <- df[,which(df_column_independent==v2_1st_explain)]
          x2 <- df[,which(df_column_independent==v1_2nd_explain)]
          y2 <- df[,which(df_column_independent==v2_2nd_explain)]
          x3 <- df[,which(df_column_independent==v1_3rd_explain)]
          y3 <- df[,which(df_column_independent==v2_3rd_explain)]
          x4 <- df[,which(df_column_independent==v1_4th_explain)]
          y4 <- df[,which(df_column_independent==v2_4th_explain)]
          
          drawparti(grouping = df[,coldependent], x=x1, y=y1,
                    method=method_partimat, xlab=v1_1st_explain, ylab=v2_1st_explain)
          iter = iter + 1
          if(iter == iteration_stop){break}
          drawparti(grouping = df[,coldependent], x=x2, y=y2,
                    method=method_partimat, xlab=v1_2nd_explain, ylab=v2_2nd_explain)
          iter = iter + 1
          if(iter == iteration_stop){break}
          drawparti(grouping = df[,coldependent], x=x3, y=y3,
                    method=method_partimat, xlab=v1_3rd_explain, ylab=v2_3rd_explain)
          iter = iter + 1
          if(iter == iteration_stop){break}
          drawparti(grouping = df[,coldependent], x=x4, y=y4,
                    method=method_partimat, xlab=v1_4th_explain, ylab=v2_4th_explain)
          iter = iter + 1
          if(iter == iteration_stop){break}
        }
        return(pos_combi_real)
      }
      
      writeLines("Discriminant Analysis Purposes -> Performs a multivariate test of differences between groups.")
      writeLines("In addition, discriminant analysis is used to determine the minimum number of dimensions needed to describe these differences")
      writeLines("==================================================================")
      writeLines("=================== Discriminant Analysis types: =================")
      writeLines("==== 1) LDA (Linear Discriminant Analysis) =======================")
      writeLines("==== 2) QDA (Quadratic Discriminant Analysis) ====================")
      writeLines("==== 3) MDA (Mixture Discriminant Analysis) ======================")
      writeLines("==== 4) RMDA (Robust Mixture Discriminant Analysis) ==============")
      writeLines("==== 5) FDA (Flexible Discriminant Analysis) =====================")
      writeLines("==== 6) RDA (Regularized Discriminant Analysis) ==================")
      writeLines("==================================================================")
      
      if(type_analysis==1){
        lda <- lda(formula, data=data)
        lda_mapping <- predict(lda)$x
        print(head(lda_mapping))
        x11()
        enhanced_plot <- plot_factor(lda_mapping, data[[target_var]], target_var)
        print(enhanced_plot)
        
        writeLines("LDA Cross Tabulation")
        lda_values <- predict(lda)
        conf_matrix <- CrossTable(data[[target_var]], lda_values$class)
        print(conf_matrix)
        
        if(ncol(lda_mapping) > 1){
          writeLines("Density Distribution by LDA")
          x11()
          writeLines("Plot Histogram of Density distribution in LDA Dim 1")
          ldahist(data = lda_values$x[,1], g=data[[target_var]])
          x11()
          writeLines("Plot Histogram of Density distribution in LDA Dim 2")
          ldahist(data = lda_values$x[,2], g=data[[target_var]])
          
          writeLines("LDA 2 Dimension Scatterplot")
          x11()
          plot(lda_values$x[,1],lda_values$x[,2]) # make a scatterplot
          text(lda_values$x[,1],lda_values$x[,2], data[[target_var]], 
               cex=0.7,pos=4,col="red") 
        }
        
        if(draw_partition_matrix){
          partimat_lda_df <- partimat_break(data, target_var, 
                                            method_partimat = "lda")
          return(list(lda, lda_values, partimat_lda_df))
        }
        return(list(lda, lda_values, conf_matrix))
      }
      else if(type_analysis==2){
        qda <- qda(formula, data=data)
        
        writeLines("QDA Cross Tabulation")
        qda_values <- predict(qda)
        conf_matrix <- CrossTable(data[[target_var]], qda_values$class)
        print(conf_matrix)
        
        totaldim <- length(colnames(qda_values$posterior))
        writeLines(paste0("QDA Dimension = ", length(colnames(qda_values$posterior))))
        for(qda_dim in 1:(totaldim-1)){
          writeLines("QDA 2 Dimension Scatterplot")
          writeLines(paste0("Plot Dimension 1 with ", (qda_dim+1)))
          x11()
          plot(qda_values$posterior[,1],qda_values$posterior[,(qda_dim+1)]) # make a scatterplot
          text(qda_values$posterior[,1],qda_values$posterior[,(qda_dim+1)], data[[target_var]], 
               cex=0.7,pos=4,col="red")
        }
        if(draw_partition_matrix){
          partimat_qda_df <- partimat_break(wine, "Type", method_partimat = "qda")
          return(list(qda, qda_values, partimat_qda_df))
        }
        return(list(qda, qda_values, conf_matrix))
      }
      else if(type_analysis==3){
        
        writeLines("MDA using MDA Package approach")
        mda <- mda(formula, data=data)
        
        writeLines("MDA Cross Tabulation")
        mda_values <- predict(mda, newdata=data)
        print(CrossTable(data[[target_var]], mda_values))
        
        writeLines("MDA Percent Explained Variances and total Dimension created")
        print(mda$percent.explained)
        
        writeLines("MDA using RobustDA Package approach")
        #G = Cluster of Mixture Components 1:5
        mda2 = MclustDA(data[,-(which(colnames(data)==target_var))], 
                        data[[target_var]], G=mixture_cluster_component) 
        summary(mda2, parameters = TRUE)
        res_mda = predict(mda2)$cl
        conf_matrix <- CrossTable(data[[target_var]], res_mda)
        print(conf_matrix)
        
        x11()
        plot(mda2, what = "classification", main="Predicted Classification Plot")
        x11()
        plot(mda2, what = "scatterplot", main="Known Classification Plot")
        x11()
        plot(mda2, what = "error", main="Miss Classification Marked Plot")
        return(list(mda, mda_values, mda2, res_mda, conf_matrix))
      }
      else if(type_analysis==4){
        data_numeric <- data[,-(which(colnames(data)==target_var))]
        data_numeric <- data_numeric %>%
          mutate_if(is.integer, as.numeric)
        
        rmda <- rmda(data_numeric, 
                     as.numeric(data[[target_var]]), K=k_group, 
                     model = mclust_model_pattern) 
        res_rmda <- predict(rmda, X=data_numeric)$cl
        conf_matrix <- CrossTable(data[[target_var]], res_rmda)
        print(conf_matrix)
        
        x11()
        plot(data_numeric, col=res_rmda, pch=(18:19)[res_rmda],
             main='Predicted Classification Plot with RMDA')
        return(list(rmda, res_rmda, conf_matrix))
      }
      else if(type_analysis==5){
        fda <- fda(formula, data=data)
        
        writeLines("FDA Cross Tabulation")
        fda_values <- predict(fda, newdata=data)
        conf_matrix <- CrossTable(data[[target_var]], fda_values)
        print(conf_matrix)
        
        writeLines("FDA Percent Explained Variances and total Dimension created")
        print(fda$percent.explained)
        
        if(length(fda$fit$fitted.values) > 0){
          writeLines("FDA 2 Dimension Scatterplot")
          x11()
          plot(fda$fit$fitted.values[,1],fda$fit$fitted.values[,2], main="FDA 2 Dimension Scatterplot") # make a scatterplot
          text(fda$fit$fitted.values[,1],fda$fit$fitted.values[,2], data[[target_var]], 
               cex=0.7,pos=4,col="red")
        }else{
          writeLines("Not Enough Dimension to create FDA Dimension Scatterplot!")
        }
        
        return(list(fda, fda_values, conf_matrix))
      }
      else if(type_analysis==6){
        rda <- rda(formula, data=data)
        
        writeLines("RDA Cross Tabulation")
        rda_values <- predict(rda, newdata=data)
        conf_matrix <- CrossTable(data[[target_var]], rda_values$class)
        print(conf_matrix)
        
        totaldim <- length(colnames(rda_values$posterior))
        print(totaldim)
        writeLines(paste0("RDA Dimension = ", length(colnames(rda_values$posterior))))
        for(rda_dim in 1:(totaldim-1)){
          writeLines("RDA 2 Dimension Scatterplot")
          writeLines(paste0("Plot Dimension 1 with ", (rda_dim+1)))
          x11()
          plot(rda_values$posterior[,1],rda_values$posterior[,(rda_dim+1)]) # make a scatterplot
          text(rda_values$posterior[,1],rda_values$posterior[,(rda_dim+1)], data[[target_var]], 
               cex=0.7,pos=4,col="red")
        }
        
        if(draw_partition_matrix){
          partimat_rda_df <- partimat_break(data, target_var, 
                                            method_partimat = "rda")
          return(list(rda, rda_values, partimat_rda_df))
        }
        return(list(rda, rda_values, conf_matrix))
      }
    }
    unique_label <- length(unique(data[[class_name]]))
    formula_set <- paste0(class_name, " ~ .")
    if(unique_label > 2){
      df_LDA <- discriminant_analysis(wine, formula_set, type_analysis = 1)
      df_QDA <- discriminant_analysis(wine, formula_set, type_analysis = 2)
      df_MDA <- discriminant_analysis(wine, formula_set, type_analysis = 3)
      df_RMDA <- discriminant_analysis(wine, formula_set, type_analysis = 4)
      df_FDA <- discriminant_analysis(wine, formula_set, type_analysis = 5)
      df_RDA <- discriminant_analysis(wine, formula_set, type_analysis = 6)
      list_DA <- list(df_LDA, df_QDA, df_MDA, df_RMDA, df_FDA, df_RDA)
      names(list_DA) <- c("Linear Discriminant Analysis",
                          "Quadratic Discriminant Analysis",
                          "Mixture Discriminant Analysis",
                          "Robust Mixture Discriminant Analysis",
                          "Flexible Discriminant Analysis",
                          "Regularized Discriminant Analysis")
      return(list_DA)
    }
    else{
      message("Not Possible to make Discriminant Analysis with at least 2 Components!")
    }
  }
}

dim_reduction_fast_2D_mapping <- function(df, class_label=""){
  dimension_reduction_by_dimred <- function(data, my_formula, knn_num=2, method="LaE",
                                            legend_pos="topright", n_dimension=2, parameter_guide=FALSE){
    library(dimRed)
    library(lle)
    data_label <- data[[unlist(strsplit(my_formula, " ~ "))[1]]]
    my_formula <- as.formula(my_formula)
    dat <- as.dimRedData(my_formula, data)
    technic_name <- ""
    dimen <- NULL
    dimension_params <- NULL
    if(method == "LaE"){
      technic_name <- "Laplacian Eigenmaps"
      dimen <- dimRed::LaplacianEigenmaps()
      writeLines("Laplacian Eigenmaps use a kernel and were originally developed to separate non-convex clusters under the name spectral clustering.")
      writeLines("Laplacian Eigenmaps Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions.")
        writeLines("sparse -> A character vector specifying hot to make the graph sparse")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("eps -> an epsilon neighborhood graph is constructed, else a dense distance matrix is used.")
        writeLines("t -> Parameter for the transformation of the distance matrix by w=exp(-d^2/t), 
                 larger values give less weight to differences in distance, t == Inf treats all distances != 0 equally.")
        writeLines("norm -> logical, should the normed laplacian be used?")
      }
    }
    else if(method == "Hessian"){
      technic_name <- "Hessian LLE (Locally Linear Embedding)"
      dimen <- dimRed::HLLE()
      writeLines("HLLE uses local hessians to approximate the curvines and is an extension to non-convex subsets in lowdimensional space.")
      writeLines("Hessian LLE (Local Linear Embeddin) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
      }
    }
    else if(method == "ICA"){
      technic_name <- "ICA (Independent Component Analysis)"
      dimen <- dimRed::FastICA()
      writeLines("ICA is used for blind signal separation of different sources. It is a linear Projection.")
      writeLines("ICA (Independent Component Analysis) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
      }
    }
    else if(method == "Isomap"){
      technic_name <- "Isomap (Isometric Mapping)"
      dimen <- dimRed::Isomap()
      writeLines("The Isomap algorithm approximates a manifold using geodesic distances on a k nearest neighbor graph.")
      writeLines("Then classical scaling is performed on the resulting distance matrix.")
      writeLines("Isomap Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$knn <- knn_num
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("get_geod -> Should the geodesic distance matrix be kept, if TRUE, access it as getOtherData(x)$geod")
      }
    }
    else if(method == "DrL"){
      technic_name <- "Distributed Recursive Layout"
      dimen <- dimRed::DrL()
      writeLines("DrL uses a complex algorithm to avoid local minima in the graph embedding which uses several steps.")
      writeLines("DrL (Distributed Recursive Graph Layout) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DM"){
      technic_name <- "Diffusion Maps"
      dimen <- dimRed:DiffusionMaps()
      writeLines("Diffusion Maps uses a diffusion probability matrix to robustly approximate a manifold.")
      writeLines("Diffusion Maps (Local Linear Embeddin) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$eps <- "auto"
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("d -> a function transforming a matrix row wise into a distance matrix or dist object, e.g. dist.")
        writeLines("eps -> The epsilon parameter that determines the diffusion weight matrix from a distance matrix d, exp(-d^2/eps), 
                 if set to 'auto' it will be set to the median distance to the 0.01*n nearest neighbor.")
        writeLines("t -> Time-scale parameter. The recommended value, 0, uses multiscale geometry.")
        writeLines("delta -> Sparsity cut-off for the symmetric graph Laplacian, a higher value results in more sparsity and faster calculation. The predefined value is 10^-5.")
      }
    }
    else if(method == "FR"){
      technic_name <- "Fruchterman Reingold"
      dimen <- dimRed::FruchtermanReingold()
      writeLines("Fruchterman Reingold Graph Layout algorithm. puts the data into a circle and puts connected points close to each other.")
      writeLines("Fruchterman Reingold Graph Layout Parameters.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DRR"){
      technic_name <- "PCA with Ridge Regression"
      dimen <- dimRed::DRR()
      writeLines('DRR is a non-linear extension of PCA that uses Kernel Ridge regression.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("lambda -> regularization parameter for the ridge regression.")
        writeLines("kernel -> The kernel to use for KRR, defaults to rbfdot")
        writeLines("kernel.pars -> A list with kernel parameters, elements depend on the kernel used, rbfdot uses sigma.")
        writeLines("pca -> logical, should an initial pca step be performed, defaults to TRUE.")
        writeLines("pca.center -> logical, should the data be centered before the pca step. Defaults to TRUE.")
        writeLines("pca.scale -> logical, should the data be scaled before the pca ste. Defaults to FALSE.")
        writeLines("fastcv -> logical, should fastCV from the CVST package be used instead of normal cross-validation.")
        writeLines("fastcv.test -> If fastcv = TRUE, separate test data set for fastcv.")
        writeLines("cv.folds -> if fastcv = FALSE, specifies the number of folds for crossvalidation.")
        writeLines("fastkrr.nblocks -> integer, higher values sacrifice numerical accuracy for speed and less memory, see below for details")
        writeLines("verbose -> logical, should the cross-validation results be printed out.")
      }
    }
    else if(method == "KK"){
      technic_name <- "Kamada Kawai"
      dimen <- dimRed::KamadaKawai()
      writeLines("Kamada Kawai is Graph embedding algorithms se the data as a graph. 
               Between the nodes of the graph exist attracting and repelling forces 
               which can be modeled as electrical fields or springs connecting the nodes.")
      writeLines("The graph is then forced into a lower dimensional representation")
      writeLines("That tries to represent the forces between the nodes accurately")
      writeLines('by minimizing the total energy of the attracting and repelling forces.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "LLE"){
      technic_name <- "Locally Linear Embedding"
      dimen <- dimRed::LLE()
      writeLines('LLE approximates the points in the manifold by linear combination of its neighbors.')
      writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=50")
      }
    }
    else if(method == "MMDS"){
      technic_name <- "Metric MDS (Multi Dimensional Scaling)"
      dimen <- dimRed::MDS()
      writeLines('Metric MDS tries to maintain distances in high- and low-dimensional space,')
      writeLines('it has the advantage over PCA that arbitrary distance functions can be used, but it is high computational')
      writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NMDS"){
      technic_name <- "Non Metric Multi Dimensional Scaling"
      dimen <- dimRed::nMDS()
      writeLines('Non Metric MDS is a non-linear extension of MDS using monotonic regression')
      writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NNMF"){
      technic_name <- "Non Negative Matrix Factorization"
      dimen <- dimRed::NNMF()
      writeLines("NNMF is a method for decomposing a matrix into a smaller dimension such that the constraint that the data (and the projection) are not negative is taken into account.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("method -> character, which algorithm should be used. See nmf for possible values. Defaults to brunet")
        writeLines("nrun -> integer, the number of times the computations are conducted. See nmf")
        writeLines("seed -> integer, a value to control the random numbers used.")
        writeLines("options -> named list, other options to pass to nmf")
      }
    }
    else if(method == "AE"){
      library(tensorflow)
      technic_name <- "Auto Encoder"
      dimen <- dimRed::AutoEncoder()
      writeLines('Autoencoders are neural networks that try to reproduce their input. Consider this method unstable, as the internals may still be changed.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of dimensions for reduction.")
        writeLines("n_hidden -> The number of neurons in the hidden layers, the length specifies the number of layers, 
                 the length must be impair, the middle number must be the same as ndim.")
        writeLines("activation -> The activation functions for the layers, one of tanh, sigmoid, relu, elu")
        writeLines("Everything else will silently be ignored and there will be no activation function for the layer.")
        writeLines("weight_decay -> the coefficient for weight decay, set to 0 if no weight decay desired.")
        writeLines("learning_rate -> The learning rate for gradient descend")
        writeLines("graph -> Optional: A list of bits and pieces that define the autoencoder in tensorflow, see details.")
        writeLines("keras_graph -> Optional: A list of keras layers that define the encoder and decoder, specifying this, will ignore all other topology related variables, see details.")
        writeLines("batchsize -> If NA, all data will be used for training, else only a random subset of size batchsize will be used")
        writeLines("n_steps -> the number of training steps.")
      }
    }
    else if(method == "KPCA"){
      technic_name <- "Kernel PCA"
      dimen <- dimRed::AutoEncoder()
      writeLines('Kernel PCA is a nonlinear extension of PCA using kernel methods.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions, defaults to 2")
        writeLines("kpar -> A list with the parameters for the kernel function, defaults to list(sigma = 0.1)")
        writeLines("kernel -> The kernel function, either as a function or a character vector with the name of the kernel. Defaults to rbfdot")
      }
    }
    else if(method == "PCA_L1"){
      technic_name <- "Regularization PCA L1"
      dimen <- dimRed::PCA_L1()
      writeLines("PCA transforms the data so that the L2 reconstruction error is minimized or the variance of the projected data is maximized.")
      writeLines("This is sensitive to outliers, L1 PCA minimizes the L1 reconstruction error or maximizes the sum of the L1 norm of the projected observations.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("center -> logical, should the data be centered, defaults to TRUE.")
        writeLines("scale -> logical, should the data be scaled, defaults to FALSE.")
        writeLines("fun -> character or function, the method to apply, see the pcaL1 package")
      }
    }
    emb <- dimen@fun(dat, dimension_params) 
    dimensional_data <- data.frame(emb@data@data)
    return(dimensional_data)
  }
  
  tSNE <- function(data_train, perplex_param=15, 
                   check_duplicate=TRUE, use_initial_pca=TRUE,
                   partial_pca_enable=FALSE){
    library(Rtsne)
    library(irlba)
    
    data_train <- as.matrix(data_train)
    
    # You can change the value of perplexity and see how the plot changes
    tsne_results <- Rtsne(data_train, perplexity=perplex_param, 
                          check_duplicates = check_duplicate,
                          pca = use_initial_pca,
                          partial_pca = partial_pca_enable)
    return(tsne_results)
  }
  
  uniform_manifold_approximation_projection <- function(data){
    library(umap)
    umap_result <- umap(data)
    return(umap_result)
  }
  
  tf_synchronize <- function(){
    library(tensorflow)
    rm(list=c("tf")) #Delete first if there any tf variable in Global Environment
    tf_compat_names <- names(tf$compat$v1)  
    for(x in 2:length(tf_compat_names)){
      tf[[tf_compat_names[x]]] <- tf$compat$v1[[tf_compat_names[x]]]
    }
  }
  
  trimap_pacmap_from_python <- function(df, method="trimap", apply_pca=TRUE, point_size=0.5,
                                        trimap_default_parameter = list(n_inliers=10L, n_outliers=5L, 
                                                                        n_random=5L, distance="euclidean",
                                                                        weight_adj=500L, lr=1000L, n_iters=400L),
                                        pacmap_default_parameter = list(n_dims=2L, MN_ratio=0.5, FP_ratio=2, n_iters=450L,
                                                                        use_neighbors=TRUE)){
    #https://pypi.org/project/trimap/
    #https://pypi.org/project/pacmap/
    library(ggplot2)
    
    reticulate_set <- function(log=FALSE){
      reticulate::use_python("C:/Users/User/AppData/Local/Programs/Python/Python38") #can only be assigned once per session
      library(reticulate)
      if(log){
        writeLines('Set Reticulate with Default Path')
        print(py_config)
      }
    }
    
    reticulate_set()
    python_pandas <- import("pandas")
    python_pacmap <- import("pacmap")
    python_numpy <- import("numpy")
    python_trimap <- import("trimap")
    
    df_pandas <- reticulate::r_to_py(df)
    nparray <- df_pandas$values
    nparray <- nparray$astype(python_numpy$float)
    data_transformed <- NULL
    if(method=="trimap"){
      embedding_trimap <- python_trimap$TRIMAP(n_inliers=trimap_default_parameter[[1]], n_outliers=trimap_default_parameter[[2]], 
                                               n_random=trimap_default_parameter[[3]], distance=trimap_default_parameter[[4]],
                                               weight_adj=trimap_default_parameter[[5]], lr=trimap_default_parameter[[6]], 
                                               n_iters=trimap_default_parameter[[7]], apply_pca = apply_pca)
      X_transformed_trimap <- embedding_trimap$fit_transform(nparray)
      data_transformed <- data.frame(X_transformed_trimap)
    }
    else if(method=="pacmap"){
      if(pacmap_default_parameter[[5]]){
        embedding_pacmap <- python_pacmap$PaCMAP(n_dims=pacmap_default_parameter[[1]], MN_ratio=pacmap_default_parameter[[2]], 
                                                 FP_ratio=pacmap_default_parameter[[3]], num_iters = pacmap_default_parameter[[4]]) 
        if(apply_pca){
          X_transformed_pacmap <- embedding_pacmap$fit_transform(nparray, init="pca")
          data_transformed <- data.frame(X_transformed_pacmap) 
        }
        else{
          X_transformed_pacmap <- embedding_pacmap$fit_transform(nparray, init="random")
          data_transformed <- data.frame(X_transformed_pacmap) 
        }
      }
      else{
        embedding_pacmap <- python_pacmap$PaCMAP(n_dims=pacmap_default_parameter[[1]], n_neighbors=NULL,
                                                 MN_ratio=pacmap_default_parameter[[2]], 
                                                 FP_ratio=pacmap_default_parameter[[3]]) 
        if(apply_pca){
          X_transformed_pacmap <- embedding_pacmap$fit_transform(nparray, init="pca")
          data_transformed <- data.frame(X_transformed_pacmap) 
        }
        else{
          X_transformed_pacmap <- embedding_pacmap$fit_transform(nparray, init="random")
          data_transformed <- data.frame(X_transformed_pacmap) 
        }
      }
    }
    return(data_transformed)
  }
  
  tf_synchronize()
  writeLines("========================== Using 2D Mapping Techniques ================================")
  writeLines("1) Executing Laplacian Eigenmaps Dimension Reductions")
  formula_set <- paste0(class_label," ~ .")
  dim_laplace <- dimension_reduction_by_dimred(df, formula_set, method="LaE") #Suitable
  writeLines("2) Executing t-SNE (t-Distributed Stochastic Neighbor Embedding) Dimension Reductions")
  dim_tsne <- tSNE(df)
  writeLines("3) Executing UMAP (Uniform Manifold Approximation Projection) Dimension Reductions")
  dim_umap <- uniform_manifold_approximation_projection(df)
  writeLines("4) Executing TriMAP (Triplets Manifold Approximation Projection) Dimension Reductions")
  dim_trimap <- trimap_pacmap_from_python(df, method="trimap")
  writeLines("5) Executing PaCMAP (Pairwise Controlled Manifold Approximation Projection) Dimension Reductions")
  dim_pacmap <- trimap_pacmap_from_python(df, method="pacmap")
  writeLines("6) Executing Auto Encoder (AE) from Tensorflow")
  dim_AE <- dimension_reduction_by_dimred(df, formula_set, method="AE")
  writeLines("7) Executing Kernel-PCA from Tensorflow")
  tf$disable_eager_execution()
  dim_KPCA <- dimension_reduction_by_dimred(df, formula_set, method="KPCA")
  tf$enable_eager_execution()
  list_2d_map <- list(dim_laplace, dim_tsne, dim_umap, dim_trimap, dim_pacmap, dim_AE, dim_KPCA)
  names(list_2d_map) <- c("Laplacian Eigenmaps",
                          "t-Distributed Stochastic Neighbor Embedding",
                          "Uniform Manifold Approximation Projection",
                          "Triplets Manifold Approximation Projection",
                          "Pairwise Controlled Manifold Approximation Projection",
                          "Auto Encoder",
                          "Kernel-PCA")
  return(list_2d_map)
  
}

exhaustive_dimension_reduction <- function(dframe, classlabel="", 
                                           exception_dim_model = c(5,23,35,48,79,81)){
  reference_dim_reduction <- "C:/Users/User/Desktop/Data Science Journey/Data Science with R/R Automation/Other_Dimension_tools.csv"
  library(Rdimtools)
  library(data.table)
  
  dimension_reduction_by_dimred <- function(data, my_formula, knn_num=2, method="LaE",
                                            legend_pos="topright", n_dimension=2, 
                                            parameter_guide=FALSE){
    library(dimRed)
    library(lle)
    data_label <- data[[unlist(strsplit(my_formula, " ~ "))[1]]]
    my_formula <- as.formula(my_formula)
    dat <- as.dimRedData(my_formula, data)
    technic_name <- ""
    dimen <- NULL
    dimension_params <- NULL
    if(method == "LaE"){
      technic_name <- "Laplacian Eigenmaps"
      dimen <- dimRed::LaplacianEigenmaps()
      writeLines("Laplacian Eigenmaps use a kernel and were originally developed to separate non-convex clusters under the name spectral clustering.")
      writeLines("Laplacian Eigenmaps Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions.")
        writeLines("sparse -> A character vector specifying hot to make the graph sparse")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("eps -> an epsilon neighborhood graph is constructed, else a dense distance matrix is used.")
        writeLines("t -> Parameter for the transformation of the distance matrix by w=exp(-d^2/t), 
                 larger values give less weight to differences in distance, t == Inf treats all distances != 0 equally.")
        writeLines("norm -> logical, should the normed laplacian be used?")
      }
    }
    else if(method == "Hessian"){
      technic_name <- "Hessian LLE (Locally Linear Embedding)"
      dimen <- dimRed::HLLE()
      writeLines("HLLE uses local hessians to approximate the curvines and is an extension to non-convex subsets in lowdimensional space.")
      writeLines("Hessian LLE (Local Linear Embeddin) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
      }
    }
    else if(method == "ICA"){
      technic_name <- "ICA (Independent Component Analysis)"
      dimen <- dimRed::FastICA()
      writeLines("ICA is used for blind signal separation of different sources. It is a linear Projection.")
      writeLines("ICA (Independent Component Analysis) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
      }
    }
    else if(method == "Isomap"){
      technic_name <- "Isomap (Isometric Mapping)"
      dimen <- dimRed::Isomap()
      writeLines("The Isomap algorithm approximates a manifold using geodesic distances on a k nearest neighbor graph.")
      writeLines("Then classical scaling is performed on the resulting distance matrix.")
      writeLines("Isomap Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$knn <- knn_num
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("get_geod -> Should the geodesic distance matrix be kept, if TRUE, access it as getOtherData(x)$geod")
      }
    }
    else if(method == "DrL"){
      technic_name <- "Distributed Recursive Layout"
      dimen <- dimRed::DrL()
      writeLines("DrL uses a complex algorithm to avoid local minima in the graph embedding which uses several steps.")
      writeLines("DrL (Distributed Recursive Graph Layout) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DM"){
      technic_name <- "Diffusion Maps"
      dimen <- dimRed::DiffusionMaps()
      writeLines("Diffusion Maps uses a diffusion probability matrix to robustly approximate a manifold.")
      writeLines("Diffusion Maps (Local Linear Embeddin) Parameters")
      dimension_params <- dimen@stdpars
      dimension_params$eps <- "auto"
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("d -> a function transforming a matrix row wise into a distance matrix or dist object, e.g. dist.")
        writeLines("eps -> The epsilon parameter that determines the diffusion weight matrix from a distance matrix d, exp(-d^2/eps), 
                 if set to 'auto' it will be set to the median distance to the 0.01*n nearest neighbor.")
        writeLines("t -> Time-scale parameter. The recommended value, 0, uses multiscale geometry.")
        writeLines("delta -> Sparsity cut-off for the symmetric graph Laplacian, a higher value results in more sparsity and faster calculation. The predefined value is 10^-5.")
      }
    }
    else if(method == "FR"){
      technic_name <- "Fruchterman Reingold"
      dimen <- dimRed::FruchtermanReingold()
      writeLines("Fruchterman Reingold Graph Layout algorithm. puts the data into a circle and puts connected points close to each other.")
      writeLines("Fruchterman Reingold Graph Layout Parameters.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DRR"){
      technic_name <- "PCA with Ridge Regression"
      dimen <- dimRed::DRR()
      writeLines('DRR is a non-linear extension of PCA that uses Kernel Ridge regression.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("lambda -> regularization parameter for the ridge regression.")
        writeLines("kernel -> The kernel to use for KRR, defaults to rbfdot")
        writeLines("kernel.pars -> A list with kernel parameters, elements depend on the kernel used, rbfdot uses sigma.")
        writeLines("pca -> logical, should an initial pca step be performed, defaults to TRUE.")
        writeLines("pca.center -> logical, should the data be centered before the pca step. Defaults to TRUE.")
        writeLines("pca.scale -> logical, should the data be scaled before the pca ste. Defaults to FALSE.")
        writeLines("fastcv -> logical, should fastCV from the CVST package be used instead of normal cross-validation.")
        writeLines("fastcv.test -> If fastcv = TRUE, separate test data set for fastcv.")
        writeLines("cv.folds -> if fastcv = FALSE, specifies the number of folds for crossvalidation.")
        writeLines("fastkrr.nblocks -> integer, higher values sacrifice numerical accuracy for speed and less memory, see below for details")
        writeLines("verbose -> logical, should the cross-validation results be printed out.")
      }
    }
    else if(method == "KK"){
      technic_name <- "Kamada Kawai"
      dimen <- dimRed::KamadaKawai()
      writeLines("Kamada Kawai is Graph embedding algorithms se the data as a graph. 
               Between the nodes of the graph exist attracting and repelling forces 
               which can be modeled as electrical fields or springs connecting the nodes.")
      writeLines("The graph is then forced into a lower dimensional representation")
      writeLines("That tries to represent the forces between the nodes accurately")
      writeLines('by minimizing the total energy of the attracting and repelling forces.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "LLE"){
      technic_name <- "Locally Linear Embedding"
      dimen <- dimRed::LLE()
      writeLines('LLE approximates the points in the manifold by linear combination of its neighbors.')
      writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=50")
      }
    }
    else if(method == "MMDS"){
      technic_name <- "Metric MDS (Multi Dimensional Scaling)"
      dimen <- dimRed::MDS()
      writeLines('Metric MDS tries to maintain distances in high- and low-dimensional space,')
      writeLines('it has the advantage over PCA that arbitrary distance functions can be used, but it is high computational')
      writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NMDS"){
      technic_name <- "Non Metric Multi Dimensional Scaling"
      dimen <- dimRed::nMDS()
      writeLines('Non Metric MDS is a non-linear extension of MDS using monotonic regression')
      writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NNMF"){
      technic_name <- "Non Negative Matrix Factorization"
      dimen <- dimRed::NNMF()
      writeLines("NNMF is a method for decomposing a matrix into a smaller dimension such that the constraint that the data (and the projection) are not negative is taken into account.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("method -> character, which algorithm should be used. See nmf for possible values. Defaults to brunet")
        writeLines("nrun -> integer, the number of times the computations are conducted. See nmf")
        writeLines("seed -> integer, a value to control the random numbers used.")
        writeLines("options -> named list, other options to pass to nmf")
      }
    }
    else if(method == "AE"){
      library(tensorflow)
      technic_name <- "Auto Encoder"
      dimen <- dimRed::AutoEncoder()
      writeLines('Autoencoders are neural networks that try to reproduce their input. Consider this method unstable, as the internals may still be changed.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of dimensions for reduction.")
        writeLines("n_hidden -> The number of neurons in the hidden layers, the length specifies the number of layers, 
                 the length must be impair, the middle number must be the same as ndim.")
        writeLines("activation -> The activation functions for the layers, one of tanh, sigmoid, relu, elu")
        writeLines("Everything else will silently be ignored and there will be no activation function for the layer.")
        writeLines("weight_decay -> the coefficient for weight decay, set to 0 if no weight decay desired.")
        writeLines("learning_rate -> The learning rate for gradient descend")
        writeLines("graph -> Optional: A list of bits and pieces that define the autoencoder in tensorflow, see details.")
        writeLines("keras_graph -> Optional: A list of keras layers that define the encoder and decoder, specifying this, will ignore all other topology related variables, see details.")
        writeLines("batchsize -> If NA, all data will be used for training, else only a random subset of size batchsize will be used")
        writeLines("n_steps -> the number of training steps.")
      }
    }
    else if(method == "KPCA"){
      technic_name <- "Kernel PCA"
      dimen <- dimRed::AutoEncoder()
      writeLines('Kernel PCA is a nonlinear extension of PCA using kernel methods.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions, defaults to 2")
        writeLines("kpar -> A list with the parameters for the kernel function, defaults to list(sigma = 0.1)")
        writeLines("kernel -> The kernel function, either as a function or a character vector with the name of the kernel. Defaults to rbfdot")
      }
    }
    else if(method == "PCA_L1"){
      technic_name <- "Regularization PCA L1"
      dimen <- dimRed::PCA_L1()
      writeLines("PCA transforms the data so that the L2 reconstruction error is minimized or the variance of the projected data is maximized.")
      writeLines("This is sensitive to outliers, L1 PCA minimizes the L1 reconstruction error or maximizes the sum of the L1 norm of the projected observations.")
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("center -> logical, should the data be centered, defaults to TRUE.")
        writeLines("scale -> logical, should the data be scaled, defaults to FALSE.")
        writeLines("fun -> character or function, the method to apply, see the pcaL1 package")
      }
    }
    emb <- dimen@fun(dat, dimension_params) 
    dimensional_data <- data.frame(emb@data@data)
    return(dimensional_data)
  }
  
  colnames_df <- colnames(dframe)
  index_classlabel <- which(colnames_df %in% classlabel)
  dframe[,index_classlabel] <- as.factor(dframe[,index_classlabel])
  matrix_df <- as.matrix(dframe[,-index_classlabel])
  dim_tools <- read.csv(reference_dim_reduction)
  class_vector <- dframe[,index_classlabel]
  dimred_mod_list <- list()
  my_formula <- paste0(classlabel, " ~ .")
  
  writeLines("Executing LLE (Locally Linear Embedding)")
  start_time <- Sys.time()
  df_LLE <- dimension_reduction_by_dimred(dframe, my_formula, method="LLE") 
  end_time <- Sys.time()
  writeLines(paste0("================== Execution Time: ", round(round(end_time - start_time,2) / 60,2)," Minutes ======================"))
  writeLines("Executing FR (Fruchtermann Reingold)")
  start_time <- Sys.time()
  df_FR <- dimension_reduction_by_dimred(dframe, my_formula, method="FR") 
  end_time <- Sys.time()
  writeLines(paste0("================== Execution Time: ", round(round(end_time - start_time,2) / 60,2)," Minutes ======================"))
  writeLines("Executing DM (Diffusion Maps)")
  start_time <- Sys.time()
  df_DM <- dimension_reduction_by_dimred(dframe, my_formula, method="DM") 
  end_time <- Sys.time()
  writeLines(paste0("================== Execution Time: ", round(round(end_time - start_time,2) / 60,2)," Minutes ======================"))
  writeLines("Executing PCA with Ridge Regression")
  start_time <- Sys.time()
  df_DRR <- dimension_reduction_by_dimred(dframe, my_formula, method="DRR") 
  end_time <- Sys.time()
  writeLines(paste0("================== Execution Time: ", round(round(end_time - start_time,2) / 60,2)," Minutes ======================"))
  writeLines("Executing NNMF (Non Negative Matrix Factorization")
  start_time <- Sys.time()
  library(dplyr)
  dframe_numeric <- dframe %>% select(where(is.numeric))
  bool_positive <- lapply(dframe_numeric, FUN = function(x) any(x >= 0))
  final_bool <- any(unlist(bool_positive))
  if(final_bool){
    df_NNMF <- dimension_reduction_by_dimred(dframe, my_formula, method="NNMF") 
    end_time <- Sys.time()
    writeLines(paste0("================== Execution Time: ", round(round(end_time - start_time,2) / 60,2)," Minutes ======================"))
  }
  else{
     message("Dataset Contains Negative Number! NNMF Algorithm cannot proceed")
  }
  dimred_mod_list <- c(dimred_mod_list, list(df_LLE), list(df_FR), list(df_DM), list(df_DRR), list(df_NNMF))
  names(dimred_mod_list) <- c("Locally Linear Embedding", "Fruchterman Reingold", 
                              "Diffusion Maps", "PCA with Ridge Regression", 
                              "Non Negative Matrix Factorization")
  for(xdim in 1:nrow(dim_tools)){
    string_eval <- paste0(dim_tools$callname[xdim],"(matrix_df")
    if(dim_tools$argument[xdim] %like% "ndim"){
      string_eval <- paste0(string_eval, ",ndim = 2")
    }
    if(dim_tools$argument[xdim] %like% "label"){
      if(class(dframe[[classlabel]]) == "character" || class(dframe[[classlabel]]) == "factor"){
        string_eval <- paste0(string_eval, ',label = as.character(class_vector)') 
      }
    }
    if(dim_tools$argument[xdim] %like% "response"){
      if(class(dframe[[classlabel]]) == "integer" || class(dframe[[classlabel]]) == "numeric"){
        string_eval <- paste0(string_eval, ',response = as.character(class_vector)')
      }
    }
    if(!(dim_tools$argument[xdim] %like% "data1") && !(dim_tools$argument[xdim] %like% "data2") && 
       !xdim %in% exception_dim_model){
      writeLines(paste0(xdim,") Executing: ", dim_tools$callname[xdim]))
      writeLines(paste0("Algorithm Description: ", dim_tools$description[xdim]))
      string_eval <- paste0(string_eval,")")
      writeLines(string_eval)
      test_eval <- tryCatch({
        if(dim_tools$argument[xdim] %like% "label" && class(dframe[[classlabel]]) == "numeric"){
          message('Skipped this Method since it is not possible for the dataset structure')
          dimred_mod_list <- c(dimred_mod_list, NULL)
        }
        else if(dim_tools$argument[xdim] %like% "response" && class(dframe[[classlabel]]) == "factor"){
          message('Skipped this Method since it is not possible for the dataset structure')
          dimred_mod_list <- c(dimred_mod_list, NULL)
        }
        else{
          start_time <- Sys.time()
          result_model <- eval(parse(text=stringwar_eval))
          dimred_mod_list <- c(dimred_mod_list, list(result_model))
          end_time <- Sys.time()
          writeLines(paste0("================== Execution Time: ", round(round(end_time - start_time,2) / 60,2)," Minutes ======================"))
          names(dimred_mod_list)[length(dimred_mod_list)] <- dim_tools$model_name[xdim]
        }
      },
      error = function(cond){
        writeLines('Error Happens when Execution!')
        message(cond)
        writeLines("")
        dimred_mod_list <- c(dimred_mod_list, NULL)
      })
    }
    else{
      message(paste0(xdim,') Method is stuck/too long to execute/Is not for Singular Datasets, skipped!'))
      dimred_mod_list <- c(dimred_mod_list, NULL)
    }
  }
  return(dimred_mod_list)
}

dim_2D_plotting <- function(dimred_list, label, limit=-1, legend_pos="topleft"){
  if(limit != -1){
    for(b in 1:limit){
      if(class(dimred_list[[b]]) == "Rdimtools" || class(dimred_list[[b]]) == "list"){
        x11()
        main_label <- paste0(names(dimred_list)[b], " 2D Projection")
        x_label <- paste0(names(dimred_list)[b], "(Dim 1)")
        y_label <- paste0(names(dimred_list)[b], "(Dim 2)")
        plot(x=dimred_list[[b]]$Y[,1], y=dimred_list[[b]]$Y[,2],
             xlab=x_label, ylab=y_label, main=main_label, col=label)
        legend(x=legend_pos, legend = unique(label), col=unique(label), pch=1)
      }
      else{
        x11()
        main_label <- paste0(names(dimred_list)[b], " 2D Projection")
        plot(x=dimred_list[[b]], main=main_label, col=label)
        legend(x=legend_pos, legend = unique(label), col=unique(label), pch=1)
      }
    }
  }
  else{
    for(b in 1:length(dimred_list)){
      if(class(dimred_list[[b]]) == "Rdimtools"){
        x11()
        main_label <- paste0(names(dimred_list)[b], " 2D Projection")
        x_label <- paste0(names(dimred_list)[b], "(Dim 1)")
        y_label <- paste0(names(dimred_list)[b], "(Dim 2)")
        plot(x=dimred_list[[b]]$Y[,1], y=dimred_list[[b]]$Y[,2], 
             xlab=x_label, ylab=y_label, main=main_label, col=label)
        legend(x=legend_pos, legend = unique(label), col=unique(label), pch=1)
      }
      else{
        x11()
        main_label <- paste0(names(dimred_list)[b], " 2D Projection")
        plot(x=dimred_list[[b]], main=main_label, col=label)
        legend(x=legend_pos, legend = unique(label), col=unique(label), pch=1)
      }
    }
  }
}

#-------------------------------------------------------------------------------------------------------------------------
########################### 15) Gravity Model Analysis ###################################################################
#-------------------------------------------------------------------------------------------------------------------------

gravity_model <- function(df, model_type=1, dependent_var, distance_var, 
                          addin_regressor_var=c(), 
                          original_code, destination_code,
                          original_income, destination_income,
                          origin_filter, destination_filter,
                          multiway_var_covar_tetrads = TRUE,
                          robust_gamma_maximum_likelihood = TRUE){
  library(gravity)
  writeLines("============================== Gravity Model Type Explanation ============================================")
  writeLines("1) ddm estimates gravity models via double demeaning the left hand side and right hand side of the gravity equation")
  writeLines("2) bvu estimates gravity models via Bonus vetus OLS with simple averages")
  writeLines("3) bvw estimates gravity models via Bonus vetus OLS with income-weights")
  writeLines("4) ppml estimates gravity models in their multiplicative form via Poisson Pseudo Maximum Likelihood.")
  writeLines("5) gpml estimates gravity models in their multiplicative form via Gamma Pseudo Maximum Likelihood.")
  writeLines("6) nbpml estimates gravity models in their multiplicative form via Negative Binomial Pseudo Maximum Likelihood.")
  writeLines("7) tetrads estimates gravity models by taking the ratio of the ratio of flows.")
  writeLines("8) Use All Available type of Gravity Model")
  writeLines("==========================================================================================================")
  if(model_type==1){
    writeLines("============= Make a Double Demeaning Gravity Model ================")
    
    fit <- ddm(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      code_origin = original_code,
      code_destination = destination_code,
      data = gravity_no_zeros
    )
    
    print(summary(fit))
    
    return(fit)
  }
  else if(model_type==2){
    writeLines("============= Make a Bonus Vetus Gravity Model ================")
    
    fit2 <- bvu(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      income_origin = original_income,
      income_destination = destination_income,
      code_origin = original_code,
      code_destination = destination_code,
      data = gravity_no_zeros
    )
    
    print(summary(fit2))
    
    return(fit2)
  }
  else if(model_type==3){
    writeLines("============= Make a Bonus Vetus with Weighting Attribute Gravity Model ================")
    
    fit3 <- bvw(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      income_origin = original_income,
      income_destination = destination_income,
      code_origin = original_code,
      code_destination = destination_code,
      data = gravity_no_zeros
    )
    
    print(summary(fit3))
    
    return(fit3)
  }
  else if(model_type==4){
    writeLines("============= Make a Poisson Pseudo Maximum Likelihood Gravity Model ================")
    
    fit4 <- ppml(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      data = gravity_no_zeros
    )
    
    print(summary(fit4))
    
    return(fit4)
  }
  else if(model_type==5){
    writeLines("============= Make a Gamma Pseudo Maximum Likelihood Gravity Model ================")
    
    fit5 <- gpml(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      robust = TRUE,
      data = gravity_no_zeros
    )
    
    print(fit5)
    print(summary(fit5))
    
    return(fit5)
  }
  else if(model_type==6){
    writeLines("============= Make a Negative Binomial Pseudo Maximum Likelihood Gravity Model ================")
    
    fit6 <- nbpml(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      robust = robust_gamma_maximum_likelihood,
      data = gravity_no_zeros
    )
    
    print(fit6)
    print(summary(fit6))
    
    return(fit6)
  }
  else if(model_type==7){
    writeLines("============= Make a Tetrads Gravity Model ================")
    
    fit7 <- tetrads(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      code_origin = original_code,
      code_destination = destination_code,
      multiway = multiway_var_covar_tetrads,
      filter_origin = "CHN",
      filter_destination = "USA",
      data = gravity_no_zeros
    )
    
    print(fit7)
    print(summary(fit7))
    
    return(fit7)
  }
  else if(model_type==8){
    writeLines("Make all kinds of Gravity Model")
    writeLines("=============== 1) Double Deameaning Gravity Model =======================")
    fit <- ddm(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      code_origin = original_code,
      code_destination = destination_code,
      data = gravity_no_zeros
    )
    
    print(summary(fit))
    
    writeLines("=============== 2) Bonus Vetus Gravity Model =======================")
    fit2 <- bvu(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      income_origin = original_income,
      income_destination = destination_income,
      code_origin = original_code,
      code_destination = destination_code,
      data = gravity_no_zeros
    )
    
    print(summary(fit2))
    
    writeLines("============ 3) Bonus Vetus with Weighing Attribute Gravity Model ==================")
    fit3 <- bvw(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      income_origin = original_income,
      income_destination = destination_income,
      code_origin = original_code,
      code_destination = destination_code,
      data = gravity_no_zeros
    )
    
    print(summary(fit3))
    
    writeLines("============ 4) Pseudo Poisson Maximum Likelihood Gravity Model ==================")
    fit4 <- ppml(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      data = gravity_no_zeros
    )
    
    print(summary(fit4))
    
    writeLines("============ 5) Gamma Poisson Maximum Likelihood Gravity Model ==================")
    fit5 <- gpml(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      robust = TRUE,
      data = gravity_no_zeros
    )
    
    print(fit5)
    print(summary(fit5))
    
    writeLines("============ 6) Negative Binomial Maximum Likelihood Gravity Model ==================")
    fit6 <- nbpml(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      robust = robust_gamma_maximum_likelihood,
      data = gravity_no_zeros
    )
    
    print(fit6)
    print(summary(fit6))
    
    writeLines("========================== 7) Tetrads Gravity Model =============================")
    fit7 <- tetrads(
      dependent_variable = dependent_var,
      distance = distance_var,
      additional_regressors = addin_regressor_var,
      code_origin = original_code,
      code_destination = destination_code,
      multiway = multiway_var_covar_tetrads,
      filter_origin = "CHN",
      filter_destination = "USA",
      data = gravity_no_zeros
    )
    
    print(fit7)
    print(summary(fit7))
    
    return(list(fit, fit2, fit3, fit4, fit5, fit6, fit7))
  }
}

#example
grav_mod1 <- gravity_model(gravity_no_zeros, 1, dependent_var="flow", distance_var = "distw", 
                           addin_regressor_var=c("rta", "comcur", "contig"), 
                           original_code = "iso_o", destination_code = "iso_d",
                           original_income = "gdp_o", destination_income = "gdp_d",
                           origin_filter = "AFG", destination_filter = "ARG")

grav_mod_all <- gravity_model(gravity_no_zeros, 8, dependent_var="flow", distance_var = "distw", 
                           addin_regressor_var=c("rta", "comcur", "contig"), 
                           original_code = "iso_o", destination_code = "iso_d",
                           original_income = "gdp_o", destination_income = "gdp_d",
                           origin_filter = "AFG", destination_filter = "ARG")

#-------------------------------------------------------------------------------------------------------------------------
########################### 16) Local Outlier Factor Analysis ############################################################
#-------------------------------------------------------------------------------------------------------------------------

local_outlier_factor_neighbor_comparison <- function(df, neighbor=c(2:10), n_outlier=5){
  library(DMwR)
  library(Rlof)
  outlier.scores <- lof(df, k=neighbor)
  outlier.scores <- data.frame(outlier.scores)
  for(x_neighbor in 1:length(neighbor)){
    sorted_outlier <- sort(outlier.scores[,x_neighbor])[1:n_outlier]
    outliers_idx <- order(outlier.scores[,x_neighbor], decreasing=T)[1:n_outlier]
    writeLines(paste0("Sorted Outlier Scores for K = ",neighbor[x_neighbor]))
    print(sorted_outlier)
    writeLines("Outlier Index is in: ")
    print(outliers_idx)
  }
}

local_outlier_factor <- function(df, neighbor=5, n_outlier=5, remove_outlier=FALSE){
  library(DMwR)
  library(Rlof)
  outlier.scores <- lofactor(df, k=neighbor)
  plot(density(outlier.scores))
  outliers <- order(outlier.scores, decreasing=T)[1:n_outlier]
  print(outliers)
  n <- nrow(iris2)
  labels <- 1:n
  labels[-outliers] <- "."
  biplot(prcomp(df), cex=.8, xlabs=labels)
  pch <- rep(".", n)
  pch[outliers] <- "+"
  col <- rep("black", n)
  col[outliers] <- "red"
  pairs(df, pch=pch, col=col)
}

#-------------------------------------------------------------------------------------------------------------------------
########################### 17) Fuzzy Rule Based System Machine Learning  ################################################
#-------------------------------------------------------------------------------------------------------------------------

#AVAILABLE IMPLIFICATION FUNCTION
#DIENES_RESHER
#LUKASIEWICHZ
#ZADEH
#GOGUEN
#GODEL
#SHARP
#MIZUMOTO
#DUBOIS_PRADE
#MIN

fuzzy_supervise_space_partition <- function(df, sample_proportion=1, 
                                            train_proportion=0.5,dependent_col,
                                            method="WM", num_label=5, iteration=50,
                                            step_size=0.05, norm_type=1, 
                                            fuzzifier_type=5 , defuzzifier_type=5, 
                                            implification_function="ZADEH"){
  writeLines("================================= These Function contains algorithm of: =======================================")
  writeLines("1) Wang and Mendel's technique (WM): It is used to solve regression tasks. See WM.")
  writeLines("2) Chi's technique (FRBCS.CHI): It is used to solve classification tasks. See FRBCS.CHI.")
  writeLines("3) Ishibuchi's technique using weight factor (FRBCS.W): It is used to solve classification tasks. See FRBCS.W.")
  
  library(frbs)
  writeLines("===========================================================")
  writeLines(paste0("Building Fuzzy Model ", method))
  writeLines("===========================================================")
  if(norm_type==1){
    writeLines("Using Tnorm with 'MIN' Parameter")
    writeLines("Using Snorm with 'MAX' Parameter")
  }
  else if(norm_type==2){
    writeLines("Using Tnorm with 'HAMACHER' Parameter")
    writeLines("Using Snorm with 'HAMACHER' Parameter")
  }
  else if(norm_type==3){
    writeLines("Using Tnorm with 'YAGER' Parameter")
    writeLines("Using Snorm with 'YAGER' Parameter")
  }
  else if(norm_type==4){
    writeLines("Using Tnorm with 'PRODUCT' Parameter")
    writeLines("Using Snorm with 'SUM' Parameter")
  }
  else if(norm_type==5){
    writeLines("Using Tnorm with 'BOUNDED' Parameter")
    writeLines("Using Snorm with 'BOUNDED' Parameter")
  }
  
  if(method=="WM"){
    if(defuzzier_type==1){
      writeLines("Using Defuzzifier with 'WAM' (Weighted Average Method) Parameter")
    }
    else if(defuzzier_type==2){
      writeLines("Using Defuzzifier with 'FIRST.MAX' (First Maxima) Parameter")
    }
    else if(defuzzier_type==3){
      writeLines("Using Defuzzifier with 'LAST.MAX' (Lastr Maxima) Parameter")
    }
    else if(defuzzier_type==4){
      writeLines("Using Defuzzifier with 'MEAN.MAX' (Mean Maxima) Parameter")
    }
    else if(defuzzier_type==5){
      writeLines("Using Defuzzifier with 'COG' (Center of Gravity) Parameter")
    }
  }
  else if(method=="FRBCS.W" || method=="FRBCS.CHI"){
    if(fuzzifier_type==1){
      writeLines("Using Fuzzifier with 'TRIANGLE' (a,b,c) Parameter")
    }
    else if(fuzzifier_type==2){
      writeLines("Using Fuzzifier with 'Left TRAPEZOID' (a,b,c,d) Parameter")
    }
    else if(fuzzifier_type==3){
      writeLines("Using Fuzzifier with 'Right TRAPEZOID' (a,b,c,d) Parameter")
    }
    else if(fuzzifier_type==4){
      writeLines("Using Fuzzifier with 'Middle TRAPEZOID' (a,b,c,d) Parameter")
    }
    else if(fuzzifier_type==5){
      writeLines("Using Fuzzifier with 'GAUSSIAN' (mean, variance) Parameter")
    }
    else if(fuzzifier_type==6){
      writeLines("Using Fuzzifier with 'SIGMOID' (y dan c) Parameter")
    }
    else if(fuzzifier_type==7){
      writeLines("Using Fuzzifier with 'BELL' (a,b,c) Parameter")
    }
  }
  
  if(sample_proportion > 0 && sample_proportion <= 1){
    sample_num <- round(nrow(df) * sample_proportion)
    sample_index <- sample(sample_num, sample_num)
    df <- df[sample_index, ]
    
    df_independent <- df[,-which(colnames(df)==dependent_col)]
    df_dependent <- matrix(df[,which(colnames(df)==dependent_col)], ncol=1)
    train_num <- round(nrow(df) * train_proportion)
    train_index <- sample(train_num, train_num)
    
    train_data <- df[train_index,-which(colnames(df)==dependent_col)]
    train_label <- df[train_index, which(colnames(df)==dependent_col)]
    test_data <- df[-train_index,-which(colnames(df)==dependent_col)]
    test_label <- df[-train_index, which(colnames(df)==dependent_col)]
    
    range_data <- c()
    control <- list()
    
    for(range_fuzzy in 1:ncol(df)){
      range_data <- c(range_data, min(df[,range_fuzzy]), max(df[,range_fuzzy]))
    }
    range_data<-matrix(range_data, nrow=2)
    print(range_data)
    
    if(method=="WM"){
      control <- list(num.labels = num_label, type.mf = fuzzifier_type, type.defuz = defuzzifier_type, type.tnorm = norm_type,
                      type.snorm = norm_type, type.implication.func = implification_function, name="WM Model")
      ## generate fuzzy model
      object <- frbs.learn(df, range_data, method, control)
      res.fit <- predict(object, df_independent)
      res.test <- predict(object, test_data)
      y.pred <- res.test
      y.real <- test_label
      
      bench <- cbind(y.pred, y.real)
      colnames(bench) <- c("pred. val.", "real. val.")
      
      print("Comparison Wang Mendel Prediction Vs Real Value")
      print(bench)
      
      residuals <- (y.real - y.pred)
      MSE <- mean(residuals^2)
      RMSE <- sqrt(mean(residuals^2))
      SMAPE <- mean(abs(residuals)/(abs(y.real) + abs(y.pred))/2)*100
      
      err <- c(MSE, RMSE, SMAPE)
      names(err) <- c("MSE", "RMSE", "SMAPE")
      
      print("Error Measurement: ")
      print(err) 
      
      op <- par(mfrow = c(2, 1))
      x1 <- seq(from = 1, to = nrow(res.fit))
      result.fit <- cbind(df_dependent, res.fit)
      plot(x1, result.fit[, 1], col="red", 
           main = "Dataset: Fitting phase (the training data(red) Vs Sim. result(blue))", 
           type = "l", ylab = dependent_col)
      lines(x1, result.fit[, 2], col="blue")
      
      result.test <- data.frame(cbind(test_label, res.test))
      colnames(result.test) <- c("Test Actual", "Test Predictions")
      x2 <- seq(from = 1, to = nrow(result.test))
      plot(x2, result.test[, 1], col="red", 
           main = "Dataset: Predicting phase (the Real Data(red) Vs Sim. result(blue))", 
           type = "l", ylab = dependent_col)
      lines(x2, result.test[, 2], col="blue", type = "l")
      par(op)
      return(list(object, y.real, y.pred))
    }
    else if(method=="FRBCS.W" || method=="FRBCS.CHI"){
      control <- list(num.labels = num_label, type.mf = fuzzifier_type, type.tnorm = norm_type, 
                      type.snorm = norm_type, type.implication.func = implification_function) 
      object <- frbs.learn(df, range_data, method, control)
      res.test <- predict(object, test_data)
      err <- 100*sum(test_label!=res.test)/nrow(test_label)
      print("The result: ")
      result.test <- data.frame(cbind(test_label, res.test))
      colnames(result.test) <- c("Test Actual", "Test Predictions")
      print(result.test)
      writeLines("FRBCS.W: percentage Error on Dataset")
      print(err)
      return(list(object, result.test, err))
    }
  }
  else if(sample_proportion == 0){
    writeLines("Please Use Sample greater than 0!")
  }
}

fuzzy_supervise_neural_network <- function(df, sample_proportion=1, 
                                           train_proportion=0.5,dependent_col,
                                           method="ANFIS", num_label=5, iteration=50,
                                           step_size=0.05, norm_type=1, defuzzifier_type=5, 
                                           implification_function="ZADEH"){
  writeLines("===================================== These Function contains algorithm of: ====================================")
  writeLines("1) The Adaptive-Network-based Fuzzy Inference System (ANFIS): It is used to solve regression tasks. See ANFIS.")
  writeLines("2) The Hybrid Neural Fuzzy Inference System (HYFIS): It is used to solve regression tasks. See HyFIS.")
  library(frbs)
  writeLines("===========================================================")
  writeLines(paste0("Building Fuzzy Model ", method))
  writeLines("===========================================================")
  if(norm_type==1){
    writeLines("Using Tnorm with 'MIN' Parameter")
    writeLines("Using Snorm with 'MAX' Parameter")
  }
  else if(norm_type==2){
    writeLines("Using Tnorm with 'HAMACHER' Parameter")
    writeLines("Using Snorm with 'HAMACHER' Parameter")
  }
  else if(norm_type==3){
    writeLines("Using Tnorm with 'YAGER' Parameter")
    writeLines("Using Snorm with 'YAGER' Parameter")
  }
  else if(norm_type==4){
    writeLines("Using Tnorm with 'PRODUCT' Parameter")
    writeLines("Using Snorm with 'SUM' Parameter")
  }
  else if(norm_type==5){
    writeLines("Using Tnorm with 'BOUNDED' Parameter")
    writeLines("Using Snorm with 'BOUNDED' Parameter")
  }
  
  if(method=="HyFIS"){
    if(defuzzier_type==1){
      writeLines("Using Defuzzifier with 'WAM' (Weighted Average Method) Parameter")
    }
    else if(defuzzier_type==2){
      writeLines("Using Defuzzifier with 'FIRST.MAX' (First Maxima) Parameter")
    }
    else if(defuzzier_type==3){
      writeLines("Using Defuzzifier with 'LAST.MAX' (Lastr Maxima) Parameter")
    }
    else if(defuzzier_type==4){
      writeLines("Using Defuzzifier with 'MEAN.MAX' (Mean Maxima) Parameter")
    }
    else if(defuzzier_type==5){
      writeLines("Using Defuzzifier with 'COG' (Center of Gravity) Parameter")
    }
  }
  
  if(sample_proportion > 0 && sample_proportion <= 1){
    sample_num <- round(nrow(df) * sample_proportion)
    sample_index <- sample(sample_num, sample_num)
    df <- df[sample_index, ]
    
    df_independent <- df[,-which(colnames(df)==dependent_col)]
    df_dependent <- matrix(df[,which(colnames(df)==dependent_col)], ncol=1)
    train_num <- round(nrow(df) * train_proportion)
    train_index <- sample(train_num, train_num)
    
    train_data <- df[train_index,-which(colnames(df)==dependent_col)]
    train_label <- df[train_index, which(colnames(df)==dependent_col)]
    test_data <- df[-train_index,-which(colnames(df)==dependent_col)]
    test_label <- df[-train_index, which(colnames(df)==dependent_col)]
    
    range_data <- c()
    control <- list()
    
    for(range_fuzzy in 1:ncol(df)){
      range_data <- c(range_data, min(df[,range_fuzzy]), max(df[,range_fuzzy]))
    }
    range_data<-matrix(range_data, nrow=2)
    print(range_data)
    
    if(method=="ANFIS"){
      control <- list(num.labels = num_label, max.iter = iteration, 
                      step.size = step_size, type.tnorm = norm_type, 
                      type.snorm = norm_type, type.implication.func = implification_function, 
                      name = "ANFIS Model")
    }
    else if(method=="HyFIS"){
      control <- list(num.labels = num_label, max.iter = iteration, 
                      step.size = step_size, type.tnorm = norm_type, 
                      type.snorm = norm_type, type.defuz = defuzzifier_type, 
                      type.implication.func = implification_function, 
                      name = "HyFIS Model")
    }
    
    ## generate fuzzy model
    object <- frbs.learn(df, range_data, method, control)
    res.fit <- predict(object, df_independent)
    res.test <- predict(object, test_data)
    y.pred <- res.test
    y.real <- test_label
    
    bench <- cbind(y.pred, y.real)
    colnames(bench) <- c("pred. val.", "real. val.")
    
    print("Comparison ANFIS Vs Real Value on Gas Furnace Data Set")
    print(bench)
    
    residuals <- (y.real - y.pred)
    MSE <- mean(residuals^2)
    RMSE <- sqrt(mean(residuals^2))
    SMAPE <- mean(abs(residuals)/(abs(y.real) + abs(y.pred))/2)*100
    
    err <- c(MSE, RMSE, SMAPE)
    names(err) <- c("MSE", "RMSE", "SMAPE")
    
    print("Error Measurement: ")
    print(err) 
    
    op <- par(mfrow = c(2, 1))
    x1 <- seq(from = 1, to = nrow(res.fit))
    result.fit <- cbind(df_dependent, res.fit)
    plot(x1, result.fit[, 1], col="red", 
         main = "Dataset: Fitting phase (the training data(red) Vs Sim. result(blue))", 
         type = "l", ylab = dependent_col)
    lines(x1, result.fit[, 2], col="blue")
    
    result.test <- data.frame(cbind(test_label, res.test))
    colnames(result.test) <- c("Test Actual", "Test Predictions")
    x2 <- seq(from = 1, to = nrow(result.test))
    plot(x2, result.test[, 1], col="red", 
         main = "Dataset: Predicting phase (the Real Data(red) Vs Sim. result(blue))", 
         type = "l", ylab = dependent_col)
    lines(x2, result.test[, 2], col="blue", type = "l")
    par(op)
    return(list(object, y.real, y.pred))
  }
  else if(sample_proportion == 0){
    writeLines("Please Use Sample greater than 0!")
  }
}

fuzzy_supervise_clustering <- function(df, sample_proportion=1, 
                                       train_proportion=0.5,dependent_col,
                                       method="SBC", iteration=50,
                                       step_size=0.05, radius_threshold=0.5, 
                                       upperthreshold=0.75, lowerthreshold=0.25,
                                       triangular_width=2){
  writeLines("===================================== This Function contains algorithm of =========================================")
  writeLines("1) The subtractive clustering and fuzzy c-means (SBC): It is used to solve regression tasks. See SBC.")
  writeLines("2) The dynamic evolving neural-fuzzy inference system (DENFIS): It is used to solve regression tasks. See DENFIS.")
  
  library(frbs)
  writeLines("===========================================================")
  writeLines(paste0("Building Fuzzy Model ", method))
  writeLines("===========================================================")
  if(sample_proportion > 0 && sample_proportion <= 1){
    sample_num <- round(nrow(df) * sample_proportion)
    sample_index <- sample(sample_num, sample_num)
    df <- df[sample_index, ]
    
    df_independent <- df[,-which(colnames(df)==dependent_col)]
    df_dependent <- matrix(df[,which(colnames(df)==dependent_col)], ncol=1)
    train_num <- round(nrow(df) * train_proportion)
    train_index <- sample(train_num, train_num)
    
    train_data <- df[train_index,-which(colnames(df)==dependent_col)]
    train_label <- df[train_index, which(colnames(df)==dependent_col)]
    test_data <- df[-train_index,-which(colnames(df)==dependent_col)]
    test_label <- df[-train_index, which(colnames(df)==dependent_col)]
    
    range_data <- c()
    control <- list()
    
    for(range_fuzzy in 1:ncol(df)){
      range_data <- c(range_data, min(df[,range_fuzzy]), max(df[,range_fuzzy]))
    }
    range_data<-matrix(range_data, nrow=2)
    print(range_data)
    
    if(method=="SBC"){
      control <- list(r.a = radius_threshold, eps.high = upperthreshold, 
                      eps.low = lowerthreshold, name = "SBC Model")
    }
    else if(method=="DENFIS"){
      control <- list(Dthr = radius_threshold, max.iter = iteration, 
                      step.size = step_size, d = triangular_width, name = "DENFIS Model")
    }
    
    ## generate fuzzy model
    object <- frbs.learn(df, range_data, method, control)
    res.fit <- predict(object, df_independent)
    res.test <- predict(object, test_data)
    y.pred <- res.test
    y.real <- test_label
    
    bench <- cbind(y.pred, y.real)
    colnames(bench) <- c("pred. val.", "real. val.")
    
    print("Comparison ANFIS Vs Real Value on Gas Furnace Data Set")
    print(bench)
    
    residuals <- (y.real - y.pred)
    MSE <- mean(residuals^2)
    RMSE <- sqrt(mean(residuals^2))
    SMAPE <- mean(abs(residuals)/(abs(y.real) + abs(y.pred))/2)*100
    
    err <- c(MSE, RMSE, SMAPE)
    names(err) <- c("MSE", "RMSE", "SMAPE")
    
    print("Error Measurement: ")
    print(err) 
    
    op <- par(mfrow = c(2, 1))
    x1 <- seq(from = 1, to = nrow(res.fit))
    result.fit <- cbind(df_dependent, res.fit)
    plot(x1, result.fit[, 1], col="red", 
         main = "Dataset: Fitting phase (the training data(red) Vs Sim. result(blue))", 
         type = "l", ylab = dependent_col)
    lines(x1, result.fit[, 2], col="blue")
    
    result.test <- data.frame(cbind(test_label, res.test))
    colnames(result.test) <- c("Test Actual", "Test Predictions")
    x2 <- seq(from = 1, to = nrow(result.test))
    plot(x2, result.test[, 1], col="red", 
         main = "Dataset: Predicting phase (the Real Data(red) Vs Sim. result(blue))", 
         type = "l", ylab = dependent_col)
    lines(x2, result.test[, 2], col="blue", type = "l")
    par(op)
    return(list(object, y.real, y.pred))
  }
  else if(sample_proportion == 0){
    writeLines("Please Use Sample greater than 0!")
  }
}

fuzzy_supervise_genetic_algorithm <- function(df, sample_proportion=1, 
                                              train_proportion=0.5,dependent_col,
                                              method="GFS.Thrift", num_label=5, iteration=50,
                                              population_size=15, crossover_probability=0.9, 
                                              mutant_probability=0.3, max_genetic_generation=50, 
                                              epsilon=0.85, max_tuning=300, 
                                              dcare_probability=0.5, gccl_probability=0.5, 
                                              max_rules=50, num_class=3, k_lower=0, k_upper=1,
                                              norm_type=1, fuzzifier_type=1, rule_selection=TRUE,
                                              mode_tuning="LOCAL",
                                              defuzzifier_type=1, implification_function="ZADEH"){
  
  writeLines("=============================================== This Function contains algorithm of: ===================================================")
  writeLines("1) Genetic Fuzzy with Thrift's method (GFS.THRIFT): It is used to solve regression tasks. See GFS.Thrift.")
  writeLines("2) The Genetic fuzzy systems for fuzzy rule learning based on the MOGUL methodology (GFS.FR.MOGUL): It is used to solve regression tasks. See GFS.FR.MOGUL.")
  writeLines("3) The Ishibuchi's method based on genetic cooperative-competitive learning (GFS.GCCL): It is used to solve classification tasks. See GFS.GCCL.")
  writeLines("4) The Ishibuchi's method based on hybridization of genetic cooperative-competitive learning (GCCL) and Pittsburgh (FH.GBML): It is used to solve classification tasks. See FH.GBML.")
  writeLines("5) The structural learning algorithm on vague environtment (SLAVE): It is used to solve classification tasks. See SLAVE.")
  writeLines("6) The genetic for lateral tuning and rule selection of linguistic fuzzy system (GFS.LT.RS): It is used to solve regression tasks. See GFS.LT.RS.")
  
  library(frbs)
  writeLines("===========================================================")
  writeLines(paste0("Building Fuzzy Model ", method))
  writeLines("===========================================================")
  writeLines("Basic Parameters for Genetic Algorithms")
  writeLines(paste0("Population Size = ", population_size))
  writeLines(paste0("Crossover Percentage = ", crossover_probability))
  writeLines(paste0("Mutant Percentage = ", mutant_probability))
  writeLines(paste0("Max Generation = ", max_genetic_generation))
  
  if(method=="GFS.Thrift" || method=="GFS.LT.RS"){
    writeLines("With GFS Thrift method used, also:")
    if(norm_type==1){
      writeLines("Using Tnorm with 'MIN' Parameter")
      writeLines("Using Snorm with 'MAX' Parameter")
    }
    else if(norm_type==2){
      writeLines("Using Tnorm with 'HAMACHER' Parameter")
      writeLines("Using Snorm with 'HAMACHER' Parameter")
    }
    else if(norm_type==3){
      writeLines("Using Tnorm with 'YAGER' Parameter")
      writeLines("Using Snorm with 'YAGER' Parameter")
    }
    else if(norm_type==4){
      writeLines("Using Tnorm with 'PRODUCT' Parameter")
      writeLines("Using Snorm with 'SUM' Parameter")
    }
    else if(norm_type==5){
      writeLines("Using Tnorm with 'BOUNDED' Parameter")
      writeLines("Using Snorm with 'BOUNDED' Parameter")
    }
    if(fuzzifier_type==1){
      writeLines("Using Fuzzifier with 'TRIANGLE' (a,b,c) Parameter")
    }
    else if(fuzzifier_type==2){
      writeLines("Using Fuzzifier with 'Left TRAPEZOID' (a,b,c,d) Parameter")
    }
    else if(fuzzifier_type==3){
      writeLines("Using Fuzzifier with 'Right TRAPEZOID' (a,b,c,d) Parameter")
    }
    else if(fuzzifier_type==4){
      writeLines("Using Fuzzifier with 'Middle TRAPEZOID' (a,b,c,d) Parameter")
    }
    else if(fuzzifier_type==5){
      writeLines("Using Fuzzifier with 'GAUSSIAN' (mean, variance) Parameter")
    }
    else if(fuzzifier_type==6){
      writeLines("Using Fuzzifier with 'SIGMOID' (y dan c) Parameter")
    }
    else if(fuzzifier_type==7){
      writeLines("Using Fuzzifier with 'BELL' (a,b,c) Parameter")
    }
    
    if(defuzzier_type==1){
      writeLines("Using Defuzzifier with 'WAM' (Weighted Average Method) Parameter")
    }
    else if(defuzzier_type==2){
      writeLines("Using Defuzzifier with 'FIRST.MAX' (First Maxima) Parameter")
    }
    else if(defuzzier_type==3){
      writeLines("Using Defuzzifier with 'LAST.MAX' (Lastr Maxima) Parameter")
    }
    else if(defuzzier_type==4){
      writeLines("Using Defuzzifier with 'MEAN.MAX' (Mean Maxima) Parameter")
    }
    else if(defuzzier_type==5){
      writeLines("Using Defuzzifier with 'COG' (Center of Gravity) Parameter")
    }
  }
  if(method=="GFS.GCCL" || method=="FH.GMBL" || method=="SLAVE"){
    writeLines("Using Num Classes Parameter")
    writeLines(paste0("Classes Used: ", num_class))
  }
  if(method=="FH.GBML"){
    writeLines("Using Special Parameter for FH.GBML Method:")
    writeLines(paste0("Maximum Number of Rules Used: ", max_rules))
    writeLines(paste0("Probability of Don't Care Attributes: ", dcare_probability))
    writeLines(paste0("Probability of GCCL Attributes: ", gccl_probability))
  }
  if(method=="GFS.FR.MOGUL" || method=="SLAVE"){
    writeLines("Using Special Parameter for either GFS.FR.MOGUL or SLAVE Method:")
    writeLines(paste0("Epsilon = ", epsilon))
  }
  if(method=="SLAVE"){
    writeLines("Using Special Parameter for SLAVE Method:")
    writeLines(paste0("K Lower = ", k_lower))
    writeLines(paste0("K Upper = ", k_upper))
  }
  if(method=="GFS.LT.RS"){
    writeLines("Using Special Parameter for either GFS.LT.RS Method:")
    writeLines(paste0("Rule Selection = ", rule_selection))
    writeLines(paste0("Mode Tuning = ", mode_tuning))
  }
  
  if(sample_proportion > 0 && sample_proportion <= 1){
    sample_num <- round(nrow(df) * sample_proportion)
    sample_index <- sample(sample_num, sample_num)
    df <- df[sample_index, ]
    
    df_independent <- df[,-which(colnames(df)==dependent_col)]
    df_dependent <- matrix(df[,which(colnames(df)==dependent_col)], ncol=1)
    train_num <- round(nrow(df) * train_proportion)
    train_index <- sample(train_num, train_num)
    
    train_data <- df[train_index,-which(colnames(df)==dependent_col)]
    train_label <- df[train_index, which(colnames(df)==dependent_col)]
    test_data <- df[-train_index,-which(colnames(df)==dependent_col)]
    test_label <- df[-train_index, which(colnames(df)==dependent_col)]
    
    range_data <- c()
    control <- list()
    
    for(range_fuzzy in 1:ncol(df)){
      range_data <- c(range_data, min(df[,range_fuzzy]), max(df[,range_fuzzy]))
    }
    range_data<-matrix(range_data, nrow=2)
    print(range_data)
    
    if(method=="GFS.Thrift"){
      control <- list(popu.size = population_size, num.labels = num_label, 
                      persen_cross = crossover_probability, 
                      max.gen = max_genetic_generation, persen_mutant = mutant_probability, 
                      type.defuz = defuzzifier_type, type.tnorm = norm_type, 
                      type.snorm = norm_type, type.mf = fuzzifier_type, 
                      type.implication.func = implification_function, name="GFS Thrift Model") 
    }
    else if(method=="GFS.FR.MOGUL"){
      control <- list(persen_cross = crossover_probability, max.iter = iteration, 
                      max.gen = max_genetic_generation, 
                      max.tune = max_tuning, persen_mutant = mutant_probability, 
                      epsilon = epsilon, 
                      name="GFS FR Mogul Model")
    }
    else if(method=="GFS.GCCL"){
      control <- list(popu.size = population_size, num.class = num_class, 
                      num.labels = num_label, persen_cross = crossover_probability, 
                      max.gen = max_genetic_generation, persen_mutant = mutant_probability,
                      name="GFS GCCL Model") 
    }
    else if(method=="FH.GBML"){
      control <- list(popu.size = population_size, max.num.rule = max_rules, 
                      num.class = num_class, persen_cross = crossover_probability, 
                      max.gen = max_genetic_generation, persen_mutant = mutant_probability, 
                      p.dcare = dcare_probability, p.gccl = gccl_probability, 
                      name="FH GBML Model") 
    }
    else if(method=="SLAVE"){
      control <- list(num.class = num_class, num.labels = num_label, 
                      persen_cross = crossover_probability, 
                      max.iter = iteration, max.gen = max_genetic_generation, 
                      persen_mutant = mutant_probability, k.lower = k_lower, k.upper = k_upper, 
                      epsilon = epsilon, name="SLAVE Model") 
    }
    else if(method=="GFS.LT.RS"){
      control <- list(popu.size = population_size, num.labels = num_label, 
                      persen_mutant = mutant_probability,
                      max.gen = max_genetic_generation, mode.tuning = mode_tuning, 
                      type.tnorm = norm_type, type.snorm = norm_type, 
                      type.implication.func = implification_function, 
                      type.defuz = defuzzifier_type, 
                      rule.selection = rule_selection, name="GFS LT RS Model")
    }
    
    if(method=="GFS.Thrift" || method=="GFS.FR.MOGUL" || method=="GFS.LT.RS"){
      object <- frbs.learn(df, range_data, method, control)
      res.fit <- predict(object, df_independent)
      res.test <- predict(object, test_data)
      y.pred <- res.test
      y.real <- test_label
      
      bench <- cbind(y.pred, y.real)
      colnames(bench) <- c("pred. val.", "real. val.")
      
      print("Comparison Model Prediction Vs Real Value")
      print(bench)
      
      residuals <- (y.real - y.pred)
      MSE <- mean(residuals^2)
      RMSE <- sqrt(mean(residuals^2))
      SMAPE <- mean(abs(residuals)/(abs(y.real) + abs(y.pred))/2)*100
      
      err <- c(MSE, RMSE, SMAPE)
      names(err) <- c("MSE", "RMSE", "SMAPE")
      
      print("Error Measurement: ")
      print(err) 
      
      op <- par(mfrow = c(2, 1))
      x1 <- seq(from = 1, to = nrow(res.fit))
      result.fit <- cbind(df_dependent, res.fit)
      plot(x1, result.fit[, 1], col="red", 
           main = "Dataset: Fitting phase (the training data(red) Vs Sim. result(blue))", 
           type = "l", ylab = dependent_col)
      lines(x1, result.fit[, 2], col="blue")
      
      result.test <- data.frame(cbind(test_label, res.test))
      colnames(result.test) <- c("Test Actual", "Test Predictions")
      x2 <- seq(from = 1, to = nrow(result.test))
      plot(x2, result.test[, 1], col="red", 
           main = "Dataset: Predicting phase (the Real Data(red) Vs Sim. result(blue))", 
           type = "l", ylab = dependent_col)
      lines(x2, result.test[, 2], col="blue", type = "l")
      par(op)
      return(list(object, y.real, y.pred))
    }
    else if(method=="GFS.GCCL" || method=="FH.GBML" || method=="SLAVE"){
      object <- frbs.learn(df, range_data, method, control)
      res.test <- predict(object, test_data)
      err <- 100*sum(test_label!=res.test)/nrow(test_label)
      print("The result: ")
      result.test <- data.frame(cbind(test_label, res.test))
      colnames(result.test) <- c("Test Actual", "Test Predictions")
      print(result.test)
      writeLines("Model percentage Error on Dataset")
      print(err)
      return(list(object, result.test, err))
    }
    
  }
  else if(sample_proportion == 0){
    writeLines("Please Use Sample greater than 0!")
  }
}

fuzzy_supervised_gradient_descent <- function(df, sample_proportion=1, 
                                              train_proportion=0.5, dependent_col,
                                              method="FS.HGD", num_label=5, iteration=50,
                                              step_size=0.05, norm_type=1, alpha_heuristic=1, 
                                              implification_function="ZADEH"){
  
  writeLines("================================== This function contains algorithm of: ===========================================")
  writeLines("1) The FRBS using heuristics and Gradient Descent method (FS.HGD): It is used to solve regression tasks. See FS.HGD")
  writeLines("2) The Fuzzy Inference Rules by Descent method (FIR.DM): It is used to solve regression tasks. See FIR.DM")
  
  library(frbs)
  writeLines("===========================================================")
  writeLines(paste0("Building Fuzzy Model ", method))
  writeLines("===========================================================")
  if(norm_type==1){
    writeLines("Using Tnorm with 'MIN' Parameter")
    writeLines("Using Snorm with 'MAX' Parameter")
  }
  else if(norm_type==2){
    writeLines("Using Tnorm with 'HAMACHER' Parameter")
    writeLines("Using Snorm with 'HAMACHER' Parameter")
  }
  else if(norm_type==3){
    writeLines("Using Tnorm with 'YAGER' Parameter")
    writeLines("Using Snorm with 'YAGER' Parameter")
  }
  else if(norm_type==4){
    writeLines("Using Tnorm with 'PRODUCT' Parameter")
    writeLines("Using Snorm with 'SUM' Parameter")
  }
  else if(norm_type==5){
    writeLines("Using Tnorm with 'BOUNDED' Parameter")
    writeLines("Using Snorm with 'BOUNDED' Parameter")
  }
  
  if(sample_proportion > 0 && sample_proportion <= 1){
    sample_num <- round(nrow(df) * sample_proportion)
    sample_index <- sample(sample_num, sample_num)
    df <- df[sample_index, ]
    
    df_independent <- df[,-which(colnames(df)==dependent_col)]
    df_dependent <- matrix(df[,which(colnames(df)==dependent_col)], ncol=1)
    train_num <- round(nrow(df) * train_proportion)
    train_index <- sample(train_num, train_num)
    
    train_data <- df[train_index,-which(colnames(df)==dependent_col)]
    train_label <- df[train_index, which(colnames(df)==dependent_col)]
    test_data <- df[-train_index,-which(colnames(df)==dependent_col)]
    test_label <- df[-train_index, which(colnames(df)==dependent_col)]
    
    range_data <- c()
    control <- list()
    
    for(range_fuzzy in 1:ncol(df)){
      range_data <- c(range_data, min(df[,range_fuzzy]), max(df[,range_fuzzy]))
    }
    range_data<-matrix(range_data, nrow=2)
    print(range_data)
    
    if(method=="FS.HGD"){
      control <- list(num.labels = num_label, max.iter = iteration, step.size = step_size, 
                      alpha.heuristic = alpha_heuristic, type.tnorm = norm_type, 
                      type.snorm = norm_type, type.implication.func = implification_function, 
                      name = "FS HGD Model") 
    }
    else if(method=="FIR.DM"){
      control <- list(num.labels = num_label, max.iter = iteration, step.size = step_size, 
                      name = "FIR.DM Model", type.tnorm = norm_type, 
                      type.snorm = norm_type, type.implication.func = implification_function)  
    }
    
    ## generate fuzzy model
    object <- frbs.learn(df, range_data, method, control)
    res.fit <- predict(object, df_independent)
    res.test <- predict(object, test_data)
    y.pred <- res.test
    y.real <- test_label
    
    bench <- cbind(y.pred, y.real)
    colnames(bench) <- c("pred. val.", "real. val.")
    
    print("Comparison Model Vs Real Value")
    print(bench)
    
    residuals <- (y.real - y.pred)
    MSE <- mean(residuals^2)
    RMSE <- sqrt(mean(residuals^2))
    SMAPE <- mean(abs(residuals)/(abs(y.real) + abs(y.pred))/2)*100
    
    err <- c(MSE, RMSE, SMAPE)
    names(err) <- c("MSE", "RMSE", "SMAPE")
    
    print("Error Measurement: ")
    print(err) 
    
    op <- par(mfrow = c(2, 1))
    x1 <- seq(from = 1, to = nrow(res.fit))
    result.fit <- cbind(df_dependent, res.fit)
    plot(x1, result.fit[, 1], col="red", 
         main = "Dataset: Fitting phase (the training data(red) Vs Sim. result(blue))", 
         type = "l", ylab = dependent_col)
    lines(x1, result.fit[, 2], col="blue")
    
    result.test <- data.frame(cbind(test_label, res.test))
    colnames(result.test) <- c("Test Actual", "Test Predictions")
    x2 <- seq(from = 1, to = nrow(result.test))
    plot(x2, result.test[, 1], col="red", 
         main = "Dataset: Predicting phase (the Real Data(red) Vs Sim. result(blue))", 
         type = "l", ylab = dependent_col)
    lines(x2, result.test[, 2], col="blue", type = "l")
    par(op)
    return(list(object, y.real, y.pred))
  }
  else if(sample_proportion == 0){
    writeLines("Please Use Sample greater than 0!")
  }
}

##-------------------------------------- Example Usage -----------------------------------------------------------
data(frbsData)
df_sample <- data.frame(frbsData$MackeyGlass1000.dt)
df_sample2 <- data.frame(frbsData$GasFurnance.dt)
wm <- fuzzy_supervise_space_partition(df_sample, dependent_col="X5",method="WM", num_label=5, iteration=50,
                                      step_size=0.05, norm_type=1, 
                                      fuzzifier_type=5 , defuzzifier_type=5, 
                                      implification_function="ZADEH")

frbcs.w <- fuzzy_supervise_space_partition(df_sample, dependent_col="X5",method="FRBCS.W", num_label=5, iteration=50,
                                           step_size=0.05, norm_type=1, 
                                           fuzzifier_type=5 , defuzzifier_type=5, 
                                           implification_function="ZADEH")

frbcs.chi <- fuzzy_supervise_space_partition(df_sample, dependent_col="X5",method="FRBCS.CHI", num_label=5, iteration=50,
                                             step_size=0.05, norm_type=1, 
                                             fuzzifier_type=5 , defuzzifier_type=5, 
                                             implification_function="ZADEH")

anfis <- fuzzy_supervise_neural_network(df_sample, dependent_col="X5", method = "ANFIS", num_label=5, iteration=50,
                                        step_size=0.05, norm_type=1, 
                                        fuzzifier_type=5 , defuzzifier_type=5, 
                                        implification_function="ZADEH")

hyfis <- fuzzy_supervise_neural_network(df_sample, dependent_col="X5", method = "HyFIS", num_label=5, iteration=50,
                                        step_size=0.05, norm_type=1, 
                                        fuzzifier_type=5 , defuzzifier_type=5, 
                                        implification_function="ZADEH")

sbc <- fuzzy_supervise_clustering(df_sample, dependent_col="X5",
                                  method="SBC", iteration=50,
                                  step_size=0.05, radius_threshold=0.5, 
                                  upperthreshold=0.75, lowerthreshold=0.25,
                                  triangular_width=2)

denfis <- fuzzy_supervise_clustering(df_sample, dependent_col="X5",
                                     method="DENFIS", iteration=50,
                                     step_size=0.05, radius_threshold=0.5, 
                                     upperthreshold=0.75, lowerthreshold=0.25,
                                     triangular_width=2)

gfs.thrift <- fuzzy_supervise_genetic_algorithm(df_sample, sample_proportion=1, 
                                                train_proportion=0.5,dependent_col,
                                                method="GFS.Thrift", num_label=5, iteration=50,
                                                population_size=15, crossover_probability=0.9, 
                                                mutant_probability=0.3, max_genetic_generation=50, 
                                                epsilon=0.85, max_tuning=300, 
                                                dcare_probability=0.5, gccl_probability=0.5, 
                                                max_rules=50, num_class=3, k_lower=0, k_upper=1,
                                                norm_type=1, fuzzifier_type=1, rule_selection=TRUE,
                                                mode_tuning="LOCAL",
                                                defuzzifier_type=1, implification_function="ZADEH")

gfs.fr.mogul <- fuzzy_supervise_genetic_algorithm(df_sample, sample_proportion=1, 
                                                  train_proportion=0.5,dependent_col,
                                                  method="GFS.FR.MOGUL", num_label=5, iteration=50,
                                                  population_size=15, crossover_probability=0.9, 
                                                  mutant_probability=0.3, max_genetic_generation=50, 
                                                  epsilon=0.85, max_tuning=300, 
                                                  dcare_probability=0.5, gccl_probability=0.5, 
                                                  max_rules=50, num_class=3, k_lower=0, k_upper=1,
                                                  norm_type=1, fuzzifier_type=1, rule_selection=TRUE,
                                                  mode_tuning="LOCAL",
                                                  defuzzifier_type=1, implification_function="ZADEH")

gfs.gccl <- fuzzy_supervise_genetic_algorithm(df_sample, sample_proportion=1, 
                                              train_proportion=0.5,dependent_col,
                                              method="GFS.GCCL", num_label=5, iteration=50,
                                              population_size=15, crossover_probability=0.9, 
                                              mutant_probability=0.3, max_genetic_generation=50, 
                                              epsilon=0.85, max_tuning=300, 
                                              dcare_probability=0.5, gccl_probability=0.5, 
                                              max_rules=50, num_class=3, k_lower=0, k_upper=1,
                                              norm_type=1, fuzzifier_type=1, rule_selection=TRUE,
                                              mode_tuning="LOCAL",
                                              defuzzifier_type=1, implification_function="ZADEH")

fh.gbml <- fuzzy_supervise_genetic_algorithm(df_sample, sample_proportion=1, 
                                             train_proportion=0.5,dependent_col,
                                             method="FH.GBML", num_label=5, iteration=50,
                                             population_size=15, crossover_probability=0.9, 
                                             mutant_probability=0.3, max_genetic_generation=50, 
                                             epsilon=0.85, max_tuning=300, 
                                             dcare_probability=0.5, gccl_probability=0.5, 
                                             max_rules=50, num_class=3, k_lower=0, k_upper=1,
                                             norm_type=1, fuzzifier_type=1, rule_selection=TRUE,
                                             mode_tuning="LOCAL",
                                             defuzzifier_type=1, implification_function="ZADEH")

slave <- fuzzy_supervise_genetic_algorithm(df_sample, sample_proportion=1, 
                                           train_proportion=0.5,dependent_col,
                                           method="SLAVE", num_label=5, iteration=50,
                                           population_size=15, crossover_probability=0.9, 
                                           mutant_probability=0.3, max_genetic_generation=50, 
                                           epsilon=0.85, max_tuning=300, 
                                           dcare_probability=0.5, gccl_probability=0.5, 
                                           max_rules=50, num_class=3, k_lower=0, k_upper=1,
                                           norm_type=1, fuzzifier_type=1, rule_selection=TRUE,
                                           mode_tuning="LOCAL",
                                           defuzzifier_type=1, implification_function="ZADEH")

gfs.lt.rs <- fuzzy_supervise_genetic_algorithm(df_sample, sample_proportion=1, 
                                               train_proportion=0.5,dependent_col,
                                               method="GFS.LT.RS", num_label=5, iteration=50,
                                               population_size=15, crossover_probability=0.9, 
                                               mutant_probability=0.3, max_genetic_generation=50, 
                                               norm_type=1, fuzzifier_type=1, rule_selection=TRUE,
                                               mode_tuning="LOCAL",
                                               defuzzifier_type=1, implification_function="ZADEH")

fs.hgd <- fuzzy_supervised_gradient_descent(df, sample_proportion=1, 
                                            train_proportion=0.5, dependent_col="X5",
                                            method="FS.HGD", num_label=5, iteration=50,
                                            step_size=0.05, norm_type=1, alpha_heuristic=1, 
                                            implification_function="ZADEH")

fir.dm <- fuzzy_supervised_gradient_descent(df, sample_proportion=1, 
                                            train_proportion=0.5, dependent_col="X5",
                                            method="FIR.DM", num_label=5, iteration=50,
                                            step_size=0.05, norm_type=1, alpha_heuristic=1, 
                                            implification_function="ZADEH")



#-------------------------------------------------------------------------------------------------------------------------
########################### 18) Instance Based Classification  ###########################################################
#-------------------------------------------------------------------------------------------------------------------------

#all numerical analysis
instance_classification <- function(df, dependent_var, method="", train_proportion=1, knn_unit=10,
                                    som_xgrid = 20, som_ygrid=20, som_iteration=1000, som_n_cluster=3,
                                    lowess_sensitivity = c(0.001,0.01,0.1,0.25,0.5,0.75,1), 
                                    lowess_independent=c(), inspect_matrix=FALSE){
  
  library(mlr)
  library(mlbench) 
  library(poliscidata)
  library(lasso2)
  library(class) #KNN (k-Nearest Neighbor) + LVQ (Learning Vector Quantization)
  library(kohonen) #SOM (Self Organizing Maps)
  library(factoextra)
  library(datasets)
  library(gmodels) #For Cross Tables
  library(lazy) #For Lowess Regression
  
  length_independent <- length(colnames(df[,which(colnames(df)!=dependent_var)]))
  matrix_independent <- as.matrix(df[,which(colnames(df)!=dependent_var)])
  matrix_dependent <- as.matrix(df[,which(colnames(df)==dependent_var)])
  
  if(inspect_matrix){
    writeLines("============================ Matrix Independent ===================================")
    print(head(matrix_independent))
    writeLines("============================ Matrix Dependent =====================================")
    print(head(matrix_dependent))
  }
  
  train_data <- NULL
  test_data <- NULL
  
  train_data_predictor <- NULL
  train_data_label <- NULL
  test_data_predictor <- NULL
  test_data_label <- NULL
  
  matrix_train_data_predictor <- NULL
  matrix_train_data_label <- NULL
  matrix_test_data_predictor <- NULL
  matrix_test_data_label <- NULL
  
  if(train_proportion < 1){
    writeLines(paste0("Using Train Proportion with ",train_proportion*100,"% and Test Proportion with ", (1-train_proportion) * 100), "%")
    ## set the seed to make your partition reproducible
    set.seed(123)
    smp_size <- floor(train_proportion * nrow(df))
    train_ind <- sample(seq_len(nrow(df)), size = smp_size)
    train_data <- df[train_ind, ]
    test_data <- df[-train_ind, ]
    train_data_predictor <- train_data[,which(colnames(train_data) != dependent_var)]
    train_data_label <- train_data[,which(colnames(train_data) == dependent_var)]
    test_data_predictor <- test_data[,which(colnames(test_data) != dependent_var)]
    test_data_label <- test_data[,which(colnames(test_data) == dependent_var)]
    if(method == "som"){
      matrix_train_data_predictor <- scale(train_data_predictor)
      matrix_train_data_label <- as.matrix(train_data_label)
      matrix_test_data_predictor <- scale(test_data_predictor, 
                                          center = attr(matrix_train_data_predictor, "scaled:center"),
                                          scale = attr(matrix_train_data_predictor, "scaled:scale"))
      matrix_test_data_label <- as.matrix(test_data_label)
    }
    else{
      matrix_train_data_predictor <- data.matrix(train_data_predictor)
      matrix_train_data_label <- as.matrix(train_data_label)
      matrix_test_data_predictor <- data.matrix(test_data_predictor)
      matrix_test_data_label <- as.matrix(test_data_label) 
    }
  }
  else if(train_proportion == 1){
    writeLines(paste0("Using Train dataset as both train and test"))
    train_data_predictor <- df[,which(colnames(df) != dependent_var)]
    train_data_label <- df[,which(colnames(df) == dependent_var)]
    test_data_predictor <- df[,which(colnames(df) != dependent_var)]
    test_data_label <- df[,which(colnames(df) == dependent_var)]
    if(method == "som"){
      matrix_train_data_predictor <- scale(train_data_predictor)
      matrix_train_data_label <- as.matrix(train_data_label)
      matrix_test_data_predictor <- scale(test_data_predictor, 
                                          center = attr(matrix_train_data_predictor, "scaled:center"),
                                          scale = attr(matrix_train_data_predictor, "scaled:scale"))
      matrix_test_data_label <- as.matrix(test_data_label)
    }
    else{
      matrix_train_data_predictor <- data.matrix(train_data_predictor)
      matrix_train_data_label <- as.matrix(train_data_label)
      matrix_test_data_predictor <- data.matrix(test_data_predictor)
      matrix_test_data_label <- as.matrix(test_data_label) 
    }
  }
  
  if(method=="knn"){
    knn_pred <- knn(train = train_data_predictor, test = test_data_predictor, 
                    cl = train_data_label, k=knn_unit)
    conf_matrix = CrossTable(x=test_data_label, y=knn_pred, 
                             prop.chisq=FALSE)
    
    writeLines("Correct Classification by KNN")
    print(conf_matrix$t)
    correct_percentage_each_category = diag(conf_matrix$prop.row)
    num_class <- seq(1:length(correct_percentage_each_category))
    writeLines(paste0(correct_percentage_each_category, " Percent Correctly Classified of Class ", num_class))
    overall_correct_percent = sum(diag(conf_matrix$t)) / sum(conf_matrix$t)
    writeLines(paste("Overall Correct Percentages: ", round(overall_correct_percent, 2)))
    return(knn_pred)
  }
  else if(method=="lvq"){
    #Building LVQ Code Book 
    codeBook = lvqinit(train_data_predictor, train_data_label, k = knn_unit)
    buildCodeBook = olvq1(train_data_predictor, train_data_label, codeBook)
    predict = lvqtest(buildCodeBook, test_data_predictor)
    conf_matrix = CrossTable(x=test_data_label, y=predict, prop.chisq=FALSE)
    
    writeLines("Correct Classification by KNN")
    print(conf_matrix$t)
    correct_percentage_each_category = diag(conf_matrix$prop.row)
    num_class <- seq(1:length(correct_percentage_each_category))
    writeLines(paste0(correct_percentage_each_category, " Percent Correctly Classified of Class ", num_class))
    overall_correct_percent = sum(diag(conf_matrix$t)) / sum(conf_matrix$t)
    writeLines(paste("Overall Correct Percentages: ", round(overall_correct_percent, 2)))
    return(predict)
  }
  else if(method=="usom"){
    som_grid <- somgrid(xdim = som_xgrid, ydim=som_ygrid, topo="hexagonal")
    unsupervised_som_model <- som(matrix_independent, grid=som_grid, 
                                  rlen=som_iteration, alpha=c(0.05,0.01), 
                                  keep.data = TRUE)
    
    writeLines("Cluster Existing SOM Model and apply the Cluster Position to current DF")
    som_data_position = unsupervised_som_model$unit.classif
    set.seed(100)
    fviz_nbclust(unsupervised_som_model$codes[[1]], kmeans, method = "wss")
    set.seed(100)
    clust <- kmeans(unsupervised_som_model$codes[[1]], som_n_cluster)
    plot(unsupervised_som_model, type = "codes", bgcol = rainbow(som_n_cluster)[clust$cluster], main = "Cluster Map")
    add.cluster.boundaries(unsupervised_som_model, clust$cluster)
    
    # know cluster each data
    som_cluster <- data.frame(df, cluster = clust$cluster[unsupervised_som_model$unit.classif])
    print("Top 10 DF with SOM Cluster")
    print(head(som_cluster, 10))
    print("Bottom 10 DF with SOM Cluster")
    print(tail(som_cluster, 10))
    
    writeLines("Plot the distance from each nodes weights to the samples represented by that node which is reduced")
    x11()
    plot(unsupervised_som_model, type="changes")
    
    writeLines("visualise the count of how many samples are mapped to each node on the map")
    writeLines("This metric can be used as a measure of map quality ideally the sample distribution is relatively uniform")
    x11()
    plot(unsupervised_som_model, type="count")
    
    writeLines("This visualisation is of the distance between each node and its neighbours.")
    writeLines("Typically viewed with a grayscale palette,")
    writeLines("Areas of low neighbour distance indicate groups of nodes that are similar.")
    x11()
    plot(unsupervised_som_model, type="dist.neighbours")
    
    writeLines("The default visualisation of the weight vectors is a a fan diagram")
    writeLines("where individual fan representations of the magnitude of each variable in the weight vector is shown for each node")
    x11()
    plot(unsupervised_som_model, type="codes")
    
    # Heatmaps are perhaps the most important visualisation possible for Self-Organising Maps. 
    # The use of a weight space view as in (4) 
    # that tries to view all dimensions on the one diagram is unsuitable for a high-dimensional (>7 variable) SOM
    # A SOM heatmap allows the visualisation of the distribution of a single variable across the map
    
    my_palette <- colors()[c(8, 5, 30, 53, 118, 72)]
    #Heatmap for each Variable side by side
    if(length_independent < 5)
    {
      par(mfrow=c(2,2))
    }
    else if(length_independent >=5 && length_independent <= 10){
      par(mfrow=c(2,5))
    }
    else if(length_independent > 10){
      if(length_independent%%5 == 0){
        multiplier = (length_independent/5)
      }
      else{
        multiplier = (length_independent/5) + 1
      }
      par(mfrow=c(multiplier,5))
    }
    
    for(indep in 1:length_independent){
      plot(unsupervised_som_model, type="property", property = unsupervised_som_model$codes[[1]][,indep], 
           main=colnames(unsupervised_som_model$data[[1]])[indep])
    } 
    
    ## use hierarchical clustering to cluster the codebook vectors
    som_hc_cluster <- cutree(hclust(dist(unsupervised_som_model$codes[[1]])), som_n_cluster)
    
    # plot these results:
    x11()
    plot(unsupervised_som_model, type="mapping", bgcol = my_palette[som_hc_cluster], main = "Clusters")
    add.cluster.boundaries(unsupervised_som_model, som_hc_cluster)
    x11()
    plot(unsupervised_som_model, type="mapping", pchs = 19, shape = "round")
    add.cluster.boundaries(unsupervised_som_model, som_hc_cluster)
    return(list(unsupervised_som_model, som_cluster, som_hc_cluster))
  }
  else if(method=="som"){
    som_grid <- somgrid(xdim = som_xgrid, ydim=som_ygrid, topo="hexagonal")
    testXY <- list(measurements = matrix_test_data_predictor, 
                   vintages = test_data_label)
    
    # classification & predict
    set.seed(100)
    supervised_som <- xyf(matrix_train_data_predictor, 
                          train_data_label, 
                          som_grid, rlen = som_iteration)
    
    clust <- kmeans(supervised_som$codes[[1]], som_n_cluster)
    pred <- predict(supervised_som, newdata = testXY)
    #print(table(Predict = pred$predictions[[2]], Actual = test_data_label))
    plot(supervised_som, type = "codes", 
         bgcol = rainbow(9)[clust$cluster], main = "Cluster SOM")
    add.cluster.boundaries(supervised_som, clust$cluster)
    return(list(supervised_som, pred))
  }
}

#all numerical analysis
loess_fit <- function(df, dependent_var, sensitivity=c(0.9,0.5,0.3,0.001), 
                      color=c("#ff8400","#fff75c","#4feb34","#34c0eb")){
  if(length(sensitivity) != length(color)){
    message("Sensitivity length is not match with Color length! please input a correct length for both!")
    stop()
  }
  independent_var <- colnames(df)[which(colnames(df) != dependent_var)]
  for(h in 1:length(independent_var)){
    x11()
    full_formula <- as.formula(paste0("df$",dependent_var, " ~ df$", independent_var[h]))
    formula <- as.formula(paste0(dependent_var, " ~ ", independent_var[h]))
    plot(formula, data=df)
    abline(lm(full_formula), col="Red")
    mod1 = lowess(Theoph$conc~Theoph$Wt)
    for(x in 1:length(sensitivity)){
      if(x == 1){
        lowessgraph = lines(lowess(full_formula, f=sensitivity[x]), col=color[x]) 
      }
      else if(x > 1){
        lines(lowess(full_formula, f=sensitivity[x]), col=color[x])
      }
    }
  }
}

library(mlbench)
library(dplyr)
data(Zoo)
zoo_numeric <- Zoo %>% mutate_if(is.logical, as.numeric)

knn_1 <- instance_classification(zoo_numeric, "type", "knn", knn_unit=2, train_proportion = 1)
knn_2 <- instance_classification(zoo_numeric, "type", "knn", knn_unit=40, train_proportion = 1)
knn_3 <- instance_classification(zoo_numeric, "type", "knn", knn_unit=2, train_proportion = 0.6)
lvq_1 <- instance_classification(zoo_numeric, "type", "lvq", knn_unit=2, train_proportion = 1)
lvq_2 <- instance_classification(zoo_numeric, "type", "lvq", knn_unit=40, train_proportion = 1)
lvq_3 <- instance_classification(zoo_numeric, "type", "lvq", knn_unit=2, train_proportion = 0.6)
u_som <- instance_classification(zoo_numeric, "type", "usom", som_xgrid = 5, som_ygrid = 5, 
                                 som_iteration = 500, som_n_cluster = 2)
s_som <- instance_classification(zoo_numeric, "type", "som", som_xgrid = 5, som_ygrid = 5, 
                                 som_iteration = 500, som_n_cluster = 2, train_proportion = 0.6, 
                                 inspect_matrix = TRUE)
loess <- loess_fit(zoo_numeric, "type")
loess_2 <- loess_fit(zoo_numeric, "breathes")
loess_3 <- loess_fit(Satellite, "classes")
loess_4 <- loess_fit(Satellite, "x.30")



#-------------------------------------------------------------------------------------------------------------------------
########################### 19) Person-Item Parameter Model  #############################################################
#-------------------------------------------------------------------------------------------------------------------------

person_item_parameter_model <- function(whole_df, one_hot_encode_df, ICC_item_group=6,
                                        factor_group_col,
                                        factor_group1_name="group1", 
                                        factor_group2_name="group2",
                                        rasch_start_zero=FALSE){
  library("eRm")
  library("ltm")
  library("difR")
  num_colnames <- length(colnames(one_hot_encode_df))
  ICC_group <- ceiling(num_colnames / ICC_item_group)
  writeLines("========================== Estimating Item Parameter ==========================================")
  writeLines("1) Using Rasch Model with CML Estimation (Conditional Maximum Likelihood)")
  res_rm_1 <- NULL
  
  if(rasch_start_zero==TRUE)
  {
    res_rm_1 <- RM(one_hot_encode_df, sum0 = FALSE)
  }
  else if(rasch_start_zero==FALSE){
    res_rm_1 <- RM(one_hot_encode_df, sum0 = TRUE)
  }
  
  res_rm_1                   # Short summary of item parameters
  summary(res_rm_1)          # Longer summary of item parameters
  betas <- -coef(res_rm_1)   # Item difficulty parameters
  print(summary(betas))
  
  for(g in 1:length(ICC_group)){
    if(g<length(ICC_group)){
      x11()
      plotjointICC(res_rm_1, item.subset =  (ICC_item_group * (g-1) + 1):(ICC_item_group * g), cex = .6)
      writeLines(paste0("Item Difficulty Parameters Group ",g))
      print(mean(betas[(ICC_item_group * (g-1) + 1):(ICC_item_group * g)]))
    }
    else if(g==length(ICC_group)){
      x11()
      plotjointICC(res_rm_1, item.subset =  (ICC_item_group * (g-1) + 1):num_colnames, cex = .6)
      writeLines("Item Difficulty Parameters Last Group")
      print(mean(betas[(ICC_item_group * (g-1) + 1):num_colnames]))
    }
  }
  
  #plotjointICC(res_rm_1, item.subset =  1:12, cex = .6)
  #plotjointICC(res_rm_1, item.subset = 13:24, cex = .6)
  #mean(betas[ 1:12])    
  #mean(betas[13:24]) 
  
  x11()
  plotPImap(res_rm_1, cex.gen = .55)
  x11()
  plotPImap(res_rm_1, cex.gen = .55, sorted = TRUE)
  
  #When setting sum0 = FALSE, the first item parameter is set to zero. 
  #These are the only two options implemented in RM()
  tmp1 <- RM(dat_1, sum0 = FALSE)
  round(coef(tmp1), 2)
  
  #Note on Item Parameters in eRm Package
  summary(res_rm_1)
  -sum(res_rm_1$etapar)
  
  writeLines("2) Using Rasch Model with MML Estimation (Marginal Maximum Likelihood)")
  #the model is now fit using marginal maximum likelihood (MML) estimation using the function rasch() from the ltm package. 
  #Therein, the model is identified by assuming a standard normal distribution of the person parameters
  res_rm_2 <- rasch(one_hot_encode_df)
  res_rm_2
  cor(coef(res_rm_2)[, 1], betas)
  
  writeLines("3) Using 2PL Model (with Latent Variable)")
  res_2pl_1 <- ltm(one_hot_encode_df ~ z1)
  res_2pl_1
  
  for(g in 1:length(ICC_group)){
    if(g<length(ICC_group)){
      x11()
      plot(res_2pl_1, items =  (ICC_item_group * (g-1) + 1):(ICC_item_group * g))
    }
    else if(g==length(ICC_group)){
      x11()
      plot(res_2pl_1, items =  (ICC_item_group * (g-1) + 1):num_colnames)
    }
  }
  #plot(res_2pl_1, items =  1:12)
  #plot(res_2pl_1, items = 13:24)
  
  writeLines("4) Make Model Fitting between Rasch and 2PL Model")
  writeLines("Make Anova table of model performance betwenn Rasch and 2PL Model")
  writeLines("Hypothesis Testing")
  writeLines("H0: Both Model do fit equally well")
  writeLines("H1: Both Model do not fit equally well (one model is better than another)")
  writeLines("(2) anova result for RASCH Model and 2PL Model")
  anova_mml <- anova(res_rm_2, res_2pl_1)
  print(anova_mml)
  writeLines('Correlation value between 2 models in (2)')
  print(cor(coef(res_rm_2)[, 1], coef(res_2pl_1)[, 1]))
  
  writeLines("4a) Andersen's Likelihood-Ratio Test and Graphical Model Check")
  writeLines("Hypothesis Testing")
  writeLines("H0: Model is Good Fitted")
  writeLines("H1: Model is Bad Fitted")
  writeLines("From this test, we may know what Individuals may badly fitted")
  writeLines("so next time we can exclude it from item estimation, to see if the Likelihood-ratio test went better")
  lrt_1 <- LRtest(res_rm_1, 
                  splitcr = whole_df[[factor_group_col]])
  print(lrt_1)
  print(summary(lrt_1))
  plotGOF(lrt_1, conf = list(), 
          tlab = "number",
          xlab = factor_group1_name, 
          ylab = factor_group2_name)
  
  writeLines("4b) Wald Test (Comparison of 2 Groups) --> used for individual items")
  writeLines("Hypothesis Testing")
  writeLines("H0: Individual Item is Good Fitted")
  writeLines("H1: Individual Item is Bad Fitted")
  writeLines("Negative values indicate that the item is easier for the second group")
  wd <- Waldtest(res_rm_1, splitcr = whole_df[[factor_group_col]])
  print(wd)
  print(summary(wd))
  
  writeLines("================================ 4c) Infit and Outfit ====================================")
  writeLines("Infit and outfit statistics are not based on the comparison of groups, but on residuals.")
  writeLines("Hypothesis Testing")
  writeLines("H0: Residuals shows good fitted in Individual")
  writeLines("H1: Residuals shows bad fitted in Individual")
  pp_ml_1 <- person.parameter(res_rm_1)
  itemfit(pp_ml_1)
  plotPWmap(res_rm_1)
  
  writeLines("========================= 5) DIF (Differential Item Functioning) =========================")
  writeLines("5a) Mantel-Haenszel Non Parametric Test")
  writeLines("H0: Not Significant DIF")
  writeLines("H1: Significant DIF")
  writeLines("For item who haves a Significant DIF")
  writeLines("+: The positive effect size indicates that the item is easier for the focal group")
  writeLines("-: The negative effect size indicates that the item is harder for the focal group")
  tmp1 <- difMH(dat_1, group = whole_df[[factor_group_col]], focal.name = 1)
  tmp1
  plot(tmp1)
  
  writeLines("5b) Lord's Chi-Square-Test (Design Specifically for 2PL Model)")
  writeLines("H0: Item contains Uniform (crossing) DIF")
  writeLines("H1: Item contains Non-Uniform (crossing) DIF")
  writeLines("Explanation")
  writeLines("Item that contains Uniform DIF indicates that the item is easier for the focal group")
  writeLines("Item that contains Non-Uniform DIF indicates that the item is harder for the focal group")
  tmp2 <- difLord(dat_1, group = whole_df[[factor_group_col]], focal.name = 1, model = "2PL")
  tmp2
  plot(tmp2)
  
  writeLines("========================== Estimating Person Parameter ==========================================")
  writeLines("1) Maximum likelihood (ML) estimation")
  ml_person <- person.parameter(res_rm_1)
  pp_ml <- coef(ml_person)
  
  writeLines("2) Empirical Bayes (EB) estimation")
  pp_map <- factor.scores(res_rm_2, method = "EB", resp.patterns = one_hot_encode_df)
  
  writeLines("3) Expected a Posteriori (EAP) estimation")
  pp_eap <- factor.scores(res_rm_2, method = "EAP", resp.patterns = one_hot_encode_df)
  
  writeLines("4) Make Benchmark of 3 kinds of estimation done")
  estimation_benchmark <- data.frame(ML = pp_ml, MAP = pp_map$score.dat$z1, 
                                     EAP = pp_eap$score.dat$z1)
  estimation_cor <- round(cor(estimation_benchmark), 4)
  print(head(estimation_benchmark))
  writeLines("Estimation Correlation")
  print(estimation_cor)
  return(list(estimation_benchmark, estimation_cor))
}

#example usage
data(verbal, package = "difR")
person_item_parameter_model(verbal, verbal[,1:24], ICC_item_group=12, factor_group_col = "Gender", 
                            factor_group1_name = "Woman", factor_group2_name = "Man")




#-------------------------------------------------------------------------------------------------------------------------
########################### 20) Social Network Analysis  #################################################################
#-------------------------------------------------------------------------------------------------------------------------

social_network_analysis <- function(df_orig, from_list, to_list, add_vertex_attr=c(), 
                                    vertex_neighbour_analysis_limit = 50,
                                    vertex_special_relationship="", vertex_interest="", edge_weight="", 
                                    mode_direct=FALSE, add_edge_attr=c(), vertice_n_step=3, 
                                    network_randomization=1000,
                                    maximal_clique_length_plot=4, make_plot=FALSE){
  library(igraph)
  writeLines("====================== Preprocessing: Making Vertex and Edge Dataframe =============================")
  df_edge <- df_orig[,which(colnames(df_orig) %in% c(from_list, to_list))]
  vertex_unit <- unique(c(unlist(df_orig[from_list]), 
                          unlist(df_orig[to_list])))
  df_vertex <- data.frame(name=vertex_unit)
  if(length(add_vertex_attr) > 0){
    for(add in 1:length(add_vertex_attr)){
      df_vertex <- data.frame(cbind(df_vertex, 
                                    df_orig[,which(colnames(df_orig) %in% c(add_vertex_attr[add]))]))
    }
  }
  if(length(add_edge_attr) > 0){
    for(add in 1:length(add_edge_attr)){
      df_edge <- data.frame(cbind(df_edge, 
                                  df_orig[,which(colnames(df_orig) %in% c(add_edge_attr[add]))]))
    }
  }
  
  writeLines("Vertex DF")
  print(head(df_vertex))
  writeLines("Vertex Length")
  print(nrow(df_vertex))
  writeLines("Edge DF")
  print(head(df_edge))
  writeLines("Edge From Length")
  print(length(unique(unlist(df_orig[from_list]))))
  writeLines("Edge To Length")
  print(length(unique(unlist(df_orig[to_list]))))
  
  writeLines("====================== 1) Convert dataframe to make a Graph Object =============================")
  df.mat <- as.matrix(df_orig)
  g <- graph_from_data_frame(d = df_edge, vertices = df_vertex, 
                             directed = mode_direct)
  if(make_plot==TRUE){
    x11()
    plot(g, layout = layout_nicely(g))
  }
  
  writeLines("============================== 2) Checking Network Basic Vertices and Edges =================================")
  writeLines("Network Vertices List")
  print(V(g))
  writeLines("Network Edge List")
  print(E(g))
  writeLines(paste0("Size of the Vertices: ",gorder(g)))
  writeLines(paste0("Size of the Edges: ",gsize(g)))
  
  writeLines("Vertex Attributes")
  print(vertex_attr(g))
  writeLines("Edge Attributes")
  print(edge_attr(g))
  
  if(make_plot==TRUE && edge_weight != ""){
    x11()
    weighteval <- paste0("E(g)$",edge_weight)
    weight <- eval(parse(text=weighteval))
    plot(g, edge.color = 'black',
         edge.width = weight,
         layout = layout_nicely(g))
  }
  
  writeLines("Does Vertices in Network use Direction to each other?")
  print(is.directed(g))
  writeLines("Does Network have weight Applied?")
  print(is.weighted(g))
  
  writeLines("========================= 3) Network Neighbours Relationship, Diamater ====================")
  all_vertex <- vertex_attr(g)$name
  
  if(vertex_interest==""){
    if(vertex_neighbour_analysis_limit == -1){
      for(ver in 1:length(all_vertex)){
        if(mode_direct==FALSE){
          writeLines(paste0("================================== Vertex ",ver, " ==============================="))
          writeLines(paste0("All Neighbor for Vertex ", all_vertex[ver]))
          print(neighbors(g, all_vertex[ver], mode = c('all')))
          writeLines(paste0("All Incidents for Vertex ", all_vertex[ver]))
          print(incident(g, all_vertex[ver], mode = c('all')))
          writeLines(paste0("Vertex Available Path with ",vertice_n_step," step from ",all_vertex[ver]))
          print(ego(g, vertice_n_step, all_vertex[ver], mode="all"))
        }
        else if(mode_direct==TRUE){
          writeLines(paste0("================================== Vertex ",ver, " ==============================="))
          writeLines(paste0("All Neighbor Reachable from Vertex ", all_vertex[ver]))
          print(neighbors(g, all_vertex[ver], mode = c('out')))
          writeLines(paste0("All Neighbor Source to Vertex ", all_vertex[ver]))
          print(neighbors(g, all_vertex[ver], mode = c('in')))
          writeLines(paste0("All Incidents Reachable from Vertex ", all_vertex[ver]))
          print(incident(g, all_vertex[ver], mode = c('out')))
          writeLines(paste0("All Incidents Source to Vertex ", all_vertex[ver]))
          print(incident(g, all_vertex[ver], mode = c('in')))
          writeLines(paste0("Vertex Reachable with ",vertice_n_step," step from ",all_vertex[ver]))
          print(ego(g, vertice_n_step, all_vertex[ver], mode="out"))
          writeLines(paste0("Vertex Source with ",vertice_n_step," step to ",all_vertex[ver]))
          print(ego(g, vertice_n_step, all_vertex[ver], mode="in"))
        }
      }
    }
    else{
      vertex_index <- sample(all_vertex, vertex_neighbour_analysis_limit)
      writeLines("Sample Vertex Used")
      print(vertex_index)
      for(ver in 1:vertex_neighbour_analysis_limit){
        if(mode_direct==FALSE){
          writeLines(paste0("================================== Vertex ",vertex_index[ver], " ==============================="))
          writeLines(paste0("All Neighbor for Vertex ", vertex_index[ver]))
          print(neighbors(g, vertex_index[ver], mode = c('all')))
          writeLines(paste0("All Incidents for Vertex ", vertex_index[ver]))
          print(incident(g, vertex_index[ver], mode = c('all')))
          writeLines(paste0("Vertex Available Path with ",vertice_n_step," step from ",vertex_index[ver]))
          print(ego(g, vertice_n_step, vertex_index[ver], mode="all"))
        }
        else if(mode_direct==TRUE){
          writeLines(paste0("================================== Vertex ",ver, " ==============================="))
          writeLines(paste0("All Neighbor Reachable from Vertex ", vertex_index[ver]))
          print(neighbors(g, vertex_index[ver], mode = c('out')))
          writeLines(paste0("All Neighbor Source to Vertex ", vertex_index[ver]))
          print(neighbors(g, vertex_index[ver], mode = c('in')))
          writeLines(paste0("All Incidents Reachable from Vertex ", vertex_index[ver]))
          print(incident(g, vertex_index[ver], mode = c('out')))
          writeLines(paste0("All Incidents Source to Vertex ", vertex_index[ver]))
          print(incident(g, vertex_index[ver], mode = c('in')))
          writeLines(paste0("Vertex Reachable with ",vertice_n_step," step from ",vertex_index[ver]))
          print(ego(g, vertice_n_step, vertex_index[ver], mode="out"))
          writeLines(paste0("Vertex Source with ",vertice_n_step," step to ",vertex_index[ver]))
          print(ego(g, vertice_n_step, vertex_index[ver], mode="in"))
        }
      }
    }
  }
  else{
    if(mode_direct==FALSE){
      writeLines(paste0("================================== Vertex of Interest = ",vertex_interest, " ==============================="))
      writeLines(paste0("All Neighbor for Vertex ", vertex_interest))
      print(neighbors(g, vertex_interest, mode = c('all')))
      writeLines(paste0("All Incidents for Vertex ", vertex_interest))
      print(incident(g, vertex_interest, mode = c('all')))
      writeLines(paste0("Vertex Available Path with ",vertice_n_step," step from ",vertex_interest))
      print(ego(g, vertice_n_step, vertex_interest, mode="all"))
    }
    else if(mode_direct==TRUE){
      writeLines(paste0("================================== Vertex of Interest = ",vertex_interest, " ==============================="))
      writeLines(paste0("All Neighbor Reachable from Vertex ", vertex_interest))
      print(neighbors(g, vertex_interest, mode = c('out')))
      writeLines(paste0("All Neighbor Source to Vertex ", vertex_interest))
      print(neighbors(g, vertex_interest, mode = c('in')))
      writeLines(paste0("All Incidents Reachable from Vertex ", vertex_interest))
      print(incident(g, vertex_interest, mode = c('out')))
      writeLines(paste0("All Incidents Source to Vertex ", vertex_interest))
      print(incident(g, vertex_interest, mode = c('in')))
      writeLines(paste0("Vertex Reachable with ",vertice_n_step," step from ",vertex_interest))
      print(ego(g, vertice_n_step, vertex_interest, mode="out"))
      writeLines(paste0("Vertex Source with ",vertice_n_step," step to ",all_vertex[ver]))
      print(ego(g, vertice_n_step, vertex_interest, mode="in"))
    }
  }
  writeLines("Farthest Vertices Network(Start, Finish)")
  print(farthest_vertices(g))
  writeLines("Diameter Network (Start, Process 1, Process 2, Process n, Finish)")
  print(get_diameter(g))
  
  writeLines("========================== 4) Vertex Importance Measurement ===========================")
  writeLines("4a) Out Degree Measurement")
  writeLines("===================Value====================")
  g.out <- degree(g, mode = c("out"))
  print(sort(g.out, decreasing = TRUE))
  writeLines("=================Normalized====================")
  g.out <- degree(g, mode = c("out"), normalized = TRUE)
  print(sort(g.out, decreasing = TRUE))
  g.out <- degree(g, mode = c("out"))
  writeLines("=================Frequency==================")
  print(table(g.out))
  if(make_plot==TRUE){
    x11()
    hist(g.out, breaks = 30)
  }
  writeLines("===================Maximum==================")
  writeLines(paste("Most Maximum Vertices: ", which.max(g.out)))
  
  writeLines("4b) In Degree Measurement")
  writeLines("===================Value====================")
  g.in <- degree(g, mode = c("in"))
  print(sort(g.in, decreasing = TRUE))
  writeLines("=================Normalized====================")
  g.out <- degree(g, mode = c("out"), normalized = TRUE)
  print(sort(g.out, decreasing = TRUE))
  g.out <- degree(g, mode = c("out"))
  writeLines("=================Frequency==================")
  print(table(g.in))
  if(make_plot==TRUE){
    x11()
    hist(g.in, breaks = 30)
  }
  writeLines("===================Maximum==================")
  writeLines(paste("Most Maximum Vertices: ", which.max(g.in)))
  
  writeLines("4c) Vertex Betweenness")
  writeLines("===================Value====================")
  g.b <- betweenness(g, directed = mode_direct)
  print(sort(g.b, decreasing = mode_direct))
  writeLines("=================Normalized====================")
  g.b <- betweenness(g, directed = mode_direct, normalized = mode_direct)
  print(sort(g.b, decreasing = mode_direct))
  g.b <- betweenness(g, directed = mode_direct)
  writeLines("=================Frequency==================")
  print(table(g.b))
  if(make_plot==TRUE){
    x11()
    hist(g.b, breaks = 30)
  }
  writeLines("===================Maximum==================")
  writeLines(paste("Most Maximum Vertices: ", which.max(g.b)))
  if(make_plot==TRUE){
    x11()
    plot(g, vertex.label.color = "black",
         edge.color = 'black',
         vertex.size = sqrt(g.b)+1,
         edge.arrow.size = 0.05,
         layout = layout_nicely(g),
         main = "Network by Vertice Betweenness")
  }
  
  writeLines("4d) Vertex By Eigenvector Centrality")
  writeLines("===================Value====================")
  g.ec <- eigen_centrality(g)
  print(eigen_centrality(g)$vector)
  writeLines("===================Maximum====================")
  which.max(g.ec$vector)
  
  if(make_plot==TRUE){
    x11()
    plot(g,vertex.label.color = "black", 
         vertex.label.cex = 0.6,
         vertex.size = 25*(g.ec$vector),
         edge.color = 'gray88',
         main = "Network by Eigen Vector Centrality")
  }
  writeLines("4e) Edge Density")
  print(edge_density(g))
  
  writeLines("4f) Average Path Length")
  g.apl <- mean_distance(g, directed=mode_direct)
  print(mean_distance(g, directed=mode_direct))
  
  if(vertex_interest != ""){
    writeLines("4g) Make A Geodesic Distance from an Interest Variable")
    graph_vi <- make_ego_graph(g, diameter(g), nodes = vertex_interest, mode = c("all"))[[1]]
    
    # Get a vector of geodesic distances of all vertices from vertex 184 
    dists <- distances(graph_vi, vertex_interest)
    colors <- c("black", "red", "orange", "blue", "dodgerblue", "cyan")
    V(graph_vi)$color <- colors[dists+1]
    x11()
    plot(graph_vi, vertex.label = dists, 
         vertex.label.color = "white",
         vertex.label.cex = .6,
         edge.color = 'black',
         vertex.size = 7,
         edge.arrow.size = .05,
         main = paste0("Geodesic Distances from ",vertex_interest, "Variable")
    )
  }
  
  writeLines("===================== 5) Network Randomization with Erdos Renyi Game Randomization ============================")
  gl <- vector('list', network_randomization)
  assortativity_gl <- vector('list', network_randomization)
  values <<- 0
  if(vertex_special_relationship != "")
  {
    if(class(df[vertex_special_relationship]) == "factor"){
      eval_values <- paste0("as.numeric(factor(V(g1)$",vertex_special_relationship,"))")
      writeLines("Convert the gender attribute into a numeric value")
      values <<- eval(parse(text=eval_values))
    }
    else if(class(df[vertex_special_relationship]) != "factor"){
      values <<- as.numeric(unlist(df[vertex_special_relationship]))
    }
  }
  
  for(i in 1:network_randomization){
    writeLines(paste0("Done making",i, "Network Randomization"))
    gl[[i]] <- erdos.renyi.game(n = gorder(g), p.or.m = edge_density(g), 
                                type = "gnp")
    if(vertex_special_relationship != "" && length(values) > 1){
      assortativity_gl[[i]] <- assortativity(g, sample(values))
    }
  }
  
  if(make_plot==TRUE){
    x11()
    gl.apls <- unlist(lapply(gl, mean_distance, directed = mode_direct))
    hist(gl.apls)
    abline(v = g.apl, col = "red", lty = 3, lwd = 2)
    # Calculate the proportion of graphs with an average path length lower than our observed
    writeLines("Mean Proportion of Average Path Length Random Network that less than Observation Average Path Length")
    print(mean(gl.apls < g.apl))
  }
  
  writeLines("============================== 6) Network Sub Structure ========================================")
  writeLines("Vertex Attribute")
  print(vertex_attr(g))
  
  writeLines("6A) All Triangles that Exist in Network")
  print(matrix(triangles(g), nrow = 3))
  writeLines("Count the number of triangles exist on each Vertex Attribute.")
  count_triangles(g)
  
  writeLines("6B) Calculate the Global transitivity of the network.")
  g.tr <- transitivity(g)
  print(g.tr)
  
  writeLines("6C) Calculate the Local transitivity of the network.")
  print(transitivity(g, type = "local"))
  
  writeLines("6D) Calculate average transitivity of N random graphs from Previous Erdos Renyi Game Network Building")
  gl.tr <- lapply(gl, transitivity)
  gl.trs <- unlist(gl.tr)
  print(summary(gl.trs))
  
  if(make_plot==TRUE){
    x11()
    hist(gl.trs)
    abline(v = g.tr, col = "red", lty = 3, lwd = 2)
    writeLines("Mean Proportion of Transitivity Random Network that greater than Observation Transitivity")
    print(mean(gl.trs > g.tr))
  }
  
  writeLines("6E) Identify the largest cliques in the network")
  print(largest_cliques(g))
  
  writeLines("6F) Determine all maximal cliques in the network and assign to object 'clq'")
  clq <- max_cliques(g)
  print(clq)
  
  writeLines("Calculate the size of each maximal Clique.")
  print(table(unlist(lapply(clq, length))))
  
  index_table <- cumsum(table(unlist(lapply(clq, length))))
  n_Clique_temp <- 0
  if(make_plot==TRUE){
    for(v in 1:length(clq)){
      n_clique <- length(clq[[v]])
      if(n_Clique_temp == 0){
        n_Clique_temp <- n_clique
      }
      else{
        if(n_Clique_temp != n_clique && n_clique>=maximal_clique_length_plot){
          x11()
          range <- index_table[n_clique + 1] - index_table[n_clique] + 1
          if(range < 5){
            par(mfrow=c(2,2))
          }
          else if(range >= 5 && range<=10){
            par(mfrow=c(2,5))
          }
          else if(range > 10){
            xfrow <- 0
            if(range %% 5 == 0){
              xfrow <- range/5
              par(mfrow=c(xfrow,5))
            }
            else if(range %% 5 != 0){
              xfrow <- floor(range/5) + 1
              par(mfrow=c(xfrow,5))
            }
          }
          n_Clique_temp <- n_clique
        }
        else if(n_Clique_temp != n_clique && n_clique<maximal_clique_length_plot){
          n_Clique_temp <- n_clique
        }
      }
      gs <- as.undirected(subgraph(g, clq[[v]]))
      plot(gs, vertex.label.color = "black", 
           vertex.label.cex = 0.9,
           vertex.size = 0,
           edge.color = 'gray28',
           main = paste0("Largest Clique-",v),
           layout = layout.circle(gs)) 
      
    }
  }
  
  if(vertex_special_relationship != "")
  {
    writeLines("============================== 7) Network Special Relationship ========================================")
    
    writeLines("Assortativity Measures of Individual Interest to another Similiar Individual")
    writeLines("Negative Assortativity means Individual are avoiding/not Preferentially Similiar Individual")
    writeLines("Calculate the Assortativity of the network")
    print(assortativity(g, values))
    
    writeLines("Assortativity Degree Measures of Individual with High Degree Interest to another Similiar Individual of High Degree")
    writeLines("Negative Assortativity Degree means Individual are avoiding/not Preferentially to Similiar Individual of High Degree")
    print(assortativity.degree(g, directed = mode_direct))
    
    # Plot the distribution of assortativity values and add a red vertical line at the original observed value
    if(make_plot==TRUE){
      # Calculate the observed assortativity
      observed.assortativity <- assortativity(g, values)
      x11()
      hist(unlist(assortativity_gl))
      abline(v = observed.assortativity, col = "red", lty = 3, lwd=2)
    }
    
    if(mode_direct == TRUE){
      writeLines("Reciprocity of a Directed Network reflects the proportion of edges that are symmetrical")
      writeLines("Calculate the Reciprocity of the network")
      print(reciprocity(g)) 
    }
    else{
      writeLines("Reciprocity will not be calculated if Network is Undirected")
    }
    
    writeLines("================================= 8) Network Community Detection ===============================")
    community_algorithm <- apropos("community")
    for(comm in 1:length(community_algorithm)){
      x11()
      algo_eval <- paste0(community_algorithm[comm],"(g)")
      perform <- eval(parse(text=algo_eval))
      writeLines(paste0("Using ",community_algorithm[comm]," Community Algorithm Detection"))
      writeLines("Size of Community")
      print(sizes(perform))
      writeLines("Membership of Community")
      print(membership(g))
      writeLines("Modularity of Community")
      print(modularity(g, membership(perform)))
      if(make_plot==TRUE){
        plot(perform, g, title=paste0("Using ",community_algorithm[comm]," Community Algorithm Detection"))
      }
    }
  }
  else{
    writeLines("============================== 7) Reciprocity Relationship ========================================")
    if(mode_direct == TRUE){
      writeLines("Reciprocity of a Directed Network reflects the proportion of edges that are symmetrical")
      writeLines("Calculate the Reciprocity of the network")
      print(reciprocity(g)) 
    }
    else{
      writeLines("Reciprocity will not be calculated if Network is Undirected")
    }
    
    writeLines("================================= 8) Network Community Detection ===============================")
    
    community_algorithm <- apropos("community")
    for(comm in 1:length(community_algorithm)){
      x11()
      algo_eval <- paste0(community_algorithm[comm],"(g)")
      perform <- eval(parse(text=algo_eval))
      writeLines(paste0("Using ",community_algorithm[comm]," Community Algorithm Detection"))
      writeLines("Size of Community")
      print(sizes(perform))
      writeLines("Membership of Community")
      print(membership(g))
      writeLines("Modularity of Community")
      print(modularity(g, membership(perform)))
      if(make_plot==TRUE){
        plot(perform, g, title=paste0("Using ",community_algorithm[comm]," Community Algorithm Detection"))
      }
    }
  }
  return(g)
}

#============================== Test and example Datasets: http://snap.stanford.edu/data/#socnets ========================
library(dplyr)
network_data <- read.csv(file.choose())
network_data_2 <- read.csv(file.choose())
network_data_3 <- read.csv(file.choose())

network_data <- network_data %>%
  mutate_if(is.integer, as.character)
network_data_2 <- network_data_2 %>%
  mutate_if(is.integer, as.character)
network_data_3 <- network_data_3 %>%
  mutate_if(is.integer, as.character)

library(igraph)
library(arcdiagram)
test = read.graph(file="https://raw.githubusercontent.com/gastonstat/arcdiagram/master/lesmiserables.gml", format="gml")
g = read.graph(file="http://users.dimi.uniud.it/~massimo.franceschet/teaching/datascience/network/R/dolphin.gml", format="gml")

#============================== Correction #################
network_data <- network_data %>%
  mutate_if(is.factor, as.character)
network_data_2 <- network_data_2 %>%
  mutate_if(is.factor, as.character)
network_data_3 <- network_data_3 %>%
  mutate_if(is.factor, as.character)

test <- social_network_analysis(network_data, "node_1", "node_2", mode_direct = TRUE, 
                                vertice_n_step = 2, maximal_clique_length_plot = 5)

graph_vertex_names <- function(graph, inspect_details=FALSE){
  library(igraph)
  library(arcdiagram)
  return(names(vertex_attr(graph)))
}

arc_diagram <- function(graph_gml_object, label_var="label", group_var="group", value_var="value", 
                        inspect_component=FALSE, fill_var="", border_var=""){
  library(dplyr)
  library(igraph)
  library(graphics)
  library(arcdiagram)
  library(data.table)
  
  colors_node <- colors()
  colors_node <- colors_node[-which(colors_node %like% "grey")]
  colors_node <- colors_node[-which(colors_node %like% "gray")]
  
  edgelist = get.edgelist(graph_gml_object)
  vlabels = get.vertex.attribute(graph_gml_object, label_var)
  vgroups = get.vertex.attribute(graph_gml_object, group_var)
  n_unique_group <- length(unique(vgroups))
  unique_group <- unique(vgroups)
  colors_node <- sample(colors_node, n_unique_group, replace = FALSE)
  vborders <- rep("-", length(vlabels))
  vfill <- rep("-", length(vlabels))
  
  if(fill_var != ""){
    vfill <- get.vertex.attribute(graph_gml_object, fill_var)
  }
  else{
    if(n_unique_group > 1){
      v_unique_fill <- colors_node
      for(g in 1:n_unique_group){
        vfill[which(vgroups==unique_group[g])] <- v_unique_fill[g]
      }
    }
  }
  if(border_var != ""){
    vborders = get.vertex.attribute(graph_gml_object, border_var)
  }
  else{
    if(n_unique_group > 1){
      v_unique_borders <- colors_node
      for(g in 1:n_unique_group){
        vborders[which(vgroups==unique_group[g])] <- v_unique_borders[g]
      }
    }
  }
  
  degrees = degree(graph_gml_object)
  values = get.edge.attribute(graph_gml_object, value_var)
  
  if(inspect_component){
    writeLines("======== EdgeList =============")
    print(edgelist)
    writeLines("======== Labels ===============")
    print(vlabels)
    writeLines("======== Groups ===============")
    print(vgroups)
    writeLines("======== Fill =================")
    print(vfill)
    writeLines("======== Borders ==============")
    print(vborders)
    writeLines("======== Degrees ==============")
    print(degrees)
    writeLines("======== Values ===============")
    print(values)
    writeLines("")
    writeLines(paste0('This Graph contains ',length(vlabels), " Vertex + ", nrow(edgelist), " Edges and ",n_unique_group, " Unique Categorized Groups"))
    writeLines("")
  }
  
  library(reshape)
  x = data.frame(vgroups, degrees, vlabels, ind=1:vcount(graph_gml_object))
  y = arrange(x, desc(vgroups), desc(degrees))
  new_ord = y$ind
  
  if(inspect_component){
    writeLines("============= x object ================")
    print(head(x))
    writeLines("============= y object ================")
    print(head(y))
    writeLines("============ New Order ================")
    print(new_ord)
  }
  
  x11()
  # plot arc diagram
  arc <- arcplot(edgelist, ordering=new_ord, labels=vlabels, cex.labels=0.8,
                 show.nodes=TRUE, col.nodes=vborders, bg.nodes=vfill,
                 cex.nodes = log(degrees)+0.5, pch.nodes=21,
                 lwd.nodes = 2, line=-0.5,
                 col.arcs = hsv(0, 0, 0.2, 0.25), lwd.arcs = 1.5 * values)
  print(arc)
  legend("topleft", legend=unique(vgroups), col=unique(vfill), pch=19, cex=0.8)
  return(list(x,y,arc))
}

#============================== example usage of 2D Social Network with Arc Diagram ====================================

#example usage of graph_vertex_names
test_vertex <- graph_vertex_names(test)
g_vertex <- graph_vertex_names(g)

#example usage of arc diagram
arc_test <- arc_diagram(test, inspect_component = TRUE, fill_var="fill", border_var="border")
arc_g <- arc_diagram(g, inspect_component = TRUE, label_var="label", group_var="sex", value_var="id")

#label granulity = main or detail
hierarchial_edge_bundling_custom <- function(metanetwork, rad1 = .925, rad2 = 1, group_rad=0.5, edge_rad=0.875,
                                             sizeEdge = T,colPal = pal_insileco, type = 'all', 
                                             label_granulity="main", angle_precision=2,
                                             focus = NULL, colLinks = '#876b40', colShadow = '#f4f4f4',shadowEdge = T,
                                             cex_detail_text=1, filter_90_label=TRUE){
  library(igraph)
  library(graphicsutils)
  coordCircle <- function(theta = NULL, radius = 1) {
    data.frame(x = radius * cos(theta),
               y = radius * sin(theta))
  }
  bound <- function(metanetwork, gap = .025, addGap = T) {
    # Metanetwork list composed of "nodes" and "links"
    # Size of gap between groups on the graph
    # addGap logical whether to add gap or not
    nGroup <- as.data.frame(table(metanetwork$nodes$network))
    nGroup$Prop <- nGroup$Freq / sum(nGroup$Freq)
    nGroup$spanDeg <- 2 * pi * nGroup$Prop
    nGroup$upper <- nGroup$lower <- 0
    for(i in 2:nrow(nGroup)){
      nGroup$lower[i] <- nGroup$lower[i-1] + nGroup$spanDeg[i-1]
    }
    nGroup$upper <- nGroup$lower + nGroup$spanDeg
    
    if (addGap) {
      nGroup$lower <- nGroup$lower + gap/2
      nGroup$upper <- nGroup$upper - gap/2
    }
    nGroup
  }
  nodePos <- function(metanetwork, edgeRad = 0.975, groupRad = 0.5, gapEdge = 0.1, addGap = T) {
    # Add x and y columns to nodes and networkGroup data
    metanetwork$nodes$y <- metanetwork$nodes$x <- 0
    metanetwork$networkGroup$y <- metanetwork$networkGroup$x <- 0
    
    # Get coordinates for all networks
    for(i in 1:nrow(metanetwork$networkGroup)) {
      # Distribute points within each network space
      edgeDeg <- seq((metanetwork$networkGroup$lower[i] + (gapEdge/2)),
                     (metanetwork$networkGroup$upper[i] - (gapEdge/2)),
                     length = metanetwork$networkGroup$Freq[i])
      # Get position for each edge
      nodePos <- coordCircle(theta = edgeDeg, radius = edgeRad)
      # Add to nodes data
      metanetwork$nodes$x[metanetwork$nodes$network == metanetwork$networkGroup$Var1[i]] <- nodePos$x
      metanetwork$nodes$y[metanetwork$nodes$network == metanetwork$networkGroup$Var1[i]] <- nodePos$y
      # Distribute network groups in space
      groupDeg <- mean(c(metanetwork$networkGroup$lower[i],metanetwork$networkGroup$upper[i]))
      # Get position for each group
      groupPos <- coordCircle(theta = groupDeg, radius = groupRad)
      # Add to group data
      metanetwork$networkGroup$x[i] <- groupPos$x
      metanetwork$networkGroup$y[i] <- groupPos$y
    }
    metanetwork
  }
  colGroups <- function(metanetwork, colPal = pal_insileco) {
    # Group colors
    metanetwork$networkGroup$cols <- colPal[1:nrow(metanetwork$networkGroup)]
    # Node colors
    metanetwork$nodes$cols <- NA
    for(i in 1:nrow(metanetwork$networkGroup)) {
      metanetwork$nodes$cols[metanetwork$nodes$network == metanetwork$networkGroup$Var1[i]] <- metanetwork$networkGroup$cols[i]
    }
    metanetwork
  }
  nodeSize <- function(metanetwork, freq = T) {
    if (isTRUE(freq)) {
      nLink <- as.data.frame(table(c(metanetwork$links$from, metanetwork$links$to)), stringsAsFactors = F)
      colnames(nLink)[1L] <- 'name'
      metanetwork$nodes <- dplyr::left_join(metanetwork$nodes, nLink, by = 'name')
      metanetwork$nodes$cex <- (metanetwork$nodes$Freq / max(metanetwork$nodes$Freq))
    } else {
      metanetwork$nodes$cex <- .33
    }
    return(metanetwork)
  }
  
  #======================================= improved features ============================================
  text_angle_group <- function(metanetwork, angle_precision=2, filter_90=FALSE){
    angle <- c() #is between -90 and 90
    angle <- round(seq(0, 360, length.out=nrow(metanetwork$nodes)), angle_precision)
    if(filter_90){
      angle <- ifelse(angle > 90 & angle <270, angle-180, angle)
      angle <- ifelse(angle > 270 & angle <=360, angle-360, angle)
    }
    metanetwork$nodes$angle <- NA
    sum_node <- 0 
    for(i in 1:nrow(metanetwork$networkGroup)) {
      node_index <- which(metanetwork$nodes$network==metanetwork$networkGroup$Var1[i])
      total_individual_nodes <- length(node_index)
      metanetwork$nodes$angle[node_index] <- angle[(sum_node+1):(sum_node+total_individual_nodes)]
      sum_node <- sum_node + total_individual_nodes
    }
    return(metanetwork)
  }
  boxGroup <- function(metanetwork, rad1 = .95, rad2 = 1, colBox = NULL, names = NULL, 
                       colNames = NULL, addNames = "main", cex_text=1, ...) {
    # metanetwork = data list composed of 'nodes', 'links' & 'networkGroup'
    # rad1 = lower boundary for polygons
    # rad2 = upper boundary for polygons
    # colBox = color of boxes
    # names = names of individual networks
    # colNames = color of names
    # addNames = logical, add names of networks to graph
    if (!is.null(colNames) & length(colNames) == 1) {
      colNames <- rep(colNames, nrow(metanetwork$links))
    }
    
    if (!is.null(colBox) & length(colBox) == 1) {
      colBox <- rep(colBox, nrow(metanetwork$links))
    }
    
    for(i in 1:nrow(metanetwork$networkGroup)) {
      a <- coordCircle(theta = seq(metanetwork$networkGroup$lower[i],
                                   metanetwork$networkGroup$upper[i],
                                   length = 200),
                       radius = rad1)
      
      b <- coordCircle(theta = seq(metanetwork$networkGroup$upper[i],
                                   metanetwork$networkGroup$lower[i],
                                   length = 200),
                       radius = rad2)
      
      polygon(rbind(a, b, a[1L,]), col = colBox[i], ...)
      
      if (addNames=="main") {
        middle <- mean(c(metanetwork$networkGroup$lower[i],
                         metanetwork$networkGroup$upper[i]))
        clockwise <- if (middle > pi) F else T
        plotrix::arctext(x = as.character(metanetwork$networkGroup$Var1[i]),
                         radius = mean(c(rad1,rad2)),
                         middle = middle,
                         col = colNames[i],
                         clockwise = clockwise,
                         font = 2)
      }
      else if(addNames=="detail"){
        individual_nodes <- metanetwork$nodes[which(metanetwork$nodes$network==metanetwork$networkGroup$Var1[i]),]
        for(h in 1:nrow(individual_nodes)){
          if(cex_text < 1){
            text(individual_nodes$x[h], individual_nodes$y[h], 
                 individual_nodes$name[h], srt = individual_nodes$angle[h], 
                 col=colNames[i], cex=cex_text)
          }
          else if(cex_text >= 1){
            text(individual_nodes$x[h], individual_nodes$y[h], 
                 individual_nodes$name[h], srt = individual_nodes$angle[h], col=colNames[i])
          }
        }
      }
    }
  }
  
  # metanetwork = list composed of 'nodes', 'links' and 'networkGroup'
  # type        = type of colors:
  #                 'all' = all links with single color = `colLinks`
  #                 'focus' = focus on the links of identified network
  # focus       = character, name of network(s) to focus on;
  #                 if length(focus) == 1, all links towards a single network
  #                 if length(focus) > 1, links focused on identified networks
  # colLinks    = color of links of `type` == 'all'
  # colShadow   = color of links that we are not focused on
  
  linkCol <- function(metanetwork, type = 'all', focus = NULL, colLinks = '#876b40', colShadow = '#f4f4f4') {
    if (type == 'all') {
      metanetwork$links$cols <- colLinks
    }
    
    if (type == 'focus' & length(focus) == 1) {
      # Box colors
      focusID <- metanetwork$networkGroup$Var1 %in% focus
      colBox <- metanetwork$networkGroup$cols
      metanetwork$networkGroup$cols[!focusID] <- colShadow
      metanetwork$networkGroup$colNames <- colBox
      metanetwork$networkGroup$colNames[!focusID] <- colShadow
      
      # Link colors
      # metanetwork$links$cols <- paste0(colShadow, 88)
      metanetwork$links$cols <- colShadow
      linkCol <- data.frame(from = metanetwork$nodes$network[match(metanetwork$links$from,
                                                                   metanetwork$nodes$name)],
                            to = metanetwork$nodes$network[match(metanetwork$links$to,
                                                                 metanetwork$nodes$name)],
                            stringsAsFactors = F)
      
      linkID <- linkCol$from %in% focus & linkCol$to %in% focus
      metanetwork$links$cols[linkID] <- metanetwork$networkGroup$cols[focusID] # "cannibalism"
      
      linkID <- (linkCol$from %in% focus | linkCol$to %in% focus) & !linkID
      cols <- paste0(linkCol$from[linkID], linkCol$to[linkID])
      cols <- gsub(focus, '', cols)
      cols <- match(cols, metanetwork$networkGroup$Var1)
      cols <- metanetwork$networkGroup$colNames[cols]
      metanetwork$links$cols[linkID] <- cols
    }
    
    if (type == 'focus' & length(focus) > 1) {
      # Box colors
      focusID <- metanetwork$networkGroup$Var1 %in% focus
      colBox <- metanetwork$networkGroup$cols
      metanetwork$networkGroup$cols[!focusID] <- colShadow
      metanetwork$networkGroup$colNames <- colBox
      metanetwork$networkGroup$colNames[!focusID] <- colShadow
      
      # Link colors
      metanetwork$links$cols <- colShadow
      linkCol <- data.frame(from = metanetwork$nodes$network[match(metanetwork$links$from,
                                                                   metanetwork$nodes$name)],
                            to = metanetwork$nodes$network[match(metanetwork$links$to,
                                                                 metanetwork$nodes$name)],
                            stringsAsFactors = F)
      
      linkID <- linkCol$from %in% focus & linkCol$to %in% focus
      metanetwork$links$cols[linkID] <- colLinks
    }
    
    # Add transparency
    metanetwork$links$cols <- paste0(metanetwork$links$cols, '66')
    
    metanetwork
  }
  
  metanetwork$networkGroup <- bound(metanetwork)
  metanetwork <- nodePos(metanetwork, edgeRad = edge_rad, groupRad = group_rad)
  metanetwork <- colGroups(metanetwork, colPal = colPal)
  metanetwork <- nodeSize(metanetwork, freq = sizeEdge)
  metanetwork <- linkCol(metanetwork, type = type, focus = focus, 
                         colLinks = colLinks, colShadow = colShadow)
  
  if(label_granulity=="detail"){
    metanetwork <- text_angle_group(metanetwork, angle_precision = angle_precision, filter_90=filter_90_label)
    #metanetwork <- x_y_pos_transformation(metanetwork, xy_transform, mean_smoothing = mean_smoothing)
  }
  # Plot
  par(mar = c(0,0,0,0))
  plot0()
  boxGroup(metanetwork,
           rad1 = rad1,
           rad2 = rad2,
           colBox = metanetwork$networkGroup$cols,
           colNames = metanetwork$networkGroup$colNames,
           border = 'transparent',
           addNames = label_granulity,
           cex_text = cex_detail_text)
  plotLinks(metanetwork, col = metanetwork$links$cols)
  
  if(shadowEdge){
    points(metanetwork$nodes$x,
           metanetwork$nodes$y,
           pch = 20,
           cex = (metanetwork$nodes$cex * 5),
           col = '#d7d7d7')
  }
  
  points(metanetwork$nodes$x,
         metanetwork$nodes$y,
         pch = 20,
         cex = (metanetwork$nodes$cex * 3),
         col = metanetwork$nodes$cols)
  return(metanetwork)
}

#============================== example usage of Hierarchial Edge Bundling ====================================

# The data
metanetwork <- vector('list', 0)
metanetwork$nodes <- nodes
metanetwork$links <- links

# The plot
x11()
h1 <- hierarchial_edge_bundling_custom(metanetwork)
h2 <- hierarchial_edge_bundling_custom(metanetwork, type = 'focus', focus = 'Species')
h3 <- hierarchial_edge_bundling_custom(metanetwork, type = 'focus', focus = c('Species', 'Drivers'))
h4 <- hierarchial_edge_bundling_custom(metanetwork, type = 'focus', focus = c('Species', 'Drivers', 'Managers'))

#The Detailed Plot
#group rad -> control of the link radian, the less value the less radian of link graph pattern
#edge rad -> control of the edge radian between label and group polygon, the more value, 
#the more white gap produces between polygon and label

x11()
h5 <- hierarchial_edge_bundling_custom(metanetwork, rad1 = 0.6, rad2 = 0.7, group_rad=0.5, edge_rad=0.875,
                                       angle_precision = 3, label_granulity="detail", cex_detail_text = 1)
x11()
h6 <- hierarchial_edge_bundling_custom(metanetwork, rad1 = 0.6, rad2 = 0.7, group_rad=0.01, edge_rad=0.825,
                                       angle_precision = 3, label_granulity="detail", cex_detail_text = 1)
x11()
h7 <- hierarchial_edge_bundling_custom(metanetwork, rad1 = 0.5, rad2 = 0.6, angle_precision = 3,
                                       label_granulity="detail", cex_detail_text = 0.9,
                                       type='focus', focus=c('Species','Drivers'))

#-------------------------------------------------------------------------------------------------------------------------
########################### 21) Machine Learning Automation with Caret  ##################################################
#-------------------------------------------------------------------------------------------------------------------------

caret_mlr_h2o_loadlearners <- function(){
  library(caret)
  library(dplyr)
  library(mlr)
  writeLines("Load List Learners of Caret")
  #https://rdrr.io/cran/caret/man/models.
  #http://topepo.github.io/caret/available-models. (number 6)
  caret_topepo_learners <- read.csv("C:/Users/User/Desktop/Data Science Journey/CARET & MLR Machine Learning/caret_topepo_model.csv")
  writeLines("Load List Learners of MLR")
  mlr_learners <- listLearners() %>%
    as.data.frame() %>%
    select(class, name, short.name, package, type) 
  writeLines("Load List Learners of H2O")
  #https://docs.h2o.ai/h2o/latest-stable/h2o-docs/training-models.
  h2o_learners <- read.csv("C:/Users/User/Desktop/Data Science Journey/H2O Machine Learning/h2o_learning_list.csv")
  return(list(caret_topepo_learners, mlr_learners, h2o_learners))
}

#one time usage (install everything)
#Sys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS="true") Don't display warning from package dependencies
caret_mlr_install_library <- function(caret_learner, mlr_learner, install_special=FALSE){
  library(devtools)
  library(BiocManager)
  installed_package <<- rownames(installed.packages())
  unique_package_caret <- unique(caret_learner$Libraries)
  unique_package_mlr <- unique(mlr_learner$package)
  writeLines("Install ML Package for All Caret Available Methods")
  for(caret_row in 1:length(unique_package_caret)){
    if(grepl(",",unique_package_caret[caret_row])){
      package_name <- unlist(strsplit(unique_package_caret[caret_row], ", "))
      for(list_package in 1:length(package_name)){
        if(package_name[list_package] %in% installed_package == FALSE){
          writeLines(paste("Installing",package_name[list_package],". . ."))
          install.packages(package_name[list_package])
          writeLines("Refresh Installed Package List")
          installed_package <<- rownames(installed.packages())
        }
      }
    }
    else{
      if(unique_package_caret[caret_row] %in% installed_package == FALSE){
        writeLines(paste("Installing",unique_package_caret[caret_row],". . ."))
        install.packages(unique_package_caret[caret_row])
        writeLines("Refresh Installed Package List")
        installed_package <<- rownames(installed.packages())
      }
    }
  }
  writeLines("install ML Package for All MLR Available Methods")
  for(mlr_row in 1:length(unique_package_mlr)){
    if(grepl(",",unique_package_mlr[mlr_row])){
      package_name <- unlist(strsplit(unique_package_mlr[mlr_row], ","))
      for(list_package in 1:length(package_name)){
        if(package_name[list_package] %in% installed_package == FALSE){
          writeLines(paste("Installing",package_name[list_package],". . ."))
          install.packages(unique_package_caret[caret_row])
          writeLines("Refresh Installed Package List")
          installed_package <<- rownames(installed.packages())
        }
      }
    }
    else{
      if(unique_package_mlr[mlr_row] %in% installed_package == FALSE){
        writeLines(paste("Installing",unique_package_caret[caret_row],". . ."))
        install.packages(unique_package_mlr[mlr_row])
        writeLines("Refresh Installed Package List")
        installed_package <<- rownames(installed.packages())
      }
    }
  }
  if(install_special){
    library(renv)
    writeLines("Install special packages")
    install_url("https://cran.r-project.org/src/contrib/Archive/adaptDA/adaptDA_1.0.tar.gz")
    install_url("https://cran.r-project.org/src/contrib/Archive/elmNN/elmNN_1.0.tar.gz")
    BiocManager::install("logicFS")
    install_github('ramhiser/sparsediscrim')
    install_url("https://cran.r-project.org/src/contrib/Archive/kerndwd/kerndwd_2.0.2.tar.gz")
    BiocManager::install("gpls")
    BiocManager::install("rrcovHD")
    BiocManager::install("rrlda")
    install_url("https://cran.r-project.org/src/contrib/Archive/rrlda/rrlda_1.1.tar.gz")
    install.packages("rotationForest")
    install.packages("https://s3.ca-central-1.amazonaws.com/jeremiedb/share/mxnet/CPU/3.6/mxnet.zip", repos = NULL)
    install_url("https://cran.r-project.org/src/contrib/Archive/sdwd/sdwd_1.0.3.tar.gz")
    install_url("https://cran.r-project.org/src/contrib/Archive/foba/foba_0_1.tar.gz")
    BiocManager::install("vbmp")
    install_url("https://cran.r-project.org/src/contrib/Archive/classiFunc/classiFunc_0.1.1.tar.gz")
    install.packages("wsrf")
    renv::install("flare@1.6.0")
    renv::install("FCNN4R@0.6.2")
    #For Robust Discriminant Analysis in Caret, there is additional packages installs
    install.packages("VIM")
    install.packages("mvoutlier")
    #flare and FCNN4R cannot be installed (still search answers) 
  }
}

#---------------------------------- CARET and MLR First initialization (install library) -------------------------------
library(dplyr)
library(data.table)
learners <- caret_mlr_h2o_loadlearners()
caret_topepo <- learners[[1]]
mlr_learner <- learners[[2]]
h2o_learner <- learners[[3]]
caret_mlr_install_library(caret_topepo, mlr_learner)

caret_method_tester <- function(dummy_data, indep_var="", dep_var="", formula, 
                                resample_plan=1, task="Regression",
                                test_method, caret_model, time_limit=300, 
                                grid_param=c(), parallel_mode=FALSE, 
                                transform_matrix=FALSE, string_kernel=FALSE, 
                                method_to_train="formula",numeric_x=FALSE, log=FALSE){
  library(caret)
  library(R.utils)
  library(dplyr)
  if(string_kernel){
    dummy_data <- dummy_data %>%
      mutate_if(is.numeric, as.character)
    dummy_data <- dummy_data %>%
      mutate_if(is.integer, as.character)
    dummy_data <- dummy_data %>%
      mutate_if(is.factor, as.character)
  }
  dummy_x <- NULL
  dummy_y <- NULL
  dependent_var <- NULL
  independent_var <- NULL
  if(indep_var == "" && dep_var == ""){
    dependent_var <- strsplit(formula, " ~ ")[[1]][1]
    independent_var <- colnames(dummy_data)[which(colnames(dummy_data) != dependent_var)]
    dummy_x <- dummy_data[,-which(colnames(dummy_data) == dependent_var)]
    dummy_y <- dummy_data[,which(colnames(dummy_data) == dependent_var)]
  }
  else{
    dummy_x <- dummy_data[,which(colnames(dummy_data) == indep_var)]
    dummy_y <- dummy_data[,which(colnames(dummy_data) == dep_var)]
  }
  if(transform_matrix){
    if(indep_var == "" && dep_var == ""){
      dummy_x <- as.matrix(dummy_x)
      colnames(dummy_x) <- colnames(dummy_data[,-which(colnames(dummy_data) == dependent_var)])
      dummy_y <- as.matrix(dummy_y)
      colnames(dummy_y) <- dependent_var
    }
    else{
      dummy_x <- as.matrix(dummy_x)
      colnames(dummy_x) <- indep_var
      dummy_y <- as.matrix(dummy_y)
      colnames(dummy_y) <- dep_var
    }
  }
  if(numeric_x){
    dummy_x <- dummy_x %>%
      mutate_if(is.factor, as.numeric)
    dummy_x <- dummy_x %>%
      mutate_if(is.integer, as.numeric)
  }
  if(log){
    writeLines("Dependent Variable to test")
    print(head(dummy_y))
    writeLines("Independent Variable to test")
    print(head(dummy_x))
    writeLines("Independent Variable X")
    writeLines(paste0("is.vector = ", is.vector(dummy_x)))
    writeLines(paste0("is.data.frame = ", is.data.frame(dummy_x)))
    writeLines(paste0("is.matrix = ", is.matrix(dummy_x)))
    writeLines(paste0("is.numeric = ", is.numeric(dummy_x)))
    writeLines(paste0("is.character = ", is.character(dummy_x)))
    writeLines("Dependent Variable Y")
    writeLines(paste0("is.vector = ", is.vector(dummy_y)))
    writeLines(paste0("is.data.frame = ", is.data.frame(dummy_y)))
    writeLines(paste0("is.matrix = ", is.matrix(dummy_y)))
    writeLines(paste0("is.numeric = ", is.numeric(dummy_y)))
    writeLines(paste0("is.character = ", is.character(dummy_y)))  
  }
  formula <- as.formula(formula)
  resampling <- NULL
  if(resample_plan==0){
    resampling <- trainControl(allowParallel = parallel_mode)
  }
  else if(resample_plan==1){
    resampling <- trainControl(method = "repeatedcv",
                               number = 10,
                               repeats = 5,
                               allowParallel = parallel_mode) 
  }
  else if(resample_plan==2){
    resampling <- trainControl(method = "cv",
                               number = 5,
                               allowParallel = parallel_mode) 
  }
  else if(resample_plan==3){
    resampling <- trainControl(method = "adaptive_cv",
                               number = 10, repeats = 5,
                               allowParallel = parallel_mode,
                               adaptive = list(min = 3, alpha = 0.05, 
                                               method = "BT", complete = FALSE))
  }
  else if(resample_plan==4){
    resampling <- trainControl(method = "boot",
                               number = 5,
                               allowParallel = parallel_mode)
  }
  else if(resample_plan==5){
    resampling <- trainControl(method = "boot_all",
                               number = 5,
                               allowParallel = parallel_mode)
  }
  model<<-NULL
  tryCatch(
    expr={
      if(method_to_train=="formula"){
        if(length(grid_param) > 0){
          model <<- caret::train(formula, 
                                 data = dummy_data, 
                                 method = test_method, 
                                 trControl = resampling,
                                 tuneGrid=grid_param)
        }
        else{
          model <<- caret::train(formula, 
                                 data = dummy_data, 
                                 method = test_method, 
                                 trControl = resampling) 
        }
      }
      else if(method_to_train=="xy"){
        if(length(grid_param) > 0){
          model <<- caret::train(x=dummy_x,
                                 y=dummy_y, 
                                 method = test_method, 
                                 trControl = resampling,
                                 tuneGrid=grid_param)
          
        }
        else{
          model <<- caret::train(x=dummy_x,
                                 y=dummy_y, 
                                 method = test_method, 
                                 trControl = resampling)
        }
      }
      return(model)
    },
    error=function(cond){
      message("Test Model Failed")
      message("Here's the original error message:")
      message(cond)
      return(NULL)
    }
  )
}

#predictor_list can be list of list containing every predictor column data
caret_string_kernel_tester <- function(response_list, predictor_list, predictor_name,
                                       method_string_kernel, resample_plan=2, parallel_mode=FALSE){
  library(kernlab)
  library(caret)
  if(resample_plan==0){
    resampling <- trainControl(allowParallel = parallel_mode)
  }
  else if(resample_plan==1){
    resampling <- trainControl(method = "repeatedcv",
                               number = 10,
                               repeats = 5,
                               allowParallel = parallel_mode) 
  }
  else if(resample_plan==2){
    resampling <- trainControl(method = "cv",
                               number = 5,
                               allowParallel = parallel_mode) 
  }
  else if(resample_plan==3){
    resampling <- trainControl(method = "adaptive_cv",
                               number = 10, repeats = 5,
                               allowParallel = parallel_mode,
                               adaptive = list(min = 3, alpha = 0.05, 
                                               method = "BT", complete = FALSE))
  }
  else if(resample_plan==4){
    resampling <- trainControl(method = "boot",
                               number = 5,
                               allowParallel = parallel_mode)
  }
  else if(resample_plan==5){
    resampling <- trainControl(method = "boot_all",
                               number = 5,
                               allowParallel = parallel_mode)
  }
  
  convert_list_to_matrix <- function(big_list, list_name=c()){
    matrix_bind <- NULL
    for(x in 1:length(big_list)){
      matrix_bind <- cbind(matrix_bind, big_list[[x]])
    }
    colnames(matrix_bind) <- list_name
    return(matrix_bind)
  }
  
  tryCatch(
    expr={
      matrix_predictor <- convert_list_to_matrix(predictor_list, predictor_name)
      string_kernel_model = train(x=matrix_predictor, y=response_list,
                                  method=method_string_kernel,
                                  trControl=resampling)
      return(string_kernel_model)
    },
    error = function(cond){
      message("Test Model Failed")
      message("Here's the original error message:")
      message(cond)
      return(NULL)
    }
  )
}

#data_type="continous", "numeric", "string", ""
autocaret_data <- function(df, tasking="classification", data_type="common", 
                           model_method="formula", model_formula, model_name_save_prefix=""){
  
  library(plyr)
  library(h2o)
  h2o.init()
  writeLines("Load Tensorflow and Keras Packages first for making a Keras Machine Learning")
  library(tensorflow)
  library(devtools)
  library(keras)
  library(reticulate)
  
  long_exec_model <- c(2,4,5,6,10,11,12,47,49,50,51,56,57,58,60,61,62,73,76,81,
                       119,120,121,122,133,134,135,139,140,141,142,159,166,168,
                       172,176,177,181,182,204,210,217,236,237)
  unknown_sol_model <- c(13, 59, 91, 185)
  big_ram_model <- c(35,36,37)
  crashed_model <- c(169)
  string_kernel_classif <- c(219,221,228)
  logistic_classif <- c(104)
  multilogistic_classif <- c(144, 236)
  numeric_classif <- c(69,112,131,132,189,191,235)
  continous_classif <- c(63,64,65,103,194)
  continous_regr <- c(21,22,25,26,63,64,65,70,71,117,184,190,235)
  xy_modeling <- c(34,108,109,127,128,231,232,233,234)
  nonnormal_model <- c(long_exec_model, unknown_sol_model, big_ram_model, crashed_model,
                       string_kernel_classif, logistic_classif, multilogistic_classif,
                       numeric_classif, continous_classif, continous_regr, xy_modeling)
  all_model <- 1:238
  normal_model <- all_model[-nonnormal_model]
  executable_model <- c()
  if(data_type=="common"){
    if(model_method=="xy"){
      executable_model <- c(executable_model, xy_modeling)
      executable_model <- sort(executable_model)
    }
    else{
      executable_model <- c(executable_model, normal_model)
      executable_model <- sort(executable_model)
    }
  }
  else if(data_type=="continous"){
    if(tasking=="regression"){
      executable_model <- c(executable_model, continous_regr)
      executable_model <- sort(executable_model)
    }
    else if(tasking=="classification"){
      executable_model <- c(executable_model, continous_classif)
      executable_model <- sort(executable_model)
    }
  }
  else if(data_type=="numeric"){
    if(tasking=="regression"){
      executable_model <- c(executable_model, normal_model)
      executable_model <- sort(executable_model)
    }
    else if(tasking=="classification"){
      executable_model <- c(executable_model, numeric_classif)
      executable_model <- sort(executable_model)
    }
  }
  else if(data_type=="logistic"){
    if(tasking=="classification"){
      executable_model <- c(executable_model, logistic_classif)
      executable_model <- sort(executable_model)
    }
    else{
      writeLines("There are no models for Regression Tasking with data type of logistics")
    }
  }
  else if(data_type=="multilogistic"){
    if(tasking=="classification"){
      executable_model <- c(executable_model, multilogistic_classif)
      executable_model <- sort(executable_model)
      writeLines("Multinomial Logistic in Caret needs xy modeling, automatically change modeling to xy")
      model_method <- "xy"
    }
    else{
      writeLines("There are no models for Regression Tasking with data type of multilogistics")
    }
  }
  else if(data_type=="string"){
    if(tasking=="classification"){
      executable_model <- c(executable_model, string_kernel_classif)
      executable_model <- sort(executable_model)
    }
    else{
      writeLines("There are no models for Regression Tasking with data type of multilogistics")
    }
  }
  if(length(executable_model) > 0){
    if(model_method=="xy"){
      for(exec_model in 1:length(executable_model)){
        writeLines(paste("Execute Model",exec_model,"of",nrow(caret_topepo)))
        model <- caret_method_tester(dummy_data=df, formula=model_formula,
                                     test_method=as.character(caret_topepo$Method[exec_model]),
                                     caret_model=caret_topepo, time_limit=300,
                                     resample_plan=2, parallel_mode=FALSE,
                                     indep_var="", dep_var="",
                                     transform_matrix=FALSE, string_kernel=FALSE,
                                     method_to_train="xy",log=FALSE)
        writeLines("Inspect Model")
        print(model)
        if(is.null(model) == FALSE){
          library(stringr)
          filename <- paste0(model_name_save_prefix, "model",exec_model,"_",
                             as.character(caret_topepo$Method[exec_model]))
          filename <- str_replace_all(filename, "\\.", "_")
          saveRDS(model, paste0(filename,".rds"))
        }
      }
    }
    else if(model_method=="formula"){
      for(exec_model in 1:length(executable_model)){
        writeLines(paste("Execute Model",exec_model,"of",nrow(caret_topepo)))
        model <- caret_method_tester(dummy_data=df, formula=model_formula,
                                     test_method=as.character(caret_topepo$Method[exec_model]),
                                     caret_model=caret_topepo, time_limit=300,
                                     resample_plan=2, parallel_mode=FALSE,
                                     indep_var="", dep_var="",
                                     transform_matrix=FALSE, string_kernel=FALSE,
                                     method_to_train="formula",log=FALSE)
        writeLines("Inspect Model")
        print(model)
        if(is.null(model) == FALSE){
          library(stringr)
          filename <- paste0(model_name_save_prefix, "model",exec_model,"_",
                             as.character(caret_topepo$Method[exec_model]))
          filename <- str_replace_all(filename, "\\.", "_")
          saveRDS(model, paste0(filename,".rds"))
        }
      }
    }
    else if(data_type=="string"){
      for(exec_model in 1:length(executable_model)){
        writeLines(paste("Execute Model ",exec_model,"of",nrow(caret_topepo)))
        dependent_var <- strsplit(model_formula, " ~ ")[[1]][1]
        independent_var <- colnames(df)[which(colnames(df) != dependent_var)]
        predictor_lists <- list()
        for(g in 1:length(independent_var)){
          predictor_lists <- c(predictor_lists, list(df[[independent_var[g]]]))
        }
        model_string <- caret_string_kernel_tester(response_list=df[[dependent_var]], 
                                                   predictor_list=predictor_lists, 
                                                   predictor_name = independent_var, 
                                                   method_string_kernel=as.character(caret_topepo$Method[exec_model]), 
                                                   resample_plan=2)
        writeLines("Inspect Model")
        print(model)
        if(is.null(model) == FALSE){
          library(stringr)
          filename <- paste0(model_name_save_prefix, "model",exec_model,"_",
                             as.character(caret_topepo$Method[exec_model]))
          filename <- str_replace_all(filename, "\\.", "_")
          saveRDS(model_string, paste0(filename,".rds"))
        }
      }
    }
  }
}

#C:/Users/User/Documents/CARET Model
caret_model_benchmarking_df <- function(file_path, sorted=TRUE, regr_priority="RMSE"){
  model_classif_vec <- c()
  model_regr_vec <- c()
  model_regr_index_vec <- c()
  model_classif_index_vec <- c()
  modelrmse_result_vec <- c()
  modelmae_result_vec <- c()
  modelacc_result_vec <- c()
  
  if(length(list.files(file_path)) > 0){
    file_complete_path <- paste0(file_path, "/", list.files(file_path))
    for(h in 1:length(file_complete_path)){
      writeLines(paste0("Model no ", h))
      read_model <- readRDS(file_complete_path[h])
      print(read_model)
      if(read_model$modelType == "Regression"){
        modelname <- read_model$method
        modelrmse_result <- read_model$results$RMSE
        modelmae_result <- read_model$results$MAE
        print(paste("Modelname :", modelname))
        print(paste("ModelRMSE :", modelrmse_result))
        print(paste("ModelMAE :", modelmae_result))
        
        if(length(modelmae_result) > 1){
          modelmae_result <- min(modelmae_result, na.rm=TRUE)
        }
        if(length(modelrmse_result) > 1){
          modelrmse_result <- min(modelrmse_result, na.rm=TRUE)
        }
        model_regr_vec <- c(model_regr_vec, modelname)
        modelrmse_result_vec <- c(modelrmse_result_vec, modelrmse_result)
        modelmae_result_vec <- c(modelmae_result_vec, modelmae_result)
        model_regr_index_vec <- c(model_regr_index_vec, h)
      }
      else if(read_model$modelType == "Classification"){
        modelname <- read_model$method
        modelacc_result <- read_model$results$Accuracy
        print(paste("Modelname :", modelname))
        print(paste("ModelACC :", modelacc_result))
        
        if(length(modelacc_result) > 1){
          modelacc_result <- max(modelacc_result, na.rm=TRUE)
        }
        model_classif_vec <- c(model_classif_vec, modelname)
        modelacc_result_vec <- c(modelacc_result_vec, modelacc_result)
        model_classif_index_vec <- c(model_classif_index_vec, h)
      }
    }
    regr_benchmark_model <- data.frame(index = model_regr_index_vec,
                                       model_name=model_regr_vec, 
                                       rmse_value=modelrmse_result_vec, 
                                       mae_value = modelmae_result_vec)
    classif_benchmark_model <- data.frame(index = model_classif_index_vec,
                                          model_name=model_classif_vec,
                                          acc_value = modelacc_result_vec)
    if(sorted){
      if(regr_priority == "RMSE"){
        regr_benchmark_model <- regr_benchmark_model[order(regr_benchmark_model$rmse_value),]
      }
      else if(regr_priority == "MAE"){
        regr_benchmark_model <- regr_benchmark_model[order(regr_benchmark_model$mae_value),]
      }
      classif_benchmark_model <- classif_benchmark_model[order(classif_benchmark_model$acc_value, decreasing = TRUE), ]
    }
    rownames(regr_benchmark_model) <- NULL
    rownames(classif_benchmark_model) <- NULL
    return(list(regr_benchmark_model, classif_benchmark_model))
  }
  else{
    writeLines("No File found in specified directory!")
  }
}

#-------------------------------------------------------------------------------------------------------------------------
########################### 22) Machine Learning Automation with MLR  ####################################################
#-------------------------------------------------------------------------------------------------------------------------
mlr_hyperparam_to_csv <- function(mlr_learner, from=1, to=171, log=FALSE){
  library(data.table)
  library(stringr)
  library(mlr)
  library(stringi)
  mlr_hyperparam_df <- data.frame(ml_name = NA,
                                  param_name = NA,
                                  param_type = NA,
                                  param_default = NA,
                                  param_tunable = NA,
                                  param_range_values = NA)
  mlr_hyperparam_df <- mlr_hyperparam_df[-1,]
  for(x in from:to){
    writeLines("=============================================")
    writeLines(paste0("ML ",x, " : ", mlr_learner$class[x]))
    writeLines("=============================================")
    if(length(getParamSet(mlr_learner$class[x])$pars)==0){
      hyperparam_partition <- data.frame(ml_name = mlr_learner$class[x],
                                         param_name = NA,
                                         param_type = NA,
                                         param_default = NA,
                                         param_tunable = NA,
                                         param_range_values = NA)
      mlr_hyperparam_df <- rbind(mlr_hyperparam_df, hyperparam_partition)
    }
    else{
      param_details <- getParamSet(mlr_learner$class[x])
      hyperparam_name <- names(param_details$pars)
      writeLines("Hyperparam name")
      print(hyperparam_name)
      hyperparam_type <- as.character(sapply(param_details$pars, "[[", "type"))
      hyperparam_default <- as.character(sapply(param_details$pars, "[[", "default"))
      hyperparam_tunable <- as.character(sapply(param_details$pars, "[[", "tunable"))
      hyperparam_num_int_lower <- as.character(sapply(param_details$pars, "[[", "lower"))
      hyperparam_num_int_upper <- as.character(sapply(param_details$pars, "[[", "upper"))
      hyperparam_range <- paste0(hyperparam_num_int_lower,"-",hyperparam_num_int_upper)
      hyperparam_values <- names(unlist(sapply(param_details$pars, "[[", "values")))
      hyperparam_non_num <- c()
      for(y in 1:length(hyperparam_name)){
        hyperparam_exact_name <- paste0(hyperparam_name[y], "\\.")
        if(any(hyperparam_values %like% hyperparam_exact_name) && str_detect(hyperparam_exact_name, "\\?") == FALSE){
          text_param_vector <- hyperparam_values[hyperparam_values %like% hyperparam_exact_name]
          if(log){
            writeLines("=====================================================================")
            writeLines(paste0("Index name = ",y))
            writeLines(paste0("Hyperparameter name = ",hyperparam_exact_name))          
            writeLines("Text Param Vector Before")
            print(text_param_vector)
          }
          #check if a hyperparameter name is not a substring of another hyperparameter name, Filter if its True
          flag_delete_text_param <- c()
          for(k in 1:length(text_param_vector)){
            hyperparam_substr_start_location <- stri_locate_first(text_param_vector[k], fixed=hyperparam_name[y])[1]
            if(is.na(hyperparam_substr_start_location) == FALSE){
              if(hyperparam_substr_start_location != 1){
                flag_delete_text_param <- c(flag_delete_text_param, k)
              }
            }
          }
          if(length(flag_delete_text_param) > 0){
            writeLines("Delete Text Param in Index")
            print(flag_delete_text_param)
            text_param_vector <- text_param_vector[-flag_delete_text_param]
          }
          delete_name_parent <- paste0(hyperparam_name[y], "\\.")
          text_param_vector <- str_replace(text_param_vector, delete_name_parent, "")
          text_param <- paste0(text_param_vector, collapse = ", ")
          if(log){
            writeLines("Text Param Vector After")
            print(text_param_vector)
            writeLines("Text Param String After")
            print(text_param)
            writeLines("=====================================================================")
          }
          hyperparam_non_num <- c(hyperparam_non_num, text_param)
        }
        else{
          hyperparam_non_num <- c(hyperparam_non_num, "-")
        }
      }
      writeLines("Hyperparam Range")
      print(hyperparam_range)
      writeLines("Hyperparam Values")
      print(hyperparam_values)
      writeLines("Hyperparam non num")
      print(hyperparam_non_num)
      for(z in 1:length(hyperparam_range)){
        if(hyperparam_range[z]=="NULL-NULL" && length(hyperparam_non_num) > 0 && 
           hyperparam_type[z]!="logical"){
          hyperparam_range[z] <- hyperparam_non_num[z]
        }
        else if(hyperparam_range[z]=="NULL-NULL" && length(hyperparam_non_num) > 0 && 
                hyperparam_type[z]=="logical"){
          hyperparam_range[z] <- "TRUE, FALSE"
        }
        else if(hyperparam_type[z] %in% c("logical","numeric","discrete","integer") == FALSE){
          hyperparam_default[z] <- "-"
          hyperparam_range[z] <- "-"
        }
        else if(hyperparam_tunable[z] == FALSE){
          hyperparam_default[z] <- "-"
          hyperparam_range[z] <- "-"
        }
      }
      hyperparam_partition <- data.frame(ml_name = mlr_learner$class[x],
                                         param_name = hyperparam_name,
                                         param_type = hyperparam_type,
                                         param_default = hyperparam_default,
                                         param_tunable = hyperparam_tunable,
                                         param_range_values = hyperparam_range)
      mlr_hyperparam_df <- rbind(mlr_hyperparam_df, hyperparam_partition)
    }
  }
  #clean df first from NA and param type function
  for(g in 1:nrow(mlr_hyperparam_df)){
    if(grepl("function", mlr_hyperparam_df$param_default[g])){
      mlr_hyperparam_df$param_default[g] <- "-"
    }
  }
  return(mlr_hyperparam_df)
}

#a long string hyperparam that will later be evaluated
mlr_parambuilder <- function(mlr_hyperparameter, method_choose, log=FALSE,
                             fixed_hyperparam_name=list(), fixed_hyperparam_value=list(),
                             integer_multiplier=1.5, exclude_hyperparam=list(),
                             replace_lowrange_default_if_null=1, 
                             replace_highrange_default_if_null=5,
                             convert_hyperparam_to_integer=list()){
  library(stringr)
  mlr_hyperparam_choose <- mlr_hyperparameter[mlr_hyperparameter$ml_name==method_choose,]
  rownames(mlr_hyperparam_choose) <- NULL
  if(log){
    print(mlr_hyperparam_choose) 
  }
  mlr_param_string <- "hyperparam <<- makeParamSet("
  discrete_vector <- NULL
  integer_vector <- NULL
  numeric_vector <- NULL
  fixed_hyperparam_index <- c()
  all_param_name <- mlr_hyperparam_choose$param_name
  exclude_hyperparam_index <- c()
  if(length(exclude_hyperparam) > 0){
    exclude_hyperparam_index <- which(all_param_name %in% unlist(exclude_hyperparam))
  }
  
  #================================Append Fixed Values first if specified=======================================
  if(length(fixed_hyperparam_name) > 0){
    #Reorder Param Name by Index 
    for(a in 1:length(fixed_hyperparam_name)){
      fixed_hyperparam_index <- c(fixed_hyperparam_index, which(all_param_name == fixed_hyperparam_name[[a]]))
    }
    
    for(f in 1:length(fixed_hyperparam_name)){
      if(mlr_hyperparam_choose$param_type[fixed_hyperparam_index[f]] == "discrete"){
        discrete_string <- "c("
        if(is.numeric(fixed_hyperparam_value[[f]][1])){
          writeLines("Make Numeric Fixed Discrete Parameter. . ")
          for(g in 1:length(fixed_hyperparam_value[[f]])){
            if(g==length(fixed_hyperparam_value[[f]])){
              discrete_string <- paste0(discrete_string,fixed_hyperparam_value[[f]][g], ")")
            }
            else{
              discrete_string <- paste0(discrete_string,fixed_hyperparam_value[[f]][g],", ")
            }
          }
        }
        else if(is.character(fixed_hyperparam_value[[f]][1])){
          writeLines("Make Character Fixed Discrete Parameter. . ")
          for(g in 1:length(fixed_hyperparam_value[[f]])){
            if(g==length(fixed_hyperparam_value[[f]])){
              discrete_string <- paste0(discrete_string,'"',fixed_hyperparam_value[[f]][g],'"', ")")
            }
            else{
              discrete_string <- paste0(discrete_string,'"',fixed_hyperparam_value[[f]][g],'"',", ")
            }
          }
        }
        mlr_param_string <- paste0(mlr_param_string, 
                                   'makeDiscreteParam("',fixed_hyperparam_name[[f]],
                                   '",values = ',discrete_string, '), ')
      }
      else if(mlr_hyperparam_choose$param_type[fixed_hyperparam_index[f]] == "integer"){
        mlr_param_string <- paste0(mlr_param_string, 
                                   'makeIntegerParam("',fixed_hyperparam_name[[f]],
                                   '",lower = ',fixed_hyperparam_value[[f]][1], ',upper=',fixed_hyperparam_value[[f]][2],'), ')
      }
      else if(mlr_hyperparam_choose$param_type[fixed_hyperparam_index[f]] == "numeric"){
        if(length(convert_hyperparam_to_integer) > 0){
          check_convert_list=FALSE
          for(v in 1:length(convert_hyperparam_to_integer)){
            if(convert_hyperparam_to_integer[[v]] == fixed_hyperparam_name[[f]]){
              mlr_param_string <- paste0(mlr_param_string, 
                                         'makeIntegerParam("',fixed_hyperparam_name[[f]],
                                         '",lower = ',fixed_hyperparam_value[[f]][1], ',upper=',fixed_hyperparam_value[[f]][2],'), ')
              break
            }
            if(v==length(convert_hyperparam_to_integer)){
              check_convert_list=TRUE
            }
          }
          if(check_convert_list){
            writeLines("Make a Default Numeric Hyperparam if not listed in convert list")
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeNumericParam("',fixed_hyperparam_name[[f]],
                                       '",lower = ',fixed_hyperparam_value[[f]][1], ',upper=',fixed_hyperparam_value[[f]][2],'), ') 
          }
        }
        else{
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeNumericParam("',fixed_hyperparam_name[[f]],
                                     '",lower = ',fixed_hyperparam_value[[f]][1], ',upper=',fixed_hyperparam_value[[f]][2],'), ') 
        }
        
      }
      else if(mlr_hyperparam_choose$param_type[fixed_hyperparam_index[f]] == "logical"){
        mlr_param_string <- paste0(mlr_param_string, 
                                   'makeLogicalParam("',fixed_hyperparam_name[[f]],
                                   '",', fixed_hyperparam_value[[f]][1],', tunable=FALSE), ')
      }
    }
  }
  
  #============================== Automatically Build Hyperparameter for each available tuning ===============================
  for(d in 1:nrow(mlr_hyperparam_choose)){
    if(length(fixed_hyperparam_index) > 0 && length(exclude_hyperparam) > 0){
      if(mlr_hyperparam_choose$param_tunable[d] && 
         (d %in% fixed_hyperparam_index) == FALSE && (d %in% exclude_hyperparam_index) == FALSE){
        if(mlr_hyperparam_choose$param_type[d] == "discrete"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Discrete Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          discrete_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          discrete_vector <- as.character(unlist(strsplit(discrete_vector, ", ")))
          if(any(is.na(suppressWarnings(as.numeric(discrete_vector))))){
            if(log){
              writeLines("Making Discretes for Character Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"', ")")
              }
              else{
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"',", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
          else{
            if(log){
              writeLines("Making Discretes for Integer Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,discrete_vector[g], ")")
              }
              else{
                discrete_string <- paste0(discrete_string,discrete_vector[g],", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "integer"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Integer Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          integer_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          integer_vector <- as.character(unlist(strsplit(integer_vector, "-")))
          flag_pop <- c()
          for(g in 1:length(integer_vector)){
            if(grepl("1e",integer_vector[g])){
              integer_vector[g] <- paste0(integer_vector[g], integer_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            integer_vector <- integer_vector[-flag_pop]
          }
          integer_vector <- as.numeric(integer_vector)
          
          #this when splitting -infinite - infinite or when splitting 1e-06-Inf
          if(length(integer_vector) >= 3 && is.na(integer_vector[1])){
            integer_vector <- integer_vector[-1]
            integer_vector[1] <- integer_vector[1] * -1 
          }
          
          higher_val <- integer_vector[2]
          lower_val <- integer_vector[1]
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          if(integer_vector[2]==Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-") {
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              higher_val <- round(default_val * integer_multiplier)
            }
            else{
              higher_val <- replace_highrange_default_if_null
            }
          }
          if(integer_vector[1]==-Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-"){
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              lower_val <- round(default_val * integer_multiplier * -1)
            }
            else{
              lower_val <- replace_lowrange_default_if_null
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeIntegerParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          if(log){
            writeLines("=======================================================================")
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "numeric"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Numeric Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          numeric_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          numeric_vector <- as.character(unlist(strsplit(numeric_vector, "-")))
          flag_pop <- c()
          for(g in 1:length(numeric_vector)){
            if(grepl("1e",numeric_vector[g])){
              numeric_vector[g] <- paste0(numeric_vector[g], "-", numeric_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            numeric_vector <- numeric_vector[-flag_pop]
          }
          numeric_vector <- as.numeric(numeric_vector)
          
          #this when splitting -infinite - infinite
          if(length(numeric_vector) == 3){
            numeric_vector <- numeric_vector[-1]
            numeric_vector[1] <- numeric_vector[1] * -1 
          }
          
          higher_val <- numeric_vector[2]
          lower_val <- numeric_vector[1]
          if(higher_val == Inf && lower_val == -Inf){
            default_val <- Inf
          }
          else{
            default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
          }
          
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
            writeLines(paste0("Default Value: ", default_val))
          }
          if(default_val == Inf){
            lower_val <- -1000
            higher_val <- 1000
          }
          else{
            if(numeric_vector[2]==Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val >= 1000){
                    higher_val <- default_val 
                  }
                  else if(default_val >= 100 && default_val < 1000){
                    higher_val <- default_val * 10
                  }
                  else if(default_val >= 10 && default_val <= 100){
                    higher_val <- default_val * 100
                  }
                  else if(default_val >= 0 && default_val < 10){
                    higher_val <- default_val * 1000
                  }
                }
                else{
                  higher_val <- 1000
                }
              }
              else{
                higher_val <- 1000
              }
            }
            if(numeric_vector[1]==-Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val < -1000){
                    lower_val <- default_val * -1
                  }
                  else if(default_val >= -1000 && default_val <= -100){
                    lower_val <- default_val * -10
                  }
                  else if(default_val >= -100 && default_val <= -10){
                    lower_val <- default_val * -100
                  }
                  else if(default_val > -10 && default_val < 0){
                    lower_val <- default_val * -1000
                  }
                  else if(default_val >= 0){
                    lower_val <- default_val
                  }
                }
                else{
                  lower_val <- -1000
                }
              }
              else{
                lower_val <- -1000
              }
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeNumericParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          writeLines("=======================================================================")
        }
        else if(mlr_hyperparam_choose$param_type[d] == "logical"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Logical Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeLogicalParam("',mlr_hyperparam_choose$param_name[d],'"), ')
          if(log){
            writeLines("=======================================================================")
          }
        } 
      }
    }
    else if(length(fixed_hyperparam_index) > 0 && length(exclude_hyperparam) == 0){
      if(mlr_hyperparam_choose$param_tunable[d] && 
         (d %in% fixed_hyperparam_index) == FALSE){
        if(mlr_hyperparam_choose$param_type[d] == "discrete"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Discrete Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          discrete_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          discrete_vector <- as.character(unlist(strsplit(discrete_vector, ", ")))
          if(any(is.na(suppressWarnings(as.numeric(discrete_vector))))){
            if(log){
              writeLines("Making Discretes for Character Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"', ")")
              }
              else{
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"',", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
          else{
            if(log){
              writeLines("Making Discretes for Integer Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,discrete_vector[g], ")")
              }
              else{
                discrete_string <- paste0(discrete_string,discrete_vector[g],", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "integer"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Integer Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          integer_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          integer_vector <- as.character(unlist(strsplit(integer_vector, "-")))
          flag_pop <- c()
          for(g in 1:length(integer_vector)){
            if(grepl("1e",integer_vector[g])){
              integer_vector[g] <- paste0(integer_vector[g], integer_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            integer_vector <- integer_vector[-flag_pop]
          }
          integer_vector <- as.numeric(integer_vector)
          
          #this when splitting -infinite - infinite or when splitting 1e-06-Inf
          if(length(integer_vector) >= 3 && is.na(integer_vector[1])){
            integer_vector <- integer_vector[-1]
            integer_vector[1] <- integer_vector[1] * -1 
          }
          
          higher_val <- integer_vector[2]
          lower_val <- integer_vector[1]
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          if(integer_vector[2]==Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-") {
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              higher_val <- round(default_val * integer_multiplier)
            }
            else{
              higher_val <- replace_highrange_default_if_null
            }
          }
          if(integer_vector[1]==-Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-"){
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              lower_val <- round(default_val * integer_multiplier * -1)
            }
            else{
              lower_val <- replace_lowrange_default_if_null
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeIntegerParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          if(log){
            writeLines("=======================================================================")
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "numeric"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Numeric Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          numeric_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          numeric_vector <- as.character(unlist(strsplit(numeric_vector, "-")))
          flag_pop <- c()
          for(g in 1:length(numeric_vector)){
            if(grepl("1e",numeric_vector[g])){
              numeric_vector[g] <- paste0(numeric_vector[g], "-", numeric_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            numeric_vector <- numeric_vector[-flag_pop]
          }
          numeric_vector <- as.numeric(numeric_vector)
          
          #this when splitting -infinite - infinite
          if(length(numeric_vector) == 3){
            numeric_vector <- numeric_vector[-1]
            numeric_vector[1] <- numeric_vector[1] * -1 
          }
          
          higher_val <- numeric_vector[2]
          lower_val <- numeric_vector[1]
          if(higher_val == Inf && lower_val == -Inf){
            default_val <- Inf
          }
          else{
            default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
          }
          
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
            writeLines(paste0("Default Value: ", default_val))
          }
          if(default_val == Inf){
            lower_val <- -1000
            higher_val <- 1000
          }
          else{
            if(numeric_vector[2]==Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val >= 1000){
                    higher_val <- default_val 
                  }
                  else if(default_val >= 100 && default_val < 1000){
                    higher_val <- default_val * 10
                  }
                  else if(default_val >= 10 && default_val <= 100){
                    higher_val <- default_val * 100
                  }
                  else if(default_val >= 0 && default_val < 10){
                    higher_val <- default_val * 1000
                  }
                }
                else{
                  higher_val <- 1000
                }
              }
              else{
                higher_val <- 1000
              }
            }
            if(numeric_vector[1]==-Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val < -1000){
                    lower_val <- default_val * -1
                  }
                  else if(default_val >= -1000 && default_val <= -100){
                    lower_val <- default_val * -10
                  }
                  else if(default_val >= -100 && default_val <= -10){
                    lower_val <- default_val * -100
                  }
                  else if(default_val > -10 && default_val < 0){
                    lower_val <- default_val * -1000
                  }
                  else if(default_val >= 0){
                    lower_val <- default_val
                  }
                }
                else{
                  lower_val <- -1000
                }
              }
              else{
                lower_val <- -1000
              }
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeNumericParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          writeLines("=======================================================================")
        }
        else if(mlr_hyperparam_choose$param_type[d] == "logical"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Logical Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeLogicalParam("',mlr_hyperparam_choose$param_name[d],'"), ')
          if(log){
            writeLines("=======================================================================")
          }
        } 
      }
    }
    else if(length(fixed_hyperparam_index) == 0 && length(exclude_hyperparam) > 0){
      if(mlr_hyperparam_choose$param_tunable[d] && 
         (d %in% exclude_hyperparam_index) == FALSE){
        if(mlr_hyperparam_choose$param_type[d] == "discrete"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Discrete Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          discrete_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          discrete_vector <- as.character(unlist(strsplit(discrete_vector, ", ")))
          if(any(is.na(suppressWarnings(as.numeric(discrete_vector))))){
            if(log){
              writeLines("Making Discretes for Character Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"', ")")
              }
              else{
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"',", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
          else{
            if(log){
              writeLines("Making Discretes for Integer Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,discrete_vector[g], ")")
              }
              else{
                discrete_string <- paste0(discrete_string,discrete_vector[g],", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "integer"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Integer Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          integer_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          integer_vector <- as.character(unlist(strsplit(integer_vector, "-")))
          flag_pop <- c()
          for(g in 1:length(integer_vector)){
            if(grepl("1e",integer_vector[g])){
              integer_vector[g] <- paste0(integer_vector[g], integer_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            integer_vector <- integer_vector[-flag_pop]
          }
          integer_vector <- as.numeric(integer_vector)
          
          #this when splitting -infinite - infinite or when splitting 1e-06-Inf
          if(length(integer_vector) >= 3 && is.na(integer_vector[1])){
            integer_vector <- integer_vector[-1]
            integer_vector[1] <- integer_vector[1] * -1 
          }
          
          higher_val <- integer_vector[2]
          lower_val <- integer_vector[1]
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          if(integer_vector[2]==Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-") {
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              higher_val <- round(default_val * integer_multiplier)
            }
            else{
              higher_val <- replace_highrange_default_if_null
            }
          }
          if(integer_vector[1]==-Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-"){
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              lower_val <- round(default_val * integer_multiplier * -1)
            }
            else{
              lower_val <- replace_lowrange_default_if_null
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeIntegerParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          if(log){
            writeLines("=======================================================================")
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "numeric"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Numeric Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          numeric_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          numeric_vector <- as.character(unlist(strsplit(numeric_vector, "-")))
          flag_pop <- c()
          for(g in 1:length(numeric_vector)){
            if(grepl("1e",numeric_vector[g])){
              numeric_vector[g] <- paste0(numeric_vector[g], "-", numeric_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            numeric_vector <- numeric_vector[-flag_pop]
          }
          numeric_vector <- as.numeric(numeric_vector)
          
          #this when splitting -infinite - infinite
          if(length(numeric_vector) == 3){
            numeric_vector <- numeric_vector[-1]
            numeric_vector[1] <- numeric_vector[1] * -1 
          }
          
          higher_val <- numeric_vector[2]
          lower_val <- numeric_vector[1]
          if(higher_val == Inf && lower_val == -Inf){
            default_val <- Inf
          }
          else{
            default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
          }
          
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
            writeLines(paste0("Default Value: ", default_val))
          }
          if(default_val == Inf){
            lower_val <- -1000
            higher_val <- 1000
          }
          else{
            if(numeric_vector[2]==Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val >= 1000){
                    higher_val <- default_val 
                  }
                  else if(default_val >= 100 && default_val < 1000){
                    higher_val <- default_val * 10
                  }
                  else if(default_val >= 10 && default_val <= 100){
                    higher_val <- default_val * 100
                  }
                  else if(default_val >= 0 && default_val < 10){
                    higher_val <- default_val * 1000
                  }
                }
                else{
                  higher_val <- 1000
                }
              }
              else{
                higher_val <- 1000
              }
            }
            if(numeric_vector[1]==-Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val < -1000){
                    lower_val <- default_val * -1
                  }
                  else if(default_val >= -1000 && default_val <= -100){
                    lower_val <- default_val * -10
                  }
                  else if(default_val >= -100 && default_val <= -10){
                    lower_val <- default_val * -100
                  }
                  else if(default_val > -10 && default_val < 0){
                    lower_val <- default_val * -1000
                  }
                  else if(default_val >= 0){
                    lower_val <- default_val
                  }
                }
                else{
                  lower_val <- -1000
                }
              }
              else{
                lower_val <- -1000
              }
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeNumericParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          writeLines("=======================================================================")
        }
        else if(mlr_hyperparam_choose$param_type[d] == "logical"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Logical Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeLogicalParam("',mlr_hyperparam_choose$param_name[d],'"), ')
          if(log){
            writeLines("=======================================================================")
          }
        } 
      }
    }
    else if(length(fixed_hyperparam_index) == 0 && length(exclude_hyperparam) == 0){
      if(mlr_hyperparam_choose$param_tunable[d]){
        if(mlr_hyperparam_choose$param_type[d] == "discrete"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Discrete Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          discrete_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          discrete_vector <- as.character(unlist(strsplit(discrete_vector, ", ")))
          if(any(is.na(suppressWarnings(as.numeric(discrete_vector))))){
            if(log){
              writeLines("Making Discretes for Character Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"', ")")
              }
              else{
                discrete_string <- paste0(discrete_string,'"',discrete_vector[g],'"',", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
          else{
            if(log){
              writeLines("Making Discretes for Integer Option")
            }
            discrete_string <- "c("
            for(g in 1:length(discrete_vector)){
              if(g==length(discrete_vector)){
                discrete_string <- paste0(discrete_string,discrete_vector[g], ")")
              }
              else{
                discrete_string <- paste0(discrete_string,discrete_vector[g],", ")
              }
            }
            mlr_param_string <- paste0(mlr_param_string, 
                                       'makeDiscreteParam("',mlr_hyperparam_choose$param_name[d],
                                       '",values = ',discrete_string, '), ')
            if(log){
              writeLines("=======================================================================")
            }
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "integer"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Integer Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          integer_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          integer_vector <- as.character(unlist(strsplit(integer_vector, "-")))
          flag_pop <- c()
          for(g in 1:length(integer_vector)){
            if(grepl("1e",integer_vector[g])){
              integer_vector[g] <- paste0(integer_vector[g], integer_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            integer_vector <- integer_vector[-flag_pop]
          }
          integer_vector <- as.numeric(integer_vector)
          
          #this when splitting -infinite - infinite or when splitting 1e-06-Inf
          if(length(integer_vector) >= 3 && is.na(integer_vector[1])){
            integer_vector <- integer_vector[-1]
            integer_vector[1] <- integer_vector[1] * -1 
          }
          
          higher_val <- integer_vector[2]
          lower_val <- integer_vector[1]
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          if(integer_vector[2]==Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-") {
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              higher_val <- round(default_val * integer_multiplier)
            }
            else{
              higher_val <- replace_highrange_default_if_null
            }
          }
          if(integer_vector[1]==-Inf){
            if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
               mlr_hyperparam_choose$param_default[d] != "-"){
              default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
              if(log){
                writeLines(paste0("Default_value = ",default_val))
              }
              lower_val <- round(default_val * integer_multiplier * -1)
            }
            else{
              lower_val <- replace_lowrange_default_if_null
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeIntegerParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          if(log){
            writeLines("=======================================================================")
          }
        }
        else if(mlr_hyperparam_choose$param_type[d] == "numeric"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Numeric Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          numeric_vector <- as.character(mlr_hyperparam_choose$param_range_values[d])
          numeric_vector <- as.character(unlist(strsplit(numeric_vector, "-")))
          flag_pop <- c()
          #this when dealing with ranges of 1e-10:1e-04
          for(g in 1:length(numeric_vector)){
            if(grepl("1e",numeric_vector[g])){
              numeric_vector[g] <- paste0(numeric_vector[g], "-", numeric_vector[g+1])
              flag_pop <- c(flag_pop, g+1)
            }
          }
          if(length(flag_pop) > 0){
            numeric_vector <- numeric_vector[-flag_pop]
          }
          numeric_vector <- as.numeric(numeric_vector)
          
          #this when splitting -infinite - infinite
          if(length(numeric_vector) == 3){
            numeric_vector <- numeric_vector[-1]
            numeric_vector[1] <- numeric_vector[1] * -1 
          }
          
          higher_val <- numeric_vector[2]
          lower_val <- numeric_vector[1]
          if(higher_val == Inf && lower_val == -Inf){
            default_val <- Inf
          }
          else{
            default_val <- as.numeric(as.character(mlr_hyperparam_choose$param_default[d]))
          }
          
          if(log){
            writeLines("Original Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
            writeLines(paste0("Default Value: ", default_val))
          }
          if(default_val == Inf){
            lower_val <- -1000
            higher_val <- 1000
          }
          else{
            if(numeric_vector[2]==Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val >= 1000){
                    higher_val <- default_val 
                  }
                  else if(default_val >= 100 && default_val < 1000){
                    higher_val <- default_val * 10
                  }
                  else if(default_val >= 10 && default_val <= 100){
                    higher_val <- default_val * 100
                  }
                  else if(default_val >= 0 && default_val < 10){
                    higher_val <- default_val * 1000
                  }
                }
                else{
                  higher_val <- 1000
                }
              }
              else{
                higher_val <- 1000
              }
            }
            if(numeric_vector[1]==-Inf){
              if(mlr_hyperparam_choose$param_default[d] != "NULL" && 
                 mlr_hyperparam_choose$param_default[d] != "-"){
                if(default_val < 0){
                  writeLines("Transform default value into absolute positive")
                  default_val <- default_val <- abs(default_val)
                }
                if(log){
                  writeLines(paste0("Default_value = ",default_val))
                }
                if(default_val != 0){
                  if(default_val < -1000){
                    lower_val <- default_val * -1
                  }
                  else if(default_val >= -1000 && default_val <= -100){
                    lower_val <- default_val * -10
                  }
                  else if(default_val >= -100 && default_val <= -10){
                    lower_val <- default_val * -100
                  }
                  else if(default_val > -10 && default_val < 0){
                    lower_val <- default_val * -1000
                  }
                  else if(default_val >= 0){
                    lower_val <- default_val
                  }
                }
                else{
                  lower_val <- -1000
                }
              }
              else{
                lower_val <- -1000
              }
            }
          }
          if(log){
            writeLines("Converted to Reliable Boundary")
            writeLines(paste0("High Boundary: ", higher_val))
            writeLines(paste0("Lower Boundary: ", lower_val))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeNumericParam("',mlr_hyperparam_choose$param_name[d],
                                     '",lower = ',lower_val, ',upper=',higher_val,'), ')
          writeLines("=======================================================================")
        }
        else if(mlr_hyperparam_choose$param_type[d] == "logical"){
          if(log){
            writeLines("=======================================================================")
            writeLines(paste0("Make Logical Parameter: ", mlr_hyperparam_choose$param_name[d]))
          }
          mlr_param_string <- paste0(mlr_param_string, 
                                     'makeLogicalParam("',mlr_hyperparam_choose$param_name[d],'"), ')
          if(log){
            writeLines("=======================================================================")
          }
        } 
      }
    }
  }
  #Convert End Comma to End Brackets or if nothing in param set convert ( to ()
  if(grepl(", $", mlr_param_string))
  {
    mlr_param_string <- sub(", $", ')', mlr_param_string)
  }
  if(grepl("\\($", mlr_param_string)){
    mlr_param_string <- sub("\\($", '()', mlr_param_string)
  }
  return(mlr_param_string)
}

mlr_method_tester <- function(df, mlr_hyperparameter, target_var, method, resample_method="CV", 
                              plothyperpars=FALSE, xplot_param="", zplot_param="", 
                              partial_dep_learner="regr.glm", search_type="random",
                              fixed_hyperparam_name=list(), fixed_hyperparam_value=list(), 
                              exclude_hyperparam=list(), tune_iteration=20, seed_random=FALSE,
                              integer_multiplier = 1.5, parambuilder_log=FALSE,
                              replace_lowrange_default_if_null=1, replace_highrange_default_if_null=5,
                              create_dummy_feature=FALSE, convert_hyperparam_to_integer=list()){
  
  library(mlr)
  task <- NULL
  lrn <- NULL
  
  #1 ==== Check if there any fixed logical to tuned, either TRUE or FALSE, 
  ## ==== Create a pars_vals_list if fixed logical needed ====
  logical_fixed <- FALSE
  hyperparam_nulled <- c()
  par_vals_fixed <- "par_vals_list <<- list("
  if(length(fixed_hyperparam_value) > 0){
    for(a in 1:length(fixed_hyperparam_value)){
      if(length(fixed_hyperparam_value[[a]])==1){
        if(is.logical(fixed_hyperparam_value[[a]])){
          logical_fixed <- TRUE
          par_vals_fixed <- paste0(par_vals_fixed, fixed_hyperparam_name[[a]], "=", fixed_hyperparam_value[[a]], ", ")
          hyperparam_nulled <- c(hyperparam_nulled, paste0("hyperparam$pars$",fixed_hyperparam_name[[a]], "= NULL"))
        }
      }
    }
    #Convert End Comma to End Brackets
    if(grepl(", $", par_vals_fixed))
    {
      par_vals_fixed <- sub(", $", ')', par_vals_fixed)
    }
    else{
      par_vals_fixed <- paste0(par_vals_fixed,")")
    }
    writeLines("Using Fixed Param Values")
    print(par_vals_fixed)
    eval(parse(text=par_vals_fixed))
  }
  if(length(exclude_hyperparam) > 0){
    for(b in 1:length(exclude_hyperparam)){
      hyperparam_nulled <- c(hyperparam_nulled, paste0("hyperparam$pars$",exclude_hyperparam[[b]], "= NULL"))
    } 
  }
  
  #2 ================================= Convert response variable to character ======================================
  df[,which(colnames(df)==target_var)] <- as.character(df[,which(colnames(df)==target_var)])
  
  #3 ================================== Conditional Tasking for different method type ============================
  if(grepl("classif.", method)){
    if(logical_fixed){
      writeLines("Executed Fixed Logical Hyperparameter")
      task <- makeClassifTask(data = df, 
                              target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE, par.vals = par_vals_list)
    }
    else{
      task <- makeClassifTask(data = df, 
                              target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE)
    }
  }
  else if(grepl("regr.", method)){
    if(logical_fixed){
      writeLines("Executed Fixed Logical Hyperparameter")
      task <- makeRegrTask(data = df, 
                           target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE, par.vals = par_vals_list)
    }
    else{
      task <- makeRegrTask(data = df, 
                           target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE)
    }
  }
  else if(grepl("surv.", method)){
    if(logical_fixed){
      writeLines("Executed Fixed Logical Hyperparameter")
      task <- makeSurvTask(data = df, 
                           target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE, par.vals = par_vals_list)
    }
    else{
      task <- makeSurvTask(data = df, 
                           target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE)
    }
  }
  else if(grepl("cluster.", method)){
    if(logical_fixed){
      writeLines("Executed Fixed Logical Hyperparameter")
      task <- makeClusterTask(data = df, 
                              target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE, par.vals = par_vals_list)
    }
    else{
      task <- makeClusterTask(data = df, 
                              target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE)
    }
  }
  else if(grepl("multilabel.", method)){
    if(logical_fixed){
      writeLines("Executed Fixed Logical Hyperparameter")
      task <- makeMultilabelTask(data = df, 
                                 target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE, par.vals = par_vals_list)
    }
    else{
      task <- makeMultilabelTask(data = df, 
                                 target = target_var)
      lrn <- makeLearner(cl = method, fix.factors.prediction = TRUE)
    }
  }
  
  #3b ================================== If using Dummy Features for some ML ========================
  if(create_dummy_feature){
    #Convert Back to Factor the response variable
    df[,which(colnames(df)==target_var)] <- as.factor(df[,which(colnames(df)==target_var)])
    df <- createDummyFeatures(df)
  }
  
  writeLines("DF Structures")
  print(str(df))
  
  #4 ================================ Build Param Set based by Method defined ============================
  writeLines("Generating Param Builder")
  if(length(fixed_hyperparam_value) > 0 && length(exclude_hyperparam) > 0){
    param_to_eval <- mlr_parambuilder(mlr_hyperparameter, method,
                                      fixed_hyperparam_name = fixed_hyperparam_name, 
                                      fixed_hyperparam_value = fixed_hyperparam_value,
                                      exclude_hyperparam = exclude_hyperparam,
                                      integer_multiplier = integer_multiplier, 
                                      log = parambuilder_log, 
                                      replace_lowrange_default_if_null = replace_lowrange_default_if_null,
                                      replace_highrange_default_if_null = replace_highrange_default_if_null,
                                      convert_hyperparam_to_integer = convert_hyperparam_to_integer)
    eval(parse(text=param_to_eval)) 
  }
  else if(length(fixed_hyperparam_value) == 0 && length(exclude_hyperparam) > 0){
    param_to_eval <- mlr_parambuilder(mlr_hyperparameter, method,
                                      exclude_hyperparam = exclude_hyperparam,
                                      integer_multiplier = integer_multiplier, 
                                      log = parambuilder_log, 
                                      replace_lowrange_default_if_null = replace_lowrange_default_if_null,
                                      replace_highrange_default_if_null = replace_highrange_default_if_null,
                                      convert_hyperparam_to_integer = convert_hyperparam_to_integer)
    eval(parse(text=param_to_eval)) 
  }
  else if(length(fixed_hyperparam_value) > 0 && length(exclude_hyperparam) == 0){
    param_to_eval <- mlr_parambuilder(mlr_hyperparameter, method,
                                      fixed_hyperparam_name = fixed_hyperparam_name, 
                                      fixed_hyperparam_value = fixed_hyperparam_value,
                                      integer_multiplier = integer_multiplier, 
                                      log = parambuilder_log, 
                                      replace_lowrange_default_if_null = replace_lowrange_default_if_null,
                                      replace_highrange_default_if_null = replace_highrange_default_if_null,
                                      convert_hyperparam_to_integer = convert_hyperparam_to_integer)
    eval(parse(text=param_to_eval)) 
  }
  else{
    param_to_eval <- mlr_parambuilder(mlr_hyperparameter, method,
                                      integer_multiplier = integer_multiplier, 
                                      log = parambuilder_log, 
                                      replace_lowrange_default_if_null = replace_lowrange_default_if_null,
                                      replace_highrange_default_if_null = replace_highrange_default_if_null,
                                      convert_hyperparam_to_integer = convert_hyperparam_to_integer)
    eval(parse(text=param_to_eval)) 
  }
  
  #5 ================================ Exclude some Param Set defined by user ============================
  if(length(hyperparam_nulled) > 0){
    writeLines("Hyperparam to be cleared")
    print(hyperparam_nulled)
    for(f in 1:length(hyperparam_nulled)){
      eval(parse(text=hyperparam_nulled[f]))
    }
  }
  
  #6 ================================ Build Tuning by Param Set ============================
  if(seed_random){
    set.seed(123)
  }
  else{
    set.seed(NULL)
  }
  hyper_search <- NULL
  if(search_type == "grid"){
    hyper_search <- makeTuneControlGrid()
    resampling_method <- makeResampleDesc(resample_method)
    
    # Perform tuning
    if(length(hyperparam$pars) > 0){
      lrn_tune <- tuneParams(learner = lrn, task = task, 
                             resampling = resampling_method, 
                             control = hyper_search, par.set = hyperparam, 
                             measures=list(acc,mmce))
    }
    else{
      lrn_tune <- tuneParams(learner = lrn, task = task, 
                             resampling = resampling_method, 
                             control = hyper_search, measures=list(acc,mmce))
    }
  }
  else if(search_type == "random"){
    hyper_search <- makeTuneControlRandom(maxit = tune_iteration)
    resampling_method <- makeResampleDesc(resample_method)
    
    # Perform tuning
    if(length(hyperparam$pars) > 0){
      lrn_tune <- tuneParams(learner = lrn, task = task, 
                             resampling = resampling_method, 
                             control = hyper_search, par.set = hyperparam, 
                             measures=list(acc,mmce))
    }
    else{
      lrn_tune <- tuneParams(learner = lrn, task = task, 
                             resampling = resampling_method, 
                             control = hyper_search, measures=list(acc,mmce))
    }
  }
  
  writeLines("Inspect Model tuning")
  print(lrn_tune)
  
  #7 ============================= Build Summary of Hyperpars Effect from the last tuning ============================
  #Making xplot and zplot param automatically for hyper pars (select 2 hyperparam numeric first found) 
  #excluding ml_name which don't have no more than 2 integer hyperparams
  hyperpar_effects <- generateHyperParsEffectData(lrn_tune, partial.dep = TRUE)
  return(hyperpar_effects$data)
}

#-------------------------------------------------------------------------------------------------------------------------
########################### 23) Various Regression Analysis  #############################################################
#-------------------------------------------------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------------------
######################## A) Standardization Method - Regression Helper #########################
#-----------------------------------------------------------------------------------------------
common_normalization <- function(x, type=1, multiplier=1){
  min_max_norm <- function(x, multiplier) {
    return(((x - min(x)) / (max(x) - min(x))) * multiplier)
  }
  
  #more robust to outlier
  z_score_norm <- function(x, multiplier){
    return(((x - mean(x)) / sd(x)) * multiplier)
  }
  
  mean_norm <- function(x, multiplier){
    return(((x- mean(x)) / (max(x) - min(x))) * multiplier)
  }
  
  occurence_norm <- function(x, multiplier){
    return(((x - min(x)) / sum(x - min(x))) * multiplier)
  }
  
  #more robust to outlier
  IQR_norm <- function(x, multiplier){
    med <- median(x)
    iqr <- IQR(x, na.rm = TRUE)
    return(((x - med) / iqr) * multiplier)
  }

  min_max_denorm <- function(x, min_x, max_x, multiplier){
    return(((x/multiplier) * (max_x - min_x) + min_x))
  }

  z_score_denorm <- function(x, mean_x, sd_x, multiplier){
    return(((x/multiplier) * sd_x) + mean_x)
  }

  mean_denorm <- function(x, mean_x, max_x, min_x, multiplier){
    return((x/multiplier) * (max_x - min_x) + mean_x)
  }
  
  occurence_denorm <- function(x, min_x, sum_x_min_x, multiplier){
    return((x/multiplier) * sum_x_min_x + min_x)
  }
  
  #more robust to outlier
  IQR_norm <- function(x, multiplier){
    med <- median(x)
    iqr <- IQR(x, na.rm = TRUE)
    return(((x - med) / iqr) * multiplier)
  }

  IQR_denorm <- function(x, med_x, iqr_x, multiplier){
    return(((x/multiplier) * iqr) + med_x)
  }

  
  
  writeLines("Common Normalization Included is: ")
  writeLines("1) Min-Max Normalization")
  writeLines("2) Z-Score Normalization")
  writeLines("3) Mean Normalization")
  writeLines("4) Occurence Normalization")
  writeLines("5) IQR Normalization")
  
  if(type==1){
    return(min_max_norm(x, multiplier))
  }else if(type==2){
    return(z_score_norm(x, multiplier))
  }else if(type==3){
    return(mean_norm(x, multiplier))
  }else if(type==4){
    return(occurence_norm(x, multiplier))
  }else if(type==5){
    return(IQR_norm(x, multiplier))
  }
  
}
community_ecologist_standardization <- function(df, method_use=1){
  library(vegan)
  writeLines("Method Available for Community Ecologist Standardization")
  writeLines("1) total: divide by margin total")
  writeLines("2) max: divide by margin maximum")
  writeLines("3) frequency: divide by margin total and multiply by the number of non-zero items, so that the average of non-zero entries is one")
  writeLines("4) normalize: make margin sum of squares equal to one ")
  writeLines("5) range: standardize values into range 0 ... 1 (default MARGIN = 2). If all values are constant, they will be transformed to 0")
  writeLines("6) rank, rrank: rank replaces abundance values by their increasing ranks leaving zeros unchanged, and rrank is similar but uses relative ranks with maximum 1")
  writeLines("7) standardize: scale x to zero mean and unit variance ")
  writeLines("8) pa: scale x to presence/absence scale (0/1)")
  writeLines("9) chi.square: divide by row sums and square root of column sums, and adjust for square root of matrix total")
  writeLines("10) hellinger: square root of method = 'total'")
  writeLines("11) log: logarithmic transformation as suggested by Anderson et al. with using formula log_b (x) + 1")
  if(method_use==1){
    data_transform <- decostand(df, "total")
  }
  else if(method_use==2){
    data_transform <- decostand(df, "max")
  }
  else if(method_use==3){
    data_transform <- decostand(df, "frequency")
  }
  else if(method_use==4){
    data_transform <- decostand(df, "normalize")
  }
  else if(method_use==5){
    data_transform <- decostand(df, "range")
  }
  else if(method_use==6){
    data_transform <- decostand(df, "rank")
  }
  else if(method_use==7){
    data_transform <- decostand(df, "standardize")
  }
  else if(method_use==8){
    data_transform <- decostand(df, "pa")
  }
  else if(method_use==9){
    data_transform <- decostand(df, "chi.square")
  }
  else if(method_use==10){
    data_transform <- decostand(df, "hellinger")
  }
  else if(method_use==11){
    data_transform <- decostand(df, "log")
  }
  return(data_transform)
}

#for Continous Variables
standardize_package_continous_scale <- function(df, multiplier=1, useformula=FALSE, formula){
  library(standardize)
  boolean_numeric <- unlist(lapply(df[1:length(colnames(df))], is.numeric))
  boolean_numeric <- as.vector(which(boolean_numeric == TRUE))
  if(useformula){
    formula <- as.formula(formula)
    scale_df <- df[,boolean_numeric]
    scale_df <- scale_by(formula, df, scale = multiplier)
  }
  else{
    scale_df <- df[,boolean_numeric]
    scale_df <- as.data.frame(lapply(df, scale))
    scale_df <- scale_df * multiplier
  }
  return(scale_df)
}
#for Factor Variables
standardize_factor <- function(x, factor_var, multiplier=1, order_factor=FALSE){
  if(order_factor == FALSE){
    options(contrasts = c("contr.treatment", "contr.poly"))
    contrasts(x[[factor_var]]) <- named_contr_sum(x[[factor_var]], scale = 0.5)
  }
  else if(order_factor == TRUE){
    contrasts(x[[factor_var]]) <- scaled_contr_poly(x[[factor_var]], scale = 0.5)
  }
  return(contrasts(x[[factor_var]]))
}

#-----------------------------------------------------------------------------------------------
######################## B) Applying Types of Regression #######################################
#-----------------------------------------------------------------------------------------------
stepwise_modeling <- function(df, dependent_col="", independent_col="",
                              stepwise_method="forward", 
                              integer_response_regr_method="poisson",
                              factor_response_regr_method="logit",
                              multi_probit_draw=200, multi_probit_burn=100, 
                              multi_probit_thin=25){
  model_type <- c()
  model_formula <- c()
  model_aic <- c()
  response_type <- class(df[[dependent_col]])
  my_formula_initial <- as.formula(paste0(dependent_col, " ~ 1"))
  my_formula <- as.formula(paste0(dependent_col, " ~ ",paste0(independent_col, collapse=" + ")))
  n_sample <- nrow(df)
  real_n <- nrow(df)
  subdf <- df
  if(response_type=="numeric"){
    if(stepwise_method=="forward"){
      model_initial <- lm(my_formula_initial, data=df)
      model_full <- lm(my_formula, data=df)
      model_lm <- step(model_initial,
                       scope = list(upper=model_full),
                       direction="forward",
                       test="Chisq",
                       data=df)
      model_type <- c(model_type, "lm_forward_stepwise")
      model_formula <- c(model_formula, as.character(model_lm$call)[2])
      model_aic <- c(model_aic, model_lm$aic)
      return(list(model_lm, model_type, model_formula, model_aic))
    }
    else if(stepwise_method=="backward"){
      model_initial <- lm(my_formula_initial, data=df)
      model_full <- lm(my_formula, data=df)
      model_lm <- step(model_full,
                       scope = list(upper=model_initial),
                       direction="backward",
                       test="Chisq",
                       data=df)
      model_type <- c(model_type, "lm_backward_stepwise")
      model_formula <- c(model_formula, as.character(model_lm$call)[2])
      model_aic <- c(model_aic, model_lm$aic)
      return(list(model_lm, model_type, model_formula, model_aic))
    }
  }
  else if(response_type=="integer"){
    if(stepwise_method=="forward"){
      if(integer_response_regr_method == "poisson"){
        model_initial <- glm(my_formula_initial, data=df, family="poisson")
        model_full <- glm(my_formula, data=df, family="poisson")
        model_lm <- step(model_initial,
                         scope = list(upper=model_full),
                         direction="backward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "poisson_forward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
      else if(integer_response_regr_method == "negative_binomial"){
        model_initial <- glm.nb(my_formula_initial, data=df)
        model_full <- glm.nb(my_formula, data=df)
        model_lm <- step(model_initial,
                         scope = list(upper=model_full),
                         direction="backward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "negative_binomial_forward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
      else if(integer_response_regr_method == "quasi_poisson"){
        model_initial <- glm(my_formula_initial, data=df, family="quasipoisson")
        model_full <- glm(my_formula, data=df, family="quasipoisson")
        model_lm <- step(model_initial,
                         scope = list(upper=model_full),
                         direction="backward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "quasi_poisson_forward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
    }
    else if(stepwise_method=="backward"){
      if(integer_response_regr_method == "poisson"){
        model_initial <- glm(my_formula_initial, data=df, family="poisson")
        model_full <- glm(my_formula, data=df, family="poisson")
        model_lm <- step(model_full,
                         scope = list(upper=model_initial),
                         direction="backward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "poisson_backward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
      else if(integer_response_regr_method == "negative_binomial"){
        model_initial <- glm.nb(my_formula_initial, data=df)
        model_full <- glm.nb(my_formula, data=df)
        model_lm <- step(model_full,
                         scope = list(upper=model_initial),
                         direction="backward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "negative_binomial_backward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
      else if(integer_response_regr_method == "quasi_poisson"){
        model_initial <- glm(my_formula_initial, data=df, family="quasipoisson")
        model_full <- glm(my_formula, data=df, family="quasipoisson")
        model_lm <- step(model_full,
                         scope = list(upper=model_initial),
                         direction="backward",
                         test="Chisq",
                         data=df)
        model_type <- c(model_type, "quasi_poisson_backward_stepwise")
        model_formula <- c(model_formula, as.character(model_lm$call)[2])
        model_aic <- c(model_aic, model_lm$aic)
        return(list(model_lm, model_type, model_formula, model_aic))
      }
    }
  }
  else if(response_type=="factor"){
    if(stepwise_method=="forward"){
      if(factor_response_regr_method=="logit"){
        if(length(unique(df[[dependent_col]])) == 2){
          model_initial <- glm(my_formula_initial, data=df, family = binomial(link="logit"))
          model_full <- glm(my_formula, data=df, family = binomial(link="logit"))
          model_lm <- step(model_initial,
                           scope = list(upper=model_full),
                           direction="forward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "logit_foward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(length(unique(df[[dependent_col]])) > 2){
          model_initial <- multinom(formula=my_formula_initial, data = df)
          model_full <- multinom(formula=my_formula, data = df)
          model_lm <- step(model_initial,
                           scope = list(upper=model_full),
                           direction="forward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "multilogit_forward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
      }
      else if(factor_response_regr_method=="probit"){
        if(length(unique(df[[dependent_col]])) == 2){
          model_initial <- glm(my_formula_initial, data=df, family = binomial(link="probit"))
          model_full <- glm(my_formula, data=df, family = binomial(link="probit"))
          model_lm <- step(model_initial,
                           scope = list(upper=model_full),
                           direction="forward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "probit_foward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(length(unique(df[[dependent_col]])) > 2){
          model_initial <- mnp(formula = my_formula_initial, 
                               data = df, n.draws = multi_probit_draw, 
                               burnin = multi_probit_burn,
                               thin = multi_probit_thin, verbose = TRUE)
          model_full <- mnp(formula = my_formula, 
                            data = df, n.draws = multi_probit_draw, 
                            burnin = multi_probit_burn,
                            thin = multi_probit_thin, verbose = TRUE)
          model_lm <- step(model_initial,
                           scope = list(upper=model_full),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "multiprobit_forward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
      }
    }
    else if(stepwise_method=="backward"){
      if(factor_response_regr_method=="logit"){
        if(length(unique(df[[dependent_col]])) == 2){
          model_initial <- glm(my_formula_initial, data=df, family = binomial(link="logit"))
          model_full <- glm(my_formula, data=df, family = binomial(link="logit"))
          model_lm <- step(model_full,
                           scope = list(upper=model_initial),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "logit_backward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(length(unique(df[[dependent_col]])) > 2){
          model_initial <- multinom(formula=my_formula_initial, data = df)
          model_full <- multinom(formula=my_formula, data = df)
          model_lm <- step(model_full,
                           scope = list(upper=model_initial),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "multilogit_backward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
      }
      else if(factor_response_regr_method=="probit"){
        if(length(unique(df[[dependent_col]])) == 2){
          model_initial <- glm(my_formula_initial, data=df, family = binomial(link="probit"))
          model_full <- glm(my_formula, data=df, family = binomial(link="probit"))
          model_lm <- step(model_full,
                           scope = list(upper=model_initial),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "probit_backward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
        else if(length(unique(df[[dependent_col]])) > 2){
          model_initial <- mnp(formula = my_formula_initial, 
                               data = df, n.draws = multi_probit_draw, 
                               burnin = multi_probit_burn,
                               thin = multi_probit_thin, verbose = TRUE)
          model_full <- mnp(formula = my_formula, 
                            data = df, n.draws = multi_probit_draw, 
                            burnin = multi_probit_burn,
                            thin = multi_probit_thin, verbose = TRUE)
          model_lm <- step(model_full,
                           scope = list(upper=model_initial),
                           direction="backward",
                           test="Chisq",
                           data=df)
          model_type <- c(model_type, "multiprobit_backward_stepwise")
          model_formula <- c(model_formula, as.character(model_lm$call)[2])
          model_aic <- c(model_aic, model_lm$aic)
          return(list(model_lm, model_type, model_formula, model_aic))
        }
      }
    }
  }
  
  sampling_result <- data.frame(model_type, model_formula, model_aic, n_model, model_rep)
  return(sampling_result)
}

DID_Regression <- function(df, dependent_var, interaction1="", interaction2=""){
  #1) validate that interaction var is correct (in binary 0 1)
  validate=0
  unique_int1 <- sort(unique(df[[interaction1]]))
  unique_int2 <- sort(unique(df[[interaction2]]))
  if(length(unique_int1) & all(unique_int1 ==  c(0,1))){
    validate <- validate + 1
  }
  if(length(unique_int1) & all(unique_int1 ==  c(0,1))){
    validate <- validate + 1
  }
  if(validate==2){
    #2) Perform DID using * interaction for both interaction1 and 2
    formula <- as.formula(dependent_var, " ~ ", interaction1, "*", interaction2)
    didreg = lm(formula, data = df)
    print(summary(didreg))
    return(didreg)
  }
}

regression_type_tips <- function(){
  writeLines("=============================== Tips which Types of Regression to use ========================================")
  writeLines("1) Choose Linear Regression if Independent and Dependent Variable are mostly can be fitted linearly")
  writeLines("2) Choose Polynomial Regression if Dependent variable have a large value than its Predictor (Independent variable)")
  writeLines("3) Choose Logistic/Probit Regression if Dependent Variable have 2 or more Outcomes (Binary or Multinomial) or Dependent is likely Categorical")
  writeLines("3b) Logistic Regression can be used if Homoscedastity assumpton is violated & Error is not normally distributed")
  writeLines("3c) Difference Between Probit and Logit Regression is: Logit and probit differ in how they define f(*).")
  writeLines("3d) The logit model uses something called the cumulative distribution function of the logistic distribution.")
  writeLines("3e) The probit model uses something called the cumulative distribution function of the standard normal distribution to define f()")
  writeLines("4) Choose Quantile Regression if we want to fit regression in specific Quantile of data (Good with data lots of Outlier)")
  writeLines("5) Choose Ridge Regression to help making Regression prevented from overfitting by using Regularization and Loss Function")
  writeLines("5b) Ridge Regression helpful to Large number of variables but Low Observations, and also if data have a high Multicollinearity, and if data is not normally distributed")
  writeLines("6) Lasso Regression is almost similiar to Ridge Regression, but with additional in-built variable selection, choose Lasso if model perform better than Ridge")
  writeLines("7) Elastic Net Regression is similiar to both Ridge and Lasso Regression, recommended to use if data contains highly correlated independent variables")
  writeLines("8) Principal Component Regression (PCR) if you have many independent variables OR multicollinearity exist in your data")
  writeLines("8b) PCR uses Dimensionality Reduction and Multicollinearity Removal to original features")
  writeLines("8c) PCR creates new Features which all created Features uncorrelated to each other and with max-min variance in order")
  writeLines("8d) PCR Components is a function of all features, so it cannot be used to explain which factor is affecting the dependent variable to what extent")
  writeLines("9) PLS is an alternate technique of PC Regression when you have independent variables highly correlated, also helpful if ther are large number of independent variables")
  writeLines("9b) Both techniques create new independent variables called components which are linear combinations of the original predictor variables")
  writeLines("9c) but PCR creates components to explain the observed variability in the predictor variables, without considering the response variable at all.")
  writeLines("9d) While PLS takes the dependent variable into account, and therefore often leads to models that are able to fit the dependent variable with fewer components.")
  writeLines("10) Support vector regression (SVM) can solve both linear and non-linear models. SVM uses non-linear kernel functions (such as polynomial) to find the optimal solution for non-linear models.")
  writeLines("10b) The main idea of SVR is to minimize error, individualizing the hyperplane which maximizes the margin.")
  writeLines("11 Ordinal Regression is used to predict ranked values, Example of ordinal variables - Survey responses (1 to 6 scale), patient reaction to drug dose (none, mild, severe)")
  writeLines("11b) Note for Choosing Poisson, Negative Binomial, and Quasi Poisson Regression")
  writeLines("11c) When the variance of count data is greater than the mean count, it is a case of overdispersion")
  writeLines("11d) The opposite of the previous statement is a case of under-dispersion")
  writeLines("12) Poisson regression is used when dependent variable has count data")
  writeLines("12b) The dependent variable must meet the following conditions for able to proceed with Poisson Regressions")
  writeLines("12c) 1) The dependent variable has a Poisson distribution.")
  writeLines("12d) 2) Counts cannot be negative.")
  writeLines("12e) 3) This method is not suitable on non-whole numbers.")
  writeLines("13) Negative binomial regression is similiar to Poisson Regression, difference is that NB Regression does not assume distribution of count having variance equal to its mean.")
  writeLines("13b) While poisson regression assumes the variance equal to its mean.")
  writeLines("14) Quasi Poisson Regression is Negative Binomial alternate regression, the difference is variance of a quasi-Poisson model is a linear function of the mean")
  writeLines("14b) while the variance of a negative binomial model is a quadratic function of the mean")
  writeLines("14c) Quasi Poisson Regression can also be used for over and under dispersed count data")
  writeLines("15) Cox Regression is suitable for time-to-event data")
  writeLines("15b) Example 1: Time from customer opened the account until attrition.")
  writeLines("15c) Example 2: Time after cancer treatment until death.")
  writeLines("15d) Example 3: Time from first heart attack to the second.")
  writeLines("16a) Tobit Regression is used to estimate linear relationships between variables when censoring exists in the dependent variable")
  writeLines("16b) Left censoring is when the event of interest has already occurred before enrollment'")
  writeLines("16c) Right censoring occurs when the event of interest does not occur before the end of study")
  writeLines("17) Stepwise Regression Forward and Backward (In order to fit Regression modelling step by step)")
  writeLines("18) MARS (Multivariate Adaptive Regression Splines)")
  writeLines("18b) is a generalized cross validation (GCV) to compare the performance of model subsets")
  writeLines("18c) in order to choose the best subset: lower values of GCV are better")
  writeLines("18d) Choose GCV instead RSS, since RSS on the training data is inadequate for comparing models") 
  writeLines("18e) because the RSS always increases as MARS terms are dropped.")
  # MARS Varibale Importance Notes:
  # https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline
  writeLines("19) LARS Modeling is Nonparametric Regression, Flexible Modeling & Prediction with Span Parameter")
  writeLines("20) least-angle regression (LARS) is an algorithm for fitting linear regression models to high-dimensional data")
}

data_regression <- function(df, coly, colx, type=1, 
                            poly_num=2, category_method = "logit",  
                            multi_probit_draw=200, multi_probit_burn=100, 
                            multi_probit_thin=25, stepwise_logit=TRUE, 
                            quantile_num=25, quantile_seq=TRUE, quantile_max=95, quantile_jump=5,
                            lambda_path_vector=c(), alpha_elastic_net=0.5, 
                            ncomp_pls=3, ordinal_enable_ftable = FALSE,
                            relationship="*",
                            lefttobit=0, righttobit=0,
                            loess_optimal_span=-1, coldate,
                            ggscatmat_enabled=FALSE, ggscatmat_factor_color="",
                            lars_method=1){
  library(lmtest)
  library(datasets)
  library(car)
  library(tseries)
  library(gvlma)
  library(e1071)
  library(quantreg) #For Quantile Regression
  library(glmnet) #For Ridge Regression
  library(pls) #For PCR (Principal Component Regression)
  library(plsdepot) 
  library(car)
  library(e1071)
  library(ordinal) #For Ordinal Regression
  library(MASS)
  library(survival) #For Tobit Regression
  library(AER) #For Affairs Dataset
  library(GGally) #For Stepwise Regression
  library(olsrr) #For Stepwise Regression
  library(earth) #For MARS (Mulivariate Adaptive Regression Splines)
  library(VGAM) #For Tobit Regression 2
  library(RcmdrPlugin.survival) #For Plot enrichment 
  library(ggiraphExtra)
  library(lattice)
  library(Metrics) #Use RMSE Metrics for evaluate Prediction over Actual Values in Regression
  library(gridExtra)
  library(gmodels) #For Cross Tables
  library(reshape2) #Reshape data 
  library(effects) #Plot Effects from Ordinal Regression
  library(dplyr)
  library(sandwich) #Make a Robust Standard Error for Poisson Regression Model
  library(msm) #Use DeltaMethod Function to calculate incident rate ratis for Poisson Regression
  library(lars) #Least Angle Regression
  library(mlogit) # Multinomial Logistic and Probit Regression 
  library(MNP) #Multinomial Logistic and Probit Dataset
  library(stringr)
  library(rlist)
  library(nnet)
  library(broom)
  
  if(type==1){
    writeLines("=======================================================================================")
    writeLines("======================== Making a Linear Regression Model/OLS Model====================")
    writeLines("=======================================================================================")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], " + ")
      }
    }
    
    model = lm(eval(parse(text=stringeval)), data=df.omit)
    model <- update(model, stringeval)
    
    x11()
    observed <- df[,coly]
    predicted = model$fitted.values
    df_comparison = data.frame(cbind(observed, predicted))
    rmse_value = rmse(observed, predicted)
    x = 1:nrow(df)
    plot(x, observed, col='blue', main="Observed and Predicted comparison of data", 
         sub=paste0("RMSE value of ", round(rmse_value, 3)))
    points(x, predicted,col='red')
    lines(x, observed, lwd=2, col="blue")
    lines(x, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8)
    
    if (length(colx) > 1)
    {
      writeLines("====================== Variable Correlation Matrix ===========================\n")
      print(cor(df)) 
    }
    writeLines("\n====================== Model Interpretation ==================================")
    robust_model = coeftest(model, vcov. = vcovHC, type = "HC1")
    print(robust_model)
    coef_table = cbind(robust_model, LL=confint(robust_model)[,1], UL = confint(robust_model)[,2])
    
    print("Regression Coefficients in Linear Regression")
    print(coef_table)
    
    writeLines("====================== Model Linearity in Plotting ===========================\n")
    
    x11()
    linearity_plotting = avPlots(model) #1 plot
    linear_test = gvlma(model)
    writeLines("=================== General Assumption Checking with GVLMA ===================\n")
    print(linear_test)
    
    x11()
    par(mfrow=c(1,3))
    writeLines("\n============================== Model Assumption Checking =====================\n")
    writeLines("1) Does the model Contains Autocorrelation? (These Only Applies to Time Series)")
    writeLines("H0: Model Contains Autocorrelation in it")
    writeLines("H1: Model Doesn't Contains Autocorrelation in it")
    writeLines("\nUsing Durbin Watson Test")
    autocorrelation_test = dwtest(model)
    print(autocorrelation_test)
    if(autocorrelation_test$p.value <= 0.05){
      message_autocor = paste0("With p-value of ",autocorrelation_test$p.value," Reject Null Hypothesis, Model Contains Autocorrelation!!! (Assumptions are not Acceptable)") 
    }
    else if(autocorrelation_test$p.value > 0.05){
      message_autocor = paste0("With p-value of ",autocorrelation_test$p.value," Accept Null Hypothesis, Model doesn't contains Autocorrelation (Assumptions are Acceptable)") 
    }
    writeLines("=========================== CONCLUSION 1 ==================================\n")
    writeLines(message_autocor)
    writeLines("\n===========================================================================\n")
    
    writeLines("\n2a) Does the model have a Normal Distribution in Error terms?")
    writeLines("H0: Residual have a Normal Distribution")
    writeLines("H1: Residual don't have a Normal Distribution")
    
    writeLines("\n2b) Does the model Contains Skewed Distribution?")
    writeLines("H0: Model Distribution is moderately Skewed or follows Normal Distribution")
    writeLines("H1: Model Distribution is Highly Skewed")
    writeLines("Note: Negative Skewness means Distribution mostly Skewed to the right")
    writeLines("Note: Positive Skewness means Distribution mostly Skewed to the left")
    
    writeLines("\n2c) Does the model Contains Kurtosis like Normal Distribution?")
    writeLines("H0: Model Distribution Kurtosis nearly follows Normal Distribution")
    writeLines("H1: Model Distribution Kurtosis differs from Normal Distribution\n")
    
    writeLines("\nUsing Shapiro Wilk Test")
    qqPlot(model$residuals) #2 plot
    plot(density(model$residuals), main="Model Residual Distibution", 
         ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(model$residuals), 2)))
    polygon(density(model$residuals), col="orange")
    normal_test = shapiro.test(model$residuals)
    kurtosis_pvalue = linear_test$GlobalTest$DirectionalStat2$pvalue
    skewness_pvalue = linear_test$GlobalTest$DirectionalStat1$pvalue
    print(normal_test)
    writeLines(paste("Kurtosis pvalue = ", kurtosis_pvalue, "(Look in Assumption Checking in GVLMA)"))
    writeLines(paste("Skewness pvalue = ", skewness_pvalue, "(Look in Assumption Checking in GVLMA)"))
    
    writeLines("\n\n=========================== CONCLUSION 2 ==================================\n")
    if(normal_test$p.value <= 0.05){
      message_normality = paste0("With Shapiro Wilk test p-value of ",normal_test$p.value," Reject Null Hypothesis, Residual don't have a Normal Distribution Error!!! (Assumptions are not Acceptable)") 
    }
    else if(normal_test$p.value > 0.05){
      message_normality = paste0("With Shapiro Wilk test p-value of ",normal_test$p.value," Accept Null Hypothesis, Residual have a Normal Distribution Error (Assumptions are Acceptable)") 
    }
    if(kurtosis_pvalue <= 0.05){
      message_kurtosis = paste0("With Kurtosis p-value of ",kurtosis_pvalue," Reject Null Hypothesis, Model Distribution is Highly Skewed (Assumptions are not Acceptable)") 
    }
    else if(kurtosis_pvalue > 0.05){
      message_kurtosis = paste0("With Kurtosis p-value of ",kurtosis_pvalue," Accept Null Hypothesis, Model Distribution is moderately Skewed or follows Normal Distribution Error (Assumptions are Acceptable)") 
    }
    if(skewness_pvalue <= 0.05){
      message_skewness = paste0("With Skewness p-value of ",skewness_pvalue," Reject Null Hypothesis, Model Distribution Kurtosis differs from Normal Distribution (Assumptions are not Acceptable)") 
    }
    else if(skewness_pvalue > 0.05){
      message_skewness = paste0("With Skewness p-value of ",skewness_pvalue," Accept Null Hypothesis, Model Distribution Kurtosis nearly follows Normal Distribution (Assumptions are Acceptable)") 
    }
    writeLines(message_normality)
    writeLines(message_kurtosis)
    writeLines(message_skewness)
    writeLines("\n===============================================================================\n")
    writeLines("\n3) Does the Error is Homoscedastity (Error have same Variance for all observation)?")
    writeLines("H0: Residual is Homoscedastity (Low Difference in Variance terms)")
    writeLines("H1: Residual is Heteroscedastity (High Difference in Variance terms)\n")
    writeLines("\nUsing Bresuch Pagan Test to Identify Homoscedastity")
    variance_test = bptest(model)
    print(variance_test)
    if(variance_test$p.value <= 0.05){
      message_variance = paste0("With p-value of ",variance_test$p.value," Reject Null Hypothesis, Residual is Heteroscedastity!!! (Assumptions are not Acceptable)") 
    }
    else if(variance_test$p.value > 0.05){
      message_variance = paste0("With p-value of ",variance_test$p.value," Accept Null Hypothesis, Residual is Homoscedastity (Assumptions are Acceptable)") 
    }
    writeLines("=========================== CONCLUSION 3 ==================================\n")
    writeLines(message_variance)
    writeLines("\n===========================================================================\n")
    if (length(colx) > 1)
    {
      writeLines("\n4) Does the Variable have Multicollinearity (Variable Contains High Dependency to others)?")
      writeLines("H0: Doesn't Contains Multicollinearity (Variables each are Independently)")
      writeLines("H1: Contains Multicollinearity (There is one or more Variables are dependent)\n")
      writeLines("\nChecking Multicollinearity with VIF Test")
      multicollinear_test = vif(model)
      print(multicollinear_test)
      if(length(which((multicollinear_test<10)==FALSE)) > 0){
        message_multicollinear = paste0("With VIF>10 applies to 1 or many variables then Reject Null Hypothesis, Model Contains Multicollinearity in Variable!!! (Assumptions are not Acceptable)") 
      }
      else if(length(which((multicollinear_test<10)==FALSE)) == 0){
        message_multicollinear = paste0("With VIF<10 applies to all variables then Accept Null Hypothesis, Model Doesn't Contains Multicollinearity in Variable (Assumptions are Acceptable)") 
      }
      writeLines("\n=========================== CONCLUSION 4 ==================================\n")
      writeLines(message_multicollinear)
      writeLines("\n===========================================================================\n")
    }
    writeLines("\n5) Do Independent Variable Linear against Dependent Variable?")
    writeLines("H0: There is Linearity relationship between Independent and Dependent Variable")
    writeLines("H1: There is Nonlinearity relationship between Independent and Dependent Variable\n")
    linear_pvalue = linear_test$GlobalTest$GlobalStat4$pvalue
    writeLines(paste("Linearity pvalue = ", linear_pvalue, "(Look in Assumption Checking in GVLMA)"))
    if(linear_pvalue >= 0.05){
      message_linearity = paste0("With p-value of ",linear_pvalue," Accept Null Hypothesis, There is Linearity relationship between Independent and Dependent Variable (Assumptions are Acceptable)")
    }
    else if(linear_pvalue < 0.05){
      message_linearity = paste0("With p-value of ",linear_pvalue," Reject Null Hypothesis, There is Nonlinearity relationship between Independent and Dependent Variable (Assumptions are not Acceptable)") 
    }
    writeLines("\n=========================== CONCLUSION 5 ==================================\n")
    writeLines(message_linearity)
    writeLines("\n===========================================================================\n")
    writeLines("6) Is there any Outliers in residual?")
    writeLines("Using Boxplot to Identify Outliers")
    outlier_checking = boxplot(model$residuals)$out
    writeLines("\n=========================== CONCLUSION 6 ==================================\n")
    writeLines(paste("There is = ", length(outlier_checking), "Residual Categorized as an Outlier"))
    writeLines("Residual Observation as an Outlier: ")
    print(outlier_checking)
    writeLines("\n===========================================================================\n")
    model_relationship <- tidy(model)
    return(list(model, model_relationship, coef_table, df_comparison))
  }
  else if(type==2){
    writeLines("=======================================================================================")
    writeLines("======================== Making a Polynomial Regression Model =========================")
    writeLines("=======================================================================================")
    stringeval = paste0(coly," ~ poly(",colx,",",poly_num,")")
    model = lm(eval(parse(text=stringeval)), data=df)
    model <- update(model, stringeval)
    
    writeLines("\n====================== Model Interpretation ==================================")
    robust_model = coeftest(model, vcov. = vcovHC, type = "HC1")
    print(robust_model)
    coef_table = cbind(robust_model, LL=confint(robust_model)[,1], UL = confint(robust_model)[,2])
    
    print("Regression Coefficients in Polynomial Regression")
    print(coef_table)
    
    x11()
    observed <- df[,coly]
    predicted = model$fitted.values
    df_comparison = data.frame(cbind(observed, predicted))
    rmse_value = rmse(observed, predicted)
    x = 1:nrow(df)
    plot(x, observed, col='blue', main="Observed and Predicted comparison of data", 
         sub=paste0("RMSE value of ", round(rmse_value, 3)))
    points(x, predicted,col='red')
    lines(x, observed, lwd=2, col="blue")
    lines(x, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8)
    
    x11()
    writeLines("====================== Variable Correlation Matrix ===========================\n")
    print(cor(df))
    par(mfrow=c(1,3))
    writeLines("\n============================== Model Assumption Checking =====================\n")
    writeLines("1) Does the model Contains Autocorrelation? (These Only Applies to Time Series)")
    writeLines("H0: Model Contains Autocorrelation in it")
    writeLines("H1: Model Doesn't Contains Autocorrelation in it")
    writeLines("\nUsing Durbin Watson Test")
    autocorrelation_test = dwtest(model)
    print(autocorrelation_test)
    if(autocorrelation_test$p.value <= 0.05){
      message_autocor = paste0("With p-value of ",autocorrelation_test$p.value," Reject Null Hypothesis, Model Contains Autocorrelation!!! (Assumptions are not Acceptable)") 
    }
    else if(autocorrelation_test$p.value > 0.05){
      message_autocor = paste0("With p-value of ",autocorrelation_test$p.value," Accept Null Hypothesis, Model doesn't contains Autocorrelation (Assumptions are Acceptable)") 
    }
    writeLines("=========================== CONCLUSION 1 ==================================\n")
    writeLines(message_autocor)
    writeLines("\n===========================================================================\n")
    
    writeLines("\n2) Does the model have a Normal Distribution in Error terms?")
    writeLines("H0: Residual have a Normal Distribution")
    writeLines("H1: Residual don't have a Normal Distribution")
    
    writeLines("\nUsing Shapiro Wilk Test")
    qqPlot(model$residuals) #2 plot
    plot(density(model$residuals), main="Model Residual Distibution", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(model$residuals), 2)))
    polygon(density(model$residuals), col="orange")
    normal_test = shapiro.test(model$residuals)
    
    writeLines("\n\n=========================== CONCLUSION 2 ==================================\n")
    if(normal_test$p.value <= 0.05){
      message_normality = paste0("With Shapiro Wilk test p-value of ",normal_test$p.value," Reject Null Hypothesis, Residual don't have a Normal Distribution Error!!! (Assumptions are not Acceptable)") 
    }
    else if(normal_test$p.value > 0.05){
      message_normality = paste0("With Shapiro Wilk test p-value of ",normal_test$p.value," Accept Null Hypothesis, Residual have a Normal Distribution Error (Assumptions are Acceptable)") 
    }
    writeLines(message_normality)
    writeLines("\n===============================================================================\n")
    writeLines("\n3) Does the Error is Homoscedastity (Error have same Variance for all observation)?")
    writeLines("H0: Residual is Homoscedastity (Low Difference in Variance terms)")
    writeLines("H1: Residual is Heteroscedastity (High Difference in Variance terms)\n")
    writeLines("\nUsing Bresuch Pagan Test to Identify Homoscedastity")
    variance_test = bptest(model)
    print(variance_test)
    if(variance_test$p.value <= 0.05){
      message_variance = paste0("With p-value of ",variance_test$p.value," Reject Null Hypothesis, Residual is Heteroscedastity!!! (Assumptions are not Acceptable)") 
    }
    else if(variance_test$p.value > 0.05){
      message_variance = paste0("With p-value of ",variance_test$p.value," Accept Null Hypothesis, Residual is Homoscedastity (Assumptions are Acceptable)") 
    }
    writeLines("\n=========================== CONCLUSION 3 ==================================\n")
    writeLines(message_variance)
    writeLines("\n===========================================================================\n")
    writeLines("4) Is there any Outliers in residual?")
    writeLines("Using Boxplot to Identify Outliers")
    outlier_checking = boxplot(model$residuals)$out
    writeLines("\n=========================== CONCLUSION 4 ==================================\n")
    writeLines(paste("There is = ", length(outlier_checking), "Residual Categorized as an Outlier"))
    writeLines("Residual Observation as an Outlier: ")
    print(outlier_checking)
    writeLines("\n===========================================================================\n")
    model_relationship <- tidy(model)
    return(list(model, model_relationship, coef_table, df_comparison))
  }
  else if(type==3){
    #https://rcompanion.org/rcompanion/e_07.
    writeLines("=======================================================================================")
    writeLines("================= Making a Logistic/Probit Regression Model ===========================")
    writeLines("=======================================================================================")
    stringevalnull <- paste0(coly," ~ 1")
    stringeval <- paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval <- paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval <- paste0(stringeval, colx[x], " + ")
      }
    }
    unique_coly <- length(unique(df.omit[,coly])) 
    model.null = NULL
    model.full = NULL
    
    if(unique_coly == 2)
    {
      if(category_method == "logit")
      {
        writeLines("================= Choose Logistic Regression to fit ===========================")
        model.null = glm(eval(parse(text=stringevalnull)),
                         data=df, family = binomial(link="logit"))
        model.full = glm(eval(parse(text=stringeval)),
                         data=df, family = binomial(link="logit"))
        model.null <- update(model.null, stringevalnull)
        model.full <- update(model.full, stringeval)
        if(stepwise_logit == TRUE){
          model_step <- step(model.null,
                             scope = list(upper=model.full),
                             direction="both",
                             test="Chisq",
                             data=df)
          print(model_step)
          model.final = model_step 
          writeLines("\n====================== Model Interpretation ==================================")
          print(summary(model.final))
          print(anova(model.final, model.null, test="Chisq"))
          x11()
          observed <- df[,coly]
          predicted = as.numeric(model.final$fitted.values)
          if(all(round(predicted,1) == 0.5)){
            writeLines("Warning, Optimal Stepwise Logistic Model Produce Flat Probability!")
          }else{
            predicted[which(predicted < 0.5)] = 0
            predicted[which(predicted >= 0.5)] = 1
          }
          df_comparison = data.frame(cbind(observed, predicted))
          CrossTable(df_comparison$observed, df_comparison$predicted)
          rmse_value = rmse(observed, predicted)
          x = 1:nrow(df)
          plot(x, observed, col='blue', main="Observed and Predicted comparison of data", 
               sub=paste0("RMSE value of ", round(rmse_value, 3)))
          points(x, predicted,col='red')
          lines(x, observed, lwd=2, col="blue")
          lines(x, predicted, lwd=2, col="red")
          legend("bottomleft", legend=c("Observed", "Predicted"), 
                 col=c("blue","red"), lty=1:2, cex=0.8)
          
          return(list(model.final, df_comparison))
        }
        else if(stepwise_logit == FALSE){
          model.final = model.full
          writeLines("\n====================== Model Interpretation ==================================")
          print(summary(model.final))
          observed <- df[,coly]
          predicted = as.numeric(model.final$fitted.values)
          if(all(round(predicted,1) == 0.5)){
            writeLines("Warning, Optimal Stepwise Logistic Model Produce Flat Probability!")
          }else{
            predicted[which(predicted < 0.5)] = 0
            predicted[which(predicted >= 0.5)] = 1
          }
          df_comparison = data.frame(cbind(observed, predicted))
          CrossTable(df_comparison$observed, df_comparison$predicted)
          x11()
          rmse_value = rmse(observed, predicted)
          x = 1:nrow(df)
          plot(x, observed, col='blue', main="Observed and Predicted comparison of data", 
               sub=paste0("RMSE value of ", round(rmse_value, 3)))
          points(x, predicted,col='red')
          lines(x, observed, lwd=2, col="blue")
          lines(x, predicted, lwd=2, col="red")
          legend("bottomleft", legend=c("Observed", "Predicted"), 
                 col=c("blue","red"), lty=1:2, cex=0.8)
          return(list(model.final, df_comparison))
        }
      }
      else if(category_method == "probit")
      {
        writeLines("================= Choose Probit Regression to fit ===========================")
        model.null = glm(eval(parse(text=stringevalnull)),
                         data=df, family = binomial(link="probit"))
        model.full = glm(eval(parse(text=stringeval)),
                         data=df, family = binomial(link="probit"))
        model.null <- update(model.null, stringevalnull)
        model.full <- update(model.full, stringeval)
        if(stepwise_logit == TRUE){
          model_step <- step(model.null,
                             scope = list(upper=model.full),
                             direction="both",
                             test="Chisq",
                             data=df.omit)
          print(model_step)
          model.final = model_step 
          print(anova(model.final, model.null, test="Chisq"))
          print("Prediction Values: ")
          print(model.final$fitted.values)
          x11()
          observed <- df[,coly]
          predicted = as.numeric(model.final$fitted.values)
          if(all(round(predicted,1) == 0.5)){
            writeLines("Warning, Optimal Stepwise Logistic Model Produce Flat Probability!")
          }else{
            predicted[which(predicted < 0.5)] = 0
            predicted[which(predicted >= 0.5)] = 1
          }
          df_comparison = data.frame(cbind(observed, predicted))
          CrossTable(df_comparison$observed, df_comparison$predicted)
          rmse_value = rmse(observed, predicted)
          x = 1:nrow(df)
          plot(x, observed, col='blue', main="Observed and Predicted comparison of data", 
               sub=paste0("RMSE value of ", round(rmse_value, 3)))
          points(x, predicted,col='red')
          lines(x, observed, lwd=2, col="blue")
          lines(x, predicted, lwd=2, col="red")
          legend("bottomleft", legend=c("Observed", "Predicted"), 
                 col=c("blue","red"), lty=1:2, cex=0.8)
          
          return(list(model.final, df_comparison))
        }
        else if(stepwise_logit == FALSE){
          model.final = model.full
          observed <- df[,coly]
          writeLines("\n====================== Model Interpretation ==================================")
          print(summary(model.final))
          predicted = as.numeric(model.final$fitted.values)
          if(all(round(predicted,1) == 0.5)){
            writeLines("Warning, Optimal Stepwise Logistic Model Produce Flat Probability!")
          }else{
            predicted[which(predicted < 0.5)] = 0
            predicted[which(predicted >= 0.5)] = 1
          }
          df_comparison = data.frame(cbind(observed, predicted))
          CrossTable(df_comparison$observed, df_comparison$predicted)
          x11()
          rmse_value = rmse(observed, predicted)
          x = 1:nrow(df)
          plot(x, observed, col='blue', main="Observed and Predicted comparison of data", 
               sub=paste0("RMSE value of ", round(rmse_value, 3)))
          points(x, predicted,col='red')
          lines(x, observed, lwd=2, col="blue")
          lines(x, predicted, lwd=2, col="red")
          legend("bottomleft", legend=c("Observed", "Predicted"), 
                 col=c("blue","red"), lty=1:2, cex=0.8)
          return(list(model.final, df_comparison))
        }
      }
    }
    else if(unique_coly > 2)
    {
      if(category_method == "logit")
      {
        #https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/#:~:text=Multinomial%20logistic%20regression%20is%20used,page%20uses%20the%20following%20packages.
        formula_string <<- stringeval
        multinom_model <- multinom(formula=formula_string, data = df)
        multinom_model <- update(multinom_model, formula_string)
        print(summary(multinom_model))
        z_statistic <- summary(multinom_model)$coefficients / summary(multinom_model)$standard.errors
        p_value <- (1 - pnorm(abs(z_statistic), 0, 1)) * 2
        writeLines("P Value for Multinomial Models")
        print(p_value)
        predicted_multinom <- predict(multinom_model, newdata = df, "probs")
        writeLines("Predicted Values: ")
        print(predicted_multinom)
        highest_prob <- apply(predicted_multinom, 1, which.max)
        highest_prob <- levels(df[,which(colnames(df) %in% c(coly))])[highest_prob]
        df_comparison = data.frame(Actual = df[,which(colnames(df) %in% c(coly))], Predict = highest_prob)
        CrossTable(df_comparison$Actual, df_comparison$Predict)
        return(list(df_comparison))
      }
      else if(category_method == "probit")
      {
        formula_string <<- stringeval
        print(formula_string)
        model.full <- mnp(formula = formula_string, 
                          data = df, n.draws = multi_probit_draw, 
                          burnin = multi_probit_burn,
                          thin = multi_probit_thin, verbose = TRUE)
        #this is to update model call since mnp call is sensitive to use to predict models
        #and update function requires variable from global
        model.full <- update(model.full, formula_string) 
        rm(list=c("formula_string"))
        writeLines("\n====================== Model Interpretation ==================================")
        print(summary(model.full))
        mprobit_prediction <- predict(model.full, newdata = df)$p
        print(mprobit_prediction)
        highest_prob <- apply(mprobit_prediction, 1, which.max)
        df_comparison = data.frame(Actual = df[,coly], Predict = highest_prob)
        CrossTable(df_comparison$Actual, df_comparison$Predict)
        return(list(model.full, df_comparison))
      }
    }
  }
  else if(type==4){
    writeLines("=======================================================================================")
    writeLines("======================== Making a Quantile Regression Model ===========================")
    writeLines("=======================================================================================\n")
    writeLines("Quantile Regression Parameter:\n")
    if (quantile_seq == TRUE){ 
      writeLines("Quantile Type: Sequence") 
      writeLines(paste0("Quantile Regression Start: ",quantile_num))
      writeLines(paste0("Quantile Regression End: ",quantile_max))
      writeLines(paste0("Quantile Regression Loop By: ",quantile_jump))
    }
    else if (quantile_seq == FALSE){ 
      writeLines("Quantile Type: Fixed") 
      writeLines(paste0("Quantile Regression Start: ",quantile_num))
    }
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], " + ")
      }
    }
    if(quantile_seq == TRUE){
      model = rq(eval(parse(text=stringeval)),data = df, 
                 tau = seq(quantile_num/100, quantile_max/100, quantile_jump/100)) 
    }
    else if(quantile_seq == FALSE){
      model = rq(eval(parse(text=stringeval)),data = df, 
                 tau = quantile_num) 
    }
    model <- update(model, stringeval)
    print("Model Interpretation")
    print(summary(model))
    
    #coef_table = cbind(coef(summary(model)), LL=confint(model)[,1], UL = confint(model)[,2])
    #print("Regression Coefficients in Quantile Regression")
    #print(coef_table)
    
    x11()
    plot(model)
    
    quantile_predict <- predict(model)
    colnames(quantile_predict) <- colnames(model$coefficients)
    print("Model Fitted Values")
    print(head(quantile_predict))
    
    length_tau <- length(colnames(quantile_predict))
    tau_colnames <- colnames(quantile_predict)
    mfrow_x = 1
    mfrow_y = 1
    
    if(length_tau == 1){
      mfrow_x = 1
      mfrow_y = 1
    }
    else if(length_tau > 1 && length_tau < 5){
      mfrow_x = 2
      mfrow_y = 2
    }
    else if(length_tau >= 5 && length_tau <=10){
      mfrow_x = 5
      mfrow_y = 2
    }
    else if(length_tau > 10){
      divider = as.integer(length_tau / 5)
      if(divider %% 5 != 0){
        divider = divider + 1
      }
      mfrow_x = 5
      mfrow_y = divider
    }
    
    x11()
    par(mfrow=c(mfrow_x, mfrow_y))
    df_comparison = data.frame()
    for(plot in 1:length_tau){
      observed <- df[,coly]
      predicted = quantile_predict[,plot]
      df_part = cbind(tau = tau_colnames[plot], observed = observed, predicted = predicted)
      df_comparison = rbind(df_comparison, df_part)
      rmse_value = rmse(observed, predicted)
      x = 1:nrow(df)
      plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with ",tau_colnames[plot]), 
           sub=paste0("RMSE value of ", round(rmse_value, 3)))
      points(x, predicted,col='red')
      lines(x, observed, lwd=2, col="blue")
      lines(x, predicted, lwd=2, col="red")
      legend("bottomleft", legend=c("Observed", "Predicted"), 
             col=c("blue","red"), lty=1:2, cex=0.8) 
    }
    
    return(list(model, quantile_predict, df_comparison))
  }
  else if(type==5){
    indexy = grep(coly, colnames(df))
    dependent_var = df[indexy]
    independent_var = df[-indexy]
    set.seed(123) #Setting the seed to get similar results.
    if(length(lambda_path_vector) == 0){
      writeLines("=======================================================================================")
      writeLines("======================== Making a Ridge Regression Model ==============================")
      writeLines("=======================================================================================\n")
      writeLines("Note: Alpha=0 for Ridge Regression in glmnet parameter")
      writeLines("Using generalized models, need input Matrix to further process!")
      model = cv.glmnet(y=as.matrix(dependent_var),x=as.matrix(independent_var),alpha = 0)
      print("Model Interpretation")
      print(summary(model))
      x11()
      plot(model, sub="MSE Distribution by ridge Model")
      
      ridge_coeff = predict(model, type = "coefficients")
      print(ridge_coeff)
      
      ridge_predict = predict(model, newx=as.matrix(independent_var))
      colnames(ridge_predict) <- c("ridge Model")
      
      x11()
      df_comparison = data.frame()
      observed <- df[,coly]
      predicted = ridge_predict[,1]
      df_comparison = as.data.frame(cbind(observed = observed, predicted = predicted))
      rmse_value = rmse(observed, predicted)
      x = 1:nrow(df)
      plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with Ridge Regression"), 
           sub=paste0("RMSE value of ", round(rmse_value, 3)))
      points(x, predicted,col='red')
      lines(x, observed, lwd=2, col="blue")
      lines(x, predicted, lwd=2, col="red")
      legend("bottomleft", legend=c("Observed", "Predicted"), 
             col=c("blue","red"), lty=1:2, cex=0.8) 
      
      
      return(list(model, ridge_coeff, ridge_predict, df_comparison))
    }else{
      writeLines("===================================================================================================")
      writeLines("============== Making a Custom Generalized Model with Set of Lambdas ==============================")
      writeLines("===================================================================================================\n")
      writeLines("Note: Alpha=1 for ridge Regression in glmnet parameter")
      writeLines("Using generalized models, need input Matrix to further process!")
      model = cv.glmnet(as.matrix(dependent_var),as.matrix(independent_var),
                        alpha = 1,lambda = lambda_path_vector)
      lambda_path_vector_summary = summary(lambda_path_vector)
      names(lambda_path_vector_summary) = paste(names(lambda_path_vector_summary), "Lambda")
      lambda_colname <- paste(names(lambda_path_vector_summary), as.numeric(lambda_path_vector_summary))
      
      print("Model Interpretation")
      print(model)
      x11()
      plot(model, sub="MSE Distribution by Log Lambda")
      
      ridge_coeff = predict(model,s = c(as.numeric(lambda_path_vector_summary)),type = "coefficients")
      colnames(ridge_coeff) = lambda_colname
      print(ridge_coeff)
      
      ridge_predict = predict(model, newx=as.matrix(dependent_var), 
                              s=c(as.numeric(lambda_path_vector_summary)))
      colnames(ridge_predict) <- lambda_colname
      
      length_lambda <- length(colnames(ridge_predict))
      mfrow_x = 3
      mfrow_y = 2
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      df_comparison = data.frame()
      for(plot in 1:length_lambda){
        observed <- df[,coly]
        predicted = ridge_predict[,plot]
        df_part = cbind(lambda = lambda_colname[plot], observed = observed, predicted = predicted)
        df_comparison = rbind(df_comparison, df_part)
        rmse_value = rmse(observed, predicted)
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with ",lambda_colname[plot]), 
             sub=paste0("RMSE value of ", round(rmse_value, 3)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8) 
      }
      
      return(list(model, ridge_coeff, ridge_predict, df_comparison))
    }
  }
  else if(type==6){
    indexy = grep(coly, colnames(df))
    dependent_var = df[indexy]
    independent_var = df[-indexy]
    set.seed(123) #Setting the seed to get similar results.
    if(length(lambda_path_vector) == 0){
      writeLines("=======================================================================================")
      writeLines("======================== Making a Lasso Regression Model ==============================")
      writeLines("=======================================================================================\n")
      writeLines("Note: Alpha=1 for Lasso Regression in glmnet parameter")
      writeLines("Using generalized models, need input Matrix to further process!")
      model = cv.glmnet(y=as.matrix(dependent_var),x=as.matrix(independent_var),alpha = 1)
      print("Model Interpretation")
      print(summary(model))
      x11()
      plot(model, sub="MSE Distribution by Lasso Model")
      
      lasso_coeff = predict(model, type = "coefficients")
      print(lasso_coeff)
      
      lasso_predict = predict(model, newx=as.matrix(independent_var))
      colnames(lasso_predict) <- c("Lasso Model")
      
      x11()
      df_comparison = data.frame()
      observed <- df[,coly]
      predicted = lasso_predict[,1]
      df_comparison = as.data.frame(cbind(observed = observed, predicted = predicted))
      rmse_value = rmse(observed, predicted)
      x = 1:nrow(df)
      plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with Lasso Regression"), 
           sub=paste0("RMSE value of ", round(rmse_value, 3)))
      points(x, predicted,col='red')
      lines(x, observed, lwd=2, col="blue")
      lines(x, predicted, lwd=2, col="red")
      legend("bottomleft", legend=c("Observed", "Predicted"), 
             col=c("blue","red"), lty=1:2, cex=0.8) 
      
      
      return(list(model, lasso_coeff, lasso_predict, df_comparison))
    }else{
      writeLines("===================================================================================================")
      writeLines("============== Making a Custom Generalized Model with Set of Lambdas ==============================")
      writeLines("===================================================================================================\n")
      writeLines("Note: Alpha=1 for Lasso Regression in glmnet parameter")
      writeLines("Using generalized models, need input Matrix to further process!")
      model = cv.glmnet(as.matrix(dependent_var),as.matrix(independent_var),
                        alpha = 1,lambda = lambda_path_vector)
      lambda_path_vector_summary = summary(lambda_path_vector)
      names(lambda_path_vector_summary) = paste(names(lambda_path_vector_summary), "Lambda")
      lambda_colname <- paste(names(lambda_path_vector_summary), as.numeric(lambda_path_vector_summary))
      
      print("Model Interpretation")
      print(model)
      x11()
      plot(model, sub="MSE Distribution by Log Lambda")
      
      lasso_coeff = predict(model,s = c(as.numeric(lambda_path_vector_summary)),type = "coefficients")
      colnames(lasso_coeff) = lambda_colname
      print(lasso_coeff)
      
      lasso_predict = predict(model, newx=as.matrix(dependent_var), 
                              s=c(as.numeric(lambda_path_vector_summary)))
      colnames(lasso_predict) <- lambda_colname
      
      length_lambda <- length(colnames(lasso_predict))
      mfrow_x = 3
      mfrow_y = 2
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      df_comparison = data.frame()
      for(plot in 1:length_lambda){
        observed <- df[,coly]
        predicted = lasso_predict[,plot]
        df_part = cbind(lambda = lambda_colname[plot], observed = observed, predicted = predicted)
        df_comparison = rbind(df_comparison, df_part)
        rmse_value = rmse(observed, predicted)
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with ",lambda_colname[plot]), 
             sub=paste0("RMSE value of ", round(rmse_value, 3)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8) 
      }
      
      return(list(model, lasso_coeff, lasso_predict, df_comparison))
    }
  }
  else if(type==7){
    indexy = grep(coly, colnames(df))
    dependent_var = df[indexy]
    independent_var = df[-indexy]
    set.seed(123) #Setting the seed to get similar results.
    if(length(lambda_path_vector) == 0){
      writeLines("=======================================================================================")
      writeLines("======================== Making a Elastic Net Regression Model ==============================")
      writeLines("=======================================================================================\n")
      writeLines("Note: Alpha=0.1-0.9 for Elastic Net Regression in glmnet parameter")
      writeLines("Using generalized models, need input Matrix to further process!")
      model = cv.glmnet(y=as.matrix(dependent_var),x=as.matrix(independent_var),alpha = alpha_elastic_net)
      print("Model Interpretation")
      print(summary(model))
      x11()
      plot(model, sub="MSE Distribution by Elastic Net Model")
      
      elastic_net_coeff = predict(model, type = "coefficients")
      print(elastic_net_coeff)
      
      elastic_net_predict = predict(model, newx=as.matrix(independent_var))
      colnames(elastic_net_predict) <- c("Elastic Net Model")
      
      x11()
      df_comparison = data.frame()
      observed <- df[,coly]
      predicted = elastic_net_predict[,1]
      df_comparison = as.data.frame(cbind(observed = observed, predicted = predicted))
      rmse_value = rmse(observed, predicted)
      x = 1:nrow(df)
      plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with Elastic Net Model"), 
           sub=paste0("RMSE value of ", round(rmse_value, 3)))
      points(x, predicted,col='red')
      lines(x, observed, lwd=2, col="blue")
      lines(x, predicted, lwd=2, col="red")
      legend("bottomleft", legend=c("Observed", "Predicted"), 
             col=c("blue","red"), lty=1:2, cex=0.8) 
      
      
      return(list(model, elastic_net_coeff, elastic_net_predict, df_comparison))
    }else{
      writeLines("===================================================================================================")
      writeLines("============== Making a Custom Generalized Model with Set of Lambdas ==============================")
      writeLines("===================================================================================================\n")
      writeLines("Note: Alpha=1 for elastic_net Regression in glmnet parameter")
      writeLines("Using generalized models, need input Matrix to further process!")
      model = cv.glmnet(as.matrix(dependent_var),as.matrix(independent_var),
                        alpha = 1,lambda = lambda_path_vector)
      lambda_path_vector_summary = summary(lambda_path_vector)
      names(lambda_path_vector_summary) = paste(names(lambda_path_vector_summary), "Lambda")
      lambda_colname <- paste(names(lambda_path_vector_summary), as.numeric(lambda_path_vector_summary))
      
      print("Model Interpretation")
      print(model)
      x11()
      plot(model, sub="MSE Distribution by Log Lambda")
      
      elastic_net_coeff = predict(model,s = c(as.numeric(lambda_path_vector_summary)),type = "coefficients")
      colnames(elastic_net_coeff) = lambda_colname
      print(elastic_net_coeff)
      
      elastic_net_predict = predict(model, newx=as.matrix(dependent_var), 
                                    s=c(as.numeric(lambda_path_vector_summary)))
      colnames(elastic_net_predict) <- lambda_colname
      
      length_lambda <- length(colnames(elastic_net_predict))
      mfrow_x = 3
      mfrow_y = 2
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      df_comparison = data.frame()
      for(plot in 1:length_lambda){
        observed <- df[,coly]
        predicted = elastic_net_predict[,plot]
        df_part = cbind(lambda = lambda_colname[plot], observed = observed, predicted = predicted)
        df_comparison = rbind(df_comparison, df_part)
        rmse_value = rmse(observed, predicted)
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with ",lambda_colname[plot]), 
             sub=paste0("RMSE value of ", round(rmse_value, 3)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8) 
      }
      
      return(list(model, elastic_net_coeff, elastic_net_predict, df_comparison))
    }
  }
  else if(type==8){
    writeLines("=======================================================================================")
    writeLines("======================== Making a Principal Component Regression Model ================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], " + ")
      }
    }
    set.seed(123) #Setting the seed to get similar results.
    pcr_model <- pcr(eval(parse(text=stringeval)), data = df, scale = TRUE, validation = "CV")
    pcr_model <- update(pcr_model, stringeval)
    print("Model Interpretation")
    summary(pcr_model)
    
    #coef_table = cbind(coef(summary(pcr_model)), LL=confint(pcr_model)[,1], UL = confint(pcr_model)[,2])
    #print("Regression Coefficients in Linear Regression")
    #print(coef_table)
    
    x11()
    plot(pcr_model)
    x11()
    validationplot(pcr_model, val.type = "MSEP")
    x11()
    validationplot(pcr_model, val.type = "R2")
    best_ncomp = which(pcr_model$validation$PRESS == min(pcr_model$validation$PRESS))
    pred = predict(pcr_model, df, ncomp = best_ncomp)
    writeLines(paste("\nUsing Component = ",best_ncomp))
    writeLines("Prediction Results")
    print(pred)
    
    observed <- df[,coly]
    predicted = pred
    df_comparison = data.frame(cbind(observed, predicted))
    rmse_value = rmse(observed, predicted)
    x = 1:nrow(df)
    plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with Best Component = ",best_ncomp), 
         sub=paste0("RMSE value of ", round(rmse_value, 3)))
    points(x, predicted,col='red')
    lines(x, observed, lwd=2, col="blue")
    lines(x, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8) 
    return(list(pcr_model, predicted))
  }
  else if(type==9){
    #https://stackoverflow.com/questions/51445926/what-do-x-scores-and-y-scores-represent-in-partial-least-squares-regression-in-r
    #X-scores, usually denoted as T, are the predictors of Y and at the same time they model X. 
    #X-scores are the linear combinations of original X variables estimated with the weights coefficients 
    #denoted as w. In the same way Y-scores, denoted as , multiplied by the weights c summarize Y variables.
    #X = TP + E
    #Y = UC + F
    writeLines("=======================================================================================")
    writeLines("======================== Making a Partial Least Square Model ==========================")
    writeLines("=======================================================================================\n")
    if(length(coly)==1){
      print("Detected only have 1 Responses Variables, Conduct Univariate Partial Least Square Model!")
      plsmodel <- plsreg1(df[,colx], df[,coly], comp = ncomp_pls)
      print("Model Interpretation")
      print("R Squared of Vector PLS (How much a Component describe data in percentage)")
      print(plsmodel$R2)
      print("Explained Variance of Variables by PLS Component")
      print(plsmodel$R2Xy)
      print("Regression Coefficients by PLS Model")
      print(plsmodel$reg.coefs)
      
      #coef_table = cbind(coef(summary(plsmodel)), LL=confint(plsmodel)[,1], UL = confint(plsmodel)[,2])
      #print("Regression Coefficients in Linear Regression")
      #print(coef_table)
      
      x11()
      plot(plsmodel, what="variables", main="Variable PLS Plot mapping", where=c("t","t"))
      x11()
      par(mfrow=c(1,3))
      plot(plsmodel, what="observations", main="Observations PLS Plot with X score Mapping", where=c("t","t"))
      plot(plsmodel, what="observations", main="Observations PLS Plot with X-Y score Mapping", where=c("t","u"))
      plot(plsmodel, what="observations", main="Observations PLS Plot with Y score Mapping", where=c("u","u"))
      x11()
      par(mfrow=c(1,1))
      print("PLS Model Predictions: ")
      print(plsmodel$y.pred)
      observed <- df[,coly]
      predicted = plsmodel$y.pred
      df_comparison <- data.frame(cbind(observed, predicted))
      rmse_value = rmse(observed, predicted)
      
      x = 1:nrow(df)
      plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data with Partial Component of ",ncomp_pls), 
           sub=paste0("RMSE value of ", round(rmse_value, 3)))
      points(x, predicted,col='red')
      lines(x, observed, lwd=2, col="blue")
      lines(x, predicted, lwd=2, col="red")
      legend("bottomleft", legend=c("Observed", "Predicted"), 
             col=c("blue","red"), lty=1:2, cex=0.8) 
      return(list(plsmodel, df_comparison))
    }
    else if(length(coly) > 1){
      print("Detected have more than 1 Responses Variables, Conduct Multivariate Partial Least Square Model!")
      plsmodel <- plsreg2(df[,colx], df[,coly], comp = ncomp_pls)
      print("Model Interpretation")
      print("Variable Importance for Projection")
      print(plsmodel$VIP)
      print("Regression COefficients by PLS Model")
      print(plsmodel$reg.coefs)
      
      #coef_table = cbind(coef(summary(plsmodel)), LL=confint(plsmodel)[,1], UL = confint(plsmodel)[,2])
      #print("Regression Coefficients in Linear Regression")
      #print(coef_table)
      
      x11()
      plot(plsmodel, what="variables", main="Variable PLS Plot mapping", where=c("t","t"))
      x11()
      par(mfrow=c(1,3))
      plot(plsmodel, what="observations", main="Observations PLS Plot with X score Mapping", where=c("t","t"))
      plot(plsmodel, what="observations", main="Observations PLS Plot with X-Y score Mapping", where=c("t","u"))
      plot(plsmodel, what="observations", main="Observations PLS Plot with Y score Mapping", where=c("u","u"))
      print("PLS Model Predictions: ")
      print(plsmodel$y.pred)
      
      length_predictions <- length(coly)
      mfrow_x = 1
      mfrow_y = 1
      
      if(length_predictions == 1){
        mfrow_x = 1
        mfrow_y = 1
      }
      else if(length_predictions > 1 && length_predictions < 5){
        mfrow_x = 2
        mfrow_y = 2
      }
      else if(length_predictions >= 5 && length_predictions <=10){
        mfrow_x = 5
        mfrow_y = 2
      }
      else if(length_predictions > 10){
        divider = as.integer(length_predictions / 5)
        if(divider %% 5 != 0){
          divider = divider + 1
        }
        mfrow_x = 5
        mfrow_y = divider
      }
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      df_comparison = data.frame()
      for(response in 1:length(coly)){
        observed <- df[,coly[response]]
        predicted = plsmodel$y.pred[,response]
        df_part = cbind(reponse = coly[response], observed=observed, predicted=predicted)
        df_comparison = rbind(df_part, df_comparison)
        rmse_value = rmse(observed, predicted)
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of Feature ",coly[response], " with component = ", ncomp_pls), 
             sub=paste0("RMSE value of ", round(rmse_value, 3)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8)  
      }
      return(list(plsmodel, df_comparison))
    }
  }
  else if(type==10){
    #https://www.kdnuggets.com/2017/03/building-regression-models-support-vector-regression.
    writeLines("=======================================================================================")
    writeLines("======================== Making a Support Vector Regression Model =====================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], " + ")
      }
    }
    #################### Simple SVR Modeling #######################
    svr.model <- svm(eval(parse(text=stringeval)) , df)
    svr.model <- update(svr.model, stringeval)
    pred <- predict(svr.model, df)
    
    #################### SVR with Parameter Tuning #################
    #Tune the SVM model
    writeLines("SVM Tuning in Progress.. ")
    OptModelsvm <- tune(svm, eval(parse(text=stringeval)), data=df,
                        ranges=list(epsilon=seq(0,1,0.1), cost=1:100))
    writeLines("SVM Tuning Done!")
    
    x11()
    plot(OptModelsvm, main="SVM Regression Performance")
    svr.model.tuned = OptModelsvm$best.model
    
    #Predict Y using best model
    pred_tuned = predict(svr.model.tuned,df)
    
    print("SVR Simple Model Summary")
    print(summary(svr.model))
    print("Model Prediction")
    print(pred)
    
    print("SVR Model Tuned Summary")
    print(summary(svr.model.tuned))
    print("Model Prediction")
    print(pred_tuned)
    
    x11()
    par(mfrow=c(1, 2))
    df_comparison = data.frame()
    for(model in 1:2){
      observed <- df[,coly]
      if(model == 1){
        predicted = pred 
        df_part = cbind(model = "Simple SVR Model", observed=observed, predicted=predicted)
        df_comparison = rbind(df_comparison, df_part)
        rmse_value = rmse(observed, predicted)
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison with Simple SVR"), 
             sub=paste0("RMSE value of ", round(rmse_value, 3)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8)  
      }
      else if(model == 2){
        predicted = pred_tuned 
        df_part = cbind(model = "SVR Model Tuning", observed=observed, predicted=predicted)
        df_comparison = rbind(df_comparison, df_part)
        rmse_value = rmse(observed, predicted)
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison with SVR Tuned Model"), 
             sub=paste0("RMSE value of ", round(rmse_value, 3)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8)  
      }
    }
    return(list(svr.model, svr.model.tuned, df_comparison))
  }
  else if(type==11){
    #https://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/
    #https://www.r-bloggers.com/how-to-perform-ordinal-logistic-regression-in-r/
    #polr uses the standard formula interface in R for specifying a regression model 
    #with outcome followed by predictors. We also specify Hess=TRUE to have the model 
    #return the observed information matrix from optimization (called the Hessian) 
    #which is used to get standard errors
    writeLines("=======================================================================================")
    writeLines("======================== Making an Ordinal Regression Model ===========================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    stringcrosstable = paste0("~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
        stringcrosstable = paste0(stringcrosstable, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], " + ")
        stringcrosstable = paste0(stringcrosstable, colx[x], " + ")
      }
    }
    o.model <- polr(eval(parse(text=stringeval)), data = df, Hess=TRUE)
    o.model <- update(o.model, stringeval)
    print("Model Summary")
    print(summary(o.model))
    
    if(ordinal_enable_ftable == TRUE){
      print("Factor Grouping Cross Tables")
      print(ftable(xtabs(eval(parse(text=stringcrosstable)), data = df))) 
    }
    
    coef_table <- coef(summary(o.model))
    p <- pnorm(abs(coef_table[, "t value"]), lower.tail = FALSE) * 2
    coef_table <- cbind(coef_table, "p value" = p)
    coef_table = cbind(coef_table, LL=confint(o.model)[,1], UL = confint(o.model)[,2])
    
    print("Regression Coefficients in Ordinal Regression")
    print(coef_table)
    
    #Combine Model Prediction into one in DF
    data_combine_prediction <- cbind(df, predict(o.model, df[,colx], type = "probs"))
    print("Inspect Dataset with Predicted Probability by Model")
    print(head(data_combine_prediction))
    
    color_length = length(unique(df[,coly]))
    colors_bold = c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03",
                    "#18fc03","#03fc5e","#03fcc6","#03f0fc","#03adfc","#035afc",
                    "#be03fc","#fc03f8","#fc03a1")
    individual_feature = length(colx)
    pos_combi = expand.grid(colx, colx)
    pos_combi = pos_combi[which(pos_combi$Var1 != pos_combi$Var2),]
    row.names(pos_combi) <- NULL
    pos_combi_real <- t(apply(pos_combi, 1, sort))
    pos_combi_real <- pos_combi_real[!duplicated(pos_combi_real),]
    pos_combi_real <- as.data.frame(pos_combi_real)
    pos_combi_real$V1 = as.character(pos_combi_real$V1)
    pos_combi_real$V2 = as.character(pos_combi_real$V2)
    #print(head(pos_combi_real))
    #print(pos_combi_real$V1[1])
    
    #Plotting Individual Effects
    x11()
    P_individual <- NULL
    for(x in 1:individual_feature){
      p <- plot(Effect(focal.predictors = c(colx[x]),o.model), multiline=TRUE, 
                colors=colors_bold[1:color_length]) 
      P_individual <- c(P_individual, list(p))
    }
    ncol_adjust = 0
    if(individual_feature <= 2){
      ncol_adjust=2
    }
    else if(individual_feature > 2 && individual_feature <=6){
      ncol_adjust=3
    }
    else if(individual_feature > 6 && individual_feature <=8){
      ncol_adjust=4
    }
    else if(individual_feature >8){
      ncol_adjust=5
    }
    do.call(grid.arrange, c(P_individual,ncol=ncol_adjust))
    
    #Plotting Combination Effects
    P_combination <- NULL
    for(x in 1:nrow(pos_combi_real)){
      p <- plot(Effect(focal.predictors = c(pos_combi_real$V1[x], pos_combi_real$V2[x]),o.model), 
                multiline=TRUE, colors=colors_bold[1:color_length]) 
      P_combination <- c(P_combination, list(p))
    }
    #Plot 2 Sided for every windows
    for(y in 1:(round(length(P_combination)/2))){
      x11()
      left_range = 1 + (2*(y-1))
      right_range = 2 + (2*(y-1))
      if(right_range > length(P_combination))
      {
        p_combination_part = P_combination[left_range] 
        do.call(grid.arrange, c(p_combination_part,ncol=1))
      }
      else if(right_range <= length(P_combination)){
        p_combination_part = P_combination[left_range:right_range]
        do.call(grid.arrange, c(p_combination_part,ncol=2))
      }
    }
    return(list(o.model, coef_table, data_combine_prediction))
  }
  else if(type==12){
    #https://stats.idre.ucla.edu/r/dae/poisson-regression/
    #https://www.theanalysisfactor.com/overdispersion-in-count-models-fit-the-model-to-the-data-dont-fit-the-data-to-the-model/
    writeLines("=======================================================================================")
    writeLines("======================== Making a Poisson Regression Model ============================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], relationship)
      }
    }
    #Model Poisson
    pos.model <- glm(eval(parse(text=stringeval)), data = df, family=poisson)
    pos.model <- update(pos.model, stringeval)
    print("Model Summary")
    print(summary(pos.model))
    
    coef_table = cbind(coef(summary(pos.model)), LL=confint(pos.model)[,1], UL = confint(pos.model)[,2])
    print("Regression Coefficients in Ordinal Regression")
    print(coef_table)
    
    #Make Robust Standard Error to control for mild violation 
    #of the distribution assumption that the variance equals the mean
    #by Cameron and Trivedi (2009)
    robust_cov <- vcovHC(pos.model, type="HC0")
    std.err <- sqrt(diag(robust_cov))
    r.est <- cbind(Estimate= coef(pos.model), "Robust SE" = std.err,
                   "Pr(>|z|)" = 2 * pnorm(abs(coef(pos.model)/std.err), lower.tail=FALSE),
                   LL = coef(pos.model) - 1.96 * std.err,
                   UL = coef(pos.model) + 1.96 * std.err)
    
    #use the residual deviance to perform a goodness of fit test for the overall model
    #if the residual difference is small enough, the goodness of fit test will not be significant, 
    #indicating that the model fits the data
    
    print("Deviance Information of Model")
    deviance_info = with(pos.model, cbind(res.deviance = deviance, df = df.residual,
                                          p = pchisq(deviance, df.residual, lower.tail=FALSE)))
    writeLines("H0: Model Reasonably fit the data (Mean = Variance)")
    writeLines("H1: Model is Overdispersed or Underdispersed (Mean != Variance)")
    print(deviance_info)
    if(deviance_info[3] <= 0.05){
      message("Warning! Chi Squared test Statistically Significant! \n The Model concluded is stated over-dispersed/under-dispersed the data! \nthis may happen if mean not equal to the variance")
    }
    else if(deviance_info[3] > 0.05){
      writeLines("Chi Squared test Statistically Not Significant! Model Fits the data reasonably (Enough Fit)\nthis can be stated mean is equal to the variance")
    }
    
    #Transform the Coefficient and SE Exponentially with Delta Method
    exponent_stringeval = "list("
    for(v in 1:length(coef(pos.model)))
    {
      if(v != length(coef(pos.model))){
        exponent_stringeval = paste0(exponent_stringeval, "~ exp(x", v, "), ")
      }
      else{
        exponent_stringeval = paste0(exponent_stringeval, "~ exp(x", v, ")) ")
      }
    }
    s <- deltamethod(eval(parse(text=exponent_stringeval)), 
                     coef(pos.model), robust_cov)
    
    #Drop Old P Values and Transform to Exponent values
    rexp.est <- exp(r.est[, -3])
    
    ## replace Robust SE with the Deltamethod (applied an exponent function)
    rexp.est[, "Robust SE"] <- s
    
    print("Incident Rate Summary for Coefficients")
    print(rexp.est)
    
    coef_names_ex_intercept = names(coef(pos.model))[-1]
    coef_names_individual = coef_names_ex_intercept[-grep(":",coef_names_ex_intercept)]
    coef_exp = rexp.est[2:nrow(rexp.est)]
    df_unique_factor = data.frame()
    for(all_feature in 1:length(colx)){
      if(is.factor(df[,colx[all_feature]])==TRUE){
        unique_value = unique(df[,colx[all_feature]])
        df_unique_factor = rbind(df_unique_factor, expand.grid(colx[all_feature], unique_value))
      }
    }
    df_unique_factor$Var1 = as.character(df_unique_factor$Var1)
    df_unique_factor$Var2 = as.character(df_unique_factor$Var2)
    df_unique_factor$full_factor_name = paste0(df_unique_factor$Var1, df_unique_factor$Var2)
    df_unique_factor$used_coef = NULL
    full_name_var = df_unique_factor$full_factor_name
    
    for(h in 1:nrow(df_unique_factor)){
      if(df_unique_factor$full_factor_name[h] %in% coef_names_individual){
        df_unique_factor$used_coef[h] <- TRUE
      }
      else{
        df_unique_factor$used_coef[h] <- FALSE
      }
    }
    
    base_reference_group = df_unique_factor[which(df_unique_factor$used_coef==FALSE),]$full_factor_name
    
    for(all_feature in 1:length(coef_names_ex_intercept)){
      parent_col = df_unique_factor[which(df_unique_factor$full_factor_name == coef_names_ex_intercept[all_feature]), ]$Var1
      combination_factor_col = grep(":", coef_names_ex_intercept)
      if(is.null(parent_col) && !(all_feature %in% combination_factor_col)){ #This is for non factor variables only
        if((coef_exp[all_feature]-1) < 0)
        {
          writeLines(paste("The decreased percent change in the incident rate of", coly,"is by",
                           abs(round((coef_exp[all_feature]-1)*100,2)),"% for every unit increase in",
                           coef_names_ex_intercept[all_feature])) 
        }
        else if((coef_exp[all_feature]-1) > 0){
          writeLines(paste("The increased percent change in the incident rate of", coly,"is by",
                           abs(round((coef_exp[all_feature]-1)*100,2)),"% for every unit increase in",
                           coef_names_ex_intercept[all_feature]))
        }
      }
      else{ #This is for factor variables and the combination factor variables
        if(all_feature %in% combination_factor_col){
          print(paste("incident rate for the correlation of",coef_names_ex_intercept[all_feature], "is",round(coef_exp[all_feature],2),
                      "times the incident rate for the correlation of reference group"))
        }
        else{
          base_reference_select = base_reference_group[grep(parent_col, base_reference_group)]
          print(paste("incident rate for",coef_names_ex_intercept[all_feature], "is",round(coef_exp[all_feature],2),
                      "times the incident rate for the reference group", base_reference_select)) 
        }
      }
    }
    x11()
    par(mfrow=c(2,3))
    plot(pos.model, which=1)
    plot(pos.model, which=2)
    plot(pos.model, which=3)
    plot(pos.model, which=4)
    plot(pos.model, which=5)
    plot(pos.model, which=6)
    
    predict_pois = predict(pos.model, type="response", se.fit=TRUE)
    
    x11()
    par(mfrow=c(1,1))
    observed = df[,coly]
    predicted = predict_pois$fit 
    df_comparison = data.frame(cbind(observed, predicted))
    rmse_value = rmse(observed, predicted)
    x = 1:nrow(df)
    plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison with Poisson Regression"), 
         sub=paste0("RMSE value of ", round(rmse_value, 3)))
    points(x, predicted,col='red')
    lines(x, observed, lwd=2, col="blue")
    lines(x, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8)  
    
    return(list(pos.model, coef_table, rexp.est, df_unique_factor, df_comparison))
  }
  else if(type==13){
    #https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/
    writeLines("=======================================================================================")
    writeLines("======================== Making a Negative Binomial Model =============================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], relationship)
      }
    }
    nb.model <- glm.nb(eval(parse(text=stringeval)), data = df)
    nb.model <- update(nb.model, stringeval)
    print("Model Summary")
    print(summary(nb.model))
    
    writeLines("Checking model assumption over Poisson Model")
    pos.model<-glm(eval(parse(text=stringeval)), data = df, family=poisson)
    
    log_likelihood_delta = 2 * (logLik(nb.model) - logLik(pos.model))
    writeLines(paste0("Log Likelihood Difference between Model ", log_likelihood_delta))
    chi_square_value = pchisq(2 * (logLik(nb.model) - logLik(pos.model)), df = 1, lower.tail = FALSE)
    writeLines(paste("Chi Square pvalue :", chi_square_value))
    if(chi_square_value <= 0.05){
      writeLines("Chi Squared test Statistically Significant! \nThe Negative Binomial Model concluded explaining over-dispersed and under-dispersed data \nis more appropriate than Poisson Model")
    }
    else if(chi_square_value > 0.05){
      message("Chi Squared test Statistically Not Significant! \nThe Negative Binomial Model concluded explaining over-dispersed and under-dispersed data\nis no better than Poisson Model (need a different approach of estimation)")
    }
    
    x11()
    par(mfrow=c(2,3))
    plot(nb.model, which=1)
    plot(nb.model, which=2)
    plot(nb.model, which=3)
    plot(nb.model, which=4)
    plot(nb.model, which=5)
    plot(nb.model, which=6)
    
    coef_table = cbind(coef(summary(nb.model)), LL=confint(nb.model)[,1], UL = confint(nb.model)[,2])
    predict_negbiom = exp(predict(nb.model, df[,colx], type = "link", se.fit=TRUE)$fit)
    
    x11()
    par(mfrow=c(1,1))
    observed = df[,coly]
    predicted = predict_negbiom
    df_comparison = data.frame(cbind(observed, predicted))
    rmse_value = rmse(observed, predicted)
    x = 1:nrow(df)
    plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison with Negative Binomial Regression"), 
         sub=paste0("RMSE value of ", round(rmse_value, 3)))
    points(x, predicted,col='red')
    lines(x, observed, lwd=2, col="blue")
    lines(x, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8)  
    
    return(list(nb.model, coef_table, df_comparison))
  }
  else if(type==14){
    #https://stats.stackexchange.com/questions/157575/why-is-the-quasi-poisson-in-glm-not-treated-as-a-special-case-of-negative-binomi
    writeLines("=======================================================================================")
    writeLines("======================== Making a Quasi Poisson Regression Model ======================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], relationship)
      }
    }
    quas.pos.model <- glm(eval(parse(text=stringeval)), data = df, 
                          family="quasipoisson")
    quas.pos.model <- update(quas.pos.model, stringeval)
    print("Model Summary")
    print(summary(quas.pos.model))
    
    x11()
    par(mfrow=c(2,3))
    plot(quas.pos.model, which=1)
    plot(quas.pos.model, which=2)
    plot(quas.pos.model, which=3)
    plot(quas.pos.model, which=4)
    plot(quas.pos.model, which=5)
    plot(quas.pos.model, which=6)
    
    coef_table = cbind(coef(summary(quas.pos.model)), LL=confint(quas.pos.model)[,1], UL = confint(quas.pos.model)[,2])
    predict_quaspos = exp(predict(quas.pos.model, df[,colx], type = "link", se.fit=TRUE)$fit)
    
    x11()
    par(mfrow=c(1,1))
    observed = df[,coly]
    predicted = predict_quaspos
    df_comparison = data.frame(cbind(observed, predicted))
    rmse_value = rmse(observed, predicted)
    x = 1:nrow(df)
    plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison with Quasi Poisson Regression"), 
         sub=paste0("RMSE value of ", round(rmse_value, 3)))
    points(x, predicted,col='red')
    lines(x, observed, lwd=2, col="blue")
    lines(x, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8)  
    return(list(quas.pos.model, coef_table, df_comparison))
  }
  else if(type==15){
    writeLines("=======================================================================================")
    writeLines("======================== Making a Cox Regression Model ================================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], "+")
      }
    }
    cox.reg <- coxph(eval(parse(text=stringeval)), data = df)
    cox.reg <- update(cox.reg, stringeval)
    print("Model Summary")
    print(summary(cox.reg))
    colors_bold = c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03",
                    "#18fc03","#03fc5e","#03fcc6","#03f0fc","#03adfc","#035afc",
                    "#be03fc","#fc03f8","#fc03a1")
    x11()
    plot(cox.reg, col=colors_bold)
    return(cox.reg)
  }
  else if(type==16){
    #https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.
    writeLines("=======================================================================================")
    writeLines("======================== Making a Tobit Regression Model ==============================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], "+")
      }
    }
    if(lefttobit == 0 && righttobit == 0){
      if(sort(unique(df[,coly]))[1] == 0){
        lefttobit = sort(unique(df[,coly]))[2]
        righttobit = max(df[,coly]) 
      }
      else{
        lefttobit = sort(unique(df[,coly]))[1]
        righttobit = max(df[,coly]) 
      }
    }
    else if(lefttobit == 0){
      if(sort(unique(df[,coly]))[1] == 0){
        lefttobit = sort(unique(df[,coly]))[2]
      }
      else{
        lefttobit = sort(unique(df[,coly]))[1]
      }
    }
    if(lefttobit != 0 || righttobit !=0){
      writeLines("============= Using Tobit Regression from AER Packages ================")
      fm.tobit <- AER::tobit(eval(parse(text=stringeval)), 
                             left = lefttobit, right = righttobit, data = df) 
      fm.tobit <- update(fm.tobit, stringeval)
      writeLines("Tobit 1 Model Summary")
      print(summary(fm.tobit))
      b <- coef(fm.tobit)
      se <- sqrt(diag(vcov(fm.tobit)))
      coef_table_tobit1 = cbind(coef_table=b, LL = b-qnorm(0.975) * se, UL = b+qnorm(0.975) * se)
      print(coef_table_tobit1)
      
      #https://stats.idre.ucla.edu/r/dae/tobit-models/
      writeLines("============== Using Tobit Regression from VGAM Packages ===============")
      fm.tobit2 <- vglm(eval(parse(text=stringeval)), 
                        VGAM::tobit(Lower = lefttobit, Upper = righttobit), data = df)
      fm.tobit2 <- update(fm.tobit2, stringeval)
      writeLines("Tobit 2 Model Summary")
      print(summary(fm.tobit2))
      
      b <- coef(fm.tobit2)
      se <- sqrt(diag(vcov(fm.tobit2)))
      coef_table_tobit2 = cbind(coef_table=b, LL = b-qnorm(0.975) * se, UL = b+qnorm(0.975) * se)
      print(coef_table_tobit2)
      attach(df)
      
      #ggPredict(fm.tobit, interactive = TRUE)
      #ggPredict(fm.tobit2, interactive = TRUE)
      #dt$yhat <- fitted(fm.tobit2)[,1]
      #dt$rr <- resid(fm.tobit2, type = "response")
      #dt$rp <- resid(fm.tobit2, type = "pearson")[,1]
      #cor_eval = paste0("cor(",yhat, ",", coly,")")
      #r <- with(dat, eval(parse(text=cor_eval)))
      #print("R2 Value: ")
      #print(r**2)
      return(list(fm.tobit, fm.tobit2, coef_table_tobit1, coef_table_tobit2))
    }
    else{
      writeLines("Using Tobit Regression from Survival Packages")
      fm.tobit <- tobit(eval(parse(text=stringeval)), 
                        left = lefttobit, right=righttobit, data = df) 
      writeLines("Tobit Model Summary")
      print(summary(fm.tobit))
      return(fm.tobit)
    }
  }
  else if(type==17){
    writeLines("=======================================================================================")
    writeLines("======================== Making a Stepwise Regression Model ===========================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    stringeval2 = paste0(coly," ~ ", 1)
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], "+")
      }
    }
    #Stepwise Regresion by guru99.com
    #https://www.guru99.com/r-simple-multiple-linear-regression.#8
    #https://stackoverflow.com/questions/11617881/accessing-multiple-pages-of-plots
    writeLines("============================================================================================")
    writeLines("====================== 3 Model Stepwise Forward, Backward, and Both ========================")
    writeLines("============================================================================================")
    lm.full <- lm(eval(parse(text=stringeval)), data = df)
    lm.null <- lm(eval(parse(text=stringeval2)), data = df)
    lm.full <- update(lm.full, stringeval)
    lm.null <- update(lm.null, stringeval2)
    tryCatch({
      model.aic.forward <- step(lm.null, direction = "forward", trace = 1, 
                                scope = list(lower = lm.null, upper = lm.full))
      writeLines("Stepwise Forward Model Summary")
      print(summary(model.aic.forward))
      x11()
      par(mfrow=c(2,3))
      plot(model.aic.forward, which=1, main="Stepwise Model Fitted Values and Residual")
      plot(model.aic.forward, which=2, main="Stepwise Model Normal QQ-Plot")
      plot(model.aic.forward, which=3, main="Stepwise Model Scale Location")
      plot(model.aic.forward, which=4, main="Stepwise Model Cook Distances Distribution")
      plot(model.aic.forward, which=5, main="Stepwise Model Residual vs Leverage")
      plot(model.aic.forward, which=6, main="Stepwise Model Cook Distancs vs Leverage")
    },error=function(error_message){
      message(error_message)
      message("Stepwise Forward is not possible for this formula")
    })
    tryCatch({
      writeLines("Stepwise Backward Model Summary")
      model.aic.backward <- step(lm.full, direction = "backward", trace = 1)
      print(summary(model.aic.backward))
    },error=function(error_message){
      message(error_message)
      message("Stepwise Backward is not possible for this formula")
    })
    tryCatch({
      writeLines("Stepwise Forward-Backward Model Summary")
      model.aic.both <- step(lm.null, direction = "both", trace = 1, 
                             scope = list(lower = lm.null, upper = lm.full))
      print(summary(model.aic.both))
    },error=function(error_message){
      message(error_message)
      message("Stepwise Forward-Backward is not possible for this formula")
    })
    writeLines("=============================================================================")
    writeLines("====================== Detailed Stepwise Explanation ========================")
    writeLines("=============================================================================")
    if(ggscatmat_enabled == TRUE){
      if(ggscatmat_factor_color != ""){
        corplot <- ggscatmat(df, columns = 1:ncol(df), color=ggscatmat_factor_color)
        x11()
        par(mfrow=c(1,1))
        plot(corplot, main="Features Correlation Matrix and Density Plot")
      }
      else if(ggscatmat_factor_color == ""){
        corplot <- ggscatmat(df, columns = 1:ncol(df)) 
        x11()
        par(mfrow=c(1,1))
        plot(corplot, main="Features Correlation Matrix and Density Plot")
      }
    }
    #https://stackoverflow.com/questions/56710724/object-mtcars1-not-found-error-in-olsrr-functions
    lm.full$call$data <- df
    step_detail <- ols_step_all_possible(lm.full)
    step_detail2 <- ols_step_both_p(lm.full, details=TRUE)
    x11()
    print(plot(step_detail)) #Stepwise Model Criterion Test (Dotted)
    print(plot(step_detail2)) #Stepwise Model Criterion Test (Line)
    return(list(model.aic.forward, model.aic.backward, 
                model.aic.both, step_detail, step_detail2))
  }
  else if(type==18){
    #https://machinelearningmastery.com/non-linear-regression-in-r/
    writeLines("=======================================================================================")
    writeLines("=========== Making a Multivariate Adaptive Regression Splines Model ===================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], "+")
      }
    }
    fit <- earth(eval(parse(text=stringeval)), df)
    fit <- update(fit, stringeval)
    # summarize the fit
    writeLines("======================== MARS Model Summary ============================")
    print(summary(fit))
    # summarize the importance of input variables
    writeLines("\n======================== MARS Variable Importance =============================")
    print(evimp(fit))
    # make predictions
    predictions <- predict(fit, df)
    plot(fit, caption="MARS Model Plot Summary")
    # summarize accuracy
    mse <- mean((df[,coly] - predictions)^2)
    writeLines(paste("MSE Values Between Actual and Predictions:", mse))
    
    observed <- df[,coly]
    predicted = predictions
    df_comparison = data.frame(cbind(observed, predicted))
    rmse_value = rmse(observed, predicted)
    x = 1:nrow(df)
    plot(x, observed, col='blue', main=paste0("Observed and Predicted comparison of data"), 
         sub=paste0("RMSE value of ", round(rmse_value, 3)))
    points(x, predicted,col='red')
    lines(x, observed, lwd=2, col="blue")
    lines(x, predicted, lwd=2, col="red")
    legend("bottomleft", legend=c("Observed", "Predicted"), 
           col=c("blue","red"), lty=1:2, cex=0.8) 
    
    return(list(fit, predictions, mse, df_comparison))
  }
  else if(type==19){
    #http://r-statistics.co/Loess-Regression-With-R.
    writeLines("=======================================================================================")
    writeLines("=========== Making a LOESS Regression (Non Parametric Local Regression) ===============")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], "+")
      }
    }
    if(loess_optimal_span != -1){
      loessoptimalmod <- loess(eval(parse(text=stringeval)), data=df, span=loess_optimal_span) # include best smoothing span
      loessoptimalmod <- update(loessoptimalmod, stringeval)
      print("Model Summary with optimal Span")
      print(summary(loessoptimalmod))
    }
    loessMod10 <- loess(eval(parse(text=stringeval)), data=df, span=0.10) # 10% smoothing span
    print("Model Summary with 10% Span")
    print(summary(loessMod10))
    loessMod25 <- loess(eval(parse(text=stringeval)), data=df, span=0.25) # 25% smoothing span
    print("Model Summary with 25% Span")
    print(summary(loessMod25))
    loessMod50 <- loess(eval(parse(text=stringeval)), data=df, span=0.50) # 50% smoothing span
    print("Model Summary with 50% Span")
    print(summary(loessMod50))
    smoothed10 <- predict(loessMod10) 
    smoothed25 <- predict(loessMod25) 
    smoothed50 <- predict(loessMod50) 
    if(loess_optimal_span != -1){
      smoothedbest <- predict(loessoptimalmod)
    }
    x11()
    plot(df[,coly], x=df[,coldate], type="l", 
         main="Loess Smoothing and Prediction", xlab="Date", ylab="Unemployment (Median)")
    lines(smoothed10, x=df[,coldate], col="red")
    lines(smoothed25, x=df[,coldate], col="green")
    lines(smoothed50, x=df[,coldate], col="blue")
    if(loess_optimal_span != -1){
      lines(smoothedbest, x=df[,coldate], col="yellow")
      legend("topleft", legend=c("Smoothed with 10% span", "Smoothed with 25% span",
                                 "Smoothed with 50% span","Smoothed with optimal span"),
             col=c("red", "green","blue","yellow"), lty=1:2, cex=0.8)
      df_comparison = cbind(observed = df[,coly], date=df[,coldate], 
                            span10 = smoothed10, span25 = smoothed25, span50=smoothed50, 
                            spanbest = smoothedbest)
      return(list(smoothed10, smoothed25, smoothed50, smoothedbest, df_comparison))
    }
    else{
      legend("topleft", legend=c("Smoothed with 10% span", "Smoothed with 25% span",
                                 "Smoothed with 50% span",),
             col=c("red", "green","blue"), lty=1:2, cex=0.8)
      df_comparison = cbind(observed = df[,coly], date=df[,coldate], 
                            span10 = smoothed10, span25 = smoothed25, span50=smoothed50)
      return(list(smoothed10, smoothed25, smoothed50, df_comparison))
    }
  }
  else if(type==20){
    #https://www.rdocumentation.org/packages/lars/versions/1.2/topics/lars
    writeLines("=======================================================================================")
    writeLines("=========== Making a LARS Regression (Least Angle Regression) =========================")
    writeLines("=======================================================================================\n")
    stringeval = paste0(coly," ~ ")
    for(x in 1:length(colx))
    {
      if (x==length(colx)){
        stringeval = paste0(stringeval, colx[x])
      }  
      else if (x<length(colx)){
        stringeval = paste0(stringeval, colx[x], "+")
      }
    }
    writeLines("with Lars_Method = 1 Use Lasso Method")
    writeLines("with Lars_Method = 2 Use Lar Method")
    writeLines("with Lars_Method = 3 Use forward.stagewise Method")
    writeLines("with Lars_Method = 4 Use stepwise Method")
    if(lars_method == 1){
      lars_model <- lars(as.matrix(df[,colx]), as.matrix(df[,coly]), 
                         type="lasso", trace=TRUE)
      x11()
      plot(lars_model, main="LARS Path Model with LASSO")
      criterion_freedom = summary(lars_model)
      prediction_matrix = predict(lars_model, newx=as.matrix(df[,colx]))$fit
      length_freedom = ncol(prediction_matrix)
      mfrow_x = 2
      mfrow_y = 2
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      for(freedom in 1:length_freedom){
        observed <- df[,coly]
        predicted = prediction_matrix[,freedom]
        df_comparison = data.frame(cbind(observed, predicted))
        rmse_value = rmse(observed, predicted)
        rss_value = criterion_freedom[freedom, 2]
        cp_value = criterion_freedom[freedom, 3]
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted LARS (LASSO) comparison of data"), 
             sub=paste0("RMSE value of ", round(rmse_value, 3), " RSS Value of ", round(rss_value,0), " CP Value of ", round(cp_value,2)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8) 
        if(freedom %% 4 == 0){
          x11()
          par(mfrow=c(mfrow_x, mfrow_y))
        }
      }
      
    }
    else if(lars_method == 2){
      lars_model <- lars(as.matrix(df[,colx]), as.matrix(df[,coly]), 
                         type="lar", trace=TRUE)
      x11()
      plot(lars_model, main="LAR Path Model with LAR")
      criterion_freedom = summary(lars_model)
      prediction_matrix = predict(lars_model, newx=as.matrix(df[,colx]))$fit
      length_freedom = ncol(prediction_matrix)
      mfrow_x = 2
      mfrow_y = 2
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      for(freedom in 1:length_freedom){
        observed <- df[,coly]
        predicted = prediction_matrix[,freedom]
        df_comparison = data.frame(cbind(observed, predicted))
        rmse_value = rmse(observed, predicted)
        rss_value = criterion_freedom[freedom, 2]
        cp_value = criterion_freedom[freedom, 3]
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted LARS (LAR) comparison of data"), 
             sub=paste0("RMSE value of ", round(rmse_value, 3), " RSS Value of ", round(rss_value,0), " CP Value of ", round(cp_value,2)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8) 
        if(freedom %% 4 == 0){
          x11()
          par(mfrow=c(mfrow_x, mfrow_y))
        }
      }
      
    }
    else if(lars_method == 3){
      lars_model <- lars(as.matrix(df[,colx]), as.matrix(df[,coly]), 
                         type="forward.stagewise", trace=TRUE)
      x11()
      plot(lars_model, main="LARS Path Model with Forward Stagewise")
      criterion_freedom = summary(lars_model)
      prediction_matrix = predict(lars_model, newx=as.matrix(df[,colx]))$fit
      length_freedom = ncol(prediction_matrix)
      mfrow_x = 2
      mfrow_y = 2
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      for(freedom in 1:length_freedom){
        observed <- df[,coly]
        predicted = prediction_matrix[,freedom]
        df_comparison = data.frame(cbind(observed, predicted))
        rmse_value = rmse(observed, predicted)
        rss_value = criterion_freedom[freedom, 2]
        cp_value = criterion_freedom[freedom, 3]
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted LARS (Forward_Stagewise) comparison of data"), 
             sub=paste0("RMSE value of ", round(rmse_value, 3), " RSS Value of ", round(rss_value,0), " CP Value of ", round(cp_value,2)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8) 
        if(freedom %% 4 == 0){
          x11()
          par(mfrow=c(mfrow_x, mfrow_y))
        }
      }
    }
    else if(lars_method == 4){
      lars_model <- lars(as.matrix(df[,colx]), as.matrix(df[,coly]),
                         type="stepwise", trace=TRUE)
      x11()
      plot(lars_model, main="LARS Path Model with Stepwise")
      criterion_freedom = summary(lars_model)
      prediction_matrix = predict(lars_model, newx=as.matrix(df[,colx]))$fit
      length_freedom = ncol(prediction_matrix)
      mfrow_x = 2
      mfrow_y = 2
      
      x11()
      par(mfrow=c(mfrow_x, mfrow_y))
      for(freedom in 1:length_freedom){
        observed <- df[,coly]
        predicted = prediction_matrix[,freedom]
        df_comparison = data.frame(cbind(observed, predicted))
        rmse_value = rmse(observed, predicted)
        rss_value = criterion_freedom[freedom, 2]
        cp_value = criterion_freedom[freedom, 3]
        x = 1:nrow(df)
        plot(x, observed, col='blue', main=paste0("Observed and Predicted LARS (Stepwise) comparison of data"), 
             sub=paste0("RMSE value of ", round(rmse_value, 3), " RSS Value of ", round(rss_value,0), " CP Value of ", round(cp_value,2)))
        points(x, predicted,col='red')
        lines(x, observed, lwd=2, col="blue")
        lines(x, predicted, lwd=2, col="red")
        legend("bottomleft", legend=c("Observed", "Predicted"), 
               col=c("blue","red"), lty=1:2, cex=0.8) 
        if(freedom %% 4 == 0){
          x11()
          par(mfrow=c(mfrow_x, mfrow_y))
        }
      }
    }
    return(lars_model)
  }
}

#============================= Regression Analysis Collection Examples =================================
#======================= A) Executing Linear Regression Examples ================================
x1 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]))

#======================= B) Executing Polynomial Regression Examples ===================================
x2 = data_regression(swiss, "Fertility", "Catholic",type=2, poly_num =2)

#======================= C) Executing Logistic and Probit Regression Examples ==========================

data_x3 = AMSsurvey
#Female = 0, Male=0
levels(data_x3$sex) = c(0,1)
#Non-US = 0, US=1
levels(data_x3$citizen) = c(0,1)
data_x3$sex = as.numeric(as.character(data_x3$sex))
data_x3$citizen= as.numeric(as.character(data_x3$citizen))

x3 = regression_testing(data_x3, "citizen", c("count","count11"), category_method = "logit", stepwise_logit = TRUE)
x3 = regression_testing(data_x3, "citizen", c("count","count11"), category_method = "logit", stepwise_logit = FALSE)
x3_2 = regression_testing(data_x3, "citizen", c("count","count11"), category_method = "probit", stepwise_logit = FALSE)
x3_3 = regression_testing(data_x3, "sex", c("count","count11"), category_method = "logit", stepwise_logit = FALSE)
x3_4 = regression_testing(data_x3, "sex", c("count","count11"), category_method = "probit", stepwise_logit = FALSE)
mlogit_x3 = mlogit.data(data_x3, choice="type", shape="wide")
x3_5 = regression_testing(mlogit_x3, "type", "count", mlogit_raw_data = data_x3, 
                          category_method = "logit", mlogit_type = "independent")
x3_6 = regression_testing(data_x3, "type", "count", category_method = "probit")

x3 = data_regression(data_x3, "citizen", c("count","count11"), type = 3, category_method = "logit", stepwise_logit = TRUE)
x3 = data_regression(data_x3, "citizen", c("count","count11"), type = 3, category_method = "logit", stepwise_logit = FALSE)
x3_2 = data_regression(data_x3, "citizen", c("count","count11"), type = 3, category_method = "probit", stepwise_logit = FALSE)
x3_3 = data_regression(data_x3, "sex", c("count","count11"), type = 3, category_method = "logit", stepwise_logit = FALSE)
x3_4 = data_regression(data_x3, "sex", c("count","count11"), type = 3, category_method = "probit", stepwise_logit = FALSE)
mlogit_x3 = mlogit.data(data_x3, choice="type", shape="wide")
x3_5 = data_regression(mlogit_x3, "type", "count", type = 3, mlogit_raw_data = data_x3, 
                       category_method = "logit", mlogit_type = "independent")
x3_6 = data_regression(data_x3, "type", "count", type = 3, category_method = "probit")

reverse_concat_string <- function(vector_string, sep="[.]"){
  splitting = strsplit(vector_string, sep)
  for(h in 1:length(splitting)){
    if(length(splitting[[h]]) == 1){
      splitting[[h]] = splitting[[h]][1]
    }
    else if(length(splitting[[h]]) == 2){
      splitting[[h]] = paste0(splitting[[h]][2],".",splitting[[h]][1])
    }
  }
  splitting = unlist(splitting)
  return(splitting)
}

data(detergent)
mldata2 <- detergent
colnames(mldata2) <- str_replace(colnames(mldata2), "Price", ".Price")
colnames(mldata2) <- reverse_concat_string(colnames(mldata2))
mldata2<-mlogit.data(mldata2, varying=2:7, choice="choice", shape="wide")
unique_dependent = unique(mldata2$alt)

x3mlogit = data_regression(mldata2, "choice", "Price", mlogit_raw_data = detergent, 
                           category_method = "logit", mlogit_type = "wide",type=3)
x3mprobit = data_regression(detergent, "choice", colnames(detergent)[2:length(colnames(detergent))], 
                            category_method = "probit", type=3)

fishing <- read.csv(file.choose()) #Multinomial Fishing 1 CSV
mlogit_fishing_data <- mlogit.data(fishing, varying=4:15, choice="mode", shape="wide")
x3mlogit_fishing = data_regression(mlogit_fishing_data, "mode", "income", mlogit_raw_data = fishing, 
                                   category_method = "logit", mlogit_type="wide", type=3)

#======================= D) Executing Quantile Regression Examples =============================
data("ChickWeight")
data4 = ChickWeight
data4 <- data4 %>%
  mutate_if(is.numeric, as.integer) %>%
  mutate_if(is.factor, as.integer)
x4 = data_regression(data4, "Diet", c(colnames(data4)[1:3]), type=4, quantile_num = 5)
x4b = data_regression(data4, "Diet", c(colnames(data4)[1:3]), type=4, quantile_num = 50, quantile_seq = FALSE)

############### Function to Calculate Lambda for Ridge, Lasso, Elastic Net Examples ##########################
### https://stackoverflow.com/questions/23686067/default-lambda-sequence-in-glmnet-for-cross-validation ##

custom_lambda_seq <- function(x, y){
  set.seed(1)
  n <- 100
  ## Standardize variables: (need to use n instead of (n-1) as denominator)
  mysd <- function(y) sqrt(sum((y-mean(y))^2)/length(y))
  sx <- scale(x, scale = apply(x, 2, mysd))
  sx <- as.matrix(sx, ncol = 20, nrow = 100)
  sy <- as.vector(scale(y, scale = mysd(y)))
  ## Calculate lambda path (first get lambda_max):
  lambda_max <- max(abs(colSums(sx*sy)))/n
  epsilon <- .0001
  K <- 100
  lambdapath <- round(exp(seq(log(lambda_max), log(lambda_max*epsilon), 
                              length.out = K)), digits = 10)
  return(lambdapath)
}

swiss_custom_lambda = custom_lambda_seq(swiss[,-1], swiss[,1])

#======================= E) Executing Ridge Regression Examples ===================================
x5 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=5, 
                     lambda_path_vector = swiss_custom_lambda)

#======================= F) Executing Lasso Regression Examples ===================================
x6 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=6, 
                     lambda_path_vector = swiss_custom_lambda)

#======================= G) Executing Elastic Net Regression Examples =============================
x7_1 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.1)
x7_2 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.2)
x7_3 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.3)
x7_4 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.4)
x7_5 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.5)
x7_6 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.6)
x7_7 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.7)
x7_8 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.8)
x7_9 = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=7, 
                       lambda_path_vector = swiss_custom_lambda, alpha_elastic_net = 0.9)

#======================= H) Executing PCR (Principal Component Regression) Examples ==============
data(longley)
data8 = longley[,colnames(longley) != "Year"]
x8 <- data_regression(data8, "Employed", ".",type=8)

#======================= I) Executing Partial Least Squares Regression Examples ===============================
data(vehicles)
data9 = vehicles
x9 <- data_regression(data9, colnames(data9)[13], colnames(data9)[-13], type=9)
x9_2 <- data_regression(data9, colnames(data9)[13], colnames(data9)[-13], type=9, ncomp_pls = 5)
x9b <- data_regression(data9, colnames(data9)[c(15,16)], colnames(data9)[-c(15,16)], type=9)
x9b_2 <- data_regression(data9, colnames(data9)[c(15,16)], colnames(data9)[-c(15,16)], type=9, ncomp_pls = 5)

#======================= J) Executing SVR (Support Vector Regression) Examples ==================
x10 = data_regression(swiss, "Fertility", "Catholic",type=10)
x10b = data_regression(swiss, "Fertility", c(colnames(swiss)[2:length(colnames(swiss))]), type=10)

#======================= K) Executing Ordinal Regression Examples ======================
#Input for Ordinal Regression preferred: Data with Low Unique Values! (Categoric Features)

data("BreastCancer")
data11 = BreastCancer
data11 = data11[,-1] #Remove Id
Classfeature = data11$Class
Clthickness = data11$Cl.thickness
Cellsize = data11$Cell.size
Cellshape = data11$Cell.shape
margadhesion = data11$Marg.adhesion
epithsize = data11$Epith.c.size

data11 = data11 %>%
  mutate_if(is.factor, as.numeric)
data11$Class = Classfeature
data11$Cl.thickness = Clthickness
data11$Cell.size = Cellsize
data11$Cell.shape = Cellshape
data11$Marg.adhesion = margadhesion
data11$Epith.c.size = epithsize
data11 = na.omit(data11)

colors = c("#ffc6c2","#ffdaa6","#fff9a6","#ecffa6","#d1ffa6","#a9ffa6","#a6ffc4","#a6ffdb","#a6ffff",
           "#a6deff","#a6caff","#a6a7ff","#b9a6ff","#d7a6ff","#f0a6ff","#fca6ff","#ffa6ea","#ffa6c1",
           "#ffa6b3","#ffa6b5")
colors_bold = c("#fc0303", "#fc7703","#fcb103","#fcdf03","#dbfc03","#98fc03",
                "#18fc03","#03fc5e","#03fcc6","#03f0fc","#03adfc","#035afc",
                "#be03fc","#fc03f8","#fc03a1")

x11 = data_regression(data11, colnames(data11)[1], colnames(data11)[c(6:10)], type=11)

data(warpbreaks)
data12 = warpbreaks
ggplot(warpbreaks, aes(breaks, fill = wool)) +
  geom_histogram(binwidth=.85, position="dodge")
ggplot(warpbreaks, aes(breaks, fill = tension)) +
  geom_histogram(binwidth=.85, position="dodge")

data(quine)
data13 <- quine

#======================= L) Executing Poisson, Negative Binomial, Quasi Poisson Regression Examples =======================

x12 <- data_regression(data12, colnames(data12)[1], colnames(data12)[-1],type = 12)
x13 <- data_regression(data12, colnames(data12)[1], colnames(data12)[-1],type = 13)
x14 <- data_regression(data12, colnames(data12)[1], colnames(data12)[-1],type = 14)

#======================= M) Executing Cox Regression Examples ========================================
data(lung)
data15 <- lung 
data15$SurvObj <- with(data15, Surv(time, status == 2))
data15$SurvObj2 <- with(data15, Surv(time, status == 1))
x15 <- data_regression(data15, colnames(data15)[11], colnames(data15)[c(4,5,7,10)],15)
x15_2 <- data_regression(data15, colnames(data15)[12], colnames(data15)[c(4,5,7,10)],15)

#======================= N) Executing Tobit/Censored Regression Examples =============================
data(Affairs)
data("CASchools")
data16 <- Affairs
data16b <- CASchools

#right > left in tobit regression
x16a <- data_regression(data16, colnames(data16)[1], colnames(data16)[c(3,4,6,7,8)], 16)
x16b1 <- data_regression(data16b, colnames(data16b)[12], colnames(data16b)[c(5:11)], 16)
x16b2 <- data_regression(data16b, colnames(data16b)[13], colnames(data16b)[c(5:11)], 16)
x16b3 <- data_regression(data16b, colnames(data16b)[14], colnames(data16b)[c(5:11)], 16)

#======================= O) Executing Stepwise Regression Examples ==================================
x17 = data_regression(swiss, "Fertility", 
                      c(colnames(swiss)[2:length(colnames(swiss))]), type=17, 
                      ggscatmat_enabled = TRUE)

#======================= P) Executing MARS Regression Examples ======================================
x18 = data_regression(swiss, "Fertility", 
                      c(colnames(swiss)[2:length(colnames(swiss))]), type=18)

#Stepwise Regresion by guru99.com
#https://www.guru99.com/r-simple-multiple-linear-regression.#8

##########################################################################################################
######################## Function to Calculate SSE for LOESS Regression Span #############################
### https://stackoverflow.com/questions/24623488/how-do-i-use-a-function-with-parameters-in-optim-in-r ###
##########################################################################################################
#======================= Q) Executing LOESS Regression Examples =======================
calcSSE_LOESS <- function(x, df, formula_fit){
  sse <- 99999
  loessMod <- try(loess(formula_fit, data=df, span=x), silent=T)
  res <- try(loessMod$residuals, silent=T)
  if(class(res)!="try-error"){
    if((sum(res, na.rm=T) > 0)){
      sse <- sum(res^2)  
    }
  }
  return(sse)
}

optim(par=c(0.5), fn=calcSSE_LOESS, df=economics, 
      formula_fit=formula(uempmed ~ index), method="SANN")

data("economics")
data19 <- economics
data19$index <- 1:nrow(data19)
data19 <- data.frame(data19)
x19 <- data_regression(data19, "uempmed", "index", type=19, 
                       loess_optimal_span = 0.07176, coldate="date")

#======================= R) Executing LARS Regression Examples ============================
#https://stats.stackexchange.com/questions/58531/using-lasso-from-lars-or-glmnet-package-in-r-for-variable-selection
data("diabetes")
data20 <- diabetes
x20_1 <- data_regression(data20, "y", "x", type=20, lars_method = 1)
x20_2 <- data_regression(data20, "y", "x", type=20, lars_method = 2)
x20_3 <- data_regression(data20, "y", "x", type=20, lars_method = 3)
x20_4 <- data_regression(data20, "y", "x", type=20, lars_method = 4)

#--------------------------------------------------------------------------------------------------------------------------
################# 24) Deep Learning with Tensorflow and Keras  ###########################################################
#--------------------------------------------------------------------------------------------------------------------------
#================================= Perceptron NN Basic Model ==================================
perceptron <- function(g1_x, g1_y, g2_x, g2_y, xlabel, ylabel, mainlabel, weight=c(0.1,0.2,0.3), 
                       iter=150, eta=0.005, th=0.9, log=TRUE) {
  
  g_x = c(g1_x, g2_x)
  g_y = c(g1_y, g2_y)
  N = length(g1_x)
  group = c(rep(-1,N), rep(1,N))
  w0 = weight[1] # initial weitht
  w1 = weight[2] # initial weight
  w2 = weight[3] # initial weitht
  
  M = iter          # number of epochs to run
  verbose = log     # whether detailed weight update info is printed
  
  for (i in 1:M){
    print(paste('Epoch starts: ', i))
    ## We reshuffle the order of the datapoint for each epoch.
    index = 1:length(g_x)
    index = sample(index)
    
    for (j in index){
      y_j = w0 + w1*g_x[j] + w2*g_y[j]
      if (y_j >= 0)
      {
        pred_j = 1
      }
      else
      {
        pred_j = -1
      }
      
      w0 <- w0 + eta*(group[j] - pred_j)*1.0
      w1 <- w1 + eta*(group[j] - pred_j)*g_x[j]
      w2 <- w2 + eta*(group[j] - pred_j)*g_y[j]
      if (verbose == T){
        print(paste('  -> updating data point ', j, ' : '))
        print(paste('     -> w0: ' ,w0))
        print(paste('     -> w1: ' ,w1))
        print(paste('     -> w2: ' ,w2))
      }
    }  
    y_all = w0 + w1*g_x + w2*g_y
    y_pred = y_all
    y_pred[y_all >= 0] = 1
    y_pred[y_all< 0] = -1
    
    acc = sum(y_pred == group)/length(group)
    print(paste('Epoch ends: ', i, ' WITH final accuracy: ', acc))
    if (acc >= th){
      break
    }
  } 
  x11()
  plot(g_x, g_y, type='n', xlab=xlabel, ylab=ylabel, main= mainlabel)
  points(g1_x, g1_y, col='red')
  points(g2_x, g2_y, col='blue')
  abline(a = -1.0*w0/w2, b = -1.0*w1/w2, col='dark green', lwd=3, lty=2)
}

#--------------------------------- Perceptron Example -------------------------------------------
N = 50 # total number of data points each group
x_offset = 0.5 # group seperation on x axis
y_offset = 0.5 # group seperation on y axis

g1_x = runif(N, min = 0, max = 1)
g1_y = runif(N, min = 0, max = 1)

g2_x = runif(N, min = 0+x_offset, max = 1+x_offset)
g2_y = runif(N, min = 0+y_offset, max = 1+y_offset)

g_x = c(g1_x, g2_x)
g_y = c(g1_y, g2_y)
group = c(rep(-1,N), rep(1,N))

model_linear_combine_runif = as.formula(group ~ g_x + g_y)
lm_model = lm(model_linear_combine_runif)
gvlma_runif = gvlma(lm_model)
linear_pvalue = gvlma_runif$GlobalTest$GlobalStat4$pvalue #0.675
model_perceptron_runif = perceptron(g1_x, g1_y, g2_x, g2_y, "X", "Y", "Runif Classification Perceptron")

#================================= Neural Network with Backpropagation ============================
#it is recommended to use Neural Network algorithm after data is standardized/normalized first
neuralnet_train_predict <- function(df, var_dependent="", var_independent=c(), test_dependent, 
                                    linear.output=TRUE, neuralnet_algorithm="rprop+",
                                    hidden_node=c(10), threshold=0.01, stepmax=10000,
                                    activation_function=1,
                                    learning_rate_backprop=NULL,
                                    learning_rate_resbackprop_limit=NULL,
                                    learning_rate_resbackprop_factor=list(minus=0.5, plus=1.2),
                                    alpha_activation_func = 0.6,
                                    tlower_sshaped=0.3,
                                    tupper_sshaped=0.7,
                                    alphalower_sshaped=0.45,
                                    alphaupper_sshaped=0.55,
                                    hint=FALSE){
  
  library(pracma)
  library(neuralnet)
  
  if(hint){
    writeLines("====================================================================================================")
    writeLines("===================== Neural Network Algorithm can be a selection of ===============================")
    writeLines("====================================================================================================")
    writeLines("1) backprop (Back Propagation Algorithm)")
    writeLines("2) rprop+ (Resilient Back Propagation with Weight Backtracking)")
    writeLines("3) rprop- (Resilient Back Propagation without Weight Backtracking)")
    writeLines("4) sag (Globally Convergent algorithm which is basically rprop- with modifying 1 learning rate with the smallest absolute gradient (sag))")
    writeLines("4) slr (Globally Convergent algorithm which is basically rprop- with modifying 1 learning rate with the smallest learning rate (slr))")
    writeLines("====================================================================================================")
    
    writeLines("====================================================================================================")
    writeLines("================== Neural Network Activation Function can be a selection of ========================")
    writeLines("====================================================================================================")
    writeLines("Several activation functions that are functions of one fold x from the previous layer or layers:    ")
    writeLines("====================================================================================================")
    writeLines("1) Logistic / Sigmoid Activation Function -> f(x) = 1/(1+e^-x)")
    writeLines("2) Unit Step Activation Function -> f(x) = 0 when x<0, 1 when x>0, in otherwords 0 for negative, 1 for positive")
    writeLines("3) ReLU (Rectified Linear Unit) -> f(x) = 0 when x<0, x when x>=0, in otherwords 0 for negative, x unit for positive")
    writeLines("4) Leaky ReLU -> f(x) = 0 when x<0, 0.01x when x>=0, in otherwords 0 for negative, 0.01x unit for positive")
    writeLines("5) Parametric ReLU -> f(x) = 0 when x<0, alpha*x when x>=0, in otherwords 0 for negative, alpha*x unit for positive")
    writeLines("6) Linear Activation Function -> f(x) = x")
    writeLines("7) Swish Activation Function -> f(x) = x/(1+e^-x), self-gated activation function discovered by researchers at Google")
    writeLines("8) Tanh (Hyperbolic Tangent) Activation Function -> f(x) = e^x - e^-x / e^x + e^-x")
    writeLines("9) Softplus Activation Function -> f(x) = ln(1+ exp x)")
    writeLines("10) Gaussian Error Linear Unit Activation Function -> f(x) = 0.5*x*(1+erf(x/sqrt(2)))")
    writeLines("11) Exponential Linear Unit -> f(x) = alpha*(exp(x) - 1) when x<0, x when x>=0")
    writeLines("12) Scaled Exponential Linear Unit -> f(x) = 1.7581*(exp(x) - 1) when x<0, x when x>=0")
    writeLines("13) Arc Tan -> f(x) = tan^-1(x)")
    writeLines("14) ElliotSig/SoftSign -> f(x) = x/(1+abs(x))")
    writeLines("15) Square Nonlinearity -> f(x) = 1 when x>2, x-x^2/4 when x>=0 & x<=2, x+x^2/4 when x>=-2 & x<0, -1 when x<-2")
    writeLines("16) S-shaped rectified linear activation function -> f(x) = tlower + alphalower*(x-tlower) when x<=tlower, x when tlower < x < tupper, tupper+alphaupper(x-tupper) when x>=tupper")
    writeLines("17) Bent Identity activation function -> f(x) = (sqrt(x^2 + 1)-1)/2 + x")
    writeLines("18) Sinusoid activation function => f(x) = sin(x)")
    writeLines("19) Sinc activation function => f(x) = 1 when x=0, sin(x)/x when x!=0")
    writeLines("20) Gaussian activation function => f(x) = exp(-x^2)")
    writeLines("21) SQ-RBF activation function => f(x) = 1-(x^2/2), when abs(x) <= 1, 0.5*((2- abs(x))^2),when abs(x) > 1 & abs(x) < 2, 0,when abs(x) >= 2")
    writeLines("===================================================================================================")
  }
  
  linear <- function(x){x}
  sigmoid <- function(x){1/(1+exp(-x))}
  softplus <- function(x){log(1 + exp(x))}
  binary_step <- function(x){ifelse(x>=0, 1, 0)}
  relu <- function(x){ifelse(x>=0, x, 0)}
  leaky_relu <- function(x){ifelse(x>=0, 0.01*x, 0)}
  parametric_relu <- function(x){ifelse(x>=0, alpha_activation_func*x, 0)}
  swish <- function(x){x/(1+exp(-x))}
  hyperbolic_tangent <- function(x){(exp(x) - exp(-x)) / (exp(x) + exp(-x))}
  gelu <- function(x){0.5*x*(1+erf(x/sqrt(2)))}
  elu <- function(x){ifelse(x>=0, x, alpha_activation_func*(exp(x) - 1))}
  selu <- function(x){ifelse(x>=0, x, 1.7581*(exp(x) - 1))}
  arc_tan <- function(x){pracma::atand(x)}
  elliot_sig <- function(x){x/(1+abs(x))}
  square_nonlinearity <- function(x){ifelse(x>2, 1, ifelse(x>=0 & x<=2, x-x**2/4, ifelse(x>=-2 & x<0, x+x**2/4, -1)))}
  s_shaped_relu <- function(x, tlower=tlower_sshaped, tupper=tupper_sshaped, 
                            alphalower=alphalower_sshaped, alphaupper=alphaupper_sshaped){
    for(i in 1:length(x)){
      if(x[i] <= tlower){
        x[i] <- tlower + alphalower*(x[i]-tlower)
      }
      else if(x[i] >= tupper){
        x[i] <- tupper + alphaupper*(x[i]-tupper)
      }
    }
    return(x)
  }
  bent_identity <- function(x){(sqrt(x**2 + 1)-1)/2 + x}
  sinusoid <- function(x){sin(x)}
  sinc <- function(x){ifelse(x==0, 1, sin(x)/x)}
  gaussian <- function(x){exp(-x**2)}
  sq_rbf <- function(x){ifelse(abs(x)<=1, 1-(x**2/2), ifelse(abs(x)>1 & abs(x)<2, 0.5*((2- abs(x))**2), ifelse(abs(x)>=2, 0, NA)))}
  
  function_string <- ""
  if(activation_function==1){
    function_string <- "sigmoid"
  }
  else if(activation_function==2){
    function_string <- "binary_step"
  }
  else if(activation_function==3){
    function_string <- "relu"
  }
  else if(activation_function==4){
    function_string <- "leaky_relu"
  }
  else if(activation_function==5){
    function_string <- "parametric_relu"
  }
  else if(activation_function==6){
    function_string <- "linear"
  }
  else if(activation_function==7){
    function_string <- "swish"
  }
  else if(activation_function==8){
    function_string <- "hyperbolic_tangent"
  }
  else if(activation_function==9){
    function_string <- "softplus"
  }
  else if(activation_function==10){
    function_string <- "gelu"
  }
  else if(activation_function==11){
    function_string <- "elu"
  }
  else if(activation_function==12){
    function_string <- "selu"
  }
  else if(activation_function==13){
    function_string <- "arc_tan"
  }
  else if(activation_function==14){
    function_string <- "elliot_sig"
  }
  else if(activation_function==15){
    function_string <- "square_nonlinearity"
  }
  else if(activation_function==16){
    function_string <- "s_shaped_relu"
  }
  else if(activation_function==17){
    function_string <- "bent_identity"
  }
  else if(activation_function==18){
    function_string <- "sinusoid"
  }
  else if(activation_function==19){
    function_string <- "sinc"
  }
  else if(activation_function==20){
    function_string <- "gaussian"
  }
  else if(activation_function==21){
    function_string <- "sq_rbf"
  }
  
  formula_nnet <- ""
  if(length(var_independent)==1){
    formula_nnet <- as.formula(paste0(var_dependent, "~", var_independent[1]))
  }
  else if(length(var_independent) > 1){
    for(h in 1:length(var_independent)){
      if(h==1){
        formula_nnet <- paste0(var_dependent, "~", var_independent[h])
      }
      else if(h > 1){
        formula_nnet <- paste0(formula_nnet, " + ", var_independent[h])
      }
    }
    formula_nnet <- as.formula(formula_nnet)
  }
  
  if(neuralnet_algorithm == "backprop"){
    writeLines("Make a Traditional Backpropagation Neural Network")
    writeLines(paste0("Using Activation Function = ",function_string))
    model=neuralnet(formula = formula_nnet, 
                    data = df, 
                    linear.output = linear.output,
                    hidden = hidden_node, 
                    threshold = threshold,
                    stepmax = stepmax,
                    learningrate = learning_rate_backprop,
                    act.fct = eval(parse(text = function_string)),
                    lifesign="minimal")
    plot(model)
    predict = predict(model, newdata = test_dependent)
    
    final_output=cbind(df[[var_dependent]], df[,which(colnames(df) %in% var_independent)], as.data.frame(model$net.result))
    colnames(final_output) = c("Input", "Expected Output", "Neural Net Prediction Output")
    writeLines("Prediction Results")
    print(final_output)
    return(final_output)
  }
  else{
    writeLines("Make a Resilient Backpropagation Neural Network with/out Globally Convergent algorithm")
    writeLines(paste0("Using Activation Function = ",function_string))
    model=neuralnet(formula = formula_nnet, 
                    data = df, 
                    linear.output = linear.output,
                    hidden=hidden_node, 
                    threshold=threshold,
                    stepmax = stepmax,
                    learningrate.limit = learning_rate_resbackprop_limit,
                    learningrate.factor = learning_rate_resbackprop_factor,
                    act.fct = eval(parse(text = function_string)),
                    lifesign="minimal")
    plot(model)
    predict = predict(model, newdata = test_dependent)
    
    final_output=cbind(df[[var_dependent]], df[,which(colnames(df) %in% var_independent)], as.data.frame(model$net.result))
    colnames(final_output) = c("Input", "Expected Output", "Neural Net Prediction Output")
    writeLines("Prediction Results")
    print(final_output)
    return(final_output)
  }
}

#--------------------------------- Neural net example implementation -------------------------
#https://stackoverflow.com/questions/50376411/neural-network-error-in-plot-nn-weights-were-not-calculated
sampleinput = seq(1,50)
sampleoutput = sampleinput**2
testinput = as.data.frame(sample(sampleinput))
names(testinput) = c("sampleinput")
df = as.data.frame(cbind(sampleinput, sampleoutput))
head(df)

#neural network example test
neuralnet_dfA <- neuralnet_train_predict(df, "sampleoutput", c("sampleinput"), testinput, 
                                         hidden_node = c(5,5), neuralnet_algorithm = "backprop",
                                         activation_function = 3, threshold=0.02)
neuralnet_dfB <- neuralnet_train_predict(df, "sampleoutput", c("sampleinput"), testinput, 
                                         hidden_node = c(5,5), neuralnet_algorithm = "rprop+",
                                         activation_function = 3, stepmax = 100000, threshold = 0.01)
neuralnet_dfC <- neuralnet_train_predict(df, "sampleoutput", c("sampleinput"), testinput, 
                                         hidden_node = c(5,5), neuralnet_algorithm = "rprop-",
                                         activation_function = 3, stepmax = 100000, threshold = 0.01)
neuralnet_dfD <- neuralnet_train_predict(df, "sampleoutput", c("sampleinput"), testinput, 
                                         hidden_node = c(5,5), neuralnet_algorithm = "sag",
                                         activation_function = 3, stepmax = 100000, threshold = 0.01)
neuralnet_dfE <- neuralnet_train_predict(df, "sampleoutput", c("sampleinput"), testinput, 
                                         hidden_node = c(5,5), neuralnet_algorithm = "slr",
                                         activation_function = 3, stepmax = 100000, threshold = 0.01)

#----------------- Deep Learning Utility to Time Series Prediction ---------------
library(keras)

to_numerical <- function(one_hot_matrix){
  numeric_sequences <- dim(one_hot_matrix)[2]
  nrow_one_hot <- nrow(one_hot_matrix)
  numeric_vector <- c(rep(0, nrow_one_hot))
  for(v in 1:numeric_sequences){
    if(v == 1){
      temp <- mapply(one_hot_matrix, FUN = function(x) x)[1:nrow_one_hot]
      idx_num_vector <- which(temp == 1)
      numeric_vector[idx_num_vector] <- v
    }
    else if(v > 1){
      temp <- mapply(one_hot_matrix, FUN = function(x) x)[(nrow_one_hot*(v-1)+1):(nrow_one_hot*v)]
      idx_num_vector <- which(temp == 1)
      numeric_vector[idx_num_vector] <- v
    }
  }
  return(numeric_vector)
}

time_series_lag_matrix_no_date <- function(vector_ts, n_lags=240, 
                                   limit_length=0, split_response=TRUE){
  library(tictoc)
  tic("Time Needed to extract lags")
  if(length(vector_ts) < limit_length || limit_length <= 0){
    df_lag <- data.frame(matrix(0, nrow=length(vector_ts), ncol=(n_lags+1)))
    for(a in 1:length(vector_ts)){
      df_lag[a,] <- vector_ts[a:(a+n_lags)] 
    } 
  }
  else{
    df_lag <- data.frame(matrix(0, nrow=limit_length, ncol=(n_lags+1)))
    for(a in 1:limit_length){
      df_lag[a,] <- vector_ts[a:(a+n_lags)]
    }
  }
  toc()
  
  name_seq <- paste0("Lags-", rev(1:n_lags))
  colnames(df_lag)[1:n_lags] <- name_seq
  df_lag <- na.omit(df_lag)
  
  if(split_response){
    split <- as.matrix(df_lag[,(n_lags+1)])
    df_lag <- as.matrix(df_lag[,1:n_lags])
    return(list(x=df_lag, y=split))
  }
  else{
    return(as.matrix(df_lag))
  }
}

time_series_lag_matrix <- function(vector_ts, date_vector, n_lags=240, 
                                         convert_date=FALSE, convert_posix=FALSE,
                                         convert_posix_format="%d.%m.%Y %H:%M:%OS",
                                         limit_length=0, split_response=TRUE, 
                                         convert_matrix=TRUE){
  library(timetk)
  library(tidyquant)
  library(tibble)
  library(tictoc)
  df_timeseries <- tibble(date_vector, vector_ts)
  colnames(df_timeseries) <- c("dates","value")
  if(convert_posix){
    df_timeseries$dates <- as.POSIXct(df_timeseries$dates, 
                                      tryFormats = convert_posix_format)
  }
  if(convert_date){
    df_timeseries$dates <- as.Date(df_timeseries$dates)
  }
  if(limit_length > 0){
    df_timeseries <- df_timeseries[1:limit_length,] 
  }
  tic("Time to Extract Lags")
  df_timeseries <- df_timeseries %>% tk_xts(silent = TRUE) %>% lag.xts(k = 0:n_lags)
  toc()
  df_timeseries <- as.data.frame(df_timeseries)
  name_seq <- paste0("Lags-", 1:n_lags)
  colnames(df_timeseries)[-1] <- name_seq
  df_timeseries <- df_timeseries[,ncol(df_timeseries):1]
  df_timeseries <- na.omit(df_timeseries)
  if(limit_length > 0){
    writeLines(paste0("Selected Response Index: ", (n_lags+1), " to ", limit_length))
  }
  else{
    writeLines(paste0("Selected Response Index: ", (n_lags+1), " to ", nrow(df_timeseries))) 
  }
  if(split_response){
    response <- df_timeseries[,ncol(df_timeseries)]
    lags <- df_timeseries[,-ncol(df_timeseries)]
    if(convert_matrix){
      lags <- as.matrix(lags)
      response <- as.matrix(response)
      dim(lags) <- c(nrow(lags), ncol(lags), 1)
    }
    return(list(x=lags, y=response))
  }
  else{
    if(convert_matrix){
      df_timeseries <- as.matrix(df_timeseries)
    }
    return(df_timeseries) 
  }
}

time_series_lag_matrix_train_test <- function(vector_ts, date_vector, n_lags=240,
                                              convert_date=FALSE, convert_posix=FALSE,
                                              convert_posix_format="%d.%m.%Y %H:%M:%OS",
                                              use_which_train="number", train_num=500,
                                              train_size=0.7){
  writeLines("Reminder, use_which_train = number will use exact train size number")
  writeLines("While, use_which_train = prop will use approzimate proportion based by dataset")
  
  library(timetk)
  library(tidyquant)
  library(tibble)
  library(tictoc)
  df_timeseries <- tibble(date_vector, vector_ts)
  colnames(df_timeseries) <- c("dates","value")
  if(convert_posix){
    df_timeseries$dates <- as.POSIXct(df_timeseries$dates, 
                                      tryFormats = convert_posix_format)
  }
  if(convert_date){
    df_timeseries$dates <- as.Date(df_timeseries$dates)
  }
  tic("Time to Extract Lags")
  df_timeseries <- df_timeseries %>% tk_xts(silent = TRUE) %>% lag.xts(k = 0:n_lags)
  toc()
  
  writeLines("Preprocess Lags Dataframe..")
  df_timeseries <- as.data.frame(df_timeseries)
  name_seq <- paste0("Lags-", 1:n_lags)
  colnames(df_timeseries)[-1] <- name_seq
  df_timeseries <- df_timeseries[,ncol(df_timeseries):1]
  df_timeseries <- na.omit(df_timeseries)
  
  writeLines(paste0("Split to Train and Test with Train Size = ",train_size*100, "%"))
  response <- df_timeseries[,ncol(df_timeseries)] #this become vector
  lags <- df_timeseries[,-ncol(df_timeseries)]
  
  if(use_which_train == "prop"){
    set.seed(17)
    sample_props <- round(train_size * nrow(lags))
    lags_train <- as.matrix(lags[1:sample_props,])
    lags_test <- as.matrix(lags[(sample_props+1):nrow(lags),])
    response_train <- matrix(response[1:sample_props], ncol=1)
    response_test <- matrix(response[(sample_props+1):length(response)], ncol=1) 
  }
  else if(use_which_train == "number"){
    lags_train <- as.matrix(lags[1:train_num,])
    lags_test <- as.matrix(lags[(train_num+1):nrow(lags),])
    response_train <- matrix(response[1:train_num], ncol=1)
    response_test <- matrix(response[(train_num+1):length(response)], ncol=1) 
  }
  
  dim(lags_train) <- c(nrow(lags_train), ncol(lags_train), 1)
  dim(lags_test) <- c(nrow(lags_test), ncol(lags_test), 1)
  
  return(list(x_train=lags_train, x_test=lags_test, 
              y_train=response_train, y_test=response_test))
}

save_or_load_tfmodel <- function(model_object, model_name="", action="save"){
  library(keras)
  if(action=="save"){
    save_model_tf(model_object, model_name)
  }
  else if(action=="load"){
    model <- load_model_tf(model_name)
    return(model)
  }
}

#---------------- Keras Layer Automate Modeling ------------------------------
keras_deep_learn_model <- function(df, target_analysis="time_series", 
                                     time_series_value_var = "", 
                                     time_series_date_var = "", 
                                     compare_n_lag=20, shuffle_df=FALSE,
                                     convert_date=FALSE, convert_posix=FALSE,
                                     convert_posix_format="%d.%m.%Y %H:%M:%OS", normalize_response=FALSE,
                                     dependent_var="", independent_var="", one_hot_transform=TRUE, 
                                     network_order=c("dense"), network_batch=128, network_epoch=20,
                                     network_units=c(32), network_dense_activation=c("relu"),
                                     network_dropout_rate=c(0.2), network_cnn_padding=c("same"),
                                     classification_metric_focus="Accuracy",
                                     use_train_test=TRUE, use_validation=TRUE,
                                     fixed_train_num=-1,  pool_size=2, conv_kernel_size=3,
                                     test_split=0.15, validation_split=0.15,
                                     crop1d=c(5,5), crop2d=c(5,5,5,5), crop3d=c(5,5,5,5,5,5),
                                     compile_loss_categorical="categorical_crossentropy",
                                     compile_loss_regression="mean_squared_error",
                                     compile_optimizer="adam", learning_rate=0.01, plot_graphic=TRUE,
                                     train_set_x="", train_set_y="", test_set_x="", test_set_y="", guide=TRUE){
    
    #----------------------------- Guide --------------------------------------
    if(guide){
      writeLines("===================================================================================================")
      writeLines("=========================== Layer Creation Parameter Guide ========================================")
      writeLines("===================================================================================================")
      writeLines("")
      writeLines("=========================== Layer Types based by NN Structures ====================================")
      writeLines("1) DNN (Deep Neural Network) -> Most Practice Cases: Time series + predicting plain response y variable (flexible on numeric integer categoric)")
      writeLines("2) CNN (Convolution Neural Network) -> Most Practice Cases: Time Series + Image Classification")
      writeLines("3) LSTM (Long Short-Term Memory) -> Most Practice Cases: Time Series + NLP ")
      writeLines("3-2) LSTM -> Recommended use for a long set of training data (More complex structure than GRU)")
      writeLines("3-3) LSTM has three gates (namely input, output and forget gates).")
      writeLines("4) RNN (Recurrent Neural Network) -> Most Practice Cases: Time Series + NLP ")
      writeLines("4-2) RNN -> Recommended use for Sequence Data (faster training, computationally less expensive)")
      writeLines("5) GRU (Gated Recurrent Unit) -> Most Practice Cases: Time Series + NLP")
      writeLines("5-2) GRU -> Recommended use for a shorter set of training data (Less complex structure than LSTM)")
      writeLines("5-3) GRU has two gates (reset and update gates)")
      writeLines("6) Auto Encoders (AE) -> Most Practice Cases: Image Classification (Noising & Denoising)")
      writeLines("6-2) Auto Encoders are a set of CNN, Pooling2D, CNN-Transpose")
      writeLines("")
      writeLines("=========================== Layer Important/Finding Notes ==================================================")
      writeLines("1-1) Pooling also means Downsampling, often also used as encoder series")
      writeLines("1-2) Upsampling is the reverse of Pooling, often used as decoder series")
      writeLines("1-3) Downsampling and Upsampling helps to make the representation become approximately invariant to small translations of the input")
      writeLines("1-4) Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.")
      writeLines("2-1) In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data.")
      writeLines("2-2) In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional. Mostly used on Image data.")
      writeLines("2-3) In 3D CNN, kernel moves in 3 directions. Input and output data of 3D CNN is 4 dimensional. Mostly used on 3D Image data (MRI, CT Scans).")
      writeLines("2-4) CNN can be an alternative to DNN over Predicting Black White Images (often 3 dimensional) in dim()")
      writeLines('2-5) but DNN cannot perform as an alternative to CNN over RGB Images (often 4 dimensional) in dim()')
      writeLines("3-1) LSTM, GRU, RNN works best analyzing sequences, so to predict time series or any kind of response")
      writeLines("3-2) make sure to train sequences number (which is transformed if not exist) rather than random numbers")
      writeLines("4-1) Crop1D have 2 parameter to fill (start, end)")
      writeLines("4-2) Crop2D have 4 parameter (height and width, each of start and end)")
      writeLines('4-3) Crop3D have 6 parameter (depth height width, each of start and end)')
      writeLines('5-1) Padding in Convolution Layer adds extra pixels around the image, to prevent information loss when reducing Image dimensions.')
      writeLines("5-2) Padding = same, means applying zero padding as such the original image size is preserved.")
      writeLines("5-3) Padding = valid, means we do not apply any padding.")
      writeLines("")
      writeLines("=========================== Activation Function Types ==============================================")
      writeLines("References: https://keras.io/api/layers/")
      writeLines("1) ReLu function rectified linear unit activation function. max(x, 0)")
      writeLines("2-1) Sigmoid function. sigmoid(x) = 1 / (1 + exp(-x)) ")
      writeLines("2-2) For small values (<-5), sigmoid returns a value close to zero, and for large values (>5) the result of the function gets close to 1")
      writeLines("3-1) Softmax function. Softmax converts a vector of values to a probability distribution.")
      writeLines("3-2) The softmax of each vector x is computed as exp(x) / tf.reduce_sum(exp(x)).")
      writeLines("4) Softplus function softplus(x) = log(exp(x) + 1).")
      writeLines("5) Softsign function softsign(x) = x / (abs(x) + 1).")
      writeLines("6) Tanh function tanh(x) = sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x))).")
      writeLines("7) SeLu function (Scaled Exponential Linear Unit)")
      writeLines("7-2) if x > 0: return scale * x, if x < 0: return scale * alpha * (exp(x) - 1)")
      writeLines("7-3) where alpha and scale are pre-defined constants (alpha=1.67326324 and scale=1.05070098).")
      writeLines("8) ELu function (exponential linear unit)")
      writeLines("8-2) alpha > 0 is: x if x > 0 and alpha * (exp(x) - 1) if x < 0 ")
      writeLines("8-3) The ELU hyperparameter alpha controls the value to which an ELU saturates for negative net inputs. ELUs diminish the vanishing gradient effect.")
      writeLines("9) exponential function exp(x) ")
      writeLines("")
      writeLines("===================================================================================================")
      writeLines("=========================== Compile Stage Parameter Guide =========================================")
      writeLines("===================================================================================================")
      writeLines("======================= Loss & Metric Types ==============================================")
      writeLines("References: https://keras.io/api/losses/ and https://keras.io/api/metrics/")
      writeLines("1) binary_crossentropy: Use this cross-entropy loss for binary (0 or 1) classification applications")
      writeLines("2-1) categorical_crossentropy: Use this crossentropy loss function when there are two or more label classes")
      writeLines("2-2) (We expect labels to be provided in a one_hot representation)")
      writeLines("3-1) sparse_categorical_crossentropy: Use this crossentropy loss function when there are two or more label classes")
      writeLines("3-2) We expect labels to be provided as integers")
      writeLines("4) poisson: Computes the Poisson loss between y_true and y_pred. (loss = y_pred - y_true * log(y_pred))")
      writeLines("5) kl_divergence: Computes Kullback-Leibler divergence loss between y_true and y_pred. (loss = y_true * log(y_true / y_pred))")
      writeLines("")
      writeLines("=========================== Accuracy Types =======================================")
      writeLines("References: https://keras.io/api/metrics/accuracy_metrics/")
      writeLines("1) accuracy: Calculates how often predictions equal labels.")
      writeLines("2) binary_accuracy: Calculates how often predictions match binary labels.")
      writeLines("3) categorical_accuracy: Calculates how often predictions match one-hot labels.")
      writeLines("4) top_k_categorical_accuracy: Computes how often targets are in the top K predictions.")
      writeLines("5) sparse_top_k_categorical_accuracy: Computes how often integer targets are in the top K predictions.")
      writeLines("")
      writeLines("=========================== Optimizer Types =======================================")
      writeLines("References: https://keras.io/api/optimizers/")
      writeLines("1) SGD: Stocahastic Gradient descent (with momentum) optimizer.")
      writeLines("---------------------------------------------------------------")
      writeLines("2) RMSprop: Optimizer that implements the RMSprop algorithm.")
      writeLines("---------------------------------------------------------------")
      writeLines("2-2) The centered version additionally maintains a moving average of the gradients, and uses that average to estimate the variance.")
      writeLines("---------------------------------------------------------------")
      writeLines("3) Adam: Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.")
      writeLines("---------------------------------------------------------------")
      writeLines("3-2) computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients")
      writeLines("3-3) well suited for problems that are large in terms of data/parameters")
      writeLines("---------------------------------------------------------------")
      writeLines("4) Adadelta: Adadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension to address two drawbacks:")
      writeLines("---------------------------------------------------------------")
      writeLines("4-2) Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients")
      writeLines("4-3) This way, Adadelta continues learning even when many updates have been done")
      writeLines("4-4) Compared to Adagrad, in the original version of Adadelta you don't have to set an initial learning rate")
      writeLines("---------------------------------------------------------------")
      writeLines("5) Adagrad: Adagrad is an optimizer with parameter-specific learning rates ")
      writeLines("---------------------------------------------------------------")
      writeLines("5-2) The more updates a parameter receives, the smaller the updates.")
      writeLines("---------------------------------------------------------------")
      writeLines("6) Adamax: Optimizer that implements the Adamax algorithm.")
      writeLines("---------------------------------------------------------------")
      writeLines("6-2) It is a variant of Adam based on the infinity norm")
      writeLines("6-3) Adamax is sometimes superior to adam, specially in models with embeddings.")
      writeLines("7) Nadam: Optimizer that implements the NAdam algorithm")
      writeLines("7-2) Much like Adam is essentially RMSprop with momentum, Nadam is Adam with Nesterov momentum.")
      
    }
    library(tensorflow)
    library(keras)
    library(ggplot2)
    
    #------------------------- Validation Input ------------------------------
    bool_train_test_no_available <- (train_set_x == "" && train_set_y == "" && test_set_x == "" && test_set_y == "")
    bool_train_test_available <- (train_set_x != "" && train_set_y != "" && test_set_x != "" && test_set_y != "")
    
    train_x <- NULL
    train_y <- NULL
    test_x <- NULL
    test_y <- NULL
    classes_train_y <- NULL
    classes_test_y <- NULL
    categorical_level_y <- NULL
    min_y <- NULL
    max_y <- NULL
    compile_metric_type <- NULL
    
    to_numerical <- function(one_hot_matrix){
      numeric_sequences <- dim(one_hot_matrix)[2]
      nrow_one_hot <- nrow(one_hot_matrix)
      numeric_vector <- c(rep(0, nrow_one_hot))
      for(v in 1:numeric_sequences){
        if(v == 1){
          temp <- mapply(one_hot_matrix, FUN = function(x) x)[1:nrow_one_hot]
          idx_num_vector <- which(temp == 1)
          numeric_vector[idx_num_vector] <- v
        }
        else if(v > 1){
          temp <- mapply(one_hot_matrix, FUN = function(x) x)[(nrow_one_hot*(v-1)+1):(nrow_one_hot*v)]
          idx_num_vector <- which(temp == 1)
          numeric_vector[idx_num_vector] <- v
        }
      }
      return(numeric_vector)
    }
    
    if(bool_train_test_no_available){
      writeLines("Create Train and Test Sets Automatically")
    }
    else if(!bool_train_test_available){
      stop("Train and Test Input are not complete! Please specify input for all test train in x y format!")
    }
    else if(bool_train_test_available){
      if(class(train_set_x) %in% c("matrix","array") && class(train_set_y) %in% c("matrix","array") && 
         class(test_set_x) %in% c("matrix","array") && class(test_set_y) %in% c("matrix","array")){
        train_x <- train_set_x
        train_y <- train_set_y
        test_x <- test_set_x
        test_y <- test_set_y
      }
      else{
        stop("Train and Test Input are expected to be Matrix or Array! Please Convert the data to Matrix first!")
      }
      
      if(target_analysis == "regression"){
        if(normalize_response){
          train_y_length <- length(train_y)
          train_data <- c(as.numeric(train_y), as.numeric(test_y))
          train_data_length <- length(train_data)
          train_list <- min_max_norm(train_data)
          train_y <- train_list[[1]][1:train_y_length]
          test_y <- train_list[[1]][(train_y_length+1):train_data_length]
          min_y <- train_list[[2]]
          max_y <- train_list[[3]]
        }
        compile_metric_type <- "regression"
      }
      else if(target_analysis == "classification"){
        if(dim(train_y)[2] > 1){
          compile_loss_categorical <- "categorical_crossentropy"
          compile_metric_type <- "classification"
          classes_train_y <- to_numerical(train_y)
          classes_test_y <- to_numerical(test_y)
        }
        else if(dim(train_y)[2] == 1){
          one_hot_transform <- FALSE
          compile_loss_categorical <- "sparse_categorical_crossentropy"
          compile_metric_type <- "classification"
          classes_train_y <- train_y
          classes_test_y <- test_y
        }
      }
    }
    
    min_max_norm <- function(x, multiplier=1) {
      min_x <- min(x)
      max_x <- max(x)
      norm_result <- ((x - min(x)) / (max(x) - min(x))) * multiplier
      return(list(norm_result, min_x, max_x))
    }
    
    min_max_denorm <- function(x, min_x_input, max_x_input, multiplier_input=1){
      return(x / multiplier_input * (max_x_input - min_x_input) + min_x_input) 
    }
    
    validation_length <- c(length(network_order), length(network_units), 
                           length(network_dense_activation),
                           length(network_dropout_rate), length(network_cnn_padding))
    if(!all(validation_length == length(network_order))){
      stop("Network Order,Units,Activation, and Dropout Rate Length Differ, please input an appropriate order and units")
    }
    if(!all(network_order %in% c("dense","lstm","rnn","gru",
                                 "bidirectional_lstm", "bidirectional_rnn", "bidirectional_gru", 
                                 "average_pooling1D", "average_pooling2D", "average_pooling3D",
                                 "max_pooling1D", "max_pooling2D", "max_pooling3D",
                                 "global_average_pooling1D","global_average_pooling2D", "global_average_pooling3D",
                                 "global_max_pooling1D","global_max_pooling2D", "global_max_pooling3D",
                                 "conv1D","conv2D","conv3D",
                                 "conv1D_t","conv2D_t","conv3D_t",
                                 "crop1D","crop2D","crop3D",
                                 "upsampling1D","upsampling2D","upsampling3D",
                                 "dropout","batch_normalization","flatten"))){
      stop("Network Order contains an unknown layer to add/typo layer!")
    }
    
    #---------------- 1) Check what actions to do based on target analysis, and prepare train and test data --------------------------
    
    #also calculate layer that is sequencial
    
    if(bool_train_test_no_available){
      if(target_analysis == "time_series"){
        if(time_series_value_var == "" || time_series_date_var == ""){
          message("Please input Time Series Value and Date Variable Appropriately!")
        }
        else{
          library(tibble)
          
          df_timeseries <- NULL
          if(normalize_response){
            series_normalized <- min_max_norm(df[[time_series_value_var]])
            series <- series_normalized[[1]]
            min_y <- series_normalized[[2]]
            max_y <- series_normalized[[3]]
            time_series_date <- df[[time_series_date_var]]
            df_timeseries <- tibble(series, time_series_date)
            colnames(df_timeseries) <- c("dates","value")
          }
          else{
            time_series_value <- df[[time_series_value_var]]
            time_series_date <- df[[time_series_date_var]]
            df_timeseries <- tibble(time_series_value, time_series_date)
            colnames(df_timeseries) <- c("dates","value")
          }
          
          if(convert_posix){
            df_timeseries$dates <- as.POSIXct(df_timeseries$dates, 
                                              tryFormats = convert_posix_format)
          }
          if(convert_date){
            df_timeseries$dates <- as.Date(df_timeseries$dates)
          }
          time_series_lag_matrix_train_test <- function(vector_ts, date_vector, n_lags=240,
                                                        convert_date=FALSE, convert_posix=FALSE,
                                                        convert_posix_format="%d.%m.%Y %H:%M:%OS",
                                                        use_which_train="number", train_num=500,
                                                        train_size=0.7){
            writeLines("Reminder, use_which_train = number will use exact train size number")
            writeLines("While, use_which_train = prop will use approzimate proportion based by dataset")
            
            library(timetk)
            library(tidyquant)
            library(tibble)
            library(tictoc)
            df_timeseries <- tibble(date_vector, vector_ts)
            colnames(df_timeseries) <- c("dates","value")
            if(convert_posix){
              df_timeseries$dates <- as.POSIXct(df_timeseries$dates, 
                                                tryFormats = convert_posix_format)
            }
            if(convert_date){
              df_timeseries$dates <- as.Date(df_timeseries$dates)
            }
            tic("Time to Extract Lags")
            df_timeseries <- df_timeseries %>% tk_xts(silent = TRUE) %>% lag.xts(k = 0:n_lags)
            toc()
            
            writeLines("Preprocess Lags Dataframe..")
            df_timeseries <- as.data.frame(df_timeseries)
            name_seq <- paste0("Lags-", 1:n_lags)
            colnames(df_timeseries)[-1] <- name_seq
            df_timeseries <- df_timeseries[,ncol(df_timeseries):1]
            df_timeseries <- na.omit(df_timeseries)
            
            writeLines(paste0("Split to Train and Test with Train Size = ",train_size*100, "%"))
            response <- df_timeseries[,ncol(df_timeseries)] #this become vector
            lags <- df_timeseries[,-ncol(df_timeseries)]
            
            if(use_which_train == "prop"){
              set.seed(17)
              sample_props <- round(train_size * nrow(lags))
              lags_train <- as.matrix(lags[1:sample_props,])
              lags_test <- as.matrix(lags[(sample_props+1):nrow(lags),])
              response_train <- matrix(response[1:sample_props], ncol=1)
              response_test <- matrix(response[(sample_props+1):length(response)], ncol=1) 
            }
            else if(use_which_train == "number"){
              lags_train <- as.matrix(lags[1:train_num,])
              lags_test <- as.matrix(lags[(train_num+1):nrow(lags),])
              response_train <- matrix(response[1:train_num], ncol=1)
              response_test <- matrix(response[(train_num+1):length(response)], ncol=1) 
            }
            
            dim(lags_train) <- c(nrow(lags_train), ncol(lags_train), 1)
            dim(lags_test) <- c(nrow(lags_test), ncol(lags_test), 1)
            
            return(list(x_train=lags_train, x_test=lags_test, 
                        y_train=response_train, y_test=response_test))
          }
          time_series_proccessed <- NULL
          if(fixed_train_num > 0){
            time_series_proccessed <- time_series_lag_matrix_train_test(df_timeseries$value, df_timeseries$dates,
                                                                        n_lags = compare_n_lag, use_which_train = "number", 
                                                                        train_num = fixed_train_num)
          }
          else{
            time_series_proccessed <- time_series_lag_matrix_train_test(df_timeseries$value, df_timeseries$dates,
                                                                        n_lags = compare_n_lag, use_which_train = "prop", 
                                                                        train_size = 1-test_split)
          }
          train_x <- time_series_proccessed$x_train
          train_y <- time_series_proccessed$y_train
          test_x <- time_series_proccessed$x_test
          test_y <- time_series_proccessed$y_test
          compile_metric_type <- "regression"
        }
      }
      else if(target_analysis == "regression"){
        if(dependent_var == ""){
          stop("Please Input Dependent Variable as a Classification Target which length is finite")
        }
        else{
          df_indep <- NULL
          df_dep <- NULL
          
          if(independent_var == ""){
            df_indep <- df[,-which(colnames(df) %in% dependent_var)]
            df_dep <- df[[dependent_var]]
          }
          else{
            df_indep <- df[,which(colnames(df) %in% independent_var)]
            df_dep <- df[[dependent_var]]
          }
          
          if(normalize_response){
            df_dep_list <- min_max_norm(df_dep)
            df_dep <- df_dep_list[[1]]
            min_y <- df_dep_list[[2]]
            max_y <- df_dep_list[[3]]
          }
          
          train_size <- 0
          train_num <- 0
          use_which_train <- "NULL"
          df_dep_transform <- NULL
          
          if(fixed_train_num == -1){
            use_which_train <- "prop"
            train_size <- (1 - test_split)
          }
          else{
            use_which_train <- "number"
            train_num <- fixed_train_num
          }
          
          compile_metric_type <- "regression"
          if(use_which_train == "prop"){
            if(shuffle_df){
              set.seed(17)
              sample_indice <- sample(2, size=nrow(df), prob=c(train_size, 1-train_size), replace=TRUE)
              train_x <- as.matrix(df_indep[which(sample_indice == 1),])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[which(sample_indice == 2),])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              train_y <- as.matrix(df_dep[which(sample_indice == 1)])
              test_y <- as.matrix(df_dep[which(sample_indice == 2)])
            }
            else{
              sample_props <- round(train_size * nrow(df))
              train_x <- as.matrix(df_indep[1:sample_props,])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[(sample_props+1):nrow(df_indep),])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              train_y <- as.matrix(df_dep[1:sample_props])
              test_y <- as.matrix(df_dep[(sample_props+1):length(df_dep)])
            }
          }
          else if(use_which_train == "number"){
            if(shuffle_df){
              set.seed(17)
              sample_indice <- sample(1:nrow(df), size=train_num, replace=FALSE)
              train_x <- as.matrix(df_indep[sample_indice,])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[-sample_indice,])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              train_y <- as.matrix(df_dep[sample_indice])
              test_y <- as.matrix(df_dep[-sample_indice])
            }
            else{
              train_x <- as.matrix(df_indep[1:train_num,])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[(train_num+1):nrow(df_indep),])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              train_y <- as.matrix(df_dep[1:train_num])
              test_y <- as.matrix(df_dep[(train_num+1):length(df_dep)])
            }
          }
        }
      }
      else if(target_analysis == "classification"){
        if(dependent_var == ""){
          stop("Please Input Dependent Variable as a Classification Target which length is finite")
        }
        else{
          df_indep <- NULL
          df_dep <- NULL
          
          if(independent_var == ""){
            df_indep <- df[,-which(colnames(df) %in% dependent_var)]
            df_dep <- df[[dependent_var]]
          }
          else{
            df_indep <- df[,which(colnames(df) %in% independent_var)]
            df_dep <- df[[dependent_var]]
          }
          
          train_size <- 0
          train_num <- 0
          use_which_train <- "NULL"
          df_dep_transform <- NULL
          
          if(fixed_train_num == -1){
            use_which_train <- "prop"
            train_size <- (1 - test_split)
          }
          else{
            use_which_train <- "number"
            train_num <- fixed_train_num
          }
          
          if(class(df_dep) == "character"){
            categorical_level_y <- levels(as.factor(df_dep))
            df_dep <- as.numeric(as.factor(df_dep)) - 1
            df_dep_transform <- to_categorical(df_dep)
          }
          else if(class(df_dep) == "factor"){
            categorical_level_y <- levels(df_dep)
            df_dep <- as.numeric(df_dep) - 1
            df_dep_transform <- to_categorical(as.numeric(df_dep))
          }
          else{
            if(min(df_dep) != 0){
              df_dep <- df_dep - min(df_dep)
            }
            df_dep_transform <- to_categorical(df_dep)
          }
          
          if(one_hot_transform){
            compile_loss_categorical <- "categorical_crossentropy"
            compile_metric_type <- "classification"
          }
          else{
            compile_loss_categorical <- "sparse_categorical_crossentropy"
            compile_metric_type <- "classification"
          }
          
          if(use_which_train == "prop"){
            if(shuffle_df){
              set.seed(17)
              sample_indice <- sample(2, size=nrow(df), prob=c(train_size, 1-train_size), replace=TRUE)
              train_x <- as.matrix(df_indep[which(sample_indice == 1),])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[which(sample_indice == 2),])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              if(one_hot_transform){
                train_y <- df_dep_transform[which(sample_indice == 1),]
                test_y <- df_dep_transform[which(sample_indice == 2),]
                classes_train_y <- df_dep[which(sample_indice == 1)]
                classes_test_y <- df_dep[which(sample_indice == 2)]
              }
              else{
                train_y <- df_dep[which(sample_indice == 1)]
                test_y <- df_dep[which(sample_indice == 2)] 
              }
            }
            else{
              sample_props <- round(train_size * nrow(df))
              train_x <- as.matrix(df_indep[1:sample_props,])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[(sample_props+1):nrow(df_indep),])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              if(one_hot_transform){
                train_y <- df_dep_transform[1:sample_props,]
                test_y <- df_dep_transform[(sample_props+1):length(df_dep),] 
                classes_train_y <- df_dep[1:sample_props]
                classes_test_y <- df_dep[(sample_props+1):length(df_dep)]
              }
              else{
                train_y <- df_dep[1:sample_props]
                test_y <- df_dep[(sample_props+1):length(df_dep)] 
              }
            }
          }
          else if(use_which_train == "number"){
            if(shuffle_df){
              set.seed(17)
              sample_indice <- sample(1:nrow(df), size=train_num, replace=FALSE)
              train_x <- as.matrix(df_indep[sample_indice,])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[-sample_indice,])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              if(one_hot_transform){
                train_y <- df_dep_transform[sample_indice,]
                test_y <- df_dep_transform[-sample_indice,] 
                classes_train_y <- df_dep[sample_indice]
                classes_test_y <- df_dep[-sample_indice]
              }
              else{
                train_y <- df_dep[sample_indice]
                test_y <- df_dep[-sample_indice]
              }
            }
            else{
              train_x <- as.matrix(df_indep[1:train_num,])
              train_x <- matrix(train_x, nrow=nrow(train_x), ncol=ncol(train_x))
              test_x <- as.matrix(df_indep[(train_num+1):nrow(df_indep),])
              test_x <- matrix(test_x, nrow=nrow(test_x), ncol=ncol(test_x))
              if(one_hot_transform){
                train_y <- df_dep_transform[1:train_num,]
                test_y <- df_dep_transform[(train_num+1):length(df_dep),] 
                classes_train_y <- df_dep[1:train_num]
                classes_test_y <- df_dep[(train_num+1):length(df_dep)] 
              }
              else{
                train_y <- df_dep[1:train_num]
                test_y <- df_dep[(train_num+1):length(df_dep)] 
              }
            }
          }
        }
      }
      else if(target_analysis == "image_classification"){
        writeLines("Not Ready Yet")
        compile_metric_type <- "classification"
      }
      else if(target_analysis == "image_noise_denoise"){
        writeLines("Not Ready Yet")
        compile_metric_type <- "classification"
      }
      else if(target_analysis == "text_processing"){
        writeLines("Not Ready Yet")
        compile_metric_type <- "classification"
      }
    }
    
    if(network_order[1]=="lstm" && length(dim(train_x) == 2)){
      dim(train_x) <- c(nrow(train_x), ncol(train_x), 1)
      dim(test_x) <- c(nrow(test_x), ncol(test_x), 1)
    }
    
    writeLines('Dimension Inspect: ')
    writeLines(paste0("Length of Dimension Train X: ", length(dim(train_x))))
    print(dim(train_x))
    writeLines(paste0("Length of Dimension Train Y: ", length(dim(train_y))))
    print(dim(train_y))
    writeLines(paste0("Length of Dimension Test X: ", length(dim(test_x))))
    print(dim(test_x))
    writeLines(paste0("Length of Dimension Test Y: ", length(dim(test_y))))
    print(dim(test_y))
    
    library(data.table)
    
    #https://stackoverflow.com/questions/58399145/lstm-forecasted-a-straight-line
    input_shape <- dim(train_x)[-1]
    network_order_table <- table(network_order)
    lstm_length <- sum(as.numeric(network_order_table[which(names(network_order_table) %like% "lstm")]))
    rnn_length <- sum(as.numeric(network_order_table[which(names(network_order_table) %like% "rnn")]))
    gru_length <- sum(as.numeric(network_order_table[which(names(network_order_table) %like% "gru")]))
    all_sequences_layer <- lstm_length + rnn_length + gru_length
    sequences_layer_add <- 1
    
    #---------------- 2) Construct a Neural Net Parameter which is compatible with the datasets ------------------
    model <- keras_model_sequential()
    writeLines('Generating Layer Command..')
    layer_exp_list <- list()
    for(mod_layer in 1:length(network_order)){
      layer_str <- paste0("model %>% ")
      if(network_order[mod_layer] == "dense"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_dense(units=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              if(network_dense_activation[mod_layer] != "NULL"){
                layer_str <- paste0(layer_str, input_shape[shapes],")",
                                    ",activation='",network_dense_activation[mod_layer],"')")
              }
              else{
                layer_str <- paste0(layer_str, input_shape[shapes],")",
                                    ",activation=",network_dense_activation[mod_layer],")")
              }
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          if(network_dense_activation[mod_layer] != "NULL"){
            layer_str <- paste0(layer_str, "layer_dense(units=",network_units[mod_layer],
                                ",activation='",network_dense_activation[mod_layer],"')")
          }
          else{
            layer_str <- paste0(layer_str, "layer_dense(units=",network_units[mod_layer],
                                ",activation=",network_dense_activation[mod_layer],")")
          }
        }
      }
      else if(network_order[mod_layer] == "lstm"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_lstm(units=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes])
              if(sequences_layer_add < all_sequences_layer){
                layer_str <- paste0(layer_str, "), return_sequences=TRUE)")
                sequences_layer_add <- sequences_layer_add + 1
              }
              else{
                layer_str <- paste0(layer_str, "), return_sequences=FALSE)")
              }
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_lstm(units=",network_units[mod_layer])
          if(sequences_layer_add < all_sequences_layer){
            layer_str <- paste0(layer_str, ",return_sequences=TRUE)")
            sequences_layer_add <- sequences_layer_add + 1
          }
          else{
            layer_str <- paste0(layer_str, ",return_sequences=FALSE)")
          }
        }
      }
      else if(network_order[mod_layer] == "gru"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_gru(units=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes])
              if(sequences_layer_add < all_sequences_layer){
                layer_str <- paste0(layer_str, "), return_sequences=TRUE)")
                sequences_layer_add <- sequences_layer_add + 1
              }
              else{
                layer_str <- paste0(layer_str, "), return_sequences=FALSE)")
              }
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_gru(units=",network_units[mod_layer])
          if(sequences_layer_add < all_sequences_layer){
            layer_str <- paste0(layer_str, ",return_sequences=TRUE)")
            sequences_layer_add <- sequences_layer_add + 1
          }
          else{
            layer_str <- paste0(layer_str, ",return_sequences=FALSE)")
          }
        }
      }
      else if(network_order[mod_layer] == "rnn"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_simple_rnn(units=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes])
              if(sequences_layer_add < all_sequences_layer){
                layer_str <- paste0(layer_str, "), return_sequences=TRUE)")
                sequences_layer_add <- sequences_layer_add + 1
              }
              else{
                layer_str <- paste0(layer_str, "), return_sequences=FALSE)")
              }
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_simple_rnn(units=",network_units[mod_layer])
          if(sequences_layer_add < all_sequences_layer){
            layer_str <- paste0(layer_str, ",return_sequences=TRUE)")
            sequences_layer_add <- sequences_layer_add + 1
          }
          else{
            layer_str <- paste0(layer_str, ",return_sequences=FALSE)")
          }
        }
      }
      else if(network_order[mod_layer] == "bidirectional_lstm"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "bidirectional(layer_lstm(units=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes])
              if(sequences_layer_add < all_sequences_layer){
                layer_str <- paste0(layer_str, "), return_sequences=TRUE))")
                sequences_layer_add <- sequences_layer_add + 1
              }
              else{
                layer_str <- paste0(layer_str, "), return_sequences=FALSE))")
              }
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "bidirectional(layer_lstm(units=",network_units[mod_layer])
          if(sequences_layer_add < all_sequences_layer){
            layer_str <- paste0(layer_str, ", return_sequences=TRUE))")
            sequences_layer_add <- sequences_layer_add + 1
          }
          else{
            layer_str <- paste0(layer_str, ", return_sequences=FALSE))")
          }
        }
      }
      else if(network_order[mod_layer] == "bidirectional_gru"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "bidirectional(layer_gru(units=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes])
              if(sequences_layer_add < all_sequences_layer){
                layer_str <- paste0(layer_str, "), return_sequences=TRUE))")
                sequences_layer_add <- sequences_layer_add + 1
              }
              else{
                layer_str <- paste0(layer_str, "), return_sequences=FALSE))")
              }
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "bidirectional(layer_gru(units=",network_units[mod_layer])
          if(sequences_layer_add < all_sequences_layer){
            layer_str <- paste0(layer_str, ", return_sequences=TRUE))")
            sequences_layer_add <- sequences_layer_add + 1
          }
          else{
            layer_str <- paste0(layer_str, ", return_sequences=FALSE))")
          }
        }
      }
      else if(network_order[mod_layer] == "bidirectional_rnn"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "bidirectional(layer_simple_rnn(units=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes])
              if(sequences_layer_add < all_sequences_layer){
                layer_str <- paste0(layer_str, "), return_sequences=TRUE))")
                sequences_layer_add <- sequences_layer_add + 1
              }
              else{
                layer_str <- paste0(layer_str, "), return_sequences=FALSE))")
              }
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "bidirectional(layer_simple_rnn(units=",network_units[mod_layer])
          if(sequences_layer_add < all_sequences_layer){
            layer_str <- paste0(layer_str, ", return_sequences=TRUE))")
            sequences_layer_add <- sequences_layer_add + 1
          }
          else{
            layer_str <- paste0(layer_str, ", return_sequences=FALSE))")
          }
        }
      }
      else if(network_order[mod_layer] == "average_pooling1D"){
        layer_str <- paste0(layer_str, "layer_average_pooling_1d(pool_size=c(",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "average_pooling2D"){
        layer_str <- paste0(layer_str, "layer_average_pooling_2d(pool_size=c(",pool_size,"L,",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "average_pooling3D"){
        layer_str <- paste0(layer_str, "layer_average_pooling_3d(pool_size=c(",pool_size,"L,",pool_size,"L,",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "max_pooling1D"){
        layer_str <- paste0(layer_str, "layer_max_pooling_1d(pool_size=c(",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "max_pooling2D"){
        layer_str <- paste0(layer_str, "layer_max_pooling_2d(pool_size=c(",pool_size,"L,",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "max_pooling3D"){
        layer_str <- paste0(layer_str, "layer_max_pooling_3d(pool_size=c(",pool_size,"L,",pool_size,"L,",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "global_average_pooling1D"){
        layer_str <- paste0(layer_str, "layer_global_average_pooling_1d()")
      }
      else if(network_order[mod_layer] == "global_average_pooling2D"){
        layer_str <- paste0(layer_str, "layer_global_average_pooling_2d()")
      }
      else if(network_order[mod_layer] == "global_average_pooling3D"){
        layer_str <- paste0(layer_str, "layer_global_average_pooling_3d()")
      }
      else if(network_order[mod_layer] == "global_max_pooling1D"){
        layer_str <- paste0(layer_str, "layer_global_max_pooling_1d()")
      }
      else if(network_order[mod_layer] == "global_max_pooling2D"){
        layer_str <- paste0(layer_str, "layer_global_max_pooling_2d()")
      }
      else if(network_order[mod_layer] == "global_max_pooling3D"){
        layer_str <- paste0(layer_str, "layer_global_max_pooling_3d()")
      }
      else if(network_order[mod_layer] == "conv1D"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_conv_1d(filters=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes],")",
                                  ",kernel_size=c(",conv_kernel_size,"L)",
                                  ",activation='",network_dense_activation[mod_layer],"'",
                                  ",padding='",network_cnn_padding[mod_layer],"')")
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_conv_1d(filters=",network_units[mod_layer],
                              ",kernel_size=c(",conv_kernel_size,"L)",
                              ",activation='",network_dense_activation[mod_layer],"'",
                              ",padding='",network_cnn_padding[mod_layer],"')")
        }
      }
      else if(network_order[mod_layer] == "conv2D"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_conv_2d(filters=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes],")",
                                  ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L)",
                                  ",activation='",network_dense_activation[mod_layer],"'",
                                  ",padding='",network_cnn_padding[mod_layer],"')")
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_conv_2d(filters=",network_units[mod_layer],
                              ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L)",
                              ",activation='",network_dense_activation[mod_layer],"'",
                              ",padding='",network_cnn_padding[mod_layer],"')")
        }
      }
      else if(network_order[mod_layer] == "conv3D"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_conv_3d(filters=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes],")",
                                  ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L,",conv_kernel_size,"L)",
                                  ",activation='",network_dense_activation[mod_layer],"'",
                                  ",padding='",network_cnn_padding[mod_layer],"')")
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_conv_3d(filters=",network_units[mod_layer],
                              ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L,",conv_kernel_size,"L)",
                              ",activation='",network_dense_activation[mod_layer],"'",
                              ",padding='",network_cnn_padding[mod_layer],"')")
        }
      }
      else if(network_order[mod_layer] == "conv1D_t"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_conv_1d_transpose(filters=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes],")",
                                  ",kernel_size=c(",conv_kernel_size,"L)",
                                  ",activation='",network_dense_activation[mod_layer],"'",
                                  ",padding='",network_cnn_padding[mod_layer],"')")
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_conv_1d_transpose(filters=",network_units[mod_layer],
                              ",kernel_size=c(",conv_kernel_size,"L)",
                              ",activation='",network_dense_activation[mod_layer],"'",
                              ",padding='",network_cnn_padding[mod_layer],"')")
        }
      }
      else if(network_order[mod_layer] == "conv2D_t"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_conv_2d_transpose(filters=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes],")",
                                  ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L)",
                                  ",activation='",network_dense_activation[mod_layer],"'",
                                  ",padding='",network_cnn_padding[mod_layer],"')")
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_conv_2d_transpose(filters=",network_units[mod_layer],
                              ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L)",
                              ",activation='",network_dense_activation[mod_layer],"'",
                              ",padding='",network_cnn_padding[mod_layer],"')")
        }
      }
      else if(network_order[mod_layer] == "conv3D_t"){
        if(mod_layer == 1){
          layer_str <- paste0(layer_str, "layer_conv_3d_transpose(filters=",network_units[mod_layer],",input_shape=c(")
          for(shapes in 1:length(input_shape)){
            if(shapes==length(input_shape)){
              layer_str <- paste0(layer_str, input_shape[shapes],")",
                                  ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L,",conv_kernel_size,"L)",
                                  ",activation='",network_dense_activation[mod_layer],"'",
                                  ",padding='",network_cnn_padding[mod_layer],"')")
            }
            else{
              layer_str <- paste0(layer_str, input_shape[shapes],",")
            }
          }
        }
        else{
          layer_str <- paste0(layer_str, "layer_conv_3d_transpose(filters=",network_units[mod_layer],
                              ",kernel_size=c(",conv_kernel_size,"L,",conv_kernel_size,"L,",conv_kernel_size,"L)",
                              ",activation='",network_dense_activation[mod_layer],"'",
                              ",padding='",network_cnn_padding[mod_layer],"')")
        }
      }
      else if(network_order[mod_layer] == "upsampling1D"){
        layer_str <- paste0(layer_str, "layer_upsampling_1d(pool_size=c(",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "upsampling2D"){
        layer_str <- paste0(layer_str, "layer_upsampling_2d(pool_size=c(",pool_size,"L,",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "upsampling3D"){
        layer_str <- paste0(layer_str, "layer_upsampling_3d(pool_size=c(",pool_size,"L,",pool_size,"L,",pool_size,"L))")
      }
      else if(network_order[mod_layer] == "dropout"){
        layer_str <- paste0(layer_str, "layer_dropout(",network_dropout_rate[mod_layer],")")
      }
      else if(network_order[mod_layer] == "batch_normalization"){
        layer_str <- paste0(layer_str, "layer_batch_normalization()")
      }
      else if(network_order[mod_layer] == "crop1D"){
        layer_str <- paste0(layer_str, "layer_cropping_1d(cropping=list(c(",crop1d[1],"L,",crop1d[2],"L",")))")
      }
      else if(network_order[mod_layer] == "crop2D"){
        layer_str <- paste0(layer_str, "layer_cropping_2d(cropping=list(c(",crop2d[1],"L,",crop2d[2],"L","), c(",crop2d[3],"L,",crop2d[4],"L",")))")
      }
      else if(network_order[mod_layer] == "crop3D"){
        layer_str <- paste0(layer_str, "layer_cropping_3d(cropping=list(c(",crop3d[1],"L,",crop3d[2],"L","), c(",crop3d[3],"L,",crop3d[4],"L","), c(",crop3d[5],"L,",crop3d[6],"L",")))")
      }
      else if(network_order[mod_layer] == "flatten"){
        layer_str <- paste0(layer_str, "layer_flatten()")
      }
      layer_exp_list <- c(layer_exp_list, list(layer_str))
    }
    
    print(layer_exp_list)
    
    #---------------- 3) Build Layers, Compile and Fit Model with given settings ------------------------------------------------
    for(execute in 1:length(layer_exp_list)){
      eval(parse(text=layer_exp_list[[execute]]))
    }
    
    writeLines("Neural Network Model Frame: ")
    print(summary(model))
    
    #https://community.rstudio.com/t/use-auc-as-metric-in-keras-for-r/84573/3
    #https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5
    
    #---------------- 4) Compile and Fit Model based by Compile Metric Types --------------------
    optimizer_neuralnet <- NULL
    if(compile_optimizer == "adam"){
      optimizer_neuralnet <- optimizer_adam(lr = learning_rate)
    }
    else if(compile_optimizer == "rmsprop"){
      optimizer_neuralnet <- optimizer_rmsprop(lr = learning_rate)
    }
    else if(compile_optimizer == "sgd"){
      optimizer_neuralnet <- optimizer_sgd(lr = learning_rate)
    }
    else if(compile_optimizer == "adadelta"){
      optimizer_neuralnet <- optimizer_adadelta(lr = learning_rate)
    }
    else if(compile_optimizer == "adagrad"){
      optimizer_neuralnet <- optimizer_adagrad(lr = learning_rate)
    }
    else if(compile_optimizer == "adamax"){
      optimizer_neuralnet <- optimizer_adamax(lr = learning_rate)
    }
    else if(compile_optimizer == "nadam"){
      optimizer_neuralnet <- optimizer_nadam(lr = learning_rate)
    }
    
    writeLines(paste0("Using ", compile_metric_type, " Metric Type"))
    if(compile_metric_type == "regression"){
      writeLines(paste0("Using ", compile_loss_regression, " Loss Regression Type"))
      writeLines("Compile Model...")
      model %>% compile(
        loss = compile_loss_regression,
        optimizer = optimizer_neuralnet,
        metrics = list(tf$keras$metrics$MeanSquaredError(),
                       tf$keras$metrics$MeanAbsoluteError(),
                       tf$keras$metrics$MeanSquaredLogarithmicError(),
                       tf$keras$metrics$MeanAbsolutePercentageError(),
                       tf$keras$metrics$RootMeanSquaredError(),
                       tf$keras$metrics$CosineSimilarity(),
                       tf$keras$metrics$LogCoshError())
      )
      
      writeLines("Begin Fitting Model...")
      history <- model %>% fit(
        x = train_x,
        y = train_y,
        validation_split = validation_split, 
        batch_size = network_batch,
        epochs = network_epoch,
        view_metrics = FALSE  
      )
      
      if(plot_graphic){
        x11()
        print(plot(history, main="Metrics Summary")) 
      }
    }
    else if(compile_metric_type == "classification"){
      writeLines(paste0("Using ", compile_loss_categorical, " Loss Categorical Type"))
      unique_response_length <- length(unique(train_y))
      if(unique_response_length == 2){
        if(compile_metric_type == "classification" && classification_metric_focus == "Accuracy"){
          writeLines("Compile Model...")
          model %>% compile(
            loss = compile_loss_categorical,
            optimizer = optimizer_neuralnet,
            metrics = list(tf$keras$metrics$BinaryAccuracy(),
                           tf$keras$metrics$Precision(),
                           tf$keras$metrics$Recall())
          )
          
          writeLines("Begin Fitting Model...")
          history <- model %>% fit(
            x = train_x,
            y = train_y,
            validation_split = validation_split, 
            batch_size = network_batch,
            epochs = network_epoch,
            view_metrics = FALSE  
          )
          
          
          if(plot_graphic){
            x11()
            print(plot(history, main="Metrics Summary")) 
          }
        }
        else if(compile_metric_type == "classification" && classification_metric_focus == "TFPN"){
          writeLines("Compile Model...")
          model %>% compile(
            loss = compile_loss_categorical,
            optimizer = optimizer_neuralnet,
            metrics = list(tf$keras$metrics$AUC(),
                           tf$keras$metrics$TruePositives(),
                           tf$keras$metrics$TrueNegatives(),
                           tf$keras$metrics$FalsePositives(),
                           tf$keras$metrics$FalseNegatives())
          )
          
          writeLines('Begin Fit Model...')
          history <- model %>% fit(
            x = train_x,
            y = train_y,
            validation_split = validation_split, 
            batch_size = network_batch,
            epochs = network_epoch,
            view_metrics = FALSE  
          )
          
          if(plot_graphic){
            x11()
            print(plot(history, main="Metrics Summary")) 
          }
        }
        else if(compile_metric_type == "classification" && classification_metric_focus == "Hinge"){
          writeLines("Compile Model...")
          model %>% compile(
            loss = compile_loss_categorical,
            optimizer = optimizer_neuralnet,
            metrics = list(tf$keras$metrics$Hinge(),
                           tf$keras$metrics$SquaredHinge())
          )
          
          writeLines("Begin Fitting Model...")
          history <- model %>% fit(
            x = train_x,
            y = train_y,
            validation_split = validation_split, 
            batch_size = network_batch,
            epochs = network_epoch,
            view_metrics = FALSE  
          )
          
          if(plot_graphic){
            x11()
            print(plot(history, main="Metrics Summary")) 
          }
        } 
      }
      else{
        if(compile_metric_type == "classification" && classification_metric_focus == "Accuracy"){
          if(one_hot_transform){
            writeLines("Compile Model...")
            model %>% compile(
              loss = compile_loss_categorical,
              optimizer = optimizer_neuralnet,
              metrics = list(tf$keras$metrics$CategoricalAccuracy(),
                             tf$keras$metrics$Precision(),
                             tf$keras$metrics$Recall())
            )
            writeLines("Begin Fitting Model...")
            history <- model %>% fit(
              x = train_x,
              y = train_y,
              validation_split = validation_split, 
              batch_size = network_batch,
              epochs = network_epoch,
              view_metrics = FALSE  
            )
            
            if(plot_graphic){
              x11()
              print(plot(history, main="Metrics Summary")) 
            }
          }
          else{
            model %>% compile(
              loss = compile_loss_categorical,
              optimizer = optimizer_neuralnet,
              metrics = list(tf$keras$metrics$Accuracy(),
                             tf$keras$metrics$SparseCategoricalAccuracy(),
                             tf$keras$metrics$Precision(),
                             tf$keras$metrics$Recall())
            )
            
            history <- model %>% fit(
              x = train_x,
              y = train_y,
              validation_split = validation_split, 
              batch_size = network_batch,
              epochs = network_epoch,
              view_metrics = FALSE  
            )
            
            if(plot_graphic){
              x11()
              print(plot(history, main="Metrics Summary")) 
            }
          }
        }
        else if(compile_metric_type == "classification" && classification_metric_focus == "Poisson"){
          writeLines("Compile Model...")
          model %>% compile(
            loss = compile_loss_categorical,
            optimizer = optimizer_neuralnet,
            metrics = tf$keras$metrics$Poisson()
          )
          
          writeLines("Begin Fitting Model...")
          history <- model %>% fit(
            x = train_x,
            y = train_y,
            validation_split = validation_split, 
            batch_size = network_batch,
            epochs = network_epoch,
            view_metrics = FALSE  
          )
          
          if(plot_graphic){
            x11()
            print(plot(history, main="Metrics Summary")) 
          }
        }
        else if(compile_metric_type == "classification" && classification_metric_focus == "Hinge"){
          writeLines("Compile Model...")
          model %>% compile(
            loss = compile_loss_categorical,
            optimizer = optimizer_neuralnet,
            metrics = tf$keras$metrics$CategoricalHinge()
          )
          
          writeLines("Begin Fitting Model...")
          history <- model %>% fit(
            x = train_x,
            y = train_y,
            validation_split = validation_split, 
            batch_size = network_batch,
            epochs = network_epoch,
            view_metrics = FALSE  
          )
          
          if(plot_graphic){
            x11()
            print(plot(history, main="Metrics Summary")) 
          }
        }
      }
    }
    
    #---------------- 5) Model Evaluation, Prediction towards y_test and Compare it -------------------------------
    train_evaluation <- model %>% evaluate(train_x, train_y)
    test_evaluation <- model %>% evaluate(test_x, test_y)
    bundle_output <- NULL
    
    writeLines("Train Evaluation")
    print(train_evaluation)
    writeLines("Test Evaluation")
    print(test_evaluation)
    
    if(compile_metric_type == "regression"){
      pred_train <- model %>% predict(train_x)
      pred_test <- model %>% predict(test_x)
      normalized_data_train <- NULL
      normalized_data_test <- NULL
      
      if(normalize_response){
        normalized_train_y <- train_y
        normalized_test_y <- test_y
        normalized_pred_train <- pred_train
        normalized_pred_test <- pred_test
        normalized_data_train <- data.frame(train_y = normalized_train_y,
                                            pred_y = normalized_pred_train)
        normalized_data_test <- data.frame(test_y = normalized_test_y,
                                           pred_y = normalized_pred_test)
        train_y <- min_max_denorm(train_y, min_y, max_y, 1)
        test_y <- min_max_denorm(test_y, min_y, max_y, 1)
        pred_train <- min_max_denorm(pred_train, min_y, max_y, 1)
        pred_test <- min_max_denorm(pred_test, min_y, max_y, 1)
      }
      
      p <- NULL
      o <- NULL
      df_plot_train <- NULL
      df_plot_test <- NULL
      
      if(plot_graphic){
        x11()
        df_plot_train <- data.frame(V1 = 1:nrow(train_y), V2=train_y, pred_out=pred_train)
        o <- ggplot(df_plot_train, aes(x = V1, y = V2)) + geom_line(colour = "blue", size = 0.1, alpha = 0.6)
        o <- o + geom_line(aes(x = V1, y = pred_train), colour = "red", size = 0.1 , alpha = 0.4) 
        o <- o + ggtitle("Train Data Comparison to Prediction")
        print(o)
        x11()
        df_plot_test <- data.frame(V1 = 1:nrow(test_y), V2=test_y, pred_out=pred_test)
        p <- ggplot(df_plot_test, aes(x = V1, y = V2)) + geom_line(colour = "blue", size = 0.1, alpha = 0.6)
        p <- p + geom_line(aes(x = V1, y = pred_test), colour = "red", size = 0.1 , alpha = 0.4) 
        p <- p + ggtitle("Test Data Comparison to Prediction")
        print(p) 
      }
      else{
        df_plot_train <- data.frame(V1 = 1:nrow(train_y), V2=train_y, pred_out=pred_train)
        o <- ggplot(df_plot_train, aes(x = V1, y = V2)) + geom_line(colour = "blue", size = 0.1, alpha = 0.6)
        o <- o + geom_line(aes(x = V1, y = pred_train), colour = "red", size = 0.1 , alpha = 0.4) 
        o <- o + ggtitle("Train Data Comparison to Prediction")
        df_plot_test <- data.frame(V1 = 1:nrow(test_y), V2=test_y, pred_out=pred_test)
        p <- ggplot(df_plot_test, aes(x = V1, y = V2)) + geom_line(colour = "blue", size = 0.1, alpha = 0.6)
        p <- p + geom_line(aes(x = V1, y = pred_test), colour = "red", size = 0.1 , alpha = 0.4) 
        p <- p + ggtitle("Test Data Comparison to Prediction")
      }
      
      colnames(df_plot_train) <- c("Rows","Actual","Prediction")
      colnames(df_plot_test) <- c("Rows","Actual","Prediction")
      
      if(normalize_response){
        bundle_output <- list(layer_exp_list, model, history, train_evaluation, test_evaluation, df_plot_train, df_plot_test, 
                              normalized_data_train, normalized_data_test, train_x, train_y, test_x, test_y, o, p, min_y, max_y)
        names(bundle_output) <- c("Layer Command", "Neural Net Model", "Metric History", 
                                  "Train Evaluation", "Test Evaluation", 
                                  "Train Predict Comparison","Test Predict Comparison",
                                  "Normalized Train data", "Normalized Test data", 
                                  "Train X data Matrix","Train Y data Matrix", 
                                  "Test X data Matrix","Test Y data Matrix", 
                                  "Train Graphics", "Test Graphics", 
                                  "Min Prior Normalize Value", "Max Prior Normalize Value")
      }
      else{
        bundle_output <- list(layer_exp_list, model, history, train_evaluation, test_evaluation, df_plot_train, df_plot_test, 
                              train_x, train_y, test_x, test_y, o, p)
        names(bundle_output) <- c("Layer Command", "Neural Net Model", "Metric History", 
                                  "Train Evaluation", "Test Evaluation", "Train Predict Comparison",
                                  "Test Predict Comparison","Train X data Matrix","Train Y data Matrix", 
                                  "Test X data Matrix","Test Y data Matrix","Train Graphics", "Test Graphics")
      }
    }
    else if(compile_metric_type == "classification"){
      pred_train <- model %>% predict_classes(train_x)
      
      actual_pred_table_train <- NULL
      if(one_hot_transform){
        actual_pred_table_train <- table(actual=classes_train_y, pred=pred_train)
      }
      else{
        actual_pred_table_train <- table(actual=train_y, pred=pred_train)
      }
      
      if(!is.null(categorical_level_y)){
        rownames(actual_pred_table_train) <- categorical_level_y
        colnames(actual_pred_table_train) <- categorical_level_y
      }
      
      print(actual_pred_table_train)
      writeLines("Total of Actual Train")
      print(rowSums(actual_pred_table_train))
      
      dist_match_train <- NULL
      matching_only_train <- NULL
      
      if(plot_graphic){
        x11()
        table_df_train <- as.data.frame(actual_pred_table_train) 
        colnames(table_df_train)[1:2] <- c("actual","pred")
        table_df_train$actual <- paste0("Actual-",table_df_train$actual)
        table_df_train$pred <- paste0("Predict-",table_df_train$pred)
        dist_match_train <- ggplot(table_df_train, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Matching Distribution for Train Datasets')
        print(dist_match_train)
        
        x11()
        matching_only_train <- matrix(0, nrow=nrow(actual_pred_table_train), ncol=ncol(actual_pred_table_train))
        diag(matching_only_train) <- diag(actual_pred_table_train)
        matching_only_train <- as.table(matching_only_train)
        matching_df_train <- as.data.frame(matching_only_train)
        colnames(matching_df_train)[1:2] <- c("actual","pred")
        matching_df_train$actual <- paste0("Actual-",matching_df_train$actual)
        matching_df_train$pred <- paste0("Predict-",matching_df_train$pred)
        compare_match_train <- ggplot(matching_df_train, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Match Comparison for Train Datasets')
        
        print(compare_match_train) 
      }
      else{
        table_df_train <- as.data.frame(actual_pred_table_train) 
        colnames(table_df_train)[1:2] <- c("actual","pred")
        table_df_train$actual <- paste0("Actual-",table_df_train$actual)
        table_df_train$pred <- paste0("Predict-",table_df_train$pred)
        dist_match_train <- ggplot(table_df_train, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Matching Distribution for Train Datasets')
        matching_only_train <- matrix(0, nrow=nrow(actual_pred_table_train), ncol=ncol(actual_pred_table_train))
        diag(matching_only_train) <- diag(actual_pred_table_train)
        matching_only_train <- as.table(matching_only_train)
        matching_df_train <- as.data.frame(matching_only_train)
        colnames(matching_df_train)[1:2] <- c("actual","pred")
        matching_df_train$actual <- paste0("Actual-",matching_df_train$actual)
        matching_df_train$pred <- paste0("Predict-",matching_df_train$pred)
        compare_match_train <- ggplot(matching_df_train, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Match Comparison for Train Datasets')
      }
      
      pred_test <- model %>% predict_classes(test_x)
      actual_pred_table_test <- NULL
      
      if(one_hot_transform){
        actual_pred_table_test <- table(actual=classes_test_y, pred=pred_test)
      }
      else{
        actual_pred_table_test <- table(actual=test_y, pred=pred_test)
      }
      
      print(actual_pred_table_test)
      writeLines("Total of Actual Test")
      print(rowSums(actual_pred_table_test))
      
      dist_match_test <- NULL
      compare_match_test <- NULL
      if(plot_graphic){
        x11()
        table_df_test <- as.data.frame(actual_pred_table_test) 
        colnames(table_df_test)[1:2] <- c("actual","pred")
        table_df_test$actual <- paste0("Actual-",table_df_test$actual)
        table_df_test$pred <- paste0("Predict-",table_df_test$pred)
        dist_match_test <- ggplot(table_df_test, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Matching Distribution for Test Dataset')
        print(dist_match_test)
        
        x11()
        matching_only_test <- matrix(0, nrow=nrow(actual_pred_table_test), ncol=ncol(actual_pred_table_test))
        diag(matching_only_test) <- diag(actual_pred_table_test)
        matching_only_test <- as.table(matching_only_test)
        matching_df_test <- as.data.frame(matching_only_test)
        colnames(matching_df_test)[1:2] <- c("actual","pred")
        matching_df_test$actual <- paste0("Actual-",matching_df_test$actual)
        matching_df_test$pred <- paste0("Predict-",matching_df_test$pred)
        compare_match_test <- ggplot(matching_df_test, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Match Comparison for Test Dataset')
        print(compare_match_test) 
      }
      else{
        table_df_test <- as.data.frame(actual_pred_table_test) 
        colnames(table_df_test)[1:2] <- c("actual","pred")
        table_df_test$actual <- paste0("Actual-",table_df_test$actual)
        table_df_test$pred <- paste0("Predict-",table_df_test$pred)
        dist_match_test <- ggplot(table_df_test, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Matching Distribution for Test Dataset')
        
        matching_only_test <- matrix(0, nrow=nrow(actual_pred_table_test), ncol=ncol(actual_pred_table_test))
        diag(matching_only_test) <- diag(actual_pred_table_test)
        matching_only_test <- as.table(matching_only_test)
        matching_df_test <- as.data.frame(matching_only_test)
        colnames(matching_df_test)[1:2] <- c("actual","pred")
        matching_df_test$actual <- paste0("Actual-",matching_df_test$actual)
        matching_df_test$pred <- paste0("Predict-",matching_df_test$pred)
        compare_match_test <- ggplot(matching_df_test, aes(fill=pred, y=Freq, x=actual)) + geom_bar(position="stack", stat="identity") + ggtitle('Actual and Prediction Match Comparison for Test Dataset')
      }
      
      bundle_output <- list(layer_exp_list, model, history, train_evaluation, test_evaluation, 
                            actual_pred_table_train, table_df_train, matching_df_train,
                            actual_pred_table_test, table_df_test, matching_df_test,
                            dist_match_train, compare_match_train, 
                            dist_match_test, compare_match_test)
      names(bundle_output) <- c("Layer Command", "Neural Net Model", "Metric History", 
                                "Train Evaluation", "Test Evaluation", "Classification Table for Train dataset",
                                "Table Composition for Train dataset", "Table Actual Predict Matching for Train dataset",
                                "Classification Table for Test dataset", "Table Composition for Test dataset", 
                                "Table Actual Predict Matching for Test dataset",
                                "Train Graphic Overall Matching", "Train Graphic Intersects",
                                "Test Graphic Overall Matching", "Test Graphic Intersects")
    }
    
    return(bundle_output)
  }

#--------------------------------------------------------------------------------------------------------------------------
##################### 25) Bayesian Analysis/ Conditional Probability Analysis  ############################################
#--------------------------------------------------------------------------------------------------------------------------

naive_bayes_model <- function(df, formula, type=1, train_proportion=1, 
                              laplace_value=1, nonparametric_kernel = "gaussian", 
                              nonparametric_bandwidth_value=1, 
                              nonparametric_bandwidth_method="nrd0",
                              nonparametric_parameter_priority="none"){
  writeLines("Reminder! Naive Bayes Model need Numeric Input of Features to Continue!")
  dependent_var <- strsplit(formula, " ")[[1]][1]
  matrix_independent <- as.matrix(df[,which(colnames(df)!=dependent_var)])
  matrix_dependent <- as.matrix(df[,which(colnames(df)==dependent_var)])
  formula <- as.formula(formula)
  train_data <- NULL
  test_data <- NULL
  train_matrix_independent <- NULL
  test_matrix_independent <- NULL
  train_matrix_dependent <- NULL
  test_matrix_dependent <- NULL
  
  if(train_proportion < 1){
    ## set the seed to make your partition reproducible
    set.seed(123)
    smp_size <- floor(train_proportion * nrow(df))
    train_ind <- sample(seq_len(nrow(df)), size = smp_size)
    train_data <- df[train_ind, ]
    test_data <- df[-train_ind, ]
    train_matrix_independent <- as.matrix(train_data[,which(colnames(train_data)!=dependent_var)])
    test_matrix_independent <- as.matrix(test_data[,which(colnames(test_data)!=dependent_var)])
    train_matrix_dependent <- as.matrix(train_data[,which(colnames(train_data)==dependent_var)])
    test_matrix_dependent <- as.matrix(test_data[,which(colnames(test_data)==dependent_var)])
  }
  
  if(type==1){ # Naive Bayes Conditional Posterior Probabilities
    Naive_Bayes_Model=naiveBayes(formula, data=df)
    print(Naive_Bayes_Model)
    if(train_proportion == 1){
      Naive_Bayes_Model=naiveBayes(formula, data=df)
      print(Naive_Bayes_Model)
      NB_Predictions = predict(Naive_Bayes_Model, Titanic_dataset)
      print(table(NB_Predictions,Titanic_dataset$Survived))
      
      #Create Naive Bayes Comparison using MLR Packages with Classification Task and Naive Bayes Learner
      task = makeClassifTask(data = df, target = dependent_var)
      selected_model = makeLearner("classif.naiveBayes")
      NB_mlr = mlr::train(selected_model, task)
      print(NB_mlr$learner.model)
      
      #Predict on the dataset without passing the target feature
      predictions_mlr = as.data.frame(predict(NB_mlr, newdata = df[,which(colnames(df) != dependent_var)]))
      
      ##Confusion matrix to check accuracy
      print(table(predictions_mlr[,1], df[dependent_var]))
    }
    else if(train_proportion < 1){
      Naive_Bayes_Model=naiveBayes(formula, data=train_data)
      print(Naive_Bayes_Model)
      NB_Predictions = predict(Naive_Bayes_Model, test_data)
      print(table(NB_Predictions, test_data[dependent_var]))
      
      #Create Naive Bayes Comparison using MLR Packages with Classification Task and Naive Bayes Learner
      task = makeClassifTask(data = train_data, target = dependent_var)
      selected_model = makeLearner("classif.naiveBayes")
      NB_mlr = mlr::train(selected_model, task)
      print(NB_mlr$learner.model)
      
      #Predict on the dataset without passing the target feature
      predictions_mlr = as.data.frame(predict(NB_mlr, newdata = test_data[,which(colnames(test_data) != dependent_var)]))
      
      ##Confusion matrix to check accuracy
      print(table(predictions_mlr[,1], test_data[dependent_var]))
    }
    return(Naive_Bayes_Model)
  }
  else if(type==2){ # Bernoulli Naive Bayes (Multivariate with Binary Values and Predictor)
    #http://www.learnbymarketing.com/tutorials/naive-bayes-in-r/
    #Laplace Smoothing value = The bigger the laplace smoothing value, the more you are making the models the same
    
    if(train_proportion == 1){
      bnb <- bernoulli_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                   laplace = laplace_value)
      print(summary(bnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(bnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(bnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(bnb))
      return(bnb) 
    }
    else if(train_proportion < 1){
      bnb <- bernoulli_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                   laplace = laplace_value)
      print(summary(bnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(bnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(bnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(bnb))
      return(bnb)
    }
  }
  else if(type==3){
    ### Train the Multinomial Naive Bayes
    if(train_proportion == 1){
      mnb <- multinomial_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                     laplace = laplace_value)
      print(summary(mnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(mnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(mnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Multinomial Naive Bayes Coefficients")
      print(coef(mnb))
      return(mnb) 
    }
    else if(train_proportion < 1){
      mnb <- multinomial_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                     laplace = laplace_value)
      print(summary(mnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(mnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(mnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Multinomial Naive Bayes Coefficients")
      print(coef(mnb))
      return(mnb)
    }
    
  }
  else if(type==4){
    if(train_proportion == 1){
      if(nonparametric_parameter_priority=="none"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent)
        print(summary(nnb))
        writeLines("Prediction By Posterior Probabilities")
        print(head(predict(nnb, newdata = matrix_independent, type = "prob")))
        print(tables(nnb, which = 1))
      }
      else if(nonparametric_parameter_priority=="kernel"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                         kernel=nonparametric_kernel)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="method"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                         bw = nonparametric_bandwidth_method)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="value"){
        nnb <- nonparametric_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                         adjust = nonparametric_bandwidth_value)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      return(nnb) 
    }
    else if(train_proportion < 1){
      if(nonparametric_parameter_priority=="none"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent)
        print(summary(nnb))
        writeLines("Prediction By Posterior Probabilities")
        print(head(predict(nnb, newdata = test_matrix_independent, type = "prob")))
        print(tables(nnb, which = 1))
      }
      else if(nonparametric_parameter_priority=="kernel"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                         kernel=nonparametric_kernel)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="method"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                         bw = nonparametric_bandwidth_method)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      else if(nonparametric_parameter_priority=="value"){
        nnb <- nonparametric_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                         adjust = nonparametric_bandwidth_value)
        print(summary(nnb))
        plot(nnb, 1, prob = "conditional")
      }
      return(nnb) 
    }
  }
  else if(type==5){
    if(train_proportion == 1){
      pnb <- poisson_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                 laplace = laplace_value)
      print(summary(pnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(pnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(pnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(pnb))
      return(pnb) 
    }
    else if(train_proportion < 1){
      pnb <- poisson_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                 laplace = laplace_value)
      print(summary(pnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(pnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(pnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(pnb))
      return(pnb)
    }
  }
  else if(type==6){
    if(train_proportion == 1){
      gnb <- gaussian_naive_bayes(x = matrix_independent, y = matrix_dependent, 
                                  laplace = laplace_value)
      print(summary(gnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(gnb, newdata = matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(gnb, newdata = matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(gnb))
      return(gnb) 
    }
    else if(train_proportion < 1){
      gnb <- gaussian_naive_bayes(x = train_matrix_independent, y = train_matrix_dependent, 
                                  laplace = laplace_value)
      print(summary(gnb))
      
      writeLines("Prediction By Classification")
      print(head(predict(gnb, newdata = test_matrix_independent, type = "class")))
      writeLines("Prediction By Posterior Probabilities")
      print(head(predict(gnb, newdata = test_matrix_independent, type = "prob"))) 
      writeLines("Bernoulli Naive Bayes Coefficients")
      print(coef(gnb))
      return(gnb)
    }
  } 
}

aode_model <- function(df, dependent_col, train_proportion=1){
  library(arules)
  library(gmodels)
  library(AnDE)
  
  train_data <- NULL
  test_data <- NULL
  num_discretize = (length(colnames(df)) + 1)
  aode_df <- data.frame(lapply(df[,which(colnames(df)!=dependent_col)], discretize, breaks=5),
                        target=df[dependent_col])
  
  if(train_proportion < 1){
    ## set the seed to make your partition reproducible
    set.seed(123)
    smp_size <- floor(train_proportion * nrow(df))
    train_ind <- sample(seq_len(nrow(df)), size = smp_size)
    train_data <- aode_df[train_ind, ]
    test_data <- aode_df[-train_ind, ]
  }
  
  if(train_proportion == 1){
    AODE_Model = aode(aode_df)
    predict_aode = predict(AODE_Model, aode_df)
    print(CrossTable(as.numeric(aode_df[dependent_col]), predict_aode))
  }
  else if(train_proportion < 1){
    AODE_Model = aode(train_data)
    predict_aode = predict(AODE_Model, test_data)
    print(CrossTable(as.numeric(test_data[dependent_col]), predict_aode))
  }
  return(AODE_Model)
}

bayesian_network_prelearning <- function(df, highlight_variable){
  library(bnlearn)
  tabu_bn_learning <- tabu(df)
  hc_bn_learning <- hc(df)
  writeLines("Model Structure with Hill Climbing Learning Algorithms")
  plot(hc_bn_learning, highlight = c(highlight_variable, mb(hc_bn_learning, highlight_variable)))
  writeLines("Model Structure with Tabu Learning Algorithms")
  plot(tabu_bn_learning, highlight = c(highlight_variable, mb(tabu_bn_learning, highlight_variable)))
  return(list(hc_bn_learning, tabu_bn_learning))
}

#Use Bayesian Belief Network to classify Probability of Multivariate Factor Variable
#When Model is Created, query is available to overlook certain events with possible conditional, like below
#What is the chance that a non-smoker has a Proteins level less than 3?
#cpquery(fittedbn, event = (Proteins=="<3"), evidence = (Smoking=="no"))
#What is the chance that a non-smoker with pressure greater than 140 has a Proteins level less than 3?
#cpquery(fittedbn, event = (Proteins=="<3"), evidence = (Smoking=="no" & Pressure==">140" ))
#What is the chance that people who have pressure greater than 140 has a Proteins level less than 3?
#cpquery(fittedbn, event = (Pressure==">140"), evidence = (Proteins=="<3" ))

bayesian_belief_network <- function(df, highlight_variable, 
                                    remove_unwanted_relations=c(),
                                    use_learning="hc"){
  library(bnlearn)
  tabu_bn_learning <- tabu(df)
  hc_bn_learning <- hc(df)
  
  writeLines("Model Structure with Hill Climbing Learning Algorithms")
  plot(hc_bn_learning, highlight = c(highlight_variable, mb(hc_bn_learning, highlight_variable)))
  writeLines("Model Structure with Tabu Learning Algorithms")
  plot(tabu_bn_learning, highlight = c(highlight_variable, mb(tabu_bn_learning, highlight_variable)))
  
  if(use_learning == "hc")
  {
    #Remove unwanted relationship if any exists
    if(!is.null(remove_unwanted_relations)){
      for(j in 1:length(remove_unwanted_relations)){
        hc_bn_learning$arcs <- hc_bn_learning$arcs[-remove_unwanted_relations[j],]
      }
    } 
    fittedbn <- bn.fit(hc_bn_learning, data = df)
    print(fittedbn)
  }
  else if(use_learning == "tabu"){
    #Remove unwanted relationship if any exists
    if(!is.null(remove_unwanted_relations)){
      for(j in 1:length(remove_unwanted_relations)){
        tabu_bn_learning$arcs <- tabu_bn_learning$arcs[-remove_unwanted_relations[j],]
      }
    }
    fittedbn <- bn.fit(tabu_bn_learning, data = df)
    print(fittedbn)
  }
  return(fittedbn)
}

model_DAG_builder <- function(edge_df){
  model_DAG_individual <- ""
  model_DAG_relationship <- ""
  model_DAG <- ""
  to_vector <- levels(unique(edge_df$to))
  vertex_vector <- unique(c(unique(levels(edge_df$from)), levels(unique(edge_df$to))))
  predecessor_vector <- rep(0, length(vertex_vector))
  names(predecessor_vector) <- vertex_vector
  for(g in 1:length(to_vector)){
    edge_related <- paste0(as.character(edge_df$from[which(edge_df$to==to_vector[g])]),collapse=":")
    model_DAG_relationship <- paste0(model_DAG_relationship, "[",to_vector[g],"|",edge_related,"]")
  }
  excluded_to <- vertex_vector[-which(vertex_vector %in% to_vector)]
  model_DAG_individual <- paste0("[",excluded_to,"]", collapse="")
  model_DAG <- paste0(model_DAG_individual, model_DAG_relationship)
  return(model_DAG)
}

DAG_Bayesian_Network <- function(df, model_DAG_notation, edge_df, 
                                 network_probability_structure){
  library(bnlearn)
  library(Rgraphviz)
  dag = model2network(model_DAG_notation)
  plot(dag)
  graphviz.plot(dag)
  bn.mle <- bn.fit(dag, data = df, method = "mle")
  bn.mle
  writeLines('Possible VStructs available in Direct Acyclic Graph: ')
  vstructs(dag)
}


#--------------------------------------------------------------------------------------------------------------------------
########################################## 26) Extreme Value Analysis  ####################################################
#--------------------------------------------------------------------------------------------------------------------------

#Resources for Extreme Values Analysis 
#https://rstudio-pubs-static.s3.amazonaws.com/229066_288cf50f4d884dbc80c4cc1716d4c1b0.
#https://cran.r-project.org/web/views/ExtremeValue.
#https://rpubs.com/kanedglsk/238041

#Distribution Function availability for Extreme Value Analysis:
#1) GP: Generalized Pareto
#2) GEV: Generalized extreme value, support for 3 types of EVD dist functions which is: Gumbel, Frechet, Reverse Weibull
#Gumbel (shape = 0, light tail), Frechet (shape > 0, heavy tail), reverse Weibull (shape < 0, bounded upper tail at location - scale/shape)
#3) PP: Poisson Process
#4) Gumbel
#5) Exponential

threshold_range_graph_analysis <- function(vector, range=c(0,1), by=10, xlimit=c(0,1)){
  library(extRemes)
  writeLines("Tips of Selecting Threshold")
  writeLines("The idea is to Select a threshold whereby the graph is linear")
  writeLines("again within uncertainty bounds, as the threshold increases")
  x11()
  threshrange.plot(vector, r = range, nint = by)
  x11()
  mrlplot(vector, xlim = xlimit)
}

extreme_value_time_unit_calc <- function(year_vector, value_vector, unit="year"){
  if(unit=="year"){
    year_range <- range(year_vector)
    year_range_num = year_range[2] - year_range[1] + 1
    dim_damage = dim(value_vector)[1]
    time_unit = dim_damage / year_range_num 
  }
  else if(unit=="day"){
    day_length <- length(value_vector)
    time_unit <- 365/day_length
  }
  return(time_unit)
}

extreme_value_probability <- function(ext_fit_object, value_to_check=c()){
  library(extRemes)
  writeLines(paste0("Extreme Value object has a distribution of ", ext_fit_object$type))
  writeLines("================== Parameter Summary =========================================")
  print(ci(ext_fit_object, type = "parameter"))
  writeLines("================== Distilled Object Representation ===========================")
  print(distill(ext_fit_object))
  b <- distill(ext_fit_object)
  x <- ci(ext_fit_object, type = "parameter")
  writeLines("=================== Most Important Parameter =================================")
  print(b[1:dim(h)[1]])
  writeLines("=================== Extreme Value Probability at given point =================")
  pextRemes(ext_fit_object, value_to_check, lower.tail = FALSE)
}

#type = c("GEV", "GP", "PP", "Gumbel", "Exponential")
#method = c("MLE", "GMLE", "Bayesian", "Lmoments")
#fitD <- fevd(Dam, damage, threshold = 6, type = "GP", time.units = "2.06/year") time unit bisa diatur untuk dataset dngn observasi year yang unique
BM_analysis <- function(list_of_block, distribution_function="Gumbel",
                        plot_type=c("density","rl")){
  maxima <- unlist(lapply(list_of_block, max))
  x11()
  plot(maxima, main = "Maxima Block", type = "l")
  df_fit <- fevd(maxima, type = distribution_function)
  for(h in 1:length(plot_type)){
    if(plot_type[h] == "density"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "rl"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Return Level of ", distribution_function, " distribution for period of years")) 
    }
    else if(plot_type[h] == "primary"){
      x11()
      par(mfrow=c(2,2))
      plot(df_fit, type = plot_type[h], 
           main = paste0("POT Plot Summary with ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "probprob"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Model probabilities of ", distribution_function, " distribution against empirical probabilities")) 
    }
    else if(plot_type[h] == "qq"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical quantiles against model quantiles of ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "qq2"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical quantiles against model quantiles of ", distribution_function, " distribution with confidence bands")) 
    }
    else if(plot_type[h] == "Zplot"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Zplot for determining whether or not the random variable, Zk is independent exponentially distributed with mean 1")) 
    }
    else if(plot_type[h] == "hist"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "density"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "trace"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
  }
  return(df_fit)
}

POT_analysis <- function(data_vector, distribution_function="GEV", 
                         threshold_test=0, plot_type=c("density","rl"),
                         location.fun=~1, scale.fun = ~1, 
                         shape.fun = ~1, use.phi = FALSE, timeunits="days",
                         method="MLE", return_level_years=c(2,10,100)){
  library(extRemes)
  df_fit <- fevd(data_vector, threshold = threshold_test, 
                 type = distribution_function, time.units = timeunits, 
                 method = method, location.fun = location.fun,
                 scale.fun = scale.fun, shape.fun = shape.fun,
                 use.phi = use.phi)
  
  for(h in 1:length(plot_type)){
    if(plot_type[h] == "density"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "rl"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Return Level of ", distribution_function, " distribution for period of years")) 
    }
    else if(plot_type[h] == "primary"){
      x11()
      par(mfrow=c(2,2))
      plot(df_fit, type = plot_type[h], 
           main = paste0("POT Plot Summary with ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "probprob"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Model probabilities of ", distribution_function, " distribution against empirical probabilities")) 
    }
    else if(plot_type[h] == "qq"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical quantiles against model quantiles of ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "qq2"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical quantiles against model quantiles of ", distribution_function, " distribution with confidence bands")) 
    }
    else if(plot_type[h] == "Zplot"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Zplot for determining whether or not the random variable, Zk is independent exponentially distributed with mean 1")) 
    }
    else if(plot_type[h] == "hist"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "density"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
    else if(plot_type[h] == "trace"){
      x11()
      plot(df_fit, type = plot_type[h], 
           main = paste0("Empirical POT exceedances density vs estimated ", distribution_function, " distribution")) 
    }
  }
  writeLines(paste0("Return Level for ", return_level_years, " Years"))
  print(ci(df_fit, return.period=return_level_years))
  return(df_fit)
}


#--------------------------------------------------------------------------------------------------------------------------
########################################## 27) SVM Classifier #############################################################
#--------------------------------------------------------------------------------------------------------------------------
dataset_class_comparison <- function(my_df, class_name, x_column, y_column, 
                                     pct_threshold=0.05, make_svm_conclusion=TRUE){
  library(data.table)
  sequence_checkpoint_pos <- 10^(seq(-7,7,1))
  sequence_checkpoint_neg <- -10^(seq(7,-7,-1))
  min_x_start <- floor(min(my_df[[x_column]]))
  max_x_start <- ceiling(max(my_df[[x_column]]))
  min_y_start <- floor(min(my_df[[y_column]]))
  max_y_start <- ceiling(max(my_df[[y_column]]))
  
  if(min_x_start < 0){
    abs_diff <- abs(min_x_start - sequence_checkpoint_neg)
    min_x_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff))]
    if(min_x_start_suggest > min_x_start){
      min_x_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff)) - 1]
    }
    min_x_start <- min_x_start_suggest
  }
  else if(min_x_start > 0){
    abs_diff <- abs(min_x_start - sequence_checkpoint_pos)
    min_x_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff))]
    if(min_x_start_suggest > min_x_start){
      min_x_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff)) - 1]
    }
    min_x_start <- min_x_start_suggest
  }
  
  if(max_x_start < 0){
    abs_diff <- abs(max_x_start - sequence_checkpoint_neg)
    max_x_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff))]
    if(max_x_start_suggest < max_x_start){
      max_x_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff)) + 1]
    }
    max_x_start <- max_x_start_suggest
  }
  else if(max_x_start > 0){
    abs_diff <- abs(max_x_start - sequence_checkpoint_pos)
    max_x_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff))]
    if(max_x_start_suggest < max_x_start){
      max_x_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff)) + 1]
    }
    max_x_start <- max_x_start_suggest
  }
  
  if(min_y_start < 0){
    abs_diff <- abs(min_y_start - sequence_checkpoint_neg)
    min_y_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff))]
    if(min_y_start_suggest > min_y_start){
      min_y_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff)) - 1]
    }
    min_y_start <- min_y_start_suggest
  }
  else if(min_y_start > 0){
    abs_diff <- abs(min_y_start - sequence_checkpoint_pos)
    min_y_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff))]
    if(min_y_start_suggest > min_y_start){
      min_y_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff)) - 1]
    }
    min_y_start <- min_y_start_suggest
  }
  
  if(max_y_start < 0){
    abs_diff <- abs(max_y_start - sequence_checkpoint_neg)
    max_y_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff))]
    if(max_y_start_suggest < max_y_start){
      max_y_start_suggest <- sequence_checkpoint_neg[which(abs_diff == min(abs_diff)) + 1]
    }
    max_y_start <- max_y_start_suggest
  }
  else if(max_y_start > 0){
    abs_diff <- abs(max_y_start - sequence_checkpoint_pos)
    max_y_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff))]
    if(max_y_start_suggest < max_y_start){
      max_y_start_suggest <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff)) + 1]
    }
    max_y_start <- max_y_start_suggest
  }
  
  subset_df_list <- list()
  subset_seq_x_list <- list()
  subset_seq_y_list <- list()
  difference_x_range <- abs(diff(range(my_df[[x_column]])))
  difference_y_range <- abs(diff(range(my_df[[y_column]])))
  sequence_for_x <- 0
  sequence_for_y <- 0
  
  abs_diff <- abs(difference_x_range - sequence_checkpoint_pos)
  sequence_for_x <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff))]
  digit_minimal <- floor(log10(abs(min(my_df[[x_column]])))) + 1
  digit_maximal <- floor(log10(abs(max(my_df[[x_column]])))) + 1
  maxvalue <- max(my_df[[x_column]])
  if(digit_minimal != digit_maximal && ((maxvalue/10) >= 5)){
    sequence_for_x <- sequence_for_x/10
  }
  else if(digit_minimal != digit_maximal && ((maxvalue/10) >= 1 && (maxvalue/10) < 5)){
    sequence_for_y <- sequence_for_y/20
  }
  if(digit_minimal == 1 && digit_maximal == 1){
    sequence_for_x <- 1
  }
  
  abs_diff <- abs(difference_y_range - sequence_checkpoint_pos)
  sequence_for_y <- sequence_checkpoint_pos[which(abs_diff == min(abs_diff))]
  digit_minimal <- floor(log10(abs(min(my_df[[y_column]])))) + 1
  digit_maximal <- floor(log10(abs(max(my_df[[y_column]])))) + 1
  maxvalue <- max(my_df[[y_column]])
  if(digit_minimal != digit_maximal && ((maxvalue/10) >= 5)){
    sequence_for_y <- sequence_for_y/10
  }
  else if(digit_minimal != digit_maximal && ((maxvalue/10) >= 1 && (maxvalue/10) < 5)){
    sequence_for_y <- sequence_for_y/20
  }
  if(digit_minimal == 1 && digit_maximal == 1){
    sequence_for_y <- 1
  }
  
  unique_class <- unique(my_df[[class_name]])
  unique_class_count <- length(unique_class)
  
  custom_sequence_count <- function(data, column_sequence="", custom_sequence=c(), custom_label=c()){
    vector <- data[[column_sequence]]
    if(length(custom_sequence) > 0 && length(custom_label) > 0){
      if(length(custom_sequence) != length(custom_label)){
        message("Value and Label Parameter length is not equal, please fix parameter!")
        break
      } 
    }
    if(length(custom_label) > 0){
      sequence_countset <- c()
      #lower than minimal custom sequence
      sequence_countset <- c(sequence_countset, length(vector[vector < custom_sequence[1]]))
      names(sequence_countset)[1] <- paste0(column_sequence,"_lower_than_",custom_label[1])
      #in between custom sequence
      for(k in 1:(length(custom_sequence)-1)){
        sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[k] & vector <= custom_sequence[k+1]]))
        names(sequence_countset)[k+1] <- paste0(column_sequence,"_",custom_label[k]," to ",custom_label[k+1])
      }
      #higher than maximal custom sequence
      sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[length(custom_sequence)]]))
      names(sequence_countset)[length(sequence_countset)] <- paste0(column_sequence,"_higher_than_",
                                                                    custom_label[length(custom_label)])
      #add more attributes
      percentage_countset <- round(sequence_countset/sum(sequence_countset),3)
      names(percentage_countset) <- paste0(names(percentage_countset), " (pct)")
      order_countset <- order(percentage_countset, decreasing = TRUE)
      percentage_sort <- sort(percentage_countset, decreasing = TRUE)
      
      interval_description <- ""
      collected_pops <- 0
      idx <- 1
      bounds_before <- 0
      bounds_after <- 0
      while(collected_pops <= 0.95){
        if(collected_pops > 0){
          interval_description <- paste0(interval_description, ", ")
        }
        collected_pops <- collected_pops + percentage_sort[idx]
        percent_collected <- collected_pops * 100
        if(order_countset[idx] == 1){
          if(idx == 1){
            bounds_before <- min(custom_sequence)
            bounds_after <- custom_sequence[1]
          }else if(idx > 1){
            new_before <- min(custom_sequence)
            new_after <- custom_sequence[1]
            bounds_before <- min(c(bounds_before, new_before))
            bounds_after <- max(c(bounds_after, new_after))
          }
        }else if(order_countset[idx] > 1 && order_countset[idx] < length(order_countset)){
          if(idx == 1){
            bounds_before <- custom_sequence[order_countset[idx]-1]
            bounds_after <- custom_sequence[order_countset[idx]] 
          }else if(idx > 1){
            new_before <- custom_sequence[order_countset[idx]-1]
            new_after <- custom_sequence[order_countset[idx]]
            bounds_before <- min(c(bounds_before, new_before))
            bounds_after <- max(c(bounds_after, new_after))
          }
        }else if(order_countset[idx] == length(order_countset[idx])){
          if(idx == 1){
            bounds_before <- custom_sequence[length(custom_sequence)]
            bounds_after <- max(custom_sequence)
          }else if(idx > 1){
            new_before <- custom_sequence[length(custom_sequence)]
            new_after <- max(custom_sequence)
            bounds_before <- min(c(bounds_before, new_before))
            bounds_after <- max(c(bounds_after, new_after))
          }
        }
        interval_description <- paste0(interval_description, percent_collected, "% Population Achieved in the interval Bounds of ",
                                       bounds_before," to ",bounds_after)
        idx <- idx + 1
      }
      interval_description <- paste0(interval_description, ".")
      names(interval_description) <- "interval_desc"
      all_info <- list(c(sequence_countset, percentage_countset), interval_description)
    }
    else if(length(custom_label) == 0){
      sequence_countset <- c()
      #lower than minimal custom sequence
      sequence_countset <- c(sequence_countset, length(vector[vector < custom_sequence[1]]))
      names(sequence_countset)[1] <- paste0(column_sequence,"_lower_than_",custom_sequence[1])
      #in between custom sequence
      for(k in 1:(length(custom_sequence)-1)){
        sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[k] & vector <= custom_sequence[k+1]]))
        names(sequence_countset)[k+1] <- paste0(column_sequence,"_",custom_sequence[k]," to ",custom_sequence[k+1])
      }
      #higher than maximal custom sequence
      sequence_countset <- c(sequence_countset, length(vector[vector >= custom_sequence[length(custom_sequence)]]))
      names(sequence_countset)[length(sequence_countset)] <- paste0(column_sequence,"_higher_than_",
                                                                    custom_sequence[length(custom_sequence)])
      #add more attributes
      percentage_countset <- round(sequence_countset/sum(sequence_countset),3)
      names(percentage_countset) <- paste0(names(percentage_countset), " (pct)")
      order_countset <- order(percentage_countset, decreasing = TRUE)
      percentage_sort <- sort(percentage_countset, decreasing = TRUE)
      
      interval_description <- ""
      collected_pops <- 0
      idx <- 1
      bounds_before <- 0
      bounds_after <- 0
      while(collected_pops <= 0.95){
        if(collected_pops > 0){
          interval_description <- paste0(interval_description, ",")
        }
        collected_pops <- collected_pops + percentage_sort[idx]
        percent_collected <- collected_pops * 100
        if(order_countset[idx] == 1){
          if(idx == 1){
            bounds_before <- min(custom_sequence)
            bounds_after <- custom_sequence[1]
          }else if(idx > 1){
            new_before <- min(custom_sequence)
            new_after <- custom_sequence[1]
            bounds_before <- min(c(bounds_before, new_before))
            bounds_after <- max(c(bounds_after, new_after))
          }
        }else if(order_countset[idx] > 1 && order_countset[idx] < length(order_countset)){
          if(idx == 1){
            bounds_before <- custom_sequence[order_countset[idx]-1]
            bounds_after <- custom_sequence[order_countset[idx]] 
          }else if(idx > 1){
            new_before <- custom_sequence[order_countset[idx]-1]
            new_after <- custom_sequence[order_countset[idx]]
            bounds_before <- min(c(bounds_before, new_before))
            bounds_after <- max(c(bounds_after, new_after))
          }
        }else if(order_countset[idx] == length(order_countset[idx])){
          if(idx == 1){
            bounds_before <- custom_sequence[length(custom_sequence)]
            bounds_after <- max(custom_sequence)
          }else if(idx > 1){
            new_before <- custom_sequence[length(custom_sequence)]
            new_after <- max(custom_sequence)
            bounds_before <- min(c(bounds_before, new_before))
            bounds_after <- max(c(bounds_after, new_after))
          }
        }
        interval_description <- paste0(interval_description, percent_collected, "% Population Achieved in the interval Bounds of ",
                                       bounds_before," to ",bounds_after)
        idx <- idx + 1
      }
      interval_description <- paste0(interval_description, ".")
      names(interval_description) <- "interval_desc"
      all_info <- list(c(sequence_countset, percentage_countset), interval_description)
    }
    return(all_info)
  }
  
  for(a in 1:unique_class_count){
    subset_df <- my_df[which(my_df[[class_name]] == unique_class[a]),]
    writeLines("============================================================================")
    writeLines(paste0('Class = ',unique_class[a]))
    writeLines(paste0("SeqX(",min_x_start,",", max_x_start,",", sequence_for_x, ")"))
    writeLines(paste0("SeqY(",min_y_start,",", max_y_start,",", sequence_for_y, ")"))
    seq_x_result <- custom_sequence_count(subset_df, x_column, custom_sequence=seq(min_x_start, max_x_start, sequence_for_x))
    seq_y_result <- custom_sequence_count(subset_df, y_column, custom_sequence=seq(min_y_start, max_y_start, sequence_for_y))
    pct_x_result <- seq_x_result[[1]][which(seq_x_result[[1]] >= pct_threshold & names(seq_x_result[[1]]) %like% "pct")]
    pct_y_result <- seq_y_result[[1]][which(seq_y_result[[1]] >= pct_threshold & names(seq_y_result[[1]]) %like% "pct")]
    print(pct_x_result)
    print(pct_y_result)
    subset_df_list <- c(subset_df_list, list(subset_df))
    subset_seq_x_list <- c(subset_seq_x_list, list(pct_x_result))
    subset_seq_y_list <- c(subset_seq_y_list, list(pct_y_result))
    writeLines("============================================================================")
  }
  
  if(make_svm_conclusion){
    unlikely_svm <- TRUE
    for(a in 1:(length(subset_seq_x_list)-1)){
      names_sequence1 <- names(subset_seq_x_list[[a]])
      names_sequence2 <- names(subset_seq_x_list[[a+1]])
      if(length(names_sequence1) != length(names_sequence2)){
        if(length(names_sequence1) < length(names_sequence2)){
          names_match <- (names_sequence2 %in% names_sequence1)
          if(all(names_match) == FALSE){
            unlikely_svm = FALSE
          }
        }
        else if(length(names_sequence1) > length(names_sequence2)){
          names_match <- (names_sequence1 %in% names_sequence2)
          if(all(names_match) == FALSE){
            unlikely_svm = FALSE
          }
        }
      }
      else{
        names_match <- (names_sequence1 %in% names_sequence2)
        if(all(names_match) == FALSE){
          unlikely_svm = FALSE
        }
      }
    }
    for(b in 1:(length(subset_seq_y_list)-1)){
      names_sequence1 <- names(subset_seq_y_list[[b]])
      names_sequence2 <- names(subset_seq_y_list[[b+1]])
      if(length(names_sequence1) != length(names_sequence2)){
        if(length(names_sequence1) < length(names_sequence2)){
          names_match <- (names_sequence2 %in% names_sequence1)
          if(all(names_match) == FALSE){
            unlikely_svm = FALSE
          }
        }
        else if(length(names_sequence1) > length(names_sequence2)){
          names_match <- (names_sequence1 %in% names_sequence2)
          if(all(names_match) == FALSE){
            unlikely_svm = FALSE
          }
        }
      }
      else{
        names_match <- (names_sequence1 %in% names_sequence2)
        if(all(names_match) == FALSE){
          unlikely_svm = FALSE
        }
      }
    }
    if(unlikely_svm){
      writeLines(paste0("Conclusion: Since each class within a ",pct_threshold*100,"% threshold having similiar distribution either from ",x_column," and ",y_column))
      writeLines("SVM Unlikely can be modeled")
    }
    else{
      writeLines(paste0("Conclusion: Since each class within a ",pct_threshold*100,"% threshold not having similiar distribution from ",x_column," or ",y_column))
      writeLines("SVM Likely can be modeled")
    }
  }
  return(list(subset_df_list, subset_seq_x_list, subset_seq_y_list))
}

overlapping_density_check <- function(data, dependent_col, x_column, y_column, legend_pos="topright"){
  x11()
  library(ggplot2)
  library(overlapping)
  data_subsets <- data[,c(which(colnames(data) %in% c(dependent_col, x_column, y_column)))]
  expand_factor <- NULL
  
  if(class(data_subsets[[dependent_col]]) == "factor"){
    factor_key <- levels(data_subsets[[dependent_col]])
    data_subsets_color <- as.numeric(data_subsets[[dependent_col]])
    factor_num <- sort(unique(data_subsets_color))
    plot(data_subsets[[x_column]],data_subsets[[y_column]], 
         col = data_subsets_color, pch=18, 
         xlab=x_column, ylab=y_column)
    legend(legend_pos, legend=factor_key, col = factor_num, pch=18) 
    x11()
    x_sym <- sym(x_column)
    y_sym <- sym(y_column)
    dep_sym <- sym(dependent_col)
    data_subsets[[dependent_col]] <- as.character(data_subsets[[dependent_col]])
    density_group <- ggplot(data=data_subsets, aes(x = !! x_sym, y = !! y_sym, fill = !! dep_sym)) +
      geom_area(alpha = .75, position="identity") + 
      ggtitle("Density Among Groups")
    print(density_group)
    if(length(factor_key) > 2){
      expand_factor <- as.data.frame(t(combn(factor_key, 2)))
    }else{
      expand_factor <- data.frame(Var1 = factor_key[1], Var2 = factor_key[2])
    }
  }
  else if(class(data_subsets[[dependent_col]]) == "character"){
    data_subset_level <- as.factor(data_subsets[[dependent_col]])
    factor_key <- levels(data_subset_level)
    data_subset_color <- as.numeric(data_subset_level)
    factor_num <- sort(unique(data_subset_color))
    plot(data_subsets[[x_column]],data_subsets[[y_column]], 
         col = data_subset_color, pch=18, 
         xlab=x_column, ylab=y_column)
    legend(legend_pos, legend=factor_key, col = factor_num, pch=18) 
    x11()
    x_sym <- sym(x_column)
    y_sym <- sym(y_column)
    dep_sym <- sym(dependent_col)
    data_subsets[[dependent_col]] <- as.character(data_subsets[[dependent_col]])
    density_group <- ggplot(data=data_subsets, aes(x = !! x_sym, y = !! y_sym, fill = !! dep_sym)) +
      geom_area(alpha = .75, position="identity") + 
      ggtitle("Density Among Groups")
    print(density_group)
    if(length(factor_key) > 2){
      expand_factor <- as.data.frame(t(combn(factor_key, 2)))
    }else{
      expand_factor <- data.frame(Var1 = factor_key[1], Var2 = factor_key[2])
    }
  }
  else if(class(data_subsets[[dependent_col]]) == "integer"){
    if(min(data_subsets[[dependent_col]]) == 0){
      data_subsets[[dependent_col]] <- data_subsets[[dependent_col]] + 1
      plot(data_subsets[[x_column]],data_subsets[[y_column]], 
           col = data_subsets[[dependent_col]], pch=18, 
           xlab=x_column, ylab=y_column)
      legend(legend_pos, legend=sort(unique(data_subsets[[dependent_col]]-1)), 
             col = seq(1,length(unique(data_subsets[[dependent_col]])),1), pch=18) 
      x11()
      x_sym <- sym(x_column)
      y_sym <- sym(y_column)
      dep_sym <- sym(dependent_col)
      data_subsets[[dependent_col]] <- data_subsets[[dependent_col]] - 1
      data_subsets[[dependent_col]] <- as.character(data_subsets[[dependent_col]])
      density_group <- ggplot(data=data_subsets, aes(x = !! x_sym, y = !! y_sym, fill = !! dep_sym)) +
        geom_area(alpha = .75, position="identity") + 
        ggtitle("Density Among Groups")
      print(density_group)
      factor_key <- as.character(unique(data_subsets[[dependent_col]]))
      if(length(factor_key) > 2){
        expand_factor <- as.data.frame(t(combn(factor_key, 2)))
      }else{
        expand_factor <- data.frame(Var1 = factor_key[1], Var2 = factor_key[2])
      }
    }
    else if(min(data_subsets[[dependent_col]]) < 0){
      diff_0 <- diff(c(min(data_subsets[[dependent_col]]), 0))
      data_subsets[[dependent_col]] <- data_subsets[[dependent_col]] + diff_0 + 1
      plot(data_subsets[[x_column]],data_subsets[[y_column]], 
           col = data_subsets[[dependent_col]], pch=18, 
           xlab=x_column, ylab=y_column)
      legend(legend_pos, legend=sort(unique(data_subsets[[dependent_col]] - diff_0 - 1)), 
             col = seq(1,length(unique(data_subsets[[dependent_col]])),1), pch=18)  
      x11()
      x_sym <- sym(x_column)
      y_sym <- sym(y_column)
      dep_sym <- sym(dependent_col)
      data_subsets[[dependent_col]] <- data_subsets[[dependent_col]] - diff_0 - 1
      data_subsets[[dependent_col]] <- as.character(data_subsets[[dependent_col]])
      density_group <- ggplot(data=data_subsets, aes(x = !! x_sym, y = !! y_sym, fill = !! dep_sym)) +
        geom_area(alpha = .75, position="identity") + 
        ggtitle("Density Among Groups")
      print(density_group)
      factor_key <- as.character(unique(data_subsets[[dependent_col]]))
      if(length(factor_key) > 2){
        expand_factor <- as.data.frame(t(combn(factor_key, 2)))
      }else{
        expand_factor <- data.frame(Var1 = factor_key[1], Var2 = factor_key[2])
      }
    }
    else{
      plot(data_subsets[[x_column]],data_subsets[[y_column]], 
           col = data_subsets[[dependent_col]], pch=18, 
           xlab=x_column, ylab=y_column)
      legend(legend_pos, legend=sort(unique(data_subsets[[dependent_col]])), 
             col = seq(1,length(unique(data_subsets[[dependent_col]])),1), pch=18) 
      x11()
      x_sym <- sym(x_column)
      y_sym <- sym(y_column)
      dep_sym <- sym(dependent_col)
      data_subsets[[dependent_col]] <- as.character(data_subsets[[dependent_col]])
      density_group <- ggplot(data=data_subsets, aes(x = !! x_sym, y = !! y_sym, fill = !! dep_sym)) +
        geom_area(alpha = .75, position="identity") + 
        ggtitle("Density Among Groups")
      print(density_group)
      factor_key <- as.character(unique(data_subsets[[dependent_col]]))
      if(length(factor_key) > 2){
        expand_factor <- as.data.frame(t(combn(factor_key, 2)))
      }else{
        expand_factor <- data.frame(Var1 = factor_key[1], Var2 = factor_key[2])
      }
    }
  }
  
  writeLines("Density Group Variance, Covariance, and Correlation")
  expand_factor$overlap_x <- NULL
  expand_factor$overlap_y <- NULL
  for(g in 1:nrow(expand_factor)){
    subset_dependent1 <- data_subsets[which(data_subsets[[dependent_col]] %in% c(expand_factor[g,1])),]
    subset_dependent2 <- data_subsets[which(data_subsets[[dependent_col]] %in% c(expand_factor[g,2])),]
    density1x <- density(subset_dependent1[[x_column]])$y
    density1y <- density(subset_dependent1[[y_column]])$y
    density2x <- density(subset_dependent2[[x_column]])$y
    density2y <- density(subset_dependent2[[y_column]])$y
    writeLines(paste0("Group ",expand_factor[g,1]," with ", expand_factor[g,2]))
    writeLines(paste0("Variance for variable ",x_column," = ", round(var(density1x, density2x),3)))
    writeLines(paste0("Covariance for variable ",x_column," = ", round(cov(density1x, density2x),3)))
    writeLines(paste0("Correlation for variable ",x_column," = ", round(cor(density1x, density2x),3)))
    x11()
    x <- list(X1=density(subset_dependent1[[x_column]])$y, X2=density(subset_dependent2[[x_column]])$y)
    names(x) <- c(paste0(x_column, " for class ", expand_factor[g,1]),
                  paste0(x_column, " for class ", expand_factor[g,2]))
    out <- overlap(x, plot=TRUE)
    expand_factor$overlap_x[g] <- round(out$OV,3)
    writeLines(paste0("Overlapping Percentages for variable ",x_column," = ", round(out$OV,3)))
    writeLines(paste0("Variance for variable ",y_column," = ", round(var(density1y, density2y),3)))
    writeLines(paste0("Covariance for variable ",y_column," = ", round(cov(density1y, density2y),3)))
    writeLines(paste0("Correlation for variable ",y_column," = ", round(cor(density1y, density2y),3)))
    x11()
    x <- list(X1=density(subset_dependent1[[y_column]])$y, X2=density(subset_dependent2[[y_column]])$y)
    names(x) <- c(paste0(y_column, " for class ", expand_factor[g,1]),
                  paste0(y_column, " for class ", expand_factor[g,2]))
    out <- overlap(x, plot=TRUE)
    expand_factor$overlap_y[g] <- round(out$OV,3)
    writeLines(paste0("Overlapping Percentages for variable ",y_column," = ", round(out$OV,3)))
  }
  return(list(density_group, expand_factor))
}

svm_classify <- function(data, dependent_col, x_column, y_column, 
                         method="svm_e1071", kernel_type="", svm_cost=1){
  
  #http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/course/2015berkeley/sectionSVM.pdf
  writeLines("======= Kernel Type available for SVM (Basic SVM) with e1071 packages ==========")
  writeLines("1) Linear formula = u'*v ")
  writeLines("2) Polynomial formula = (gamma*u'*v + coef0)^degree ")
  writeLines("3) Radial Basis formula = exp(-gamma*|u-v|^2)")
  writeLines("4) Sigmoid formula = tanh(gamma*u'*v + coef0)")
  writeLines("======= Kernel Type available for SVM (Basic SVM) with Kernlab packages =========")
  writeLines("1) Radial basis Kernel (rbfdot)")
  writeLines("2) Polynomial Kernel (polydot)")
  writeLines("3) Linear Kernel (vanilladot)")
  writeLines("4) Hyperbolic Tangent Kernel (tanhdot)")
  writeLines("5) Laplacian Kernel (laplacedot)")
  writeLines("6) Bessel Kernel (besseldot)")
  writeLines("7) ANOVA RBF Kernel (anovadot)")
  writeLines("8) Spline Kernel (rbfdot)")
  writeLines("9) Stringdot Kernel (rbfdot)")
  
  writeLines("======================== Cost Hyperparameter info =================================")
  writeLines('As Stated in: https://stackoverflow.com/questions/30237742/r-ksvm-with-parameter-c')
  writeLines("larger cost will result in a more flexible model with fewer misclassifications. ")
  writeLines("In effect the cost parameter allows you to adjust the bias/variance trade-off.")
  writeLines("The greater the cost parameter, the more variance in the model and the less bias.")
  
  writeLines("Tips for Choosing SVM e1071 or Kernlab")
  writeLines("SVM e1071 is practically often used in Linear Kernel")
  writeLines("while SVM kenlab often used in Nonlinear Kernel")
  
  if(method=="svm_e1071" && kernel_type==""){
    writeLines("Using Default Kernel: Linear")
    kernel_type="linear"
  }
  else if(method=="svm_kernlab" && kernel_type==""){
    writeLines("Using Default Kernel: Radial Basis")
    kernel_type="rbfdot"
  }
  
  writeLines("Mapping the Dependent Variables to Numeric type")
  factor_key <- levels(data[[dependent_col]])
  factor_keynum <- seq(1,length(factor_key), 1)
  data[[dependent_col]] <- as.numeric(data[[dependent_col]])
  
  library(e1071)
  library(kernlab)
  data_subsets <- data[,c(which(colnames(data) %in% c(dependent_col, x_column, y_column)))]
  x_data <- data[,c(which(colnames(data) %in% c(x_column, y_column)))]
  y_data <- data[,c(which(colnames(data) %in% c(dependent_col)))]
  print(head(data_subsets))
  
  writeLines(paste0("Using Kernel = ", kernel_type))
  if(method=="svm_e1071"){
    my_formula <- as.formula(paste0(dependent_col, "~ ."))
    svm_model <- svm(my_formula,
                     data=data_subsets,
                     type="C-classification",
                     kernel=kernel_type,
                     scale=FALSE,
                     cost=svm_cost)
    print(summary(svm_model))
    if(kernel_type=="linear"){
      x11()
      plot(data_subsets[,-which(colnames(data_subsets) %in% c(dependent_col))], pch=19)
      points(data_subsets[svm_model$index, c(1,2)], col="orange", cex=2)
      w <- t(svm_model$coefs) %*% svm_model$SV
      b <- -svm_model$rho
      abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="blue", lty=3) 
    }
    x11()
    plot(data_subsets[[x_column]],data_subsets[[y_column]], 
         col = data_subsets[[dependent_col]], pch=18)
    legend("topleft", factor_key, col = factor_keynum)
    return(svm_model)
  }
  else if(method=="svm_kernlab"){
    kernfit <- ksvm(x_data, y_data, type = "C-svc", 
                    kernel = kernel_type, C = svm_cost, scaled = c())
    plot(kernfit, data = x_data)
    return(kernfit)
  }
}

#--------------------------------------------------------------------------------------------------------------------------
################################## 28) Sparklyr Functional Interfaces  ####################################################
#--------------------------------------------------------------------------------------------------------------------------

spark_config_connect <- function(memory_executor = "4GB",
                                 memory_driver = "4GB",
                                 memory_fraction = 0.9, 
                                 executor_core = 4,
                                 pivot_max_values = 50000,
                                 dynamic_memory_allocation="false",
                                 shell_driver_memory = "16G",
                                 shell_executor_memory = "16G",
                                 yarn_memory_overhead = "1g",
                                 config_yaml=TRUE){
  library(sparklyr)
  library(config)
  library(dplyr)
  conf <- spark_config()   # Load variable with spark_config()
  conf$spark.executor.memory <- memory_executor
  conf$spark.memory.fraction <- memory_fraction
  conf$spark.executor.cores <- executor_core
  conf$spark.dynamicAllocation.enabled <- dynamic_memory_allocation
  conf$spark.sql.pivotMaxValues <- pivot_max_values
  conf$`sparklyr.shell.driver-memory` <- shell_driver_memory
  conf$`sparklyr.shell.executor-memory` <- shell_executor_memory
  conf$`spark.yarn.executor.memoryOverhead` <- yarn_memory_overhead
  Sys.setenv(SPARK_HOME = "C:/Users/User/AppData/Local/spark/spark-3.1.1-bin-hadoop2.7")
  if(config_yaml){
    spark_conn <- spark_connect(master = "local") 
    print(spark_version(sc=spark_conn))
  }else{
    spark_conn <- spark_connect(master = "local", 
                                config = conf)  # Pass the conf variable 
    print(spark_version(sc=spark_conn))
  }
  return(spark_conn)
}

spark_read_data <- function(format="csv", filename=""){
  if(format=="csv"){
    csv <- spark_read_csv(spark_conn, name=filename, path=file.choose())
    writeLines("Spark DataFrame Dimensions")
    x <- c(sdf_nrow(csv), sdf_ncol(csv))
    print(x)
    return(csv)
  }
  else if(format=="json"){
    json <- spark_read_json(spark_conn, name=filename, path=file.choose())
    writeLines("Spark DataFrame Dimensions")
    x <- c(sdf_nrow(json), sdf_ncol(json))
    print(x)
    return(json)
  }
  else if(format=="parquet"){
    parquet <- spark_read_parquet(spark_conn, name=filename, path=file.choose())
    writeLines("Spark DataFrame Dimensions")
    x <- c(sdf_nrow(parquet), sdf_ncol(parquet))
    print(x)
    return(parquet)
  }
}

spark_filter_vector <- function(spark_df_name="", column_name_to_match="", 
                                vector_to_match=c(), log=FALSE, inverse=FALSE){
  string_sql <- ""
  vector_class <- class(vector_to_match)
  if(inverse){
    string_sql <- paste0("SELECT * FROM ", spark_df_name, " WHERE ",column_name_to_match, " NOT IN(")
  }else{
    string_sql <- paste0("SELECT * FROM ", spark_df_name, " WHERE ",column_name_to_match, " IN(")
  }
  for(a in 1:length(vector_to_match)){
    if(log){
      print(paste0("a = ",a))
    }
    if(a < length(vector_to_match)){
      if(vector_class=="character"){
        string_sql <- paste0(string_sql, "'",vector_to_match[a],"',")
      }else{
        string_sql <- paste0(string_sql, vector_to_match[a],",")
      }
    }
    else if(a == length(vector_to_match)){
      if(vector_class=="character"){
        string_sql <- paste0(string_sql, "'",vector_to_match[a],"')")
      }else{
        string_sql <- paste0(string_sql, vector_to_match[a],")")
      }
    }
  }
  sdf_sql(spark_conn, string_sql)
}

spark_crosstab_batches <- function(spark_df_name, total_unique_column, column_batch=9000, 
                                   column_unique_vector=c(), stop_until_batch=-1,
                                   row_crosstab="", column_crosstab="", log=FALSE){
  
  library(rlist)
  library(dplyr)
  grand_cross_table <- NULL
  spark_obj_cross_list <- list()
  spark_filter_vector <- function(spark_df_name="", column_name_to_match="", 
                                  vector_to_match=c(), log=FALSE, inverse=FALSE){
    string_sql <- ""
    if(inverse){
      string_sql <- paste0("SELECT * FROM ", spark_df_name, " WHERE ",column_name_to_match, " NOT IN(")
    }else{
      string_sql <- paste0("SELECT * FROM ", spark_df_name, " WHERE ",column_name_to_match, " IN(")
    }
    for(a in 1:length(vector_to_match)){
      if(log){
        print(paste0("a = ",a))
      }
      if(a < length(vector_to_match)){
        string_sql <- paste0(string_sql, vector_to_match[a],",")
      }
      else if(a == length(vector_to_match)){
        string_sql <- paste0(string_sql, vector_to_match[a],")")
      }
    }
    sdf_sql(spark_conn, string_sql)
  }
  
  batch_loop <- ceiling(total_unique_column / column_batch)
  start <- 0
  end <- 0
  
  for(x in 1:batch_loop){
    if(x==stop_until_batch){
      return(grand_cross_table)
    }
    if(x == 1){
      start <- 1
      end <- column_batch
    }
    else if(x==batch_loop){
      start <- 1 + (column_batch * (x-1))
      end <- total_unique_column
    }
    else{
      start <- 1 + (column_batch * (x-1))
      end <- column_batch * x
    }
    if(log){
      print(paste0("Batch Cross = ", x))
    }
    quote_columns <- enquo(column_crosstab)
    batch_sub1 <- spark_filter_vector(spark_df_name, column_crosstab, column_unique_vector[start:end])
    batch_sub2 <- spark_filter_vector(spark_df_name, column_crosstab, column_unique_vector[start:end],inverse=TRUE)
    batch_sub2 <- batch_sub2 %>% mutate(!!quote_columns := NA)
    batch_sub <- sdf_bind_rows(batch_sub1, batch_sub2)
    if(log){
      print(paste0("Row Target 1 = ",sdf_nrow(batch_sub1)))
      print("Batch 1 DF view")
      print(head(batch_sub1))
      print(paste0("Row Remaining_NULL 2 = ",sdf_nrow(batch_sub2)))
      print("Batch 2 DF view")
      print(head(batch_sub2))
      print(paste0("Row All Bind = ",sdf_nrow(batch_sub)))
      print("Batch Binded DF view")
      print(head(batch_sub))
    }
    movie_cross <- sdf_crosstab(batch_sub, row_crosstab, column_crosstab)
    if(x==1){
      grand_cross_table <- movie_cross
    }
    else{
      grand_cross_table <- sdf_bind_cols(grand_cross_table, movie_cross)
    }
  }
  return(grand_cross_table)
}

spark_read_batch_data <- function(format="csv", filename="", all_file_path=c(),
                                  mutate_label_col="", mutate_label_vector=c(),
                                  remove_dupl_based_by_col=c()){
  spark_df_obj <- NULL
  for(w in 1:length(all_file_path)){
    if(format=="csv"){
      source <- spark_read_csv(spark_conn, name=filename, path=all_file_path[w])
    }
    else if(format=="json"){
      source <- spark_read_json(spark_conn, name=filename, path=all_file_path[w])
    }
    else if(format=="parquet"){
      source <- spark_read_parquet(spark_conn, name=filename, path=all_file_path[w])
    }
    #preprocess before combine rows
    #1) Add Label
    if(length(mutate_label_col) > 0){
      value <- mutate_label_vector[w]
      enquo_label_col <- enquo(mutate_label_col)
      source <- source %>% mutate(!!enquo_label_col := value)
    }
    #2) Column Adjustments
    if(w>1){
      col_source <- sdf_ncol(source)
      col_spark <- sdf_ncol(spark_df_obj)
      print(paste0("============ Starting Batch ",w,"================"))
      print(paste0("Source Column Length", col_source))
      print(paste0("Collected Dataset Column Length", col_spark))
      if(col_source != col_spark){
        if(col_source < col_spark){
          writeLines("Adjusting Columns for Sources Dataset. . ")
          a <- colnames(spark_df_obj)
          b <- colnames(source)
          col_to_add <- a[-which(a %in% b)]
          for(f in 1:length(col_to_add)){
            value_col <- col_to_add[f]
            enquo_col_add <- enquo(value_col)
            source <- source %>% mutate(!!enquo_col_add := NA)
          }
        }
        else if(col_source > col_spark){
          writeLines("Adjusting Columns for Collected Dataset. . ")
          a <- colnames(spark_df_obj)
          b <- colnames(source)
          col_to_add <- b[-which(b %in% a)]
          for(f in 1:length(col_to_add)){
            value_col <- col_to_add[f]
            enquo_col_add <- enquo(value_col)
            spark_df_obj <- spark_df_obj %>% mutate(!!enquo_col_add := NA)
          }
        }
      }
    }
    #3) Duplicate Date Removal
    if(length(remove_dupl_based_by_col) > 0){
      source <- sdf_drop_duplicates(source, remove_dupl_based_by_col)
    }
    writeLines(paste0("Loaded Batch ",w," Dimensions"))
    x <- c(sdf_nrow(source), sdf_ncol(source))
    print(x)
    if(w==1){
      spark_df_obj <- source
    }else{
      spark_df_obj <- sdf_bind_rows(spark_df_obj, source)
    }
  }
  writeLines("Spark DataFrame Dimensions")
  x <- c(sdf_nrow(spark_df_obj), sdf_ncol(spark_df_obj))
  print(x)
  return(spark_df_obj)
}

spark_count_unique_columns <- function(spark_obj){
  uniques_count <- spark_obj %>%
    summarise_all(list(~approx_count_distinct(.))) %>% as.data.frame()
  return(uniques_count)
}

spark_distinct_value_in_column <- function(spark_obj, colname_target){
  sym_col <- sym(colname_target)
  distinct_value <- spark_obj %>% 
    select(!!sym_col) %>% 
    distinct() %>% 
    as.data.frame()
  distinct_value <- distinct_value[[colname_target]]
  print(head(distinct_value))
  return(distinct_value)
}

spark_summary_groupby <- function(spark_obj, groupby_columns, summarise_column, 
                                  arrange_by_metric="", arrange_by_column="", arrange_priority="top", 
                                  summarise_focus="descriptive", convert_df=TRUE){
  
  #https://stackoverflow.com/questions/44121728/programming-with-dplyr-using-string-as-input
  #https://stackoverflow.com/questions/59553613/using-rlang-nse-to-group-by-multiple-variables
  #https://stackoverflow.com/questions/34487641/dplyr-groupby-on-multiple-columns-using-variable-names
  library(rlang)
  enquo_groupby <- NULL
  datatype <- NULL
  enquos_vector <- c()
  method_enquo <- FALSE
  groupby_length <- 0
  if(length(groupby_columns) > 1){
    enquo_groupby <- syms(groupby_columns)
  }else{
    enquo_groupby <- sym(groupby_columns)
  }
  enquo_summarise = sym(summarise_column)
  metrics_name = c("minimum_","first_quantile_","average_","third_quantile_","max_")
  total_metric = c("total_")
  summarise_name = paste0(metrics_name, summarise_column)
  summarise_total_name = paste0(total_metric, summarise_column)
  min_var = summarise_name[1]
  f1q_var = summarise_name[2]
  avg_var = summarise_name[3]
  f3q_var = summarise_name[4]
  max_var = summarise_name[5]
  total_var = summarise_total_name[1]
  count_var = "count_groupby"
  enquo_min = sym(min_var)
  enquo_f1q = sym(f1q_var)
  enquo_avg = sym(avg_var)
  enquo_f3q = sym(f3q_var)
  enquo_max = sym(max_var)
  enquo_count = sym(count_var)
  groupby_result <- NULL
  
  if(summarise_column != ""){
    if(length(groupby_columns) > 1){
      if(summarise_focus == "descriptive"){
        groupby_result <- spark_obj %>%
          group_by(!!!enquo_groupby) %>%
          summarise(!!enquo_min := min(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_f1q := percentile_approx(!!enquo_summarise, 0.25),
                    !!enquo_avg := mean(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_f3q := percentile_approx(!!enquo_summarise, 0.75),
                    !!enquo_max := max(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_count := n())
      }else if(summarise_focus == "total"){
        groupby_result <- spark_obj %>%
          group_by(!!!enquo_groupby) %>%
          summarise(!!enquo_total := sum(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_count := n())
      }
    }else{
      if(summarise_focus == "descriptive"){
        groupby_result <- spark_obj %>%
          group_by(!!enquo_groupby) %>%
          summarise(!!enquo_min := min(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_f1q := percentile_approx(!!enquo_summarise, 0.25),
                    !!enquo_avg := mean(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_f3q := percentile_approx(!!enquo_summarise, 0.75),
                    !!enquo_max := max(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_count := n())
      }else if(summarise_focus == "total"){
        groupby_result <- spark_obj %>%
          group_by(!!enquo_groupby) %>%
          summarise(!!enquo_total := sum(!!enquo_summarise, na.rm=TRUE),
                    !!enquo_count := n())
      }
    }
    if(arrange_by_metric != ""){
      if(arrange_by_metric == "min"){
        if(arrange_priority == "top"){
          groupby_result <- groupby_result %>% arrange(desc(!!enquo_min))
        }else if(arrange_priority == "bottom"){
          groupby_result <- groupby_result %>% arrange(!!enquo_min)
        }
      }else if(arrange_by_metric == "f1q"){
        if(arrange_priority == "top"){
          groupby_result <- groupby_result %>% arrange(desc(!!enquo_f1q))
        }else if(arrange_priority == "bottom"){
          groupby_result <- groupby_result %>% arrange(!!enquo_f1q)
        }
      }else if(arrange_by_metric == "avg"){
        if(arrange_priority == "top"){
          groupby_result <- groupby_result %>% arrange(desc(!!enquo_avg))
        }else if(arrange_priority == "bottom"){
          groupby_result <- groupby_result %>% arrange(!!enquo_avg)
        }
      }else if(arrange_by_metric == "f3q"){
        if(arrange_priority == "top"){
          groupby_result <- groupby_result %>% arrange(desc(!!enquo_f3q))
        }else if(arrange_priority == "bottom"){
          groupby_result <- groupby_result %>% arrange(!!enquo_f3q)
        }
      }else if(arrange_by_metric == "max"){
        if(arrange_priority == "top"){
          groupby_result <- groupby_result %>% arrange(desc(!!enquo_max))
        }else if(arrange_priority == "bottom"){
          groupby_result <- groupby_result %>% arrange(!!enquo_max)
        }
      }else if(arrange_by_metric == "count"){
        if(arrange_priority == "top"){
          groupby_result <- groupby_result %>% arrange(desc(!!enquo_count))
        }else if(arrange_priority == "bottom"){
          groupby_result <- groupby_result %>% arrange(!!enquo_count)
        }
      }
    }else if(arrange_by_column != ""){
      enquo_arrange_col = sym(arrange_by_column)
      if(arrange_priority == "top"){
        groupby_result <- groupby_result %>% arrange(desc(!!enquo_arrange_col))
      }else if(arrange_priority == "bottom"){
        groupby_result <- groupby_result %>% arrange(!!enquo_arrange_col)
      }
    }
    if(convert_df){
      return(as.data.frame(groupby_result))
    }else{
      return(groupby_result)
    }
  }else if(summarise_column == ""){
    groupby_result <- spark_obj %>%
      group_by(!!!enquo_groupby) %>%
      summarise(frequencies := n())
    if(arrange_by_column == ""){
      if(arrange_priority == "top"){
        groupby_result <- groupby_result %>% arrange(desc(frequencies))
      }else if(arrange_priority == "bottom"){
        groupby_result <- groupby_result %>% arrange(frequencies)
      }
    }
    else if(arrange_by_column != ""){
      enquo_arrange_col = sym(arrange_by_column)
      if(arrange_priority == "top"){
        groupby_result <- groupby_result %>% arrange(desc(!!enquo_arrange_col))
      }else if(arrange_priority == "bottom"){
        groupby_result <- groupby_result %>% arrange(!!enquo_arrange_col)
      }
    }
    if(convert_df){
      return(as.data.frame(groupby_result))
    }else{
      return(groupby_result)
    }
  }
}

spark_check_na <- function(spark_obj, view_head=TRUE){
  #https://stackoverflow.com/questions/47432867/how-to-find-colums-having-missing-data-in-sparklyr
  na_list <- spark_obj %>% 
    mutate_all(is.na) %>% 
    mutate_all(as.numeric) %>%
    summarize_all(sum) %>% as.data.frame()
  writeLines("NA Detected in Columns: ")
  print(na_list[which(na_list > 0)])
  na_found <- na_list[which(na_list > 0)]
  if(view_head){
    if(ncol(na_found)==0){
      writeLines("No NA Found on Dataset, Dataset is loaded with full data")
    }else{
      colnames_spark <- colnames(spark_obj)[which(na_list > 0)]
      sym_col <- syms(colnames_spark)
      spark_obj %>%
        select(!!!sym_col) %>% print(n=20) 
    }
  }
  return(na_list)
}

#--------------------------------------------------------------------------------------------------------------------------
################################## 29) Calendar Time Conversion ###########################################################
#--------------------------------------------------------------------------------------------------------------------------
#-------------------- Time Enrichment, Date Converter, and Calendar Converter Utility -------------------------------
#unit: Decade > Year > Month > Day > Hour > Minute > Second > MilliSecond
time_unit_matching_metrics <- function(time, unit="Decade"){
  if(unit=="Decade"){
    writeLines(paste("Equals to", time, "Decade"))
    writeLines(paste("Equals to", time*100, "Year"))
    writeLines(paste("Equals to", time*100*12, "Month"))
    writeLines(paste("Equals to", time*100*365, "Day"))
    writeLines(paste("Equals to", time*100*365*24, "Hour"))
    writeLines(paste("Equals to", time*100*365*24*60, "Minute"))
    writeLines(paste("Equals to", time*100*365*24*60*60, "Second"))
    writeLines(paste("Equals to", time*100*365*24*60*60*1000, "MilliSecond"))
  }
  else if(unit=="Year"){
    writeLines(paste("Equals to", time/100, "Decade"))
    writeLines(paste("Equals to", time, "Year"))
    writeLines(paste("Equals to", time*12, "Month"))
    writeLines(paste("Equals to", time*365, "Day"))
    writeLines(paste("Equals to", time*365*24, "Hour"))
    writeLines(paste("Equals to", time*365*24*60, "Minute"))
    writeLines(paste("Equals to", time*365*24*60*60, "Second"))
    writeLines(paste("Equals to", time*365*24*60*60*1000, "MilliSecond"))
  }
  else if(unit=="Month"){
    writeLines(paste("Equals to", time/100/12, "Decade"))
    writeLines(paste("Equals to", time/12, "Year"))
    writeLines(paste("Equals to", time, "Month"))
    writeLines(paste("Equals to", time*30, "Day"))
    writeLines(paste("Equals to", time*30*24, "Hour"))
    writeLines(paste("Equals to", time*30*24*60, "Minute"))
    writeLines(paste("Equals to", time*30*24*60*60, "Second"))
    writeLines(paste("Equals to", time*30*24*60*60*1000, "MilliSecond"))
  }
  else if(unit=="Day"){
    writeLines(paste("Equals to", time/100/365, "Decade"))
    writeLines(paste("Equals to", time/365, "Year"))
    writeLines(paste("Equals to", time/30, "Month"))
    writeLines(paste("Equals to", time, "Day"))
    writeLines(paste("Equals to", time*24, "Hour"))
    writeLines(paste("Equals to", time*24*60, "Minute"))
    writeLines(paste("Equals to", time*24*60*60, "Second"))
    writeLines(paste("Equals to", time*24*60*60*1000, "MilliSecond"))
  }
  else if(unit=="Hour"){
    writeLines(paste("Equals to", time/100/365/24, "Decade"))
    writeLines(paste("Equals to", time/365/24, "Year"))
    writeLines(paste("Equals to", time/30/24, "Month"))
    writeLines(paste("Equals to", time/24, "Day"))
    writeLines(paste("Equals to", time, "Hour"))
    writeLines(paste("Equals to", time*60, "Minute"))
    writeLines(paste("Equals to", time*60*60, "Second"))
    writeLines(paste("Equals to", time*60*60*1000, "MilliSecond"))
  }
  else if(unit=="Minute"){
    writeLines(paste("Equals to", time/100/365/24/60, "Decade"))
    writeLines(paste("Equals to", time/365/24/60, "Year"))
    writeLines(paste("Equals to", time/30/24/60, "Month"))
    writeLines(paste("Equals to", time/24/60, "Day"))
    writeLines(paste("Equals to", time/60, "Hour"))
    writeLines(paste("Equals to", time, "Minute"))
    writeLines(paste("Equals to", time*60, "Second"))
    writeLines(paste("Equals to", time*60*1000, "MilliSecond"))
  }
  else if(unit=="Second"){
    writeLines(paste("Equals to", time/100/365/24/60/60, "Decade"))
    writeLines(paste("Equals to", time/365/24/60/60, "Year"))
    writeLines(paste("Equals to", time/30/24/60/60, "Month"))
    writeLines(paste("Equals to", time/24/60/60, "Day"))
    writeLines(paste("Equals to", time/60/60, "Hour"))
    writeLines(paste("Equals to", time/60, "Minute"))
    writeLines(paste("Equals to", time, "Second"))
    writeLines(paste("Equals to", time*1000, "MilliSecond"))
  }
  else if(unit=="MilliSecond"){
    writeLines(paste("Equals to", time/100/365/24/60/60/1000, "Decade"))
    writeLines(paste("Equals to", time/365/24/60/60/1000, "Year"))
    writeLines(paste("Equals to", time/30/24/60/60/1000, "Month"))
    writeLines(paste("Equals to", time/24/60/60/1000, "Day"))
    writeLines(paste("Equals to", time/60/60/1000, "Hour"))
    writeLines(paste("Equals to", time/60/1000, "Minute"))
    writeLines(paste("Equals to", time/1000, "Second"))
    writeLines(paste("Equals to", time, "MilliSecond"))
  }
}

date_converter_flex_format <- function(date_vector, format="ymd"){
  library(lubridate)
  if(format=="ymd"){
    date_vector <- ymd(date_vector)
  }
  else if(format=="ydm"){
    date_vector <- ydm(date_vector)
  }
  else if(format=="mdy"){
    date_vector <- mdy(date_vector)
  }
  else if(format=="myd"){
    date_vector <- myd(date_vector)
  }
  else if(format=="dmy"){
    date_vector <- dmy(date_vector)
  }
  else if(format=="dym"){
    date_vector <- dym(date_vector)
  }
}

date_differences <- function(date1, date2){
  library(lubridate)
  date1 <- as.Date(date1)
  date2 <- as.Date(date2)
  years_diff <- round(interval(date1, date2) / years(1),1)
  month_diff <- round(interval(date1, date2) / months(1),1)
  week_diff <- round(interval(date1, date2) / weeks(1),1)
  day_diff <- round(interval(date1, date2) / days(1),1)
  hour_diff <- round(interval(date1, date2) / hours(1),1)
  minute_diff <- round(interval(date1, date2) / minutes(1),1)
  second_diff <- interval(date1, date2) / seconds(1)
  writeLines(paste0("Years Differences Between ",date1," with ",date2," is ",years_diff, " Years"))
  writeLines(paste0("Months Differences Between ",date1," with ",date2," is ",month_diff, " Months"))
  writeLines(paste0("Weeks Differences Between ",date1," with ",date2," is ",week_diff, " Weeks"))
  writeLines(paste0("Days Differences Between ",date1," with ",date2," is ",day_diff, " Days"))
  writeLines(paste0("Hours Differences Between ",date1," with ",date2," is ",hour_diff, " Hours"))
  writeLines(paste0("Minutes Differences Between ",date1," with ",date2," is ",minute_diff, " Minutes"))
  writeLines(paste0("Seconds Differences Between ",date1," with ",date2," is ",second_diff, " Seconds"))
}

date_enrichment_info <- function(df_orig, date_col, date_prefix_colname="", 
                                 combine_colname=FALSE, log=FALSE){
  library(lubridate)
  library(dplyr)
  library(tictoc)
  
  tic()
  date_hour <- hour(df_orig[[date_col]])
  day_name <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
  
  #count date by semester and trimester approach
  sday <- function(date_vec, log_progress=FALSE){
    year_unique <- unique(year(date_vec))
    sday_vector <- rep(0, length(date_vec)) #initialize all 0
    semester_start <- as.Date(paste0(year_unique,"-01-01"))
    semester_breakpoint <- as.Date(paste0(year_unique,"-07-01"))
    
    writeLines("Calculate Semester day. .")
    for(h in 1:length(year_unique)){
      year_filtered_date_vec <- date_vec[which(year(date_vec)==year_unique[h])] 
      #writeLines(paste0("Length Condition 1 ", length(which(year_filtered_date_vec < semester_breakpoint[h]))))
      #writeLines(paste0("Length Condition 2 ", length(abs(as.numeric(difftime(semester_start[h], year_filtered_date_vec[which(year_filtered_date_vec < semester_breakpoint[h])], units="days"))))))
      sday_vector[which(year_filtered_date_vec < semester_breakpoint[h])] <- abs(as.numeric(difftime(semester_start[h], year_filtered_date_vec[which(year_filtered_date_vec < semester_breakpoint[h])], units="days")))
      #writeLines(paste0("Length Condition 1 ", length(which(year_filtered_date_vec >= semester_breakpoint[h]))))
      #writeLines(paste0("Length Condition 2 ", length(abs(as.numeric(difftime(semester_breakpoint[h], year_filtered_date_vec[which(year_filtered_date_vec >= semester_breakpoint[h])], units="days"))))))
      sday_vector[which(year_filtered_date_vec >= semester_breakpoint[h])] <- abs(as.numeric(difftime(semester_breakpoint[h], year_filtered_date_vec[which(year_filtered_date_vec >= semester_breakpoint[h])], units="days")))
    }
    sday_vector <- sday_vector + 1 #(start from day 1)
    return(sday_vector)
  }
  tday <- function(date_vec, log_progress=FALSE){
    year_unique <- unique(year(date_vec))
    tday_vector <- rep(0, length(date_vec)) #initialize all 0
    trimester_start <- as.Date(paste0(year_unique,"-01-01"))
    trimester_1st_breakpoint <- as.Date(paste0(year_unique,"-05-01"))
    trimester_2nd_breakpoint <- as.Date(paste0(year_unique,"-09-01"))
    
    writeLines("Calculate Trimester day. .")
    for(h in 1:length(year_unique)){
      year_filtered_date_vec <- date_vec[which(year(date_vec)==year_unique[h])] 
      tday_vector[which(year_filtered_date_vec < trimester_1st_breakpoint[h])] <- abs(as.numeric(difftime(trimester_start[h], year_filtered_date_vec[which(year_filtered_date_vec < trimester_1st_breakpoint[h])], units="days"))) 
      tday_vector[which(year_filtered_date_vec >= trimester_1st_breakpoint[h] & year_filtered_date_vec < trimester_2nd_breakpoint[h])] <- abs(as.numeric(difftime(trimester_1st_breakpoint[h], year_filtered_date_vec[which(year_filtered_date_vec >= trimester_1st_breakpoint[h] & year_filtered_date_vec < trimester_2nd_breakpoint[h])], units="days"))) 
      tday_vector[which(year_filtered_date_vec >= trimester_2nd_breakpoint[h])] <- abs(as.numeric(difftime(trimester_2nd_breakpoint[h], year_filtered_date_vec[which(year_filtered_date_vec >= trimester_2nd_breakpoint[h])], units="days"))) 
    }
    tday_vector <- tday_vector + 1 #(start from day 1)
    
    return(tday_vector)
  }
  #Make Trimester Label
  trimester <- function(date_vec, log_progress=FALSE){
    year_unique <- unique(year(date_vec))
    trimester_vector <- rep(0, length(date_vec))
    trimester_start <- as.Date(paste0(year_unique,"-01-01"))
    trimester_1st_breakpoint <- as.Date(paste0(year_unique,"-05-01"))
    trimester_2nd_breakpoint <- as.Date(paste0(year_unique,"-09-01"))
    
    writeLines("Mapping Trimester label. .")
    for(h in 1:length(year_unique)){
      year_filtered_date_vec <- date_vec[which(year(date_vec)==year_unique[h])] 
      trimester_vector[which(year_filtered_date_vec < trimester_1st_breakpoint[h])] <- 1 
      trimester_vector[which(year_filtered_date_vec >= trimester_1st_breakpoint[h] & 
                               year_filtered_date_vec < trimester_2nd_breakpoint[h])] <- 2
      trimester_vector[which(year_filtered_date_vec >= trimester_2nd_breakpoint[h])] <- 3
    }
    
    return(trimester_vector)
  }
  
  if(all(date_hour == 0)){ #theres no Hourly attribute in the date
    if(log){
      writeLines("Current Date doesn't have Hour Minute and Seconds Property")
      writeLines("Make Separate Dataframe...")
    }
    df <- data.frame(date=df_orig[[date_col]])
    if(log){
      print(head(df))
      writeLines("1) Extract Day Month and Year to separate values...")
    }
    df$day <- as.numeric(day(df$date))
    df$month <- as.numeric(month(df$date))
    df$year <- as.numeric(year(df$date))
    if(log){
      writeLines("2) Calculate Week Number and Year day values...")
    }
    df$week <- week(df$date)
    df$day_in_a_week <- as.numeric(wday(df$date))
    df$day_name <- day_name[df$day_in_a_week]
    df$weekday_string <- paste0("Week-",df$week,"-Day-",df$day_in_a_week," (",df$day_name,")")
    df$number_day_in_year <- as.numeric(yday(df$date))
    if(log){
      writeLines("3) Calculate Day in Trimester, Quarter and Semester values...")
      writeLines("Calculate Trimester. . .")
      df$date_trimester <- as.numeric(trimester(df$date))
      df$day_in_trimester <- as.numeric(tday(df$date))
      df$day_trimester_string <- paste0("T",df$date_trimester,"-",df$day_in_trimester)
    }
    else{
      df$date_trimester <- as.numeric(trimester(df$date))
      df$day_in_trimester <- as.numeric(tday(df$date))
      df$day_trimester_string <- paste0("T",df$date_trimester,"-",df$day_in_trimester) 
    }
    if(log){
      writeLines("4) Calculate Quarter. .")
    }
    df$date_quarter <- as.numeric(quarter(df$date))
    df$day_in_quarter <- as.numeric(qday(df$date))
    df$day_quarter_string <- paste0("Q",df$date_quarter,"-",df$day_in_quarter)
    if(log){
      writeLines("Calculate Semester. .")
      df$date_semester <- as.numeric(semester(df$date))
      df$day_in_semester <- as.numeric(sday(df$date))
      df$day_semester_string <- paste0("S",df$date_semester,"-",df$day_in_semester)
    }
    else{
      df$date_semester <- as.numeric(semester(df$date))
      df$day_in_semester <- as.numeric(sday(df$date))
      df$day_semester_string <- paste0("S",df$date_semester,"-",df$day_in_semester) 
    }
    if(log){
      writeLines("5) Convert Month to Month name and calculate day limit in that month, also provide wheter a year is a leap or not...") 
    }
    df$month_name <- month.name[as.numeric(month(df$date))]
    df$day_limit_in_month <- as.numeric(days_in_month(df$date))
    df$is_leap_year <- leap_year(df$date)
  }
  else{
    if(log){
      writeLines("Make Separate Dataframe...")
    }
    df <- data.frame(date=df_orig[[date_col]])
    if(log){
      print(head(df))
      writeLines("1) Extract Day Month and Year to separate values...")
    }
    df$day <- as.numeric(day(df$date))
    df$month <- as.numeric(month(df$date))
    df$year <- as.numeric(year(df$date))
    if(log){
      writeLines("2) Calculate Week Number and Year day values...")
    }
    df$hour <- as.numeric(hour(df$date))
    df$minute <- as.numeric(minute(df$date))
    df$second <- as.numeric(second(df$date))
    df$week <- week(df$date)
    df$day_in_a_week <- as.numeric(wday(df$date))
    df$day_name <- day_name[df$day_in_a_week]
    df$weekday_string <- paste0("Week-",df$week,"-Day-",df$day_in_a_week," (",df$day_name,")")
    df$number_day_in_year <- as.numeric(yday(df$date))
    if(log){
      writeLines("3) Calculate Day in Trimester, Quarter and Semester values...")
      writeLines("Calculate Trimester. . .")
      df$date_trimester <- as.numeric(trimester(df$date))
      df$day_in_trimester <- as.numeric(tday(df$date))
      df$day_trimester_string <- paste0("T",df$date_trimester,"-",df$day_in_trimester)
    }
    else{
      df$date_trimester <- as.numeric(trimester(df$date))
      df$day_in_trimester <- as.numeric(tday(df$date))
      df$day_trimester_string <- paste0("T",df$date_trimester,"-",df$day_in_trimester) 
    }
    if(log){
      writeLines("4) Calculate Quarter. .")
    }
    df$date_quarter <- as.numeric(quarter(df$date))
    df$day_in_quarter <- as.numeric(qday(df$date))
    df$day_quarter_string <- paste0("Q",df$date_quarter,"-",df$day_in_quarter)
    if(log){
      writeLines("5) Calculate Semester. .")
      df$date_semester <- as.numeric(semester(df$date))
      df$day_in_semester <- as.numeric(sday(df$date))
      df$day_semester_string <- paste0("S",df$date_semester,"-",df$day_in_semester)
    }
    else{
      df$date_semester <- as.numeric(semester(df$date))
      df$day_in_semester <- as.numeric(sday(df$date))
      df$day_semester_string <- paste0("S",df$date_semester,"-",df$day_in_semester)
    }
    if(log){
      writeLines("6) Convert Month to Month name and calculate day limit in that month, also provide wheter a year is a leap or not...") 
    }
    df$month_name <- month.name[as.numeric(month(df$date))]
    df$day_limit_in_month <- as.numeric(days_in_month(df$date))
    df$is_leap_year <- leap_year(df$date)
    df$day_index <- ifelse(am(df$date), "AM", "PM")
  }
  if(date_prefix_colname != ""){
    colnames(df) <- paste0(date_prefix_colname, "_", colnames(df))
  }
  if(combine_colname){
    df_orig <- cbind(df_orig, df)
    df_orig <- df_orig %>% mutate_if(is.character, as.factor)
    toc()
    return(df_orig)
  }
  else{
    df <- df %>% mutate_if(is.character, as.factor)
    toc()
    return(df)
  }
}

#-Calendar Converter 1)---------------- Chinese Converter Date --------------------
chinese_calendar_converter <- function(date_vector, 
                                       chn_unique_pathfile="C:/Users/User/Desktop/Data Science Journey/Data Science with R/R Automation/Calendar Converter References/Chinese Unique Date.csv"){
  library(lubridate)
  chn_unique_date <- read.csv(chn_unique_pathfile)
  chn_unique_date$Year <- as.character(chn_unique_date$Year)
  chn_unique_date$Date <- as.Date(chn_unique_date$Date, format="%b. %d, %Y")
  chn_unique_date$Date_Year <- year(chn_unique_date$Date) 
  chn_unique_date$Date_Month <- month(chn_unique_date$Date) 
  chn_unique_date$Date_Day <- day(chn_unique_date$Date) 
  #print(str(chn_unique_date))
  
  zodiac_sign <- c("Jia Zi","Yi Chou","Bing Yin","Ding Mao","Wu Chen",
                   "Ji Si","Geng Wu","Xin Wei","Ren Shen","Gui You",
                   "Jia Xu","Yi Hai","Bing Zi","Ding Chou","Wu Yin",
                   "Ji Mao","Geng Chen","Xin Si","Ren Wu","Gui Wei",
                   "Jia Shen","Yi You","Bing Xu","Ding Hai","Wu Zi",
                   "Ji Chou","Geng Yin","Xin Mao","Ren Chen","Gui Si",
                   "Jia Wu","Yi Wei","Bing Shen","Ding You","Wu Xu",
                   "Ji Hai","Geng Zi","Xin Chou","Ren Yin","Gui Mao",
                   "Jia Chen","Yi Si","Bing Wu","Ding Wei","Wu Shen",
                   "Ji You","Geng Xu","Xin Hai","Ren Zi","Gui Chou",
                   "Jia Yin","Yi Mao","Bing Chen","Ding Si","Wu Wu",
                   "Ji Wei","Geng Shen","Xin You","Ren Xu","Gui Hai")
  
  df <- data.frame(date_target=date_vector)
  df$chn_stem <- ""
  df$chn_branch <- ""
  df$chn_zodiac <- ""
  df$full_text_year <- ""
  df$chn_year_start <- ""
  df$chn_year_end <- ""
  class(df$chn_year_start) <- "Date"
  class(df$chn_year_end) <- "Date"
  #print(str(df))
  
  for(h in 1:nrow(df)){
    which_year <- which(chn_unique_date$Date > df$date_target[h])[1]
    df$chn_year_start[h] <- as.Date(chn_unique_date$Date[which_year-1])
    df$chn_year_end[h] <- as.Date(chn_unique_date$Date[which_year] - days(1))
    df$chn_year[h] <- chn_unique_date$Year[which_year-1]
    year_num <- year(df$date_target[h])
    #print(year_num)
    if(year_num %% 10 == 0){
      df$chn_stem[h] <- "Yang"
      df$chn_branch[h] <- "Metal"
    }
    else if(year_num %% 10 == 1){
      df$chn_stem[h] <- "Yin"
      df$chn_branch[h] <- "Metal"
    }
    else if(year_num %% 10 == 2){
      df$chn_stem[h] <- "Yang"
      df$chn_branch[h] <- "Water"
    }
    else if(year_num %% 10 == 3){
      df$chn_stem[h] <- "Yin"
      df$chn_branch[h] <- "Metal"
    }
    else if(year_num %% 10 == 4){
      df$chn_stem[h] <- "Yang"
      df$chn_branch[h] <- "Wood"
    }
    else if(year_num %% 10 == 5){
      df$chn_stem[h] <- "Yin"
      df$chn_branch[h] <- "Wood"
    }
    else if(year_num %% 10 == 6){
      df$chn_stem[h] <- "Yang"
      df$chn_branch[h] <- "Fire"
    }
    else if(year_num %% 10 == 7){
      df$chn_stem[h] <- "Yin"
      df$chn_branch[h] <- "Fire"
    }
    else if(year_num %% 10 == 8){
      df$chn_stem[h] <- "Yang"
      df$chn_branch[h] <- "Earth"
    }
    else if(year_num %% 10 == 9){
      df$chn_stem[h] <- "Yin"
      df$chn_branch[h] <- "Earth"
    }
    which_zodiac_sign <- (((year_num - 1924) %% 60) + 1)
    df$chn_zodiac[h] <- zodiac_sign[which_zodiac_sign]
    df$full_text_year[h] <- paste0("The Year of ", df$chn_branch[h]," ",df$chn_year[h], " or year of ", df$chn_zodiac[h])
  }
  return(df)
}

#-Calendar Converter 2)---------------- Julian Converter Date -----------------------------------
julian_calendar_converter <- function(date_vector){
  #------------------------------------------- Universal Calendar Property ---------------------------------
  universal_weekday_name <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
  
  library(lubridate)
  df <- data.frame(date_target=date_vector)
  julian_day_converter <- function(Y, M, D, convert_from="gregorian", modified=FALSE){
    #references: https://en.wikipedia.org/wiki/Julian_day#:~:text=The%20Julian%20day%20number%20is,(indiction%20cycle)%20%3D%207980%20years
    JDN = 0
    if(convert_from == "gregorian"){
      JDN = (1461 * (Y + 4800 + (M - 14)/12))/4 +(367 * (M - 2 - 12 * ((M - 14)/12)))/12 - (3 * ((Y + 4900 + (M - 14)/12)/100))/4 + D - 32075
    }
    else if(convert_from == "julian"){
      JDN = 367 * Y - (7 * (Y + 5001 + (M - 9)/7))/4 + (275 * M)/9 * D + 1729777
    }
    if(modified){
      JDN = JDN - 2400000.5
    }
    return(JDN)
  }
  df$julian_day <- 0
  df$modified_julian_day <- 0
  df$julian_date <- ""
  class(df$julian_date) <- "Date"
  df$julian_weekday <- ""
  for(a in 1:nrow(df)){
    df$julian_day[a] <- julian_day_converter(year(df$date_target[a]),month(df$date_target[a]), day(df$date_target[a]))
    df$modified_julian_day[a] <- julian_day_converter(year(df$date_target[a]),month(df$date_target[a]), day(df$date_target[a]), modified=TRUE)
    df$julian_date[a] <- df$date_target[a] - days(13)
    df$julian_weekday[a] <- universal_weekday_name[wday(df$date_target[a])]
  }
  return(df)
}

#-Calendar Converter 3)---------------- Hebrew Converter Date ---------------------------------------
hebrew_calendar_converter <- function(date_vector){
  #Generating Days Length Information by other Calendar
  full_seq_date <- seq(as.Date("1800-01-01"), as.Date("2100-01-01"), by="days")
  hebrew_convert_seq_date <- as.OtherDate(full_seq_date, "hebrew")$year
  df_full_seq_date <- data.frame(hebrew_year=hebrew_convert_seq_date)
  heb_count <- df_full_seq_date %>% group_by(hebrew_year) %>% summarise(length=n())
  heb_count <- as.data.frame(heb_count)
  
  #-------------------------------------------- Hebrew Calendar Property ------------------------------------
  hebrew_leap_monthday_name <- c("Tishrei","Heshvan","Kislev","Teveth","Shevat","Adar","Veadar","Nisan","Iyar","Sivan","Tammuz","Av","Elul")
  hebrew_weekday_name <- c("Yom Rishon","Yom Sheni","Yom Shlishi","Yom Revi'i", "Yom Chamishi", "Yom Shishi", "Yom Shabbat")
  
  #First, one must determine whether each year is an ordinary or leap year by its position in the 19-year Metonic cycle. Years 3, 6, 8, 11, 14, 17, and 19 are leap years.
  #Hebrew Calendar count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  df <- data.frame(date_target=date_vector)
  converted_hebrew <- as.OtherDate(df$date_target, "hebrew")
  #print(length(converted_hebrew))
  df$hebrew_year <- converted_hebrew$year
  df$hebrew_monthname <- hebrew_leap_monthday_name[converted_hebrew$month]
  df$hebrew_day <- converted_hebrew$day
  df$hebrew_date <- ""
  class(df$hebrew_date) <- "Date"
  df$hebrew_date <- as.Date(paste0(converted_hebrew$year,"-",converted_hebrew$month,"-",converted_hebrew$day))
  df$hebrew_leap_year <- ifelse(df$hebrew_year %% 19 %in% c(0,3,6,8,11,14,17), TRUE, FALSE)
  df$hebrew_weekday <- hebrew_weekday_name[wday(df$date_target)]
  v <- heb_count[which(heb_count$hebrew_year %in% unique(df$hebrew_year)),]
  names(v)[2] <- "hebrew_year_length"
  df <- merge(df, v, by="hebrew_year")
  df$year_notation <- ifelse(df$hebrew_year_length == 353, "Deficient Year",
                             ifelse(df$hebrew_year_length == 354, "Regular Year",
                                    ifelse(df$hebrew_year_length == 355, "Complete Year",
                                           ifelse(df$hebrew_year_length == 383, "Leap Deficient Year",
                                                  ifelse(df$hebrew_year_length == 384, "Leap Regular Year",
                                                         ifelse(df$hebrew_year_length == 385, "Leap Complete Year", "Incomplete"))))))
  df$notation_description <- ifelse(df$hebrew_year_length == 353, "Cheshvan and Kislev Month has 29 Days, and a year has ordinary 12 months",
                                    ifelse(df$hebrew_year_length == 354, "Cheshvan has 29 Days and Kislev has 30 Days, and a year has ordinary 12 months",
                                           ifelse(df$hebrew_year_length == 355, "Cheshvan and Kislev Month has 30 Days, and a year has ordinary 12 months",
                                                  ifelse(df$hebrew_year_length == 383, "Cheshvan and Kislev Month has 29 Days, and a year has ordinary 13 months, extra Adar month after Shevat",
                                                         ifelse(df$hebrew_year_length == 384, "Cheshvan has 29 Days and Kislev has 30 Days, and a year has ordinary 13 months, extra Adar month after Shevat",
                                                                ifelse(df$hebrew_year_length == 385, "Cheshvan and Kislev Month has 30 Days, and a year has ordinary 13 months, extra Adar month after Shevat", "Incomplete"))))))
  return(df)
}

#-Calendar Converter 4)---------------- Islamic Converter Date --------------------------------------
islamic_calendar_converter <- function(date_vector){
  #Generating Days Length Information by other Calendar
  full_seq_date <- seq(as.Date("1800-01-01"), as.Date("2100-01-01"), by="days")
  islamic_convert_seq_date <- as.OtherDate(full_seq_date, "islamic")$year
  df_full_seq_date <- data.frame(islamic_year=islamic_convert_seq_date)
  islamic_count <- df_full_seq_date %>% group_by(islamic_year) %>% summarise(length=n())
  islamic_count <- as.data.frame(islamic_count)
  
  #------------------------------------------- Islamic Calendar Property ---------------------------------------------
  islamic_monthday_name <- c("Muharram","Safar","Rabi al-Awwal", "Rabi ath-Thani", "Jumada al-Awwal", "Jumada ath-Thaniyah", "Rajab",
                             "Shaban", "Ramadan", "Shawwal", "Du al-Qadah", "Du al-Hijjah")
  islamic_monthday_description <- c("month of the forbidden", "month of the void", "month of the first spring",
                                    "month of the second spring, the last spring", "month of the first of parched land",
                                    "month of the second of parched land, the last of parched land", "month of respect, honour",
                                    "month of the scattered","month of the burning heat", "month of the raised", 
                                    "month of the one of truce/sitting", "month of the one of pilgrimage")
  islamic_weekday_name <- c("al-Ahad","al-Ithnayn","ath-Thulatha","al-Arbi'a","al-Khamis","al-Jumah","as-Sabt")
  
  
  #Islamic Calendar count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  df <- data.frame(date_target=date_vector)
  converted_islamic <- as.OtherDate(df$date_target, "islamic")
  df$islamic_year <- converted_islamic$year
  df$islamic_monthname <- islamic_monthday_name[converted_islamic$month]
  df$islamic_monthname_desc <- islamic_monthday_description[converted_islamic$month]
  df$islamic_day <- converted_islamic$day
  df$islamic_date <- ""
  class(df$islamic_date) <- "Date"
  df$islamic_date <- as.Date(paste0(converted_islamic$year,"-",converted_islamic$month,"-",converted_islamic$day))
  df$islamic_weekday <- islamic_weekday_name[wday(df$date_target)]
  v <- islamic_count[which(islamic_count$islamic_year %in% unique(df$islamic_year)),]
  names(v)[2] <- "islamic_year_length"
  df <- merge(df, v, by="islamic_year")
  return(df)
}

#-Calendar Converter 5)---------------- Persian & Kurdish $ Afghan ------------------------------------
persian_afghan_kurdish_calendar_converter <- function(date_vector){
  #Generating Days Length Information by other Calendar
  full_seq_date <- seq(as.Date("1800-01-01"), as.Date("2100-01-01"), by="days")
  persian_convert_seq_date <- as.OtherDate(full_seq_date, "persian")$year
  df_full_seq_date <- data.frame(persian_year=persian_convert_seq_date)
  persian_count <- df_full_seq_date %>% group_by(persian_year) %>% summarise(length=n())
  persian_count <- as.data.frame(persian_count)
  
  #------------------------------------ Persian & Kurdish & Afghan Calendar Property ---------------------------------------------
  persian_monthday_name <- c("Farvardin","Ordibehesht","Khordad","Tir","Mordad","Shahrivar", 
                             "Mehr", "Aban", "Azar", "Dey", "Bahman", "Esfand")
  persian_monthday_description <- c("Month of Guardian spirits","Month of Best Truth","Month of Perfection",
                                    "Month of Sirius","Month of Immortality", "Month of Desirable Dominion",
                                    "Month of Covenant","Month of Waters", "Month of Fire", "Month of the creator",
                                    "Month of Good Spirit", "Month of Holy Devotion")
  persian_weekday_name <- c("Yekshanbeh", "Doshanbeh", "Seshhanbeh", "Chaharshanbeh", 
                            "Panjshanbeh", "Jomeh", "Shanbeh")
  
  kurdish_monthday_name <- c("Xakelewe","Golan","Jozerdan","Poshper","Gelawej","Xermanan","Rezber",
                             "Gelarezan","Sermawez","Befranbar","Rebendan","Resheme")
  afghan_monthday_name <- c("Hamal","Sawr","Jawza","Saratan","Asad","Sunbula","Mizan","Aqrab",
                            "Qaws","Jaddi","Dalwa","Hout")
  
  #Persian Afghan Kurdish Calendar, count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  df <- data.frame(date_target=date_vector)
  converted_persian <- as.OtherDate(df$date_target, "persian")
  df$persian_year <- converted_persian$year
  df$persian_monthname <- persian_monthday_name[converted_persian$month]
  df$persian_monthname_desc <- persian_monthday_description[converted_persian$month]
  df$kurdish_monthname <- kurdish_monthday_name[converted_persian$month]
  df$afghan_monthname <- afghan_monthday_name[converted_persian$month]
  df$persian_day <- converted_persian$day
  df$persian_date <- ""
  class(df$persian_date) <- "Date"
  df$persian_date <- as.Date(paste0(converted_persian$year,"-",converted_persian$month,"-",converted_persian$day))
  df$persian_weekday <- persian_weekday_name[wday(df$date_target)]
  v <- persian_count[which(persian_count$persian_year %in% unique(df$persian_year)),]
  names(v)[2] <- "persian_year_length"
  df <- merge(df, v, by="persian_year")
  return(df)
}

#-Calendar Converter 6)---------------- Mayan Converter Date ------------------------------------------
mayan_calendar_converter <- function(date_vector){
  #Mayan Calendar count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  
  #devtools::install_github("edgararuiz/gregorian")
  df <- data.frame(date_target=as.Date(date_vector))
  df$mayan_baktun <- 0
  df$mayan_katun <- 0 
  df$mayan_tun <- 0 
  df$mayan_uinal <- 0
  df$mayan_kin <- 0
  df$tzolkin_day <- 0
  df$tzolkin_god <- ""
  df$haab_day <- 0 
  df$haab_month <- ""
  df$mayan_representation <- ""
  
  base_mayan_date <- as.Date("0000-01-01") - days(1136777)
  for(e in 1:nrow(df)){
    #--------------------- 1) Calculate Mayan Long Count -----------------------
    day_diff <- abs(as.numeric(difftime(base_mayan_date, df$date_target[e], units="days"))) 
    baktun_count <- floor(day_diff / 144000)
    baktun_remainder <- day_diff%%144000
    katun_count <- floor(baktun_remainder / 7200)
    katun_remainder <- baktun_remainder%%7200
    tun_count <- floor(katun_remainder / 360)
    tun_remainder <- katun_remainder%%360
    uinal_count <- floor(tun_remainder / 20)
    uinal_remainder <- tun_remainder%%20
    kin_count <- uinal_remainder
    
    df$mayan_baktun[e] <- baktun_count
    df$mayan_katun[e] <- katun_count
    df$mayan_tun[e] <- tun_count 
    df$mayan_uinal[e] <- uinal_count
    df$mayan_kin[e] <- kin_count
    
    full_round_calendar_cycles <- day_diff/18980
    full_round_calendar_kin_remainder <- day_diff%%18980
    
    #-------------- 2) Calcualte Mayan Tzolkin --------------------------
    tzolkin_by_days <- full_round_calendar_kin_remainder/13
    tzolkin_by_days_remainder <- full_round_calendar_kin_remainder%%13
    tzolkin_by_gods <- full_round_calendar_kin_remainder/20
    tzolkin_by_gods_remainder <- full_round_calendar_kin_remainder%%20
    
    #Start of Tzolkin from Maya Calendar is 4 Ahau
    tzolkin_gods_name <- c("Ahau","Imix","Ik","Akbal","Kan","Chicchan","Cimi","Manik","Lamat","Muluc","Oc","Chuen",
                           "Eb","Ben","Ix","Men","Cib","Caban","Etznab","Cauac")
    starter_tzolkin_day <- 4
    tzolkin_days <- 4 + tzolkin_by_days_remainder #--> 0 means first day = 4
    tzolkin_gods <- tzolkin_gods_name[(tzolkin_by_gods_remainder+1)] #--> 0 means first God = Ahau
    
    df$tzolkin_day[e] <- tzolkin_days
    df$tzolkin_god[e] <- tzolkin_gods
    
    #-------------- 3) Calculate Mayan Haab --------------------------
    haab_by_years <- full_round_calendar_kin_remainder/365
    haab_by_years_remainder <- full_round_calendar_kin_remainder%%365
    
    #Start of Haab from Maya Calendar is 8 Kum'Ku
    haab_month_name <- c("Kumk'u","Wayeb'","Pop","Wo'","Sip","Sotz'","Sek","Xul","Yaxk'in",
                         "Mol","Ch'en","Yax","Sak","Keh","Mak","K'ank'in","Muwan","Pax","K'ayab")
    
    if(haab_by_years_remainder >= 0 && haab_by_years_remainder <= 11){
      haab_day <- 8 + haab_by_years_remainder
      haab_month <- haab_month_name[1]
    }
    else if(haab_by_years_remainder >= 12 && haab_by_years_remainder <= 16){
      haab_day <- (haab_by_years_remainder) %% 12
      haab_month <- haab_month_name[2]
    }
    else if(haab_by_years_remainder > 16){
      #search from 0 pop to 7 Kumk'u 
      haab_days <- (haab_by_years_remainder - 17) #substract 17 days not including 8 Kum'Ku to 4 Wayeb (5 unlucky days)
      haab_month_count <- haab_days/20
      haab_day <- haab_days%%20
      haab_month <- haab_month_name[3 + haab_month_count] 
    }
    df$haab_day[e] <- haab_day 
    df$haab_month[e] <- haab_month
    df$mayan_representation[e] <- paste0("Following Date equals to ", baktun_count, " Baktun ", katun_count, " Katun ", 
                                         tun_count, " Tun ", uinal_count, " Uinal ", kin_count, " Kin and", 
                                         " with Tzolkin Calendar = ", tzolkin_days, " ",tzolkin_gods,
                                         " with Haab Calendar = ", haab_day, " ", haab_month)
  }
  return(df)
}

#-Calendar Converter 7)---------------- Indian Converter Date ----------------------------------------
indian_calendar_converter <- function(date_vector){
  #------------------------------------------- Hindi/Indian Calendar Property ---------------------------------------------
  indian_weekday_name <- c("ravivara","somavara","mangalavara","budhavara","brahaspativara","sukravara","sanivara")
  indian_monthday_name <- c("Pausa","Magha","Phalguna","Caitra","Vaisakha",
                            "Jyaistha","Asadha","Sravana","Bhadra","Asvina",
                            "Kartika","Agrahayana")
  
  #Indian Calendar count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  #start from the start of Gregorian Year, 1 January = 11 Pasha
  #New year of Indian starts from the month of Caitra, Month 1 in Indian Year, Month 4 in Gregorian Calendar
  df <- data.frame(date_target=as.Date(date_vector))
  indian_startday <- 11
  indian_month_length_normal <- c(19,30,30,30,31,31,31,31,31,30,30,30,11) #First Month 19 to next month, and last month 11 to initial new year
  indian_month_length_leap <- c(19,30,30,31,31,31,31,31,31,30,30,30,11) #First Month 19 to next month, and last month 11 to initial new year
  df$indian_year <- 0
  df$indian_monthname <- ""
  df$indian_day <- 0
  df$indian_weekday <- ""
  
  for(f in 1:nrow(df)){
    if(leap_year(df$date_target[f])){
      day_in_year <- yday(df$date_target[f]) - 1
      cumsum_indian_month <- cumsum(indian_month_length_leap)
      indian_year <- year(df$date_target[f]) - 80
      indian_month <- which(cumsum_indian_month >= day_in_year)[1]
      indian_day <- 0
      indian_monthname <- ""
      if(indian_month==1){
        indian_day <- indian_startday + day_in_year
      }
      else if(indian_month > 1){
        indian_day <- day_in_year %% cumsum_indian_month[indian_month-1] 
      }
      if(indian_month < 13){
        indian_monthname <- indian_monthday_name[indian_month]
      }
      else if(indian_month == 13){
        indian_monthname <- indian_monthday_name[1]
      }
      if(indian_month >= 4){
        indian_year <- indian_year + 1
      }
      df$indian_year[f] <- indian_year
      df$indian_monthname[f] <- indian_monthname
      df$indian_day[f] <- indian_day
      df$indian_weekday[f] <- indian_weekday_name[wday(df$date_target[f])]
    }
    else{
      day_in_year <- yday(df$date_target[f]) - 1
      cumsum_indian_month <- cumsum(indian_month_length_normal)
      indian_year <- year(df$date_target[f]) - 80
      indian_month <- which(cumsum_indian_month >= day_in_year)[1]
      indian_day <- 0
      indian_monthname <- ""
      if(indian_month==1){
        indian_day <- indian_startday + day_in_year
      }
      else if(indian_month > 1){
        indian_day <- day_in_year %% cumsum_indian_month[indian_month-1] 
      }
      if(indian_month < 13){
        indian_monthname <- indian_monthday_name[indian_month]
      }
      else if(indian_month == 13){
        indian_monthname <- indian_monthday_name[1]
      }
      if(indian_month >= 4){
        indian_year <- indian_year + 1
      }
      df$indian_year[f] <- indian_year
      df$indian_monthname[f] <- indian_monthname
      df$indian_day[f] <- indian_day
      df$indian_weekday[f] <- indian_weekday_name[wday(df$date_target[f])]
    }
  }
  return(df)
}

#-Calendar Converter 8)---------------- French Converter Date ----------------------------------------
french_calendar_converter <- function(date_vector){
  #Generating Days Length Information by other Calendar
  full_seq_date <- seq(as.Date("1800-01-01"), as.Date("2100-01-01"), by="days")
  french_convert_seq_date <- as.OtherDate(full_seq_date, "french")$year
  df_full_seq_date <- data.frame(french_year=french_convert_seq_date)
  french_count <- df_full_seq_date %>% group_by(french_year) %>% summarise(length=n())
  french_count <- as.data.frame(french_count)
  
  #French Calendar count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  df <- data.frame(date_target=date_vector)
  french_base_date <- as.Date("1792-09-22") #this base date is from the start of French Revolutional that created the Calendar
  yearly_day_france <- c(365,365,365,366)
  four_year_cum <- sum(yearly_day_france)
  monthly_name <- c("Vendemiaire","Brumaire","Frimaire","Nivose",
                    "Pluviose","Ventose","Germinal","Floreal",
                    "Prairial","Messidor","Thermidor","Fructidor")
  month_season <- c(rep("Autumn", 3), rep("Winter", 3), rep("Spring", 3), rep("Summer", 3))
  month_meaning <- c("vintage","mist","frost","snowy","rainy","windy","germination",
                     "Flower","Meadow","Harvest","Summer Heat","Fruit")
  complementary_month_name <- c("Sans-culottides")
  week_of_decade <- c("I","II","III")
  day_name_of_decade <- c("du Primidi", "du Duodi", "du Tridi", "du Quartidi",
                          "du Quintidi", "du Sextidi", "du Septidi", "du Octidi",
                          "du Nonidi", "du Decadi")
  complementary_days <- c("de La Vertu", "du Genie", "du Travail", "de l'Opinion", 
                          "des Recompenses", "de la Revolution")
  
  date_diff <- abs(as.numeric(difftime(df$date_target,french_base_date,units="days")))
  
  #------------- Annee (Year Calculation) ----------------------------------
  divide_by_4year <- floor(date_diff/four_year_cum)
  remainder_by_4year <- date_diff%%four_year_cum
  french_Annee <- 1 + (divide_by_4year * 4)
  yearly_day_france_cumsum <- cumsum(yearly_day_france)
  french_day_by_year_num <- ifelse(remainder_by_4year <= yearly_day_france_cumsum[1], remainder_by_4year,
                                   ifelse(remainder_by_4year > yearly_day_france_cumsum[1] & remainder_by_4year <= yearly_day_france_cumsum[2], remainder_by_4year %% yearly_day_france_cumsum[1],
                                          ifelse(remainder_by_4year > yearly_day_france_cumsum[2] & remainder_by_4year <= yearly_day_france_cumsum[3], remainder_by_4year %% yearly_day_france_cumsum[2],
                                                 ifelse(remainder_by_4year > yearly_day_france_cumsum[3] & remainder_by_4year <= yearly_day_france_cumsum[4], remainder_by_4year %% yearly_day_france_cumsum[3], -1))))
  french_Annee_increment <- ifelse(remainder_by_4year <= yearly_day_france_cumsum[1], 0,
                                   ifelse(remainder_by_4year > yearly_day_france_cumsum[1] & remainder_by_4year <= yearly_day_france_cumsum[2], 1,
                                          ifelse(remainder_by_4year > yearly_day_france_cumsum[2] & remainder_by_4year <= yearly_day_france_cumsum[3], 2,
                                                 ifelse(remainder_by_4year > yearly_day_france_cumsum[3] & remainder_by_4year <= yearly_day_france_cumsum[4], 3, -1)))) 
  french_Annee <- french_Annee + french_Annee_increment
  
  #------------- Mois (Month Calculation), Jous and Decade (Day and Week Calculation) ----------------------------------
  french_month_num <- floor(french_day_by_year_num / 30) + 1
  french_day_by_month <- (french_day_by_year_num %% 30)
  french_week_divide <- floor(french_day_by_month / 10) + 1
  french_day_divide <- french_day_by_month %% 10
  french_day_divide[which(french_day_divide==0)] <- 10
  french_mois <- ifelse(french_month_num == 13, 
                        complementary_month_name, 
                        monthly_name[french_month_num])
  french_weekly <- ifelse(french_month_num == 13, 
                          week_of_decade[1], 
                          week_of_decade[french_week_divide])
  french_jous <- ifelse(french_month_num == 13, 
                        complementary_days[french_day_divide], 
                        day_name_of_decade[french_day_divide])
  
  df$french_Annee <- french_Annee
  df$french_mois <- french_mois
  df$french_weekly <- french_weekly
  df$french_jous <- french_jous
  return(df)
}

#-Calendar Converter 9)---------------- Ethiopian Converter Date -------------------------------------
ethiopian_calendar_converter <- function(date_vector){
  #------------------------------------------- Ethiopian Calendar Property --------------------------------------
  ethiopian_month_name <- c("Meskerem", "Tikimt", "Hidar", "Tahsas", "Tir","Yekatit",
                            "Megabit","Meyazya","Ginbot","Sene","Hamle","Nehase",
                            "Pagume")
  #Hebrew Calendar count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  df <- data.frame(date_target=date_vector)
  #Ethiopian Calendar also have 365 days for normal year and 366 days for leap year
  gregorian_start_date <- as.Date("1907-09-12")
  ethiopian_start_date <- as.Date("1900-01-01")
  date_differences <- as.numeric(difftime(gregorian_start_date, ethiopian_start_date))
  df$ethiopian_date <- ""
  class(df$ethiopian_date) <- "Date"
  df$ethiopian_monthname <- ""
  df$ethiopian_weekday <- ""
  df$ethiopian_date <- df$date_target - days(date_differences)
  df$ethiopian_monthname <- ethiopian_month_name[month(df$ethiopian_date)]
  df$ethiopian_weekday <- universal_weekday_name[wday(df$ethiopian_date)]
  return(df)
}

#-Calendar Converter 10)--------------- Badi Converter Date ------------------------------------------
badi_calendar_converter <- function(date_vector){
  #------------------------------------------ Badi/Baha'i Calendar Property ------------------------------------------
  badi_monthname <- c("Baha","Jalal","Jamal","Azamat","Nur","Rahmat","Kalimat","Kamal","Asma",
                      "Izzat","Mashiyyat","llm","Qudrat","Qawi","Masa'il","Sharaf","Sultan",
                      "Mulk","Ayyam-ul Ha", "Ala")
  badi_dayname <-  c("Baha","Jalal","Jamal","Azamat","Nur","Rahmat","Kalimat","Kamal","Asma",
                     "Izzat","Mashiyyat","llm","Qudrat","Qawi","Masa'il","Sharaf","Sultan",
                     "Mulk", "Ala")
  badi_dayname_meaning <- c("Splendour","Glory","Beauty","Grandeur","Light","Mercy","Words",
                            "Perfection","Names","Might","Will","Knowledge","Power","Speech",
                            "Questions","Honour","Soverignty","Dominion","Loftiness")
  badi_year_cycle_name <- c("Alif","Ba","Ab","Dal","Bab","Vav","Abad","Jad","Baha","Hubb","Bahhaj","Javab","Ahad",
                            "Vahhab","Vidad","Badi","Bahi","Abha","Vahid")
  badi_year_cycle_meaning <- c("A","B","Father","D","Gate","V","Eternity","Generosity","Splendour","Love",
                               "Delightful","Answer","Single","Bountiful","Affection","Beginning","Luminous",
                               "Most Luminous","Unity")
  badi_monthname_meaning <- c("Splendour","Glory","Beauty","Grandeur","Light","Mercy","Words",
                              "Perfection","Names","Might","Will","Knowledge","Power","Speech",
                              "Questions","Honour","Soverignty","Dominion","The Days of Ha","Loftiness")
  badi_weekday_name <- c("Jam?","Kam?","Fid?","Id?","Istijl?","Istiql?","Jalal")
  
  #Badi Calendar count a new day exactly at noon 12pm
  date_vector <- date_vector + hours(12)
  df <- data.frame(date_target=as.Date(date_vector))
  df$Badi_Kull_i_Shay <- 0
  df$Badi_Vahid <- 0 
  df$Badi_Vahid_cycle_name <- ""
  df$Badi_era <- 0 
  df$Badi_month_num <- 0
  df$Badi_day_num <- 0
  df$Badi_monthname <- ""
  df$Badi_monthname_meaning <- ""
  df$Badi_dayname <- ""
  df$Badi_dayname_meaning <- ""
  df$Badi_weekday_name <- ""
  
  badi_base_date <- as.Date("1844-03-21")
  full_seq_date <- seq(as.Date("1844-01-01"),as.Date("2201-01-01"), by="days")
  df_full_expand <- data.frame(date_target = full_seq_date,
                               yearly = year(full_seq_date),
                               monthly = month(full_seq_date),
                               daily = day(full_seq_date))
  year_day_count <- df_full_expand %>% group_by(yearly) %>% summarise(day_count = n())
  year_day_count <- data.frame(year_day_count)
  year_day_count$cumsum_day <- cumsum(year_day_count$day_count) 
  diff_days <- abs(as.numeric(difftime(df$date_target, badi_base_date, units="days")))
  
  for(v in 1:length(diff_days)){
    badi_era <- as.numeric(rownames(year_day_count[which(year_day_count$cumsum_day > diff_days[v])[1]-1,]))
    days_remainder <- diff_days[v] - year_day_count$cumsum_day[which(year_day_count$cumsum_day > diff_days[v])[1]-1]
    year_length <- year_day_count$day_count[which(year_day_count$cumsum_day > diff_days[v])[1]]
    intercalary_days <- ifelse(year_length == 365, 4, 5)
    kull_i_shay <- floor(badi_era / 361) + 1
    vahid <- floor(badi_era / 19) + 1
    vahid_cycle_year <- ((badi_era - 1) %% 19) + 1
    badi_month <- ifelse(days_remainder > 342, ifelse((days_remainder - 342) > intercalary_days, 20, 19), 
                         floor(days_remainder / 19) + 1)
    badi_day <- ifelse(days_remainder > 342, ifelse((days_remainder - 342) > intercalary_days, (days_remainder - 342 - intercalary_days), (days_remainder - 342)), 
                       (days_remainder %% 19) + 1)
    badi_day <- (days_remainder %% 19) + 1
    
    df$Badi_Kull_i_Shay[v] <- kull_i_shay
    df$Badi_Vahid[v] <- vahid
    df$Badi_Vahid_cycle_name[v] <- badi_year_cycle_name[vahid_cycle_year]
    df$Badi_era[v] <- badi_era 
    df$Badi_month_num[v] <- badi_month
    df$Badi_day_num[v] <- badi_day
    df$Badi_monthname[v] <- badi_monthname[badi_month]
    df$Badi_monthname_meaning[v] <- badi_monthname_meaning[badi_month]
    df$Badi_dayname[v] <- badi_dayname[badi_day]
    df$Badi_dayname_meaning[v] <- badi_dayname[badi_day]
    df$Badi_weekday_name[v] <- badi_weekday_name[wday(df$date_target[v])]
  }
  return(df)
}

#-Calendar Converter 11)--------------- Balinese Converter Date ----------------------------------
balinese_day_converter <- function(gregorian_date, log=TRUE){
  library(lubridate)
  library(stringr)
  gregorian_date <- as.Date(gregorian_date)
  base_date <- as.Date("2017-08-22")
  year_pointer <- year(base_date - years(78))
  day_pointer <- 30
  month_pointer <- 2
  balinese_saka_monthname <- c("Kasa","Karo","Katiga","Kapat","Kalima","Kaenen","Kapitu",
                               "Kawulu","Kasanga","Kadasa","Jhista","Sadha")
  java_monthname <- c("Suro","Sapar","Mulud","Bakdo Mulud","Jumadil Awal", "Jumadil Akhir",
                      "Rejeb","Ruwah","Poso","Bodho","Apit","Besar")
  available_days_regular <- c(paste0("Day ", seq(1,15,1), " moon rise"), paste0("Day ", seq(1,15,1), " moon fall"))
  available_days_intercalary <- rep("",30)
  for(g in 1:length(available_days_regular)){
    if(g < length(available_days_regular)){
      available_days_intercalary[g] <- paste0(available_days_regular[g], "/", available_days_regular[g+1])
    }
    else if(g == length(available_days_regular)){
      available_days_intercalary[g] <- paste0(available_days_regular[g], "/", available_days_regular[1])
    }
  }
  
  selected_current_date <- available_days_intercalary[day_pointer]
  diff_date <- abs(as.numeric(difftime(gregorian_date, base_date, units="days")))
  mod_diff_date <- diff_date %% 63
  direction_forward <- (as.numeric(difftime(gregorian_date, base_date)) >= 0)
  direction_string <- ifelse(direction_forward, "Forward", "Backward")
  
  diff_date_cycle <- diff_date / 63
  intercalary_days_fixed <- ifelse(diff_date_cycle == floor(diff_date_cycle), TRUE, FALSE)
  n_intercalary_to_day <- floor(diff_date / 63)
  n_day_to_intercalary <- floor(diff_date / 63)
  n_intercalary_to_day <- ifelse(mod_diff_date > 0, 
                                 n_intercalary_to_day+1, 
                                 n_intercalary_to_day)
  n_day_to_day <- diff_date - n_intercalary_to_day - n_day_to_intercalary
  
  step_intercalary_to_day <- ifelse(direction_forward, 
                                    n_intercalary_to_day * 2, 
                                    n_intercalary_to_day*-1)
  step_day_to_intercalary <- ifelse(direction_forward, 
                                    n_day_to_intercalary, 
                                    n_day_to_intercalary*-2)
  step_day_to_day <- ifelse(direction_forward, 
                            n_day_to_day, 
                            n_day_to_day*-1)
  total_step <- step_intercalary_to_day + step_day_to_intercalary + step_day_to_day
  
  #day pointer after step can be negative
  total_step_modulus <- ifelse(direction_forward, 
                               total_step %% 30, 
                               total_step %% -30)
  day_pointer_after_step <- day_pointer + total_step_modulus
  day_pointer_after_step <- ifelse(day_pointer_after_step > 30, 
                                   day_pointer_after_step - 30, 
                                   day_pointer_after_step)
  
  #month pointer after step can be negative
  month_pointer_after_step <- month_pointer + floor(total_step / 30)
  year_pointer <- ifelse(month_pointer_after_step > 12, 
                         year_pointer + (floor(month_pointer_after_step / 12)), 
                         year_pointer)
  month_pointer_after_step <- ifelse(month_pointer_after_step > 12, 
                                     month_pointer_after_step - (ceiling(month_pointer_after_step / 12) * 12), 
                                     month_pointer_after_step)
  year_pointer <- ifelse(month_pointer_after_step <= 0, 
                         year_pointer - (ceiling((abs(month_pointer_after_step)+1) / 12)), 
                         year_pointer)
  month_pointer_after_step <- ifelse(month_pointer_after_step <= 0, 
                                     month_pointer_after_step - (floor((month_pointer_after_step-1) / 12) * 12), 
                                     month_pointer_after_step)
  
  #For Every month of Kadasa (Which is Month 10 of Balinese Calendar theres a change of year), 
  #Balinese Year increased by 1 if forward, and decrased by 1 if backward
  year_pointer <- ifelse(direction_forward, 
                         ifelse(month_pointer_after_step >= 10, year_pointer + 1, year_pointer), 
                         ifelse(month_pointer_after_step < 10, year_pointer - 1, year_pointer))
  
  day_string <- ifelse(day_pointer_after_step < 10, 
                       paste0("0", day_pointer_after_step), 
                       day_pointer_after_step)
  month_string <- ifelse(month_pointer_after_step < 10, 
                         paste0("0", month_pointer_after_step), 
                         month_pointer_after_step)
  balinese_day <- ifelse(intercalary_days_fixed, 
                         available_days_intercalary[day_pointer_after_step], 
                         available_days_regular[day_pointer_after_step])
  balinese_date <- paste0(year_pointer, "-", month_string, "-", day_string)
  cycle_day <- paste0("Day ", 63 - mod_diff_date, " of 63")
  
  balinese_df <- data.frame(original_date = gregorian_date,
                            diff_date = diff_date,
                            balinese_date,
                            balinese_day,
                            balinese_year_num = year_pointer,
                            balinese_month_num = month_pointer_after_step,
                            balinese_day_num = day_pointer_after_step,
                            cycle_day,
                            balinese_monthname = balinese_saka_monthname[month_pointer_after_step],
                            java_monthname = java_monthname[month_pointer_after_step],
                            n_intercalary_to_day,
                            n_day_to_intercalary,
                            n_day_to_day,
                            total_step,
                            direction = direction_string)
  
  if(log){
    print(head(balinese_df))
  }
  return(balinese_df)
}
balinese_pawukon_converter <- function(gregorian_date, log=TRUE, detailed_return=TRUE){
  #Balinese Pawukon Week
  #10th day week
  gregorian_date <- as.Date(gregorian_date)
  balinese_weekday_dasawara_week <- c("Pandita","Pati","Suka","Duka","Sri","Manuh","Manusa","Raja","Dewa","Raksasa")
  balinese_weekday_dasawara_order <- c("Day 1 of 10", "Day 2 of 10", "Day 3 of 10", "Day 4 of 10", "Day 5 of 10",
                                       "Day 6 of 10", "Day 7 of 10", "Day 8 of 10", "Day 9 of 10", "Day 10 of 10")
  balinese_weekday_dasawara_meaning_raw <-  c("Bijaksana","Tegas dan Dinamis","Gembira/Periang","Mudah Tersinggung dan berjiwa seni",
                                              "Feminim dan Halus", "Menurut", "Mempunyai rasa Sosial", "Jiwa Kepemimpinan",
                                              "Berbudi Luhur", "Jiwa keras dan tanpa pertimbangan")
  balinese_weekday_dasawara_meaning <- c("Wise","Bold and Dynamic", "Happy and Cheerful", "Easy to be Offended but Artistic",
                                         "Feminime and Soft", "Obedience", "Social Sense", "Leadership", "Virtuous", 
                                         "Bold Soul and without Consideration")
  df_dasawara <- data.frame(index_data=seq(1,10,1), dasawara=balinese_weekday_dasawara_week)
  
  #9th day week
  balinese_weekday_sangawara_week <- c("Dangu","Jangur","Gigis","Nohan","Ogan","Erangan","Urungan","Tulus","Dadi")
  balinese_weekday_sangawara_order <- c("Day 1 of 9", "Day 2 of 9", "Day 3 of 9", "Day 4 of 9", "Day 5 of 9",
                                        "Day 6 of 9", "Day 7 of 9", "Day 8 of 9", "Day 9 of 9")
  balinese_weekday_sangawara_meaning_raw <- c("Terang dan Gelap", "Jadi dan Batal", "Sederhana", "Gembira", 
                                              "Bingung","Dendam","Batal", "Langsung", "Jadi")
  balinese_weekday_sangawara_meaning <- c("Light and Darkness", "Execute and Halt", "Simple", "Happy",
                                          "Confused", "Hatred", "Halt", "Directly", "Execute")
  df_sangawara <- data.frame(index_data=seq(1,9,1), sangawara=balinese_weekday_sangawara_week)
  
  #8th day week
  balinese_weekday_astawara_week <- c("Sri","Indra","Guru","Yama","Ludra","Brahma","Kala","Uma")
  balinese_weekday_astawara_order <- c("Day 1 of 8", "Day 2 of 8", "Day 3 of 8", "Day 4 of 8", "Day 5 of 8",
                                       "Day 6 of 8", "Day 7 of 8", "Day 8 of 8")
  balinese_weekday_astawara_meaning_raw <- c("Pengatur", "Penggerak", "Penuntun", "Keadilan", "Peleburan",
                                             "Pencipta","Nilai", "Pemelihara/Peneliti")
  balinese_weekday_astawara_meaning <- c("Manager","Mover", "Leader", "Justice", "Smelting",
                                         "Creator","Value", "Protector and Researcher")
  df_astawara <- data.frame(index_data=seq(1,8,1), astawara=balinese_weekday_astawara_week)
  
  #7th day week
  balinese_weekday_saptawara_week <- c("Radite","Soma","Anggara","Buda","Wraspati","Sukra","Saniscara")
  balinese_weekday_saptawara_order <- c("Day 1 of 7", "Day 2 of 7", "Day 3 of 7", "Day 4 of 7", "Day 5 of 7",
                                        "Day 6 of 7", "Day 7 of 7")
  balinese_weekday_saptawara_day_meaning <- c("Sunday", "Monday", "Tuesday","Wednesday","Thursday","Friday","Saturday")
  balinese_weekday_saptawara_meaning_raw <- c("Menanam yang beruas", "Menanam Umbi-Umbian", "Menanam Sayuran daun", 
                                              "Menanam Semua Jenis Bunga", "Menanam yang menghasilkan biji",
                                              "Menanam Buah-Buahan","Menanam Tanaman sebagai Pagar")
  balinese_weekday_saptawara_meaning <- c("Plant the Segmented plant", "Plant the Tubers", "Plant the Vegetable",
                                          "Plant a Flower", "Plant which Produces Seed", "Plant the fruits",
                                          "Plant a Hedgerows")
  df_saptawara <- data.frame(index_data=seq(1,7,1), saptawara=balinese_weekday_saptawara_week)
  
  #6th day week
  balinese_weekday_sadwara_week <- c("Tungleh","Aryang","Urukung","Paniron","Was","Maulu")
  balinese_weekday_sadwara_order <- c("Day 1 of 6", "Day 2 of 6", "Day 3 of 6", "Day 4 of 6", "Day 5 of 6",
                                      "Day 6 of 6")
  balinese_weekday_sadwara_meaning_raw <- c("Tak Kekal", "Kurus", "Punah","Gemuk","Kuat", "Membiak")
  balinese_weekday_sadwara_meaning <- c("Not Eternally", "Skinny", "Extinct", "Fat", "Strong", "Breeding")
  df_sadwara <- data.frame(index_data=seq(1,6,1), sadwara=balinese_weekday_sadwara_week)
  
  #5th day week
  balinese_weekday_pancawara_week <- c("Paing","Pon","Wage","Keliwon","Umanis")
  balinese_weekday_pancawara_order <- c("Day 1 of 5", "Day 2 of 5", "Day 3 of 5", "Day 4 of 5", "Day 5 of 5")
  balinese_weekday_pancawara_meaning_raw <- c("Cipta", "Idep", "Angan","Budhi","Rasa")
  balinese_weekday_pancawara_meaning <- c("Create", "Thought", "Wishful", "Virtuous", "Tasteful")
  df_pancawara <- data.frame(index_data=seq(1,5,1), pancawara=balinese_weekday_pancawara_week)
  
  #4th day week
  balinese_weekday_caturwara_week <- c("Sri","Laba","Jaya","Menala")
  balinese_weekday_caturwara_order <- c("Day 1 of 4", "Day 2 of 4", "Day 3 of 4", "Day 4 of 4")
  balinese_weekday_caturwara_meaning_raw <- c("Kemakmuran", "Keuntungan", "Unggul", "Daerah")
  balinese_weekday_caturwara_meaning <- c("Prosperity", "Profit/Benefit", "Superior", "Area")
  df_caturwara <- data.frame(index_data=seq(1,4,1), caturwara=balinese_weekday_caturwara_week)
  
  #3rd day week
  balinese_weekday_triwara_week <- c("Pasah","Beteng","Kajeng")
  balinese_weekday_triwara_order <- c("Day 1 of 3", "Day 2 of 3", "Day 3 of 3")
  balinese_weekday_triwara_meaning_raw <- c("Tersisih", "Makmur", "Tekanan Tajam")
  balinese_weekday_triwara_meaning <- c("Left Out", "Prosperity", "Pressured")
  df_triwara <- data.frame(index_data=seq(1,3,1), triwara=balinese_weekday_triwara_week)
  
  #2nd day week
  balinese_weekday_dwiwara_week <- c("Menga","Pepet")
  balinese_weekday_dwiwara_order <- c("Day 1 of 2", "Day 2 of 2")
  balinese_weekday_dwiwara_meaning_raw <- c("Terang", "Gelap")
  balinese_weekday_dwiwara_meaning <- c("Light", "Darkness")
  df_dwiwara <- data.frame(index_data=seq(1,2,1), dwiwara=balinese_weekday_dwiwara_week)
  
  #1st day week
  balinese_weekday_ekawara_week <- "Lunga"
  balinese_weekday_ekawara_meaning_raw <- "Tunggal/Kosong"
  balinese_weekday_ekawara_meaning <- "Singularity"
  base_date <- as.Date("2017-08-20")
  
  saptawara_weekname <- c("Sinta","Landep","Ukir","Kulantir","Taulu","Gumbreg","Wariga","Warigadian",
                          "Julungwangi","Sungsang","Dunggulan","Kuningan","Langkir","Medangsia","Pujut",
                          "Pahang","Krulut","Merakih","Tambir","Medangkungan","Matal","Uye","Menail",
                          "Parangbakat","Bala","Ugu","Wayang","Kelawu","Dukut","Watugunung")
  saptawara_urip <- c(5, 4, 3, 7, 8, 6, 9)
  pancawara_urip <- c(9, 7, 4, 8, 5)
  
  if(log){
    writeLines("Calculations is based from: https://wiki2.org/en/Pawukon_calendar")
    writeLines("1) Ekawara week depends on the results of Dasawara Urip")
    writeLines("2) Dwiwara week depends on the results of Dasawara Urip")
    writeLines("3) Triwara week is a regular recurring cycle week")
    writeLines("4) Caturwara week is a recurring cycle week except")
    writeLines("there is a slight week repetition in 71th day (by 2x), since 210 cannot be divisible by 4")
    writeLines("5) Pancawara week is a regular recurring cycle week")
    writeLines("6) Sadwara week is a regular recurring cycle week")
    writeLines("7) Saptawara week is a regular recurring cycle week")
    writeLines("8) Astawara week is a recurring cycle week except")
    writeLines("there is a slight week repetition in 71th day (by 2x), since 210 cannot be divisible by 8")
    writeLines("9) Sangawara week is a recurring cycle week except")
    writeLines("there is a slight week repetition in beginning of year (by 3x), since 210 cannot be divisible by 9")
    writeLines("10) Dasawara week depends on the results of Dasawara Urip") 
    writeLines("")
  }
  
  pawukon_base_date_finder <- function(date_vector){
    library(lubridate)
    base_date <- as.Date("2017-08-20")
    difftime <- as.numeric(difftime(date_vector, base_date, units="days"))
    pawukon_cycle_diff <- floor(difftime / 210)
    base_date_vec <- rep(NA, length(date_vector))
    class(base_date_vec) <- "Date"
    for(v in 1:length(date_vector)){
      base_date_vec[v] <- base_date + days(210 * pawukon_cycle_diff[v]) 
    }
    return(base_date_vec)
  }
  
  base_date_pawukon <- pawukon_base_date_finder(gregorian_date)
  
  #+1 for counting the base date also from starting the pawukon calculation
  diff_date <- ceiling(abs(as.numeric(difftime(gregorian_date, base_date_pawukon, units="days")))) 
  if(log) writeLines(paste0("Date Vector have Difference of ",diff_date, " Days from Base Date"))
  diff_date <- diff_date + 1
  
  saptawara_weekname_result <- saptawara_weekname[ceiling(diff_date / 7)]
  saptawara_weekname_which <- which(saptawara_weekname == saptawara_weekname_result)
  
  urip_1_index <- diff_date %% 7
  urip_1_index <- ifelse(urip_1_index == 0, 7, urip_1_index)
  urip_1 <- saptawara_urip[urip_1_index]
  
  urip_2_index <- diff_date %% 5
  urip_2_index <- ifelse(urip_2_index == 0, 5, urip_2_index)
  urip_2 <- pancawara_urip[urip_2_index]
  
  urip_result <- urip_1 + urip_2 + 1
  urip_result <- ifelse(urip_result > 10, urip_result-10, urip_result)
  
  #1,2,3 week
  ekawara_result <- ifelse(urip_result %% 2 == 0, "Luang", "Not a Day of one-week")
  ekawara_meaning_raw <- ifelse(urip_result %% 2 == 0, balinese_weekday_ekawara_meaning_raw, "-")
  ekawara_meaning <- ifelse(urip_result %% 2 == 0, balinese_weekday_ekawara_meaning, "-")
  dwiwara_result <- ifelse(urip_result %% 2 == 0, "Pepet", "Menga")
  dwiwara_which <- df_dwiwara[match(dwiwara_result, df_dwiwara$dwiwara),]$index_data
  dwiwara_order <- balinese_weekday_dwiwara_order[dwiwara_which]
  dwiwara_meaning_raw <- balinese_weekday_dwiwara_meaning_raw[dwiwara_which]
  dwiwara_meaning <- balinese_weekday_dwiwara_meaning[dwiwara_which]
  
  triwara_index <- diff_date %% 3
  triwara_index <- ifelse(triwara_index == 0, 3, triwara_index)
  triwara_result <- balinese_weekday_triwara_week[triwara_index]
  triwara_which <- df_triwara[match(triwara_result, df_triwara$triwara),]$index_data
  triwara_order <- balinese_weekday_triwara_order[triwara_which]
  triwara_meaning_raw <- balinese_weekday_triwara_meaning_raw[triwara_which]
  triwara_meaning <- balinese_weekday_triwara_meaning[triwara_which]
  
  #4,8 week
  caturwara_result <- ""
  astawara_result <- ""
  
  caturwara_index <- ifelse(diff_date < 71, diff_date %% 4, 
                            ifelse(diff_date >= 71 & diff_date <=73, 71%%4, 
                                   (diff_date-2) %% 4))
  astawara_index <- ifelse(diff_date < 71, diff_date %% 8, 
                           ifelse(diff_date >= 71 & diff_date <=73, 71%%8, 
                                  (diff_date-2) %% 8))
  caturwara_index <- ifelse(caturwara_index==0, 4, caturwara_index)
  astawara_index <- ifelse(astawara_index==0, 8, astawara_index)
  
  astawara_result <- balinese_weekday_astawara_week[astawara_index]
  caturwara_result <- balinese_weekday_caturwara_week[caturwara_index]
  
  caturwara_which <- df_caturwara[match(caturwara_result, df_caturwara$caturwara),]$index_data
  caturwara_order <- balinese_weekday_caturwara_order[caturwara_which]
  astawara_which <- df_astawara[match(astawara_result, df_astawara$astawara),]$index_data
  astawara_order <- balinese_weekday_astawara_order[astawara_which]
  caturwara_meaning_raw <- balinese_weekday_caturwara_meaning_raw[caturwara_which]
  astawara_meaning_raw <- balinese_weekday_astawara_meaning_raw[astawara_which]
  caturwara_meaning <- balinese_weekday_caturwara_meaning[caturwara_which]
  astawara_meaning <- balinese_weekday_astawara_meaning[astawara_which]
  
  #5,6,7 week
  pancawara_index <- diff_date %% 5
  pancawara_index <- ifelse(pancawara_index == 0, 5, pancawara_index)
  pancawara_result <- balinese_weekday_pancawara_week[pancawara_index]
  sadwara_index <- diff_date %% 6
  sadwara_index <- ifelse(sadwara_index == 0, 6, sadwara_index)
  sadwara_result <- balinese_weekday_sadwara_week[sadwara_index]
  saptawara_index <- diff_date %% 7
  saptawara_index <- ifelse(saptawara_index == 0, 7, saptawara_index)
  saptawara_result <- balinese_weekday_saptawara_week[saptawara_index]
  
  pancawara_which <- df_pancawara[match(pancawara_result, df_pancawara$pancawara),]$index_data
  pancawara_order <- balinese_weekday_pancawara_order[pancawara_which]
  pancawara_meaning_raw <- balinese_weekday_pancawara_meaning_raw[pancawara_which]
  pancawara_meaning <- balinese_weekday_pancawara_meaning[pancawara_which]
  sadwara_which <- df_sadwara[match(sadwara_result, df_sadwara$sadwara),]$index_data
  sadwara_order <- balinese_weekday_sadwara_order[sadwara_which]
  sadwara_meaning_raw <- balinese_weekday_sadwara_meaning_raw[sadwara_which]
  sadwara_meaning <- balinese_weekday_sadwara_meaning[sadwara_which]
  saptawara_which <- df_saptawara[match(saptawara_result, df_saptawara$saptawara),]$index_data
  saptawara_order <- balinese_weekday_saptawara_order[saptawara_which]
  saptawara_meaning_raw <- balinese_weekday_saptawara_meaning_raw[saptawara_which]
  saptawara_meaning <- balinese_weekday_saptawara_meaning[saptawara_which]
  
  #9 week
  sangawara_result <- ""
  sangawara_index <- ifelse(diff_date >= 1 & diff_date <=4, 1 %% 9, (diff_date-3) %% 9)
  sangawara_index <- ifelse(sangawara_index == 0, 9, sangawara_index)
  sangawara_result <- balinese_weekday_sangawara_week[sangawara_index]                       
  sangawara_which <- df_sangawara[match(sangawara_result, df_sangawara$sangawara),]$index_data
  sangawara_order <- balinese_weekday_sangawara_order[sangawara_which]
  sangawara_meaning_raw <- balinese_weekday_sangawara_meaning_raw[sangawara_which]
  sangawara_meaning <- balinese_weekday_sangawara_meaning[sangawara_which]
  
  #10 week
  dasawara_result <- balinese_weekday_dasawara_week[urip_result]
  dasawara_which <- df_dasawara[match(dasawara_result, df_dasawara$dasawara),]$index_data
  dasawara_order <- balinese_weekday_dasawara_order[dasawara_which]
  dasawara_meaning_raw <- balinese_weekday_dasawara_meaning_raw[dasawara_which]
  dasawara_meaning <- balinese_weekday_dasawara_meaning[dasawara_which]
  
  if(log){
    writeLines("Gregorian Date")
    print(gregorian_date)
    writeLines("Urip")
    print(urip_result)
    writeLines("Ekawara property")
    print(ekawara_result)
    print(ekawara_meaning_raw)
    print(ekawara_meaning)
    writeLines("Dwiwara property")
    print(dwiwara_result)
    print(dwiwara_meaning_raw)
    print(dwiwara_meaning)
    print(dwiwara_order)
    writeLines("Triwara property")
    print(triwara_result)
    print(triwara_meaning_raw)
    print(triwara_meaning)
    print(triwara_order)
    writeLines("Caturwara property")
    print(caturwara_result)
    print(caturwara_meaning_raw)
    print(caturwara_meaning)
    print(caturwara_order)
    writeLines("Pancawara property")
    print(pancawara_result)
    print(pancawara_meaning_raw)
    print(pancawara_meaning)
    print(pancawara_order)
    writeLines("Sadwara property")
    print(sadwara_result)
    print(sadwara_meaning_raw)
    print(sadwara_meaning)
    print(sadwara_order)
    writeLines("Saptawara property")
    print(saptawara_result)
    print(saptawara_meaning_raw)
    print(saptawara_meaning)
    print(saptawara_order)
    writeLines("Astawara property")
    print(astawara_result)
    print(astawara_meaning_raw)
    print(astawara_meaning)
    print(astawara_order)
    writeLines("Sangawara property")
    print(sangawara_result)
    print(sangawara_meaning_raw)
    print(sangawara_meaning)
    print(sangawara_order)
    writeLines("Dasawara property")
    print(dasawara_result)
    print(dasawara_meaning_raw)
    print(dasawara_meaning)
    print(dasawara_order)
  }
  
  if(detailed_return){
    pawukon_df <- data.frame(date=gregorian_date, urip = urip_result,
                             ekawara_result, ekawara_meaning_raw, ekawara_meaning,
                             dwiwara_result, dwiwara_order, dwiwara_meaning_raw, dwiwara_meaning,
                             triwara_result, triwara_order, triwara_meaning_raw, triwara_meaning,
                             caturwara_result, caturwara_order, caturwara_meaning_raw, caturwara_meaning,
                             pancawara_result, pancawara_order, pancawara_meaning_raw, pancawara_meaning,
                             sadwara_result, sadwara_order, sadwara_meaning_raw, sadwara_meaning,
                             saptawara_result, saptawara_order, saptawara_meaning_raw, saptawara_meaning,
                             astawara_result, astawara_order, astawara_meaning_raw, astawara_meaning,
                             sangawara_result, sangawara_order, sangawara_meaning_raw, sangawara_meaning,
                             dasawara_result, dasawara_order, dasawara_meaning_raw, dasawara_meaning)
    if(log){
      print(head(pawukon_df))
    }
    return(pawukon_df) 
  }else{
    pawukon_df <- data.frame(date=gregorian_date, ekawara_result, dwiwara_result, triwara_result, 
                             caturwara_result, pancawara_result, sadwara_result, saptawara_result, 
                             astawara_result, sangawara_result, dasawara_result)
    if(log){
      print(head(pawukon_df))
    }
    return(pawukon_df)
  }
}

#-Calendar Converter 12)--------------- Javanese Converter Date ----------------------------------
javanese_pasaran_converter <- function(gregorian_date, log=TRUE){
}

#--------------------------------------------------------------------------------------------------------------------------
################################### 30) Cross Tabulation Method ###########################################################
#--------------------------------------------------------------------------------------------------------------------------
#---- A) Crosstab by Dr Paul Williamson ---------------------------------------------
#This Crosstab feature is for analysis purpose of insights, final data type is list not matrix
#Sources: http://rstudio-pubs-static.s3.amazonaws.com/6975_c4943349b6174f448104a5513fed59a9.
#Code Source: http://pcwww.liv.ac.uk/~william/R/crosstab.r

#-------------- Helper Function From Dr Paul Williamson ------------------------
print.crosstab <- function(x,dec.places=x$dec.places,
                           subtotals=x$subtotals,...) {
  
  ###################################################################################
  #                                                                                 #
  # Function created by Dr Paul Williamson, Dept. of Geography and Planning,        #
  # School of Environmental Sciences, University of Liverpool, UK.                  #
  #                                                                                 #
  # Adapted from the function print.ctab() in the catspec packge.                   #
  #                                                                                 #
  # Version: 12th July 2013                                                         #
  #                                                                                 #
  # Designed to provide optimal viewing of the output from crosstab()               #
  #                                                                                 #
  ###################################################################################
  
  row.vars <- x$row.vars
  col.vars <- x$col.vars
  n.row.vars <- length(row.vars)
  n.col.vars <- length(col.vars)
  n.vars <- n.row.vars + n.col.vars
  
  if (length(x$type)>1) {
    z<-length(names(dimnames(x$crosstab)))
    if (x$style=="long") {
      row.vars<-c(row.vars,z) 
    } else {
      col.vars<-c(z,col.vars)
    }
  }
  
  if (n.vars==1) {
    if (length(x$type)==1) {
      tmp <- data.frame(round(x$crosstab,x$dec.places))
      colnames(tmp)[2] <- ifelse(x$type=="frequency","Count","%")
      print(tmp,row.names=FALSE)
    } else {
      print(round(x$crosstab,x$dec.places))
    }
  }
  
  #If table has only 2 dimensions, or subtotals required for >2 dimensional table,
  #print table using ftable() on x$crosstab
  if ((n.vars == 2) | ((subtotals==TRUE) & (n.vars>2))) {
    tbl <- ftable(x$crosstab,row.vars=row.vars,col.vars=col.vars)
    if (!all(as.integer(tbl)==as.numeric(tbl))) tbl <- round(tbl,dec.places)
    print(tbl,...)
  }
  
  #If subtotals NOT required AND > 2 dimensions, print table using write.table() on x$crosstab.nosub
  if ((subtotals==FALSE) & (n.vars>2))  {
    
    t1 <- x$crosstab.nosub
    
    #Convert numbers to required decimal places, right aligned
    width <- max( nchar(t1[1,]), nchar(t1[2,]), 7 )
    dec.places <- x$dec.places
    number.format <- paste("%",width,".",dec.places,"f",sep="")
    t1[3:nrow(t1),((n.row.vars+1):ncol(t1))] <- sprintf(number.format,as.numeric(t1[3:nrow(t1),((n.row.vars+1):ncol(t1))]))
    
    #Adjust column variable label to same width as numbers, left aligned, padding with trailing spaces as required
    col.var.format <- paste("%-",width,"s",sep="")
    t1[1,(n.row.vars+1):ncol(t1)] <- sprintf(col.var.format,t1[1,(n.row.vars+1):ncol(t1)])
    #Adjust column category labels to same width as numbers, right aligned, padding with preceding spaces as required
    col.cat.format <- paste("%",width,"s",sep="")
    t1[2,(n.row.vars+1):ncol(t1)] <- sprintf(col.cat.format,t1[2,(n.row.vars+1):ncol(t1)])
    
    #Adjust row labels so that each column is of fixed width, using trailing spaces as required
    for (i in 1:n.row.vars) {
      width <- max(nchar(t1[,i])) + 2
      row.lab.format <- paste("%-",width,"s",sep="")
      t1[,i] <- sprintf(row.lab.format,t1[,i])
    }
    
    write.table(t1,quote=FALSE,col.names=FALSE,row.names=FALSE)
    
  }
  
}

crosstab <- function (..., dec.places = NULL,
                      type = NULL,
                      style = "wide",
                      row.vars = NULL,
                      col.vars = NULL,
                      percentages = TRUE, 
                      addmargins = TRUE,
                      subtotals=TRUE){
  
  #Declare function used to convert frequency counts into relevant type of proportion or percentage
  mk.pcnt.tbl <- function(tbl, type) {
    a <- length(row.vars)
    b <- length(col.vars)
    mrgn <- switch(type, column.pct = c(row.vars[-a], col.vars), 
                   row.pct = c(row.vars, col.vars[-b]),
                   joint.pct = c(row.vars[-a], col.vars[-b]),
                   total.pct = NULL)
    tbl <- prop.table(tbl, mrgn)
    if (percentages) {
      tbl <- tbl * 100
    }
    tbl
  }
  
  #Find no. of vars (all; row; col) for use in subsequent code
  n.row.vars <- length(row.vars)
  n.col.vars <- length(col.vars)
  n.vars <- n.row.vars + n.col.vars
  
  
  #Check to make sure all user-supplied arguments have valid values
  stopifnot(as.integer(dec.places) == dec.places, dec.places > -1)
  #type: see next section of code
  stopifnot(is.character(style))    
  stopifnot(is.logical(percentages))
  stopifnot(is.logical(addmargins))
  stopifnot(is.logical(subtotals))
  stopifnot(n.vars>=1)
  
  #Convert supplied table type(s) into full text string (e.g. "f" becomes "frequency")
  #If invalid type supplied, failed match gives user automatic error message
  types <- NULL
  choices <- c("frequency", "row.pct", "column.pct", "joint.pct", "total.pct")
  for (tp in type) types <- c(types, match.arg(tp, choices))
  type <- types
  
  #If no type supplied, default to 'frequency + total' for univariate tables and to
  #'frequency' for multi-dimenstional tables
  
  #For univariate table....
  if (n.vars == 1) {
    if (is.null(type)) {
      # default = freq count + total.pct  
      type <- c("frequency", "total.pct")
      #row.vars <- 1
    } else {
      #and any requests for row / col / joint.pct must be changed into requests for 'total.pct'
      type <- ifelse(type == "frequency", "frequency", "total.pct")
    }
    #For multivariate tables...
  } else if (is.null(type)) {
    # default = frequency count  
    type <- "frequency"
  }
  
  
  
  #Check for integrity of requested analysis and adjust values of function arguments as required
  
  if ((addmargins==FALSE) & (subtotals==FALSE)) {
    warning("WARNING: Request to suppress subtotals (subtotals=FALSE) ignored because no margins requested (addmargins=FALSE)")
    subtotals <- TRUE
  }
  
  if ((n.vars>1) & (length(type)>1) & (addmargins==TRUE)) {
    warning("WARNING: Only row totals added when more than one table type requested")
    #Code lower down selecting type of margin implements this...
  }
  
  if ((length(type)>1) & (subtotals==FALSE)) { 
    warning("WARNING: Can only request supply one table type if requesting suppression of subtotals; suppression of subtotals not executed")
    subtotals <- TRUE
  }
  
  if ((length(type)==1) & (subtotals==FALSE)) {
    choices <- c("frequency", "row.pct", "column.pct", "joint.pct", "total.pct")
    tp <- match.arg(type, choices)
    if (tp %in% c("row.pct","column.pct","joint.pct")) {
      warning("WARNING: subtotals can only be suppressed for tables of type 'frequency' or 'total.pct'")
      subtotals<- TRUE
    }
  }
  
  if ((n.vars > 2) & (n.col.vars>1) & (subtotals==FALSE)) 
    warning("WARNING: suppression of subtotals assumes only 1 col var; table flattened accordingly")
  
  
  if ( (subtotals==FALSE) & (n.vars>2) )  {
    #If subtotals not required AND total table vars > 2
    #Reassign all but last col.var as row vars
    #[because, for simplicity, crosstabs assumes removal of subtotals uses tables with only ONE col var]
    #N.B. Subtotals only present in tables with > 2 cross-classified vars...
    if (length(col.vars)>1) {
      row.vars <- c(row.vars,col.vars[-length(col.vars)])
      col.vars <- col.vars[length(col.vars)]
      n.row.vars <- length(row.vars)
      n.col.vars <- 1
    }
  }
  
  #If dec.places not set by user, set to 2 unlesss only one table of type frequency requested,
  #in which case set to 0.  [Leaves user with possibility of having frequency tables with > 0 dp]
  if (is.null(dec.places)) {
    if ((length(type)==1) & (type[1]=="frequency")) {
      dec.places <- 0
    } else {
      dec.places <-2
    }
  }
  
  #Take the original input data, whatever form originally supplied in,
  #convert into table format using requested row and col vars, and save as 'tbl'
  
  args <- list(...)    
  
  if (length(args) > 1) {
    if (!all(sapply(args, is.factor))) 
      stop("If more than one argument is passed then all must be factors")
    tbl <- table(...)
  }
  else {
    if (is.factor(...)) {
      tbl <- table(...)
    }
    else if (is.table(...)) {
      tbl <- eval(...)
    }
    else if (is.data.frame(...)) {
      #tbl <- table(...)
      if (is.null(row.vars) && is.null(col.vars)) {
        tbl <- table(...)
      }
      else {
        var.names <- c(row.vars,col.vars)
        A <- (...)
        tbl <- table(A[var.names])
        if(length(var.names==1)) names(dimnames(tbl)) <- var.names
        #[table() only autocompletes dimnames for multivariate crosstabs of dataframes]
      }
    }
    else if (class(...) == "ftable") {
      tbl <- eval(...)
      if (is.null(row.vars) && is.null(col.vars)) {
        row.vars <- names(attr(tbl, "row.vars"))
        col.vars <- names(attr(tbl, "col.vars"))
      }
      tbl <- as.table(tbl)
    }
    else if (class(...) == "ctab") {
      tbl <- eval(...)
      if (is.null(row.vars) && is.null(col.vars)) {
        row.vars <- tbl$row.vars
        col.vars <- tbl$col.vars
      }
      for (opt in c("dec.places", "type", "style", "percentages", 
                    "addmargins", "subtotals")) if (is.null(get(opt))) 
                      assign(opt, eval(parse(text = paste("tbl$", opt, 
                                                          sep = ""))))
      tbl <- tbl$table
    }
    else {
      stop("first argument must be either factors or a table object")
    }
  }
  
  #Convert supplied table style into full text string (e.g. "l" becomes "long")
  style <- match.arg(style, c("long", "wide"))
  
  #Extract row and col names to be used in creating 'tbl' from supplied input data
  nms <- names(dimnames(tbl))
  z <- length(nms)
  if (!is.null(row.vars) && !is.numeric(row.vars)) {
    row.vars <- order(match(nms, row.vars), na.last = NA)
  }
  if (!is.null(col.vars) && !is.numeric(col.vars)) {
    col.vars <- order(match(nms, col.vars), na.last = NA)
  }
  if (!is.null(row.vars) && is.null(col.vars)) {
    col.vars <- (1:z)[-row.vars]
  }
  if (!is.null(col.vars) && is.null(row.vars)) {
    row.vars <- (1:z)[-col.vars]
  }
  if (is.null(row.vars) && is.null(col.vars)) {
    col.vars <- z
    row.vars <- (1:z)[-col.vars]
  }
  
  #Take the original input data, converted into table format using supplied row and col vars (tbl)
  #and create a second version (crosstab) which stores results as percentages if a percentage table type is requested.
  if (type[1] == "frequency") 
    crosstab <- tbl
  else 
    crosstab <- mk.pcnt.tbl(tbl, type[1])
  
  
  #If multiple table types requested, create and add these to 
  if (length(type) > 1) {
    tbldat <- as.data.frame.table(crosstab)
    z <- length(names(tbldat)) + 1
    tbldat[z] <- 1
    pcntlab <- type
    pcntlab[match("frequency", type)] <- "Count"
    pcntlab[match("row.pct", type)] <- "Row %"
    pcntlab[match("column.pct", type)] <- "Column %"
    pcntlab[match("joint.pct", type)] <- "Joint %"
    pcntlab[match("total.pct", type)] <- "Total %"
    for (i in 2:length(type)) {
      if (type[i] == "frequency") 
        crosstab <- tbl
      else crosstab <- mk.pcnt.tbl(tbl, type[i])
      crosstab <- as.data.frame.table(crosstab)
      crosstab[z] <- i
      tbldat <- rbind(tbldat, crosstab)
    }
    tbldat[[z]] <- as.factor(tbldat[[z]])
    levels(tbldat[[z]]) <- pcntlab
    crosstab <- xtabs(Freq ~ ., data = tbldat)
    names(dimnames(crosstab))[z - 1] <- ""
  }
  
  
  #Add margins if required, adding only those margins appropriate to user request
  if (addmargins==TRUE) {
    
    vars <- c(row.vars,col.vars)
    
    if (length(type)==1) {
      if (type=="row.pct") 
      { crosstab <- addmargins(crosstab,margin=c(vars[n.vars]))
      tbl <- addmargins(tbl,margin=c(vars[n.vars]))
      }
      else 
      { if (type=="column.pct") 
      { crosstab <- addmargins(crosstab,margin=c(vars[n.row.vars]))
      tbl <- addmargins(tbl,margin=c(vars[n.row.vars]))
      }
        else 
        { if (type=="joint.pct") 
        { crosstab <- addmargins(crosstab,margin=c(vars[(n.row.vars)],vars[n.vars])) 
        tbl <- addmargins(tbl,margin=c(vars[(n.row.vars)],vars[n.vars])) 
        }
          else #must be total.pct OR frequency
          { crosstab <- addmargins(crosstab)
          tbl <- addmargins(tbl)
          }
        }
      } 
    }
    #If more than one table type requested, only adding row totals makes any sense...
    if (length(type)>1) {
      crosstab <- addmargins(crosstab,margin=c(vars[n.vars]))
      tbl <- addmargins(tbl,margin=c(vars[n.vars]))
    }
  }  
  
  #If subtotals not required, and total vars > 2, create dataframe version of table, with relevent
  #subtotal rows / cols dropped [Subtotals only present in tables with > 2 cross-classified vars]
  t1 <- NULL
  if ( (subtotals==FALSE) & (n.vars>2) )  {
    
    #Create version of crosstab in ftable format
    t1 <- crosstab 
    t1 <- ftable(t1,row.vars=row.vars,col.vars=col.vars)
    
    #Convert to a dataframe
    t1 <- as.data.frame(format(t1),stringsAsFactors=FALSE)
    
    #Remove backslashes from category names AND colnames
    t1 <- apply(t1[,],2, function(x) gsub("\"","",x))
    #Remove preceding and trailing spaces from category names to enable accurate capture of 'sum' rows/cols
    #[Use of grep might extrac category labels with 'sum' as part of a longer one or two word string...]
    t1 <- apply(t1,2,function(x) gsub("[[:space:]]*$","",gsub("^[[:space:]]*","",x)))
    
    #Reshape dataframe to that variable and category labels display as required
    #(a) Move col category names down one row; and move col variable name one column to right
    t1[2,(n.row.vars+1):ncol(t1)] <- t1[1,(n.row.vars+1):ncol(t1)]
    t1[1,] <- ""
    t1[1,(n.row.vars+2)] <- t1[2,(n.row.vars+1)]    
    #(b) Drop the now redundant column separating the row.var labels from the table data + col.var labels
    t1 <- t1[,-(n.row.vars+1)]
    
    #In 'lab', assign category labels for each variable to all rows (to allow identification of sub-totals) 
    lab <- t1[,1:n.row.vars]
    for (c in 1:n.row.vars) {
      for (r in 2:nrow(lab)) {
        if (lab[r,c]=="") lab[r,c] <- lab[r-1,c]  
      }
    }
    
    lab <- (apply(lab[,1:n.row.vars],2,function(x) x=="Sum"))
    lab <- apply(lab,1,sum)
    #Filter out rows of dataframe containing subtotals
    
    t1 <- t1[((lab==0) | (lab==n.row.vars)),]
    
    #Move the 'Sum' label associated with last row to the first column; in the process
    #setting the final row labels associated with other row variables to ""
    t1[nrow(t1),1] <- "Sum"
    t1[nrow(t1),(2:n.row.vars)] <- ""
    
    #set row and column names to NULL
    rownames(t1) <- NULL
    colnames(t1) <- NULL
    
  }
  
  
  
  #Create output object 'result' [class: crosstab]
  result <- NULL
  #(a) record of argument values used to produce tabular output
  result$row.vars <- row.vars
  result$col.vars <- col.vars
  result$dec.places <- dec.places
  result$type <- type
  result$style <- style
  result$percentages <- percentages
  result$addmargins <- addmargins
  result$subtotals <- subtotals
  
  #(b) tabular output [3 variants]
  result$table <- tbl  #Stores original cross-tab frequency counts without margins [class: table]
  result$crosstab <- crosstab #Stores cross-tab in table format using requested style(frequency/pct) and table margins (on/off)
  #[class: table]  
  result$crosstab.nosub <- t1  #crosstab with subtotals suppressed [class: dataframe; or NULL if no subtotals suppressed]  
  class(result) <- "crosstab"    
  
  #Return 'result' as output of function
  result
}
#-------------- Execution Function ----------------------------------------------------------------

#Crosstabs Executor (FACTOR X FACTOR) 
#row, column, and cross type can be a vectors
crosstab_analysis <- function(df, row_cross=c(), column_cross=c(), 
                              cross_type=c(), subtotals=FALSE, addmargin=TRUE, 
                              print_result=FALSE,
                              percentage_format=FALSE,
                              decimal=2){
  library(descr)
  writeLines("---------------Cross Tabulation Settings--------------------")
  writeLines("type = f -> frequency - frequency count")
  writeLines("type = r -> row.pct - proportion within row")
  writeLines("type = c -> col.pct - proportion within column")
  writeLines("type = j -> joint.pct - proportion within final 2 dimensions of table")
  writeLines("type = t -> total.pct - proportion of entire table")
  writeLines("Subtotals -> TRUE: Only calculate subtotal withous sum of each cross items")
  writeLines("Subtotals -> FALSE: Only calculate subtotal withous sum of each cross items")
  writeLines("Addmargins -> TRUE: Margin of Grand Column Header title is applied")
  writeLines("Addmargins -> FALSE: Margin of Grand Column Header title is not applied")
  result <- crosstab(df, row.vars = row_cross, col.vars = column_cross, type = cross_type,
                     addmargins=addmargin, subtotals=subtotals, 
                     percentages=percentage_format, dec.places=decimal)
  if(print_result){
    print(result)
  }
  return(result)
}

#example implementation
library(MASS)
data(survey)
a <- crosstab_analysis(survey, row_cross=c("Sex","Fold","Clap","Exer"), column_cross=c("M.I","Smoke"), cross_type="f")
b <- crosstab_analysis(survey, row_cross=c("Sex","M.I"), column_cross=c("Smoke"), cross_type="r")
c <- crosstab_analysis(survey, row_cross=c("Sex","Fold","Exer"), column_cross=c("Smoke"), cross_type="r")

#---- B) Crosstab with Chi Square Independence and Visualization --------------------------------
#sources: https://bookdown.org/wadetroberts/bookdown-demo/cross-tabulation.
crosstab_statistically_visual <- function(df, row_cross="", col_cross="", 
                                          title="Crosstab Data"){
  library(sjPlot)
  x <- sjPlot::tab_xtab(var.row = df[row_cross], var.col = df[col_cross], 
                        title = title, show.row.prc = TRUE)
  object <- table(df[row_cross], df[col_cross])
  mosaicplot(object, main = "Title of Graph", 
             xlab = "X-axis label", ylab = "Y-axis label", color = TRUE)
  y <- sjPlot::plot_xtab(df[row_cross], df[col_cross], 
                         margin = "row", bar.pos = "stack", coord.flip = TRUE)
}

#---- C) Crosstab with Tabyl (Visual Focus) -------------------------------------
crosstab_tabyl_percents <- function(df, row_cross="", column_cross="", 
                                    percent_method="rowwise"){
  library(janitor)
  library(skimr)
  library(rlang)
  library(gtools) #CGPfunctions depedencies
  library(CGPfunctions)
  
  sym_row <- sym(row_cross)
  sym_col <- sym(column_cross)
  skimr::skim(df)
  if(percent_method=="rowwise"){
    tabyl(df, !!sym_row, !!sym_col) %>%
      adorn_percentages("row") %>%
      adorn_pct_formatting(digits=1)
  }else if(percent_method=="columnwise"){
    tabyl(df, !!sym_row, !!sym_col) %>%
      adorn_percentages("col") %>%
      adorn_pct_formatting(digits=1) 
  }
  PlotXTabs(survey, Sex, Fold)
  PlotXTabs2(survey, Sex, Fold, results.subtitle = FALSE)
}

tree_percent_diagram <- function(df, var_vector=c(), palette_vector=c(), 
                                 customcolor_vector=c(), guide=TRUE, orientation="horizontal"){
  x11()
  library(vtree)
  if(guide){
    writeLines("Construct Tree Diagram consisting of Data Composition of Factors")
    writeLines("If Using Pallette, Here is Palette Number to Choose From: ")
    writeLines("1	Reds		4	Oranges		7	PuBu		10	PuBuGn		13	RdYlGn")
    writeLines("2	Blues		5	Purples		8	PuRd		11	BuPu		14	Set1")
    writeLines("3	Greens		6	YlGn		9	YlOrBr		12	YlOrRd		")
    writeLines("If Using Custom Color, Provide Variable Name for each Color to use as Named Vector")
    writeLines("Fold       Clap       Exer      Smoke ")
    writeLines("#e7d4e8   #99db8c9    #9ecae1   #a4e12f")
  }
  orient <- NULL
  if(orientation=="horizontal"){
    orient <- TRUE
  }else if(orientation=="vertical"){
    orient <- FALSE
  }
  if(length(palette_vector) > 0){
    vtree(df, var_vector, 
          horiz=orient, 
          palette = palette_vector)
  }else if(length(customcolor_vector) > 0){
    vtree(df, var_vector, 
          horiz=orient, 
          fillcolor = customcolor_vector) 
  }
}

#--------------------------------------------------------------------------------------------------------------------------
################################### 31) Multi Decision Criteria Making ####################################################
#--------------------------------------------------------------------------------------------------------------------------

MCDM_recommendation <- function(df, non_beneficial_var=c(), 
                                custom_weight_var=c(),
                                keep_transform=FALSE,
                                sorted_transform=TRUE){
  library(dplyr)
  if(length(custom_weight_var) == 0){
    custom_weight_var <- rep(abs(diff(round(seq(0, 1, length.out = ncol(df)+1),4))[1]), ncol(df))
    if(sum(custom_weight_var) > 1){
      oversum <- sum(custom_weight_var) - 1
      custom_weight_var[ncol(df)] <- custom_weight_var[ncol(df)] - oversum
    }
  }
  names(custom_weight_var) <- colnames(df)
  writeLines('Custom Weight Variables')
  print(custom_weight_var)
  writeLines('Variable which is not Beneficial/Not Common Interest')
  print(non_beneficial_var)
  writeLines("Executing MCDM (Multi Criteria Decision Making)")
  converted_df <- df
  if(length(non_beneficial_var) > 0){
    for(a in 1:ncol(converted_df)){
      if(colnames(converted_df)[a] %in% non_beneficial_var){
        converted_df[, a] <- (min(converted_df[, a]) / converted_df[, a]) * custom_weight_var[a]
      }else{
        converted_df[, a] <- (converted_df[, a] / max(converted_df[, a])) * custom_weight_var[a]
      }
    }
  }
  else{
    for(a in 1:ncol(converted_df)){
      converted_df[, a] <- (converted_df[, a] / max(converted_df[, a])) * custom_weight_var[a]
    }
  }
  converted_df$performance_score <- rowSums(converted_df)
  converted_df$row_index <- as.numeric(rownames(converted_df))
  converted_df <- converted_df %>% arrange(desc(performance_score))
  converted_df$rank <- 1:nrow(converted_df)
  if(keep_transform==FALSE){
    if(sorted_transform){
      df <- df[converted_df$row_index,]
      df$performance_score <- converted_df$performance_score
      df$rank <- converted_df$rank
      return(df) 
    }else{
      #revert back to row index
      converted_df <- converted_df[converted_df$row_index,]
      df$performance_score <- converted_df$performance_score
      df$rank <- converted_df$rank
      return(df)
    }
  }
  else{
    return(converted_df)
  }
}

#phone_recommendation_df <- MCDM_recommendation(phone_df, non_beneficial_var = c("price","camera_pxl","sim_slot"))
#phone_recommendation_df <- MCDM_recommendation(phone_df, non_beneficial_var = c("price","camera_pxl","sim_slot"),sorted_transform = FALSE)
#transformed_phone_recommendation <- MCDM_recommendation(phone_df, non_beneficial_var = c("price","camera_pxl","sim_slot"), keep_transform =  TRUE)

AHP_recommendation <- function(matrix_importance, only_return_criteria_weight=FALSE){
  writeLines("Executing AHP (Analytic Hierarchy Process)")
  normalized_matrix_importance <- matrix_importance
  consistency_matrix_importance <- matrix_importance
  criteria_weight <- c()
  weight_sum_value <- c()
  for(n in 1:nrow(normalized_matrix_importance)){
    normalized_matrix_importance[n,] <- normalized_matrix_importance[n,] / colSums(matrix_importance) 
    criteria_weight <- c(criteria_weight, mean(normalized_matrix_importance[n,]))
  }
  for(m in 1:nrow(consistency_matrix_importance)){
    consistency_matrix_importance[n,] <- consistency_matrix_importance[n,] * criteria_weight
    weight_sum_value <- c(weight_sum_value, sum(consistency_matrix_importance[n,]))
  }
  writeLines("============================================================================================")
  writeLines("========================== 1) Actual Matrix Importance =====================================")
  writeLines("============================================================================================")
  print(matrix_importance)
  writeLines("============================================================================================")
  writeLines("========================== 2) Normalized Matrix Results ====================================")
  writeLines("============================================================================================")
  print(normalized_matrix_importance)
  writeLines("============================================================================================")
  writeLines('========================== 3) Criteria Weight Result =======================================')
  writeLines("============================================================================================")
  print(criteria_weight)
  if(only_return_criteria_weight){
    names(criteria_weight) <- rownames(matrix_importance)
    return(criteria_weight)
  }
  writeLines("============================================================================================")
  writeLines("========================== 4) Consistency Matrix Results ===================================")
  writeLines("============================================================================================")
  print(consistency_matrix_importance)
  writeLines("============================================================================================")
  writeLines("========================== 5) Weight Sum value =============================================")
  writeLines("============================================================================================")
  print(weight_sum_value)
  ratio_weight_value <- weight_sum_value / criteria_weight
  lambda_maximum <- mean(ratio_weight_value)
  n_attribute <- nrow(normalized_matrix_importance)
  consistency_index <- (lambda_maximum - n_attribute) / (n_attribute - 1)
  random_index_table <- c(0,0,0.58,0.9,1.12,1.24,1.32,1.41,1.45,1.49,1.51,1.48,1.56,1.57,1.59)
  consistency_ratio <- 0
  writeLines("============================================================================================")
  writeLines("========================== 6) Ratio Weighted Value =========================================")
  writeLines("============================================================================================")
  print(ratio_weight_value)
  writeLines("============================================================================================")
  writeLines("========================== 7) Consistency Ratio Calculation ================================")
  writeLines("============================================================================================")
  writeLines(paste0("Lambda Maximum = ",lambda_maximum))
  writeLines(paste0("Consistency Index = ",consistency_index))
  print(consistency_index)
  if(n_attribute <= 15){
    consistency_ratio <- consistency_index / random_index_table[n_attribute]
  }else{
    consistency_ratio <- consistency_index / random_index_table[15]
  }
  writeLines(paste0('Consistency Ratio (N=',n_attribute,") = ",consistency_ratio))
  print(consistency_ratio)
  if(consistency_ratio < 0.1){
    writeLines(paste0("Conclusion: Metrics is Reasonably Consistent as Consistency Ratio is below 10% (", consistency_ratio*100,"%)"))
  }
  else if(consistency_ratio >= 0.1){
    writeLines(paste0("Conclusion: Metrics is Not Consistent as Consistency Ratio is greater than 10% (", consistency_ratio*100,"%)"))
  }
  return(list(normalized_matrix_importance, criteria_weight, consistency_matrix_importance,
              weight_sum_value, ratio_weight_value, lambda_maximum, consistency_index, consistency_ratio))
}

#AHP_analysis <- AHP_recommendation(matrix_importance)
#AHP_weight <- AHP_recommendation(matrix_importance, only_return_criteria_weight=TRUE)

ANP_analysis <- function(df, sample_n_alternative=20, standardize=TRUE, 
                         base_importance_matrix_weight=c()){
  if(standardize){
    community_ecologist_standardization <- function(df, method_use=1){
      library(vegan)
      if(method_use==1){
        data_transform <- decostand(df, "total")
      }
      else if(method_use==2){
        data_transform <- decostand(df, "max")
      }
      else if(method_use==3){
        data_transform <- decostand(df, "frequency")
      }
      else if(method_use==4){
        data_transform <- decostand(df, "normalize")
      }
      else if(method_use==5){
        data_transform <- decostand(df, "range")
      }
      else if(method_use==6){
        data_transform <- decostand(df, "rank")
      }
      else if(method_use==7){
        data_transform <- decostand(df, "standardize")
      }
      else if(method_use==8){
        data_transform <- decostand(df, "pa")
      }
      else if(method_use==9){
        data_transform <- decostand(df, "chi.square")
      }
      else if(method_use==10){
        data_transform <- decostand(df, "hellinger")
      }
      else if(method_use==11){
        data_transform <- decostand(df, "log")
      }
      return(data_transform)
    }
    df <- community_ecologist_standardization(df, 2)
  }
  if(sample_n_alternative > 0){
    df <- df[sample(1:nrow(df), sample_n_alternative),] 
  }
  
  writeLines("========== 1) Make Matrix of Alternatives ====================")
  list_matrix_alternative <- list()
  
  for(y in 1:ncol(df)){
    matrix_importance <- matrix(rep(0, nrow(df)*nrow(df)), nrow=nrow(df))
    diag(matrix_importance) <- 1
    for(x in 1:(nrow(df)-1)){
      multiplier <- 1/min(df[x,y])
      matrix_value <- round(df[,y] * multiplier,2)
      matrix_importance[x, (x+1):nrow(df)] <- matrix_value[(x+1):nrow(df)]
      matrix_importance[(x+1):nrow(df), x] <- round(1/matrix_value[(x+1):nrow(df)],2)
    }
    colnames(matrix_importance) <- paste0("A-", rownames(df))
    rownames(matrix_importance) <- paste0("A-", rownames(df))
    list_matrix_alternative <- c(list_matrix_alternative, list(matrix_importance))
  }
  names(list_matrix_alternative) <- colnames(df)
  
  writeLines("========== 2) Calculate AHP within a Set Matrix of Alternatives ==================")
  AHP_recommendation <- function(matrix_importance){
    normalized_matrix_importance <- matrix_importance
    consistency_matrix_importance <- matrix_importance
    criteria_weight <- c()
    weight_sum_value <- c()
    for(n in 1:nrow(normalized_matrix_importance)){
      normalized_matrix_importance[n,] <- normalized_matrix_importance[n,] / colSums(matrix_importance) 
      criteria_weight <- c(criteria_weight, mean(normalized_matrix_importance[n,]))
    }
    for(m in 1:nrow(consistency_matrix_importance)){
      consistency_matrix_importance[n,] <- consistency_matrix_importance[n,] * criteria_weight
      weight_sum_value <- c(weight_sum_value, sum(consistency_matrix_importance[n,]))
    }
    names(criteria_weight) <- rownames(matrix_importance)
    return(criteria_weight)
  }
  
  list_of_matrix_weight <- list()
  for(a in 1:length(list_matrix_alternative)){
    weights <- AHP_recommendation(list_matrix_alternative[[a]])
    list_of_matrix_weight <- c(list_of_matrix_weight, list(weights))
  }
  
  writeLines('========= 3) Combine All Computed Alternative Weights to Decision Matrix ============')
  convert_list_to_matrix <- function(big_list){
    matrix_bind <- NULL
    for(x in 1:length(big_list)){
      matrix_bind <- cbind(matrix_bind, big_list[[x]])
    }
    return(matrix_bind)
  }
  
  decision_matrix <- convert_list_to_matrix(list_of_matrix_weight)
  colnames(decision_matrix) <- colnames(df)
  rownames(decision_matrix) <- rownames(decision_matrix)
  
  writeLines('========= 4) Multiple Decision Matrix by Base Weight and Additive to Priority Value =======')
  #https://stackoverflow.com/questions/3643555/multiply-rows-of-matrix-by-vector
  multipled_decision_matrix <- sweep(decision_matrix, MARGIN=2, base_importance_matrix_weight, `*`)
  priority_value <- rowSums(multipled_decision_matrix)
  
  writeLines("========= 5) Define Super Matrix to represent ANP =========================================")
  super_matrix_length <- nrow(df) + ncol(df) + 1 #Goal + Alternative Set + Criterion Set
  matrix_identity_alternative <- matrix(rep(0, nrow(df)*nrow(df)), nrow=nrow(df))
  diag(matrix_identity_alternative) <- 1
  
  super_matrix <- matrix(rep(0, super_matrix_length*super_matrix_length), nrow=super_matrix_length)
  colnames(super_matrix) <- c("Goal", colnames(decision_matrix), rownames(decision_matrix))
  rownames(super_matrix) <- c("Goal", colnames(decision_matrix), rownames(decision_matrix))
  super_matrix[2:(1+ncol(df)),1] <- base_importance_matrix_weight
  super_matrix[(ncol(df)+2):super_matrix_length, 2:(1+ncol(df))] <- decision_matrix
  super_matrix[(ncol(df)+2):super_matrix_length, (ncol(df)+2):super_matrix_length] <- matrix_identity_alternative
  
  writeLines("========= 6) Define Limit Matrix from Calculated Super Matrix =========================================")
  limit_matrix <- super_matrix %*% super_matrix
  
  return(list(list_matrix_alternative, list_of_matrix_weight, decision_matrix, 
              multipled_decision_matrix, priority_value, super_matrix, limit_matrix))
}

#phone_anp3 <- ANP_analysis(phone_df, sample_n_alternative = 3, base_importance_matrix_weight = AHP_weight)
#phone_anp20 <- ANP_analysis(phone_df, sample_n_alternative = 20, base_importance_matrix_weight = AHP_weight)


allocate_importance_matrix <- function(df, importance_vector=c()){
  if(length(importance_vector) != (ncol(df)-1)){
    stop("Base Importance Vector Length is not correct! make sure it is available for all columns length")
  }
  matrix_importance <- matrix(rep(0, ncol(df)*ncol(df)), nrow=ncol(df))
  colnames(matrix_importance) <- colnames(df)
  rownames(matrix_importance) <- colnames(df)
  diag(matrix_importance) <- 1
  for(a in 1:length(importance_vector)){
    if(a==1){
      matrix_importance[a, (a+1):ncol(df)] <- importance_vector
      matrix_importance[(a+1):ncol(df), a] <- round(1/importance_vector,2)
    }
    else if(a > 1){
      importance_adjusted <- round(importance_vector / importance_vector[a-1],2)
      matrix_importance[a, (a+1):ncol(df)] <- importance_adjusted[a:length(importance_adjusted)]
      matrix_importance[(a+1):ncol(df), a] <- round(1/importance_adjusted[a:length(importance_adjusted)],2) 
    }
  }
  return(matrix_importance)
}

allocate_importance_matrix_by_attribute <- function(vector_attribute, importance_vector=c()){
  n_attr <- length(vector_attribute)
  if(length(importance_vector) != (n_attr-1)){
    stop("Base Importance Vector Length is not correct! make sure it is available for all columns length")
  }
  matrix_importance <- matrix(rep(0, n_attr*n_attr), nrow=n_attr)
  colnames(matrix_importance) <- vector_attribute
  rownames(matrix_importance) <- vector_attribute
  diag(matrix_importance) <- 1
  for(a in 1:length(importance_vector)){
    if(a==1){
      matrix_importance[a, (a+1):n_attr] <- importance_vector
      matrix_importance[(a+1):n_attr, a] <- round(1/importance_vector,2)
    }
    else if(a > 1){
      importance_adjusted <- round(importance_vector / importance_vector[a-1],2)
      matrix_importance[a, (a+1):n_attr] <- importance_adjusted[a:length(importance_adjusted)]
      matrix_importance[(a+1):n_attr, a] <- round(1/importance_adjusted[a:length(importance_adjusted)],2) 
    }
  }
  return(matrix_importance)
}

MCDM_Meta <- function(df, weight_criteria = c(),
                      criteria_most_preference = c(),
                      extreme_value_minimum=c(), extreme_value_maximum=c(),
                      ideal_value_minimum=c(), ideal_value_maximum=c(),
                      v_param = 0.5, lambda_param=0.5, simplify_result=TRUE){
  library(MCDM)
  mat <- as.matrix(df)
  if((length(extreme_value_minimum) != length(extreme_value_maximum)) ||
     (length(ideal_value_minimum) != length(ideal_value_maximum)) || 
     length(extreme_value_minimum) == 0 || length(extreme_value_maximum) == 0 ||
     length(ideal_value_minimum) == 0 || length(ideal_value_maximum) == 0 ){
    writeLines("Building MCDM Ranking without Given Extreme and Ideal Value")
    writeLines('1) MCDM by MULTIMOORA (Multi-Objective Optimization by Ration Analysis & Full Multiplicative Form)')
    df1 <- MMOORA(mat,weight_criteria,criteria_most_preference)
    colnames(df1)[c(3,5,7)] <- c("RS_Ranking","RP_Ranking","MF_Ranking")
    print(head(df1))
    writeLines("2) MCDM by TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)")
    writeLines("TOPSIS Variant: linear transformation as normalization prodecure")
    df2 <- TOPSISLinear(mat,weight_criteria,criteria_most_preference)
    colnames(df2) <- paste0("TOPSISLinear_",colnames(df2)) 
    print(head(df2))
    writeLines("3) MCDM by TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)")
    writeLines("TOPSIS Variant: Vectorial normalization prodecure.")
    df3 <- TOPSISVector(mat,weight_criteria,criteria_most_preference)
    colnames(df3) <- paste0("TOPSISVector_",colnames(df3))
    print(head(df3))
    writeLines("4) MCDM by VIKOR (VIseKriterijumska Optimizacija I Kompromisno Resenje)")
    df4 <- VIKOR(mat,weight_criteria,criteria_most_preference,v_param)
    colnames(df4) <- paste0("VIKOR_",colnames(df4))
    print(head(df4))
    writeLines("5) MCDM by WASPAS (Weighted Aggregated Sum Product Assessment)")
    df5 <- WASPAS(mat,weight_criteria,criteria_most_preference,lambda_param)
    colnames(df5) <- paste0("WASPAS_",colnames(df5))
    print(head(df5))
    if(simplify_result){
      simp_df <- data.frame(cbind(df1[,c(1,ncol(df1))], df2[,c(ncol(df2))], df3[,c(ncol(df3))], 
                                  df4[,c(ncol(df4))], df5[,c(ncol(df5))]))
      colnames(simp_df) <- c("Alternatives",
                             colnames(df1)[ncol(df1)],
                             colnames(df2)[ncol(df2)],
                             colnames(df3)[ncol(df3)],
                             colnames(df4)[ncol(df4)],
                             colnames(df5)[ncol(df5)])
      return(simp_df)
    }else{
      unite_df <- data.frame(cbind(df1, df2[,-1], df3[,-1],df4[,-1],df5[,-1]))
      return(unite_df)
    }
  }
  else{
    extreme_AB <- matrix(c(extreme_value_minimum, extreme_value_maximum), nrow=2)
    ideal_CD <- matrix(c(ideal_value_minimum, ideal_value_maximum), nrow=2)
    if(simplify_result){
      MR <- MetaRanking(mat,weight_criteria,criteria_most_preference,
                        lambda_param,v_param,extreme_AB,ideal_CD)
      print(head(MR))
      return(MR)
    }else{
      writeLines("Building MCDM Ranking with Given Extreme and Ideal Value")
      writeLines('1) MCDM by MULTIMOORA (Multi-Objective Optimization by Ration Analysis & Full Multiplicative Form)')
      df1 <- MMOORA(mat,weight_criteria,criteria_most_preference)
      colnames(df1)[c(3,5,7)] <- c("RS_Ranking","RP_Ranking","MF_Ranking")
      print(head(df1))
      writeLines("2) MCDM by TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)")
      writeLines("TOPSIS Variant: linear transformation as normalization prodecure")
      df2 <- TOPSISLinear(mat,weight_criteria,criteria_most_preference)
      colnames(df2) <- paste0("TOPSISLinear_",colnames(df2)) 
      print(head(df2))
      writeLines("3) MCDM by TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)")
      writeLines("TOPSIS Variant: Vectorial normalization prodecure.")
      df3 <- TOPSISVector(mat,weight_criteria,criteria_most_preference)
      colnames(df3) <- paste0("TOPSISVector_",colnames(df3))
      print(head(df3))
      writeLines("4) MCDM by VIKOR (VIseKriterijumska Optimizacija I Kompromisno Resenje)")
      df4 <- VIKOR(mat,weight_criteria,criteria_most_preference,v_param)
      colnames(df4) <- paste0("VIKOR_",colnames(df4))
      print(head(df4))
      writeLines("5) MCDM by WAPSAS (Weighted Aggregated Sum Product Assessment)")
      df5 <- WASPAS(mat,weight_criteria,criteria_most_preference,lambda_param)
      colnames(df5) <- paste0("WASPAS_",colnames(df5))
      print(head(df5))
      writeLines('6) MCDM by RIM (Reference Ideal Method)')
      df6 <- RIM(mat,weight_criteria, criteria_most_preference, extreme_AB, ideal_CD)
      colnames(df6) <- paste0("RIM_",colnames(df6))
      print(head(df6))
      unite_df <- data.frame(cbind(df1, df2[,-1], df3[,-1],df4[,-1],df5[,-1],df6[,-1]))
      return(unite_df)
    }
  }
}

#phone_meta <- MCDM_Meta(phone_df, weight_criteria = c(0.2, 0.25, 0.05, 0.15, 0.05, 0.1, 0.1, 0.05, 0.05), 
#                        criteria_most_preference = c('min',rep('max',8)), simplify_result = TRUE)
#library(dplyr)
#phone_by_multimoora_rcmd <- phone_df %>% 
#  mutate(rownumber = rownames(phone_df)) %>% 
#  arrange(phone_meta$MultiMooraRanking)
#phone_by_topsislinear_rcmd <- phone_df %>% 
#  mutate(rownumber = rownames(phone_df)) %>% 
#  arrange(phone_meta$TOPSISLinear_Ranking)
#phone_by_topsisvector_rcmd <- phone_df %>% 
#  mutate(rownumber = rownames(phone_df)) %>% 
#  arrange(phone_meta$TOPSISVector_Ranking)
#phone_by_vikor_rcmd <- phone_df %>% 
#  mutate(rownumber = rownames(phone_df)) %>% 
#  arrange(phone_meta$VIKOR_Ranking)
#phone_by_waspas_rcmd <- phone_df %>% 
#  mutate(rownumber = rownames(phone_df)) %>% 
#  arrange(phone_meta$WASPAS_Ranking)

#print(head(phone_by_multimoora_rcmd))
#print(head(phone_by_topsislinear_rcmd))
#print(head(phone_by_topsisvector_rcmd))
#print(head(phone_by_vikor_rcmd))
#print(head(phone_by_waspas_rcmd))

#phone_meta_detail <- MCDM_Meta(phone_df, weight_criteria = c(0.2, 0.25, 0.05, 0.15, 0.05, 0.1, 0.1, 0.05, 0.05), 
#                        criteria_most_preference = c('min',rep('max',8)), simplify_result = FALSE)

#--------------------------------------------------------------------------------------------------------------------------
#################### 32) Species by Community and Environment Analysis ####################################################
#--------------------------------------------------------------------------------------------------------------------------

#library(vegan)
#data("BCI")
#data("BCI.env")
#data("dune.taxon")
#data("dune.env")
#data("dune.phylodis")
#data("varespec")
#data("mite")
#data("mite.env")
#data("mite.pcnm")
#data("mite.xy")
#data("pyrifos")
#data("sipoo")

community_environment_divider <- function(raw_df, base_row_colname="", 
                                          community_spread_colname="",
                                          community_value_colname="",
                                          environment_columns_colname=c(),
                                          add_environment_dummy_numeric=TRUE,
                                          show_variation_partition=TRUE){
  library(rlang)
  library(tidyr)
  library(dplyr)
  writeLines("Make Community Datasets")
  base_row_sym <- sym(base_row_colname)
  comm_col_sym <- sym(community_spread_colname)
  comm_val_sym <- sym(community_value_colname)
  comm <- tree %>%
    group_by(!!base_row_sym, !!comm_col_sym) %>%
    summarise(value = mean(!!comm_val_sym, na.rm=TRUE)) %>%
    spread(!!comm_col_sym, value) %>% as.data.frame()
  comm[is.na(comm)] <- 0
  rownames(comm) <- comm[,1]
  comm <- comm[,-1]
  print(head(comm))
  writeLines("Make Environment Datasets")
  class_of_columns <- unlist(lapply(raw_df[,environment_columns_colname], class))
  env <- NULL
  if(any(class_of_columns %in% c("character","factor"))){
    #take first value of grouped vectors
    env <- aggregate(raw_df[,environment_columns_colname], 
                     by = list(raw_df[[base_row_colname]]), function(x) x[1])
    colnames(env)[1] <- base_row_colname
  }else{
    #take mean value of grouped vectors
    env <- aggregate(raw_df[,environment_columns_colname], 
                     by = list(raw_df[[base_row_colname]]), function(x) mean(x))
    colnames(env)[1] <- base_row_colname
  }
  rownames(env) <- env[,1]
  env <- env[,-1]
  env[is.na(env)] <- 0
  print(head(env))
  writeLines('Done Adjusting Raw Data to Community and Environments!')
  if(add_environment_dummy_numeric){
    library(dummies)
    dummy_list <- list()
    env_class <- unlist(lapply(env, class))
    index_dummy <- which(env_class %in% c("character","factor"))
    if(length(index_dummy) > 0){
      for(b in 1:length(index_dummy)){
        dummy_df <- as.data.frame(dummy(env[,index_dummy[b]]))
        writeLines(paste0("Adding ",colnames(env)[index_dummy[b]], " as Dummy Variable"))
        dummy_list <- c(dummy_list, list(dummy_df))
      }
      names(dummy_list) <- colnames(env)[index_dummy]
      metrics_numeric <- env[,-index_dummy]
      if(show_variation_partition){
        library(vegan)
        variation_list <- list()
        for(dum in 1:length(dummy_list)){
          writeLines("===================================================================================")
          writeLines(paste0("Variation Partitioning Using ", names(dummy_list)[dum], " as Dummy Variable"))
          variation <- varpart(comm, metrics_numeric, dummy_list[[dum]])
          variation_list <- c(variation_list, list(variation))
          print(variation)
          writeLines("===================================================================================")
        }
        return(list(comm, env, dummy_list, metrics_numeric, variation_list))
      }
      return(list(comm, env, dummy_list, metrics_numeric))
    }
    else{
      writeLines('There are No Potentially Dummy Variable found in Environment Variables!')
    }
  }
  return(list(comm, env))
}

#comm_env <- community_environment_divider(tree, "plotID", "spcode", "cover", 
#                                          environment_columns_colname = c("elev","tci","streamdist","disturb","beers"))
#comm_tree <- comm_env[[1]]
#env_tree <- comm_env[[2]]
#dummy_disturb <- comm_env[[3]][[1]]
#moisture <- comm_env[[4]]

community_label_converter <- function(df, sep="_"){
  all_unique_values <- lapply(df, unique)
  parent_count <- length(names(all_unique_values))
  parent_name <- names(all_unique_values)
  child_count <- unlist(lapply(df, FUN=function(x) length(unique(x))))
  names(child_count) <- NULL
  child_name <- unlist(all_unique_values)
  names(child_name) <- NULL
  processed_df <- data.frame(ID=seq(1,nrow(df),1))
  for(a in 1:parent_count){
    for(b in 1:child_count[a]){
      full_name <- paste0(parent_name[a],sep,child_name[b])
      values_to_convert <- df[[parent_name[a]]]
      values_converted <- ifelse(values_to_convert == child_name[b], 1, 0)
      processed_df[[full_name]] <- values_converted
    }
  }
  library(dplyr)
  processed_df <- processed_df %>% mutate_if(is.numeric, as.integer)
  return(processed_df)
}

canonical_correspondence_analysis <- function(comm_df, env_df, 
                                              partial_analysis=TRUE, 
                                              spatial_df=data.frame(),
                                              test_anova_permutation=1000){
  library(vegan)
  partial_cca_formula_maker = function(colnames_data, sep="."){
    library(stringr)
    sep_colnames = strsplit(colnames_data, sep, fixed = TRUE)
    formula_string = ""
    colname_pointer = ""
    string_concater = "Condition("
    combo_string = 1
    for(i in 1:length(sep_colnames)){
      if(i == 1){
        colname = sep_colnames[[i]][1]
        colname_pointer = colname
      }  
      else if(i > 1){
        colname = sep_colnames[[i]][1]
        if(colname_pointer == colname){
          if(combo_string == 1){
            string_concater <- paste0(string_concater, colnames_data[i-1]," + ",colnames_data[i])
            combo_string = combo_string + 1
            if(i == length(sep_colnames)){
              string_concater <- paste0(string_concater, ")")
              formula_string = paste0(formula_string, " + ", string_concater) 
            }
          }
          else if(combo_string > 1){
            string_concater <- paste0(string_concater, " + ", colnames_data[i])
            combo_string = combo_string + 1
            if(i == length(sep_colnames)){
              string_concater <- paste0(string_concater, ")")
              formula_string = paste0(formula_string, " + ", string_concater) 
            }
          }
        }
        else{
          colname_pointer = colname
          if(combo_string < 2){
            if(i==2){
              formula_string = paste0(formula_string, colnames_data[i-1]) 
            }
            else if(i>2){
              formula_string = paste0(formula_string, " + ", colnames_data[i-1]) 
            }
          }
          else if(combo_string >=2){
            string_concater <- paste0(string_concater, ")")
            formula_string = paste0(formula_string, " + ", string_concater) 
            string_concater = "Condition("
            combo_string <- 1
          }
        }
      }
      #print(paste0("Iteration ",i))
      #print(paste0("Formula: ", formula_string))
    }
    return(formula_string)
  }
  
  if(nrow(spatial_df)!=0){
    if(nrow(comm_df) == nrow(spatial_df)){
      comm_df <- cbind(comm_df, spatial_df)
      writeLines("Successfully Concated Spatial DF to Response DF")
    }
    else{
      writeLines("Row Difference between Response and Spatial Dataframes! Expected the Same row, Spatial DF ignored")
    }
  }
  
  #========================Model CCA=============================
  ccamodel <- cca(comm_df~., env_df)
  finalmodel<- ordistep(ccamodel, scope=formula(ccamodel))
  print(vif.cca(finalmodel))
  
  ccavectors <- as.matrix(scores(ccamodel, display = "bp", scaling = "species")*12.2) %>% 
    as.data.frame()
  
  site_data <- scores(ccamodel, display = "sites") %>% 
    as.data.frame() 
  
  species_data <- scores(ccamodel, display = "species") %>% 
    as.data.frame()
  
  writeLines(paste0("R Squared Adjustment Score: ", RsquareAdj(finalmodel)))
  writeLines("Testing the significance of the CCA model:")
  anova.cca(finalmodel)
  writeLines("Testing the significance of terms (environmental variables):")
  anova.cca(finalmodel, by="terms")
  writeLines("Testing the significance of CCA axes (at least the first two or three should present a significant p value):")
  anova.cca(finalmodel, by="axis")
  writeLines(paste0('Testing the significance of the CCA model by ', test_anova_permutation, " Permutation"))
  anova(finalmodel, permutations = test_anova_permutation)
  
  writeLines("plot display notes:")
  writeLines("sp = species")
  writeLines("cn = environmental gradients")
  writeLines("wa = sites")
  
  x11()
  plot(finalmodel, display=c("sp","cn","wa"))
  
  if(partial_analysis){
    #========================Partial CCA=============================
    explanatory_colnames = colnames(env_df)
    explanatory_colnames = sort(explanatory_colnames)
    
    partial_formula = formula(paste0("comm_df ~ ", 
                                     partial_cca_formula_maker(explanatory_colnames)))
    all_df = cbind(comm_df, env_df)
    partialccamodel<-cca(partial_formula, env_df)
    
    finalmodelpartial<- ordistep(partialccamodel, scope=formula(partialccamodel))
    print(vif.cca(finalmodelpartial))
    
    writeLines("Testing the significance of the Partial CCA model:")
    print(anova.cca(finalmodelpartial))
    writeLines("Testing the significance of terms (environmental variables) Partial CCA Model:")
    print(anova.cca(finalmodelpartial, by="terms"))
    writeLines("Testing the significance of CCA axes (at least the first two or three should present a significant p value) Partial CCA Model:")
    print(anova.cca(finalmodelpartial, by="axis"))
    
    x11()
    plot(finalmodelpartial, display=c("sp","cn","wa"))
    return(list(finalmodel, finalmodelpartial, ccavectors, site_data, species_data))
  }
  return(list(finalmodel,ccavectors, site_data, species_data))
}

#mm <- read.csv("https://stats.idre.ucla.edu/stat/data/mmreg.csv")
#colnames(mm) <- c("Control", "Concept", "Motivation", "Read", "Write", "Math", "Science", "Sex")
#x <- canonical_correlation_analysis_latent_var(mm, c(1:3), c(4:8), "Psychological", "Academic")

redudancy_analysis <- function(comm_df, env_df){
  library(vegan)
  rda_model <- rda(comm_df ~ ., data=env_df)
  print(summary(rda_model))
  writeLines(paste0("R Squared Adjustment Score: ", RsquareAdj(rda_model)))
  
  writeLines("plot display notes:")
  writeLines("sp = species")
  writeLines("cn = environmental gradients")
  writeLines("wa = sites")
  
  x11()
  plot(rda_model, type='n', scaling=1, main="Species Plot")
  orditorp(rda_model, display='sp', cex=0.5, scaling=1, col='blue')
  text(rda_model, display='cn', col='red')
  
  x11()
  plot(rda_model, type='n', scaling=1, main="Sites Plot")
  orditorp(rda_model, display='wa', cex=0.5, scaling=1, col='blue')
  text(rda_model, display='cn', col='red')
  
  return(rda_model)
}

canonical_correlation_analysis_latent_var <- function(df, multivar_group1, multivar_group2,
                                                      latent_name1="latent1", latent_name2="latent2"){
  library(ggplot2)
  library(GGally)
  library(CCA)
  library(CCP)
  
  writeLines(paste0("Latent Variable 1 = ", latent_name1))
  writeLines(paste0("Latent Variable 2 = ", latent_name2))
  latent1 <- mm[, multivar_group1]
  latent2 <- mm[, multivar_group2]
  x11()
  print(ggpairs(latent1))
  x11()
  print(ggpairs(latent2))
  corr_matrix <- matcor(latent1, latent2)
  print(corr_matrix)
  
  cc1 <- cc(latent1, latent2)
  print(cc1$cor)
  cc2 <- comput(latent1, latent2, cc1)
  print(cc2[3:6])
  
  writeLines("The above correlations are between observed variables and canonical variables which are known as the canonical loadings.")
  writeLines("These canonical variates are actually a type of latent variable.")
  
  writeLines("Tests of canonical dimensions")
  writeLines("Define number of observations, number of variables in first set, and number of variables in the second set.")
  rho <- cc1$cor
  n <- dim(latent1)[1]
  p <- length(latent1)
  q <- length(latent2)
  
  ## Calculate p-values using the F-approximations of different test statistics:
  wilk_test <- p.asym(rho, n, p, q, tstat = "Wilks")
  hotelling_test <- p.asym(rho, n, p, q, tstat = "Hotelling")
  pillai_test <- p.asym(rho, n, p, q, tstat = "Pillai")
  roy_test <- p.asym(rho, n, p, q, tstat = "Roy")
  writeLines("=================== Statistical Testing using Wilk's Lambda ====================================")
  print(unlist(wilk_test))
  for(p_val in 1:length(wilk_test$p.value)){
    if(wilk_test$p.value[p_val] < 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is Statistically Significant by Wilk Lambda Test"))
    }else if(wilk_test$p.value[p_val] >= 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is not Statistically Significant by Wilk Lambda Test"))
    }
  }
  writeLines("=================== Statistical Testing using Hotelling Lawley Trace ===========================")
  print(unlist(hotelling_test))
  for(p_val in 1:length(hotelling_test$p.value)){
    if(hotelling_test$p.value[p_val] < 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is Statistically Significant by Hotelling Lawley Test"))
    }else if(hotelling_test$p.value[p_val] >= 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is not Statistically Significant by Hotelling Lawley Test"))
    }
  }
  writeLines("=================== Statistical Testing using Pillai Bartlett Trace ============================")
  print(unlist(pillai_test))
  for(p_val in 1:length(pillai_test$p.value)){
    if(pillai_test$p.value[p_val] < 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is Statistically Significant by Pillai Bartlett Test"))
    }else if(pillai_test$p.value[p_val] >= 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is not Statistically Significant by Pillai Bartlett Test"))
    }
  }
  writeLines("=================== Statistical Testing using Roy's Largest Root ===============================")
  print(unlist(roy_test))
  for(p_val in 1:length(roy_test$p.value)){
    if(roy_test$p.value[p_val] < 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is Statistically Significant by Roy Largest Test"))
    }else if(roy_test$p.value[p_val] >= 0.05){
      writeLines(paste0("Canonical Dimension ",p_val," is not Statistically Significant by Roy Largest Test"))
    }
  }
  
  writeLines("standardized Latent 1 canonical coefficients diagonal matrix of Latent 1 sd's")
  s1 <- diag(sqrt(diag(cov(latent1))))
  print(s1 %*% cc1$xcoef)
  
  writeLines("standardized Latent 2 canonical coefficients diagonal matrix of Latent 2 sd's")
  s2 <- diag(sqrt(diag(cov(latent2))))
  print(s2 %*% cc1$ycoef)
  return(list(cc1, cc2, s1, s2))
}

individual_community_analysis <- function(df_community){
  library(vegan)
  writeLines("==========================================================")
  writeLines('PCA (Principal Component Analysis) Model of Community')
  writeLines("==========================================================")
  x11()
  model_pca = rda(df_community)
  print(summary(model_pca))
  ordiplot(model_pca, display = 'sp')
  orditorp(model_pca, display = 'sp')
  writeLines("==========================================================")
  writeLines('CA (Correspondence Analysis) Model of Community')
  writeLines("==========================================================")
  model_ca = cca(df_community)
  print(summary(model_ca))
  x11()
  ordiplot(model_ca, display = 'sp')
  orditorp(model_ca, display = 'sp')
  writeLines("==========================================================")
  writeLines("DCA (Detrended Correspondence Analysis) Model of Community")
  writeLines("==========================================================")
  model_dca = decorana(df_community)
  print(summary(model_dca))
  x11()
  ordiplot(model_dca, display = 'sp')
  orditorp(model_dca, display = 'sp')
  return(list(model_pca, model_ca, model_dca))
}

ecology_dataset_analysis <- function(dataset_community, dataset_env, 
                                     independent_var=c(),
                                     fit_permutation=999){
  library(vegan)
  library(tibble)
  library(tidyr)
  library(dplyr)
  
  dataset_env["site"] <- row.names(dataset_env)
  clean_background <- theme(plot.background = element_rect("white"),
                            panel.background = element_rect("white"),
                            panel.grid = element_line("white"),
                            axis.line = element_line("gray25"),
                            axis.text = element_text(size = 12, color = "gray25"),
                            axis.title = element_text(color = "gray25"),
                            legend.text = element_text(size = 12),
                            legend.key = element_rect("white"))
  attach(dataset_env)
  
  length_factor <- length(unique(dataset_env[[independent_var]]))
  random_num <- unique(round(runif(657,1,657)))
  pal1 <- colors()[random_num[1:length_factor]]
  pal2 <- colors()[random_num[(length_factor+1):(2*length_factor)]]
  
  independent_form <- ""
  if(length(independent_var) > 0){
    independent_form <- paste0(independent_var, collapse =" + ")
  }
  else if(length(independent_var) == 0){
    independent_form <- independent_var
  }
  
  writeLines("These Feature is Spesialized for Ecology Dataset with different perspective analysis")
  writeLines("======================================================================================================")
  writeLines("======================= 1) How speciose are my communities? specnumber() =============================")
  writeLines("======================================================================================================")
  writeLines("specnumber() will tell you the number of species within each sample. ")
  
  writeLines("run an analysis of variance to ask if data mean is significantly different across groups.")
  sppr <- specnumber(dataset_community)
  sppr_formula <- as.formula(paste0("sppr ~ ",independent_form))
  sppr_aov <- aov(sppr_formula, data = dataset_env)
  print(summary(sppr_aov))
  
  sppr_df <- sppr %>% enframe() 
  writeLines("Inspect Species Number")
  print(head(sppr_df))
  
  x11()
  plot_sppr <- ggplot(sppr_df, aes(x = eval(parse(text=independent_form)), y = value, 
                                   fill = eval(parse(text=independent_form)))) +
    geom_boxplot() +
    scale_fill_manual(values = pal1) +
    theme(legend.position = "topright",
          plot.background = element_rect("white"),
          panel.background = element_rect("white"),
          panel.grid = element_line("grey90"),
          axis.line = element_line("gray25"),
          axis.text = element_text(size = 12, color = "gray25"),
          axis.title = element_text(color = "gray25"),
          legend.text = element_text(size = 12)) + 
    labs(x = independent_form,
         y = "Number of Species per site",
         title = "Species richness")
  print(plot_sppr)
  
  writeLines("========================================================================================================")
  writeLines("===========================2) How diverse are my communities? diversity() ==============================")
  writeLines("========================================================================================================")
  writeLines("Using all Diversity metrics Available -> Shannon, Simpson, InvSimpson")
  shannondiv <- diversity(dataset_community, index="shannon")
  simpsondiv <- diversity(dataset_community, index="simpson")
  invsimpsondiv <- diversity(dataset_community, index="invsimpson")
  
  writeLines("Inspect Diversity Results From Shannon Metrics")
  print(head(shannondiv))
  writeLines("Inspect Diversity Results From Simpson Metrics")
  print(head(simpsondiv))
  writeLines("Inspect Diversity Results From Inverse Simpson Metrics")
  print(head(invsimpsondiv))
  
  shandiv_df <- shannondiv %>% 
    enframe() %>% 
    rename(site = name,
           shan_div = value)
  
  simpdiv_df <- simpsondiv %>%
    enframe() %>%
    rename(site = name,
           simp_div = value)
  
  invsimpdiv_df <- invsimpsondiv %>%
    enframe() %>%
    rename(site = name, 
           invsimp_div = value)
  
  div_plot_shannon_df <- shandiv_df %>% 
    full_join(dataset_env, ., by = "site") %>% 
    group_by(eval(parse(text=independent_form))) %>% 
    summarize(mean = round(mean(shan_div), 2),
              err = sd(shan_div)/sqrt(length(shan_div))) %>% 
    dplyr::mutate(label = "mean") %>% 
    unite("mean_label", label, mean, sep = " = ", remove = FALSE)
  writeLines("Shannon Diversity Dataframe")
  colnames(div_plot_shannon_df)[1] <- independent_form
  print(head(div_plot_shannon_df))
  
  div_plot_simpson_df <- simpdiv_df %>% 
    full_join(dataset_env, ., by = "site") %>% 
    group_by(eval(parse(text=independent_form))) %>% 
    summarize(mean = round(mean(simp_div), 2),
              err = sd(simp_div)/sqrt(length(simp_div))) %>% 
    dplyr::mutate(label = "mean") %>% 
    unite("mean_label", label, mean, sep = " = ", remove = FALSE)
  writeLines("Simpson Diversity Dataframe")
  colnames(div_plot_simpson_df)[1] <- independent_form
  print(head(div_plot_simpson_df))
  
  div_plot_invsimpson_df <- invsimpdiv_df %>% 
    full_join(dataset_env, ., by = "site") %>% 
    group_by(eval(parse(text=independent_form))) %>% 
    summarize(mean = round(mean(invsimp_div), 2),
              err = sd(invsimp_div)/sqrt(length(invsimp_div))) %>% 
    dplyr::mutate(label = "mean") %>% 
    unite("mean_label", label, mean, sep = " = ", remove = FALSE)
  writeLines("Inverse Simpson Diversity Dataframe")
  colnames(div_plot_invsimpson_df)[1] <- independent_form
  print(head(div_plot_invsimpson_df))
  
  x11()
  plot_shandiv <- ggplot(div_plot_shannon_df, aes(x = eval(parse(text=independent_form)), 
                                                  y = mean, fill = eval(parse(text=independent_form)))) +
    geom_col(color = "black") +
    scale_fill_manual(values = pal1) +
    geom_errorbar(aes(ymin = mean - err, ymax = mean + err), width = 0.5) +
    geom_text(aes(x = eval(parse(text=independent_form)), 
                  y = mean + err + 0.07, label = mean_label)) +
    scale_y_continuous(limits = c(0, 2.75), expand = c(0,0)) +
    clean_background + 
    theme(legend.position = "topright") +
    labs(x = independent_form,
         y = "Mean Shannon diversity",
         title = "Shannon diversity")
  print(plot_shandiv)
  
  x11()
  plot_simpdiv <- ggplot(div_plot_simpson_df, aes(x = eval(parse(text=independent_form)), y = mean, 
                                                  fill = eval(parse(text=independent_form)))) +
    geom_col(color = "black") +
    scale_fill_manual(values = pal1) +
    geom_errorbar(aes(ymin = mean - err, ymax = mean + err), width = 0.5) +
    geom_text(aes(x = eval(parse(text=independent_form)), 
                  y = mean + err + 0.07, label = mean_label)) +
    scale_y_continuous(limits = c(0, 2.75), expand = c(0,0)) +
    clean_background + 
    theme(legend.position = "topright") +
    labs(x = independent_form,
         y = "Mean Simpson diversity",
         title = "Simpson diversity")
  print(plot_simpdiv)
  
  x11()
  plot_invsimpdiv <- ggplot(div_plot_invsimpson_df, aes(x = eval(parse(text=independent_form)), y = mean, 
                                                        fill = eval(parse(text=independent_form)))) +
    geom_col(color = "black") +
    scale_fill_manual(values = pal1) +
    geom_errorbar(aes(ymin = mean - err, ymax = mean + err), width = 0.5) +
    geom_text(aes(x = eval(parse(text=independent_form)), 
                  y = mean + err + 0.07, label = mean_label)) +
    scale_y_continuous(limits = c(0, 2.75), expand = c(0,0)) +
    clean_background + 
    theme(legend.position = "topright") +
    labs(x = independent_form,
         y = "Mean Inverse Simpson diversity",
         title = "Inverse Simpson diversity")
  print(plot_invsimpdiv)
  
  shannon_form <- as.formula(paste0("shannondiv ~ ",independent_form))
  simpson_form <- as.formula(paste0("simpsondiv ~ ",independent_form))
  invsimpson_form <- as.formula(paste0("invsimpsondiv ~ ",independent_form))
  
  writeLines("Anova results from Shannon Diversity Metrics")
  aov1 <- aov(shannon_form, data = dataset_env)
  print(summary(aov1))
  writeLines("Anova results from Simpson Diversity Metrics")
  aov2 <- aov(simpson_form, data = dataset_env)
  print(summary(aov2))
  writeLines("Anova results from Inverse Simpson Diversity Metrics")
  aov3 <- aov(invsimpson_form, data = dataset_env)
  print(summary(aov3))
  
  writeLines("=========================================================================================================================")
  writeLines("================ 3) How different are my communities in species composition? adonis(), rda(), metaMDS() =================")
  writeLines("A) Assessing differences in community composition is done with permutational Multivariate Analysis of Variance, or perMANOVA.")
  writeLines("These tests are done on distances, meaning that they assess the differences between communities based on dissimilarity. ")
  writeLines("With perMANOVA, the null hypothesis is that the centroids of your groups (in ordination space as defined by the dissimilarity measure you've chosen) are equivalent for all groups.")
  writeLines("In other words, you are asking if, following some measure of (dis)similarity, the community composition of sites between groups is the same.")
  writeLines("=========================================================================================================================")
  
  community_independent_form <- as.formula(paste0(dataset_community, " ~ ", independent_form))
  
  manova_permutation <- adonis(community_independent_form, data = dataset_env)
  print(manova_permutation)
  redudancy_analysis <- rda(dataset_community)
  print(redudancy_analysis)
  
  PCAscores <- scores(redudancy_analysis, display = "sites") %>% as.data.frame() 
  writeLines("===================PCA Scores============================")
  print(PCAscores)
  
  PCAvect <- scores(redudancy_analysis, display = "species") %>% as.data.frame()
  writeLines("===================PCA Vector============================")
  print(PCAvect)
  
  if(length(independent_var) == 1)
  {
    x11()
    plot_PCA <- ggplot() +
      geom_point(data = PCAscores, aes(x = PC1, y = PC2, color = eval(parse(text=independent_form)))) +
      scale_color_manual(values = pal1) +
      geom_vline(xintercept = c(0), color = "grey70", linetype = 2) +
      geom_hline(yintercept = c(0), color = "grey70", linetype = 2) +
      geom_segment(data = PCAvect, aes(x = 0, y = 0, xend = PC1, yend = PC2), 
                   arrow = arrow(length = unit(0.2, "cm"))) + 
      geom_text(data = PCAvect, aes(x = PC1, y = PC2, label = rownames(PCAvect))) +
      clean_background +
      labs(x = "PC1 (23.57%)",
           y = "PC2 (12.23%)",
           title = "Principal Components Analysis") 
    print(plot_PCA) 
  }
  
  # extract site coords (points)
  PCA_fort_sites <- fortify(PCAscores)
  
  # extract species coords (segment)
  PCA_fort_species <- fortify(PCAvect)
  
  if(length(independent_var) == 1)
  {
    x11()
    PCA_fortify_plot <- ggplot() +
      geom_point(data = PCA_fort_sites, aes(x = PC1, y = PC2, col = eval(parse(text=independent_form)))) +
      geom_vline(xintercept = c(0), color = "grey70", linetype = 2) +
      geom_hline(yintercept = c(0), color = "grey70", linetype = 2) +
      scale_color_manual(values = pal2) +
      geom_segment(data = PCA_fort_species, aes(x = 0, xend = PC1, y = 0, yend = PC2), 
                   arrow = arrow(length = unit(0.2, "cm"))) +
      geom_text(data = PCA_fort_species, aes(x = PC1, y = PC2, label = colnames(dataset_community))) +
      clean_background +
      labs(x = "PC1 (23.57%)",
           y = "PC2 (12.23%)",
           title = "Principal Components Analysis - using fortify()")
    print(PCA_fortify_plot)
  }
  
  writeLines("=========================================================================================================================")
  writeLines("B) Most community ecologists use Non-metric Multidimensional Scaling (NMDS).")
  writeLines("You can imagine NMDS as a reduction of axes, where all your axes are the species within a sample, and each sample exists relative to others on the axes.")
  writeLines("NMDS allows you to collapse all these species axes (in this case, 48) into 2 to plot in cartesian space in order to visualize the differences between samples and sites.")
  writeLines("We'll do this by using metaMDS().")
  writeLines("=========================================================================================================================")
  
  NMDS <- metaMDS(dataset_community)
  print(NMDS)
  
  x11()
  stressplot(NMDS)
  x11()
  plot(NMDS)
  
  plot_df <- scores(NMDS, display = "sites") %>% 
    as.data.frame() 
  
  x11()
  plot_nmds <- ggplot(plot_df, aes(x = NMDS1, y = NMDS2, color = eval(parse(text=independent_form)), 
                                   shape = eval(parse(text=independent_form)))) +
    geom_point(size = 3, alpha = 0.8) +
    scale_color_manual(values = pal1) +
    stat_ellipse(linetype = 2, size = 1) +
    clean_background +
    labs(title = "NMDS")
  print(plot_nmds)
  
  writeLines("========================== 4) How do species drive the separation of my communities? envfit() ========================")
  writeLines("envfit() is a vegan function that allows you to determine the relative contribution of environmental variables to the separation of your communities along ordination axes")
  writeLines("envfit() takes the output of metaMDS() and the species matrix created")
  
  fit <- envfit(NMDS, dataset_community, perm = fit_permutation) 
  
  writeLines("extract p-values for each species") 
  fit_pvals <- fit$vectors$pvals %>% 
    as.data.frame() %>% 
    rownames_to_column("species") %>% 
    dplyr::rename("pvals" = ".")
  
  writeLines("extract coordinates for species, only keep species with p-val = 0.001")
  fit_spp <- fit %>% 
    scores(., display = "vectors") %>% 
    as.data.frame() %>% 
    rownames_to_column("species") %>% 
    full_join(., fit_pvals, by = "species") %>% 
    filter(pvals == 0.001)
  
  # new plot
  x11()
  nmds_plot_new <- ggplot(plot_df, aes(x = NMDS1, y = NMDS2)) +
    coord_fixed() +
    geom_point(aes(color = eval(parse(text=independent_form)), 
                   shape = eval(parse(text=independent_form))), size = 3, alpha = 0.8) +
    stat_ellipse(aes(color = eval(parse(text=independent_form)))) +
    scale_color_manual(values = pal2) +
    geom_segment(data = fit_spp, aes(x = 0, xend = NMDS1, y = 0, yend = NMDS2),
                 arrow = arrow(length = unit(0.25, "cm")),
                 col = "black") +
    geom_text(data = fit_spp, aes(label = species)) + clean_background
  print(nmds_plot_new)
  
  writeLines("=================== 5) How is community structure related to specific environmental variables? cca() ===========================")
  writeLines("The above ordination methods are unconstrained ordination, which means that the ordination is done only considering species counts in the site x species matrix.")
  writeLines("Constrained ordination is appropriate for what's common in ecological data: a matrix of communities (site x species) and another matrix of environmental characteristics (site x environment).")
  writeLines("Our environmental matrix is env, created up top.")
  
  #remove site colnames from df environment
  dataset_env <- dataset_env[, -which(colnames(dataset_env)=="site")]
  canonical_correspondence_analysis <- function(response_df, explanatory_df, 
                                                partial_analysis=TRUE, spatial_df=data.frame()){
    
    partial_cca_formula_maker = function(colnames_data, sep="."){
      library(stringr)
      sep_colnames = strsplit(colnames_data, sep, fixed = TRUE)
      formula_string = ""
      colname_pointer = ""
      string_concater = "Condition("
      combo_string = 1
      for(i in 1:length(sep_colnames)){
        if(i == 1){
          colname = sep_colnames[[i]][1]
          colname_pointer = colname
        }  
        else if(i > 1){
          colname = sep_colnames[[i]][1]
          if(colname_pointer == colname){
            if(combo_string == 1){
              string_concater <- paste0(string_concater, colnames_data[i-1]," + ",colnames_data[i])
              combo_string = combo_string + 1
              if(i == length(sep_colnames)){
                string_concater <- paste0(string_concater, ")")
                formula_string = paste0(formula_string, " + ", string_concater) 
              }
            }
            else if(combo_string > 1){
              string_concater <- paste0(string_concater, " + ", colnames_data[i])
              combo_string = combo_string + 1
              if(i == length(sep_colnames)){
                string_concater <- paste0(string_concater, ")")
                formula_string = paste0(formula_string, " + ", string_concater) 
              }
            }
          }
          else{
            colname_pointer = colname
            if(combo_string < 2){
              if(i==2){
                formula_string = paste0(formula_string, colnames_data[i-1]) 
              }
              else if(i>2){
                formula_string = paste0(formula_string, " + ", colnames_data[i-1]) 
              }
            }
            else if(combo_string >=2){
              string_concater <- paste0(string_concater, ")")
              formula_string = paste0(formula_string, " + ", string_concater) 
              string_concater = "Condition("
              combo_string <- 1
            }
          }
        }
        #print(paste0("Iteration ",i))
        #print(paste0("Formula: ", formula_string))
      }
      return(formula_string)
    }
    
    if(nrow(spatial_df)!=0){
      if(nrow(response_df) == nrow(spatial_df)){
        response_df <- cbind(response_df, spatial_df)
        writeLines("Successfully Concated Spatial DF to Response DF")
      }
      else{
        writeLines("Row Difference between Response and Spatial Dataframes! Expected the Same row, Spatial DF ignored")
      }
    }
    
    #========================Model CCA=============================
    ccamodel <- cca(response_df~., explanatory_df)
    finalmodel<- ordistep(ccamodel, scope=formula(ccamodel))
    print(vif.cca(finalmodel))
    
    ccavectors <- as.matrix(scores(ccamodel, display = "bp", scaling = "species")*12.2) %>% 
      as.data.frame()
    
    site_data <- scores(ccamodel, display = "sites") %>% 
      as.data.frame() 
    
    species_data <- scores(ccamodel, display = "species") %>% 
      as.data.frame()
    
    writeLines("Testing the significance of the CCA model:")
    anova.cca(finalmodel)
    writeLines("Testing the significance of terms (environmental variables):")
    anova.cca(finalmodel, by="terms")
    writeLines("Testing the significance of CCA axes (at least the first two or three should present a significant p value):")
    anova.cca(finalmodel, by="axis")
    
    writeLines("plot display notes:")
    writeLines("sp = species")
    writeLines("cn = environmental gradients")
    writeLines("wa = sites")
    
    x11()
    plot(finalmodel, display=c("sp","cn","wa"))
    
    if(partial_analysis){
      #========================Partial CCA=============================
      explanatory_colnames = colnames(explanatory_df)
      explanatory_colnames = sort(explanatory_colnames)
      
      partial_formula = formula(paste0("response_df ~ ", 
                                       partial_cca_formula_maker(explanatory_colnames)))
      all_df = cbind(response_df, explanatory_df)
      partialccamodel<-cca(partial_formula, explanatory_df)
      
      finalmodelpartial<- ordistep(partialccamodel, scope=formula(partialccamodel))
      print(vif.cca(finalmodelpartial))
      
      writeLines("Testing the significance of the Partial CCA model:")
      print(anova.cca(finalmodelpartial))
      writeLines("Testing the significance of terms (environmental variables) Partial CCA Model:")
      print(anova.cca(finalmodelpartial, by="terms"))
      writeLines("Testing the significance of CCA axes (at least the first two or three should present a significant p value) Partial CCA Model:")
      print(anova.cca(finalmodelpartial, by="axis"))
      
      x11()
      plot(finalmodelpartial, display=c("sp","cn","wa"))
      return(list(finalmodel, finalmodelpartial, ccavectors, site_data, species_data))
    }
    return(list(finalmodel,ccavectors, site_data, species_data))
  }
  cca_result <- canonical_correspondence_analysis(dataset_community, dataset_env, partial_analysis = FALSE)
  detach(dataset_env)
  return(list(sppr, div_plot_shannon_df, div_plot_simpson_df, manova_permutation, 
              redudancy_analysis, div_plot_invsimpson_df, NMDS, fit, cca_result))
}

test <- ecology_dataset_analysis(mite, mite.env, independent_var = "Substrate")

#--------------------------------------------------------------------------------------------------------------------------
############### 33) Spatial/GIS Related Analysis ##########################################################################
#--------------------------------------------------------------------------------------------------------------------------
fast_spatial_plot <- function(data, labeldata, 
                              use="mapview", divide_by="group",
                              group_divider="",
                              divide_polygon_map_kmeans=18, map_paddings=8){
  library(mapview)
  library(tidyverse)
  library(leaflet)
  library(rvest)
  library(stringr)
  library(rlang)
  
  if(class(data)[1] %in% c('sf')){
    if(use=="mapview"){
      focused_data <- data[,which(colnames(data) %in% c(labeldata, "geometry"))]
      mapview(focused_data) 
    }
    else if(use!="mapview"){
      sym_label <- sym(labeldata)
      coordinate_extract <- st_coordinates(data$geometry)
      coordinate_extract$label <- labeldata
      if(class(data$geometry)[1] %in% c("sfc_POINT")){
        writeLines("Using Leaflet Platform")
        data %>% leaflet() %>% addTiles() %>% 
          addMarkers(popup = paste0(labeldata,": ", data[[labeldata]]), 
                     clusterOptions = markerClusterOptions()) 
      }
      else if(class(data$geometry)[1] %in% c("sfc_MULTIPOLYGON")){
        writeLines("Using ggplot with geom_sf() Platform")
        if(divide_by == "group"){
          data_points <- cbind(data, st_coordinates(st_centroid(data$geometry)))
          data_points$group_numeric <- as.numeric(as.factor(data_points[[group_divider]]))
          data$group_numeric <- data_points$group_numeric
          total_unique_group <- length(unique(data$group_numeric))
          for(geom_plot in 1:(total_unique_group + 1)){
            if(geom_plot == 1){
              x11()
              j <- ggplot(data = data) + 
                geom_sf(fill= "antiquewhite") + 
                geom_text(data = data_points, aes(x=X, y=Y, label=data[[labeldata]]), 
                          color = "darkblue", fontface = "bold", check_overlap = FALSE) + 
                coord_sf(expand = FALSE) + 
                xlab("Longitude") + 
                ylab("Latitude") + 
                ggtitle("Whole Map") +
                theme(panel.grid.major = element_line(color = gray(.5), linetype = "dashed", size = 0.5), 
                      panel.background = element_rect(fill = "aliceblue"))
              print(j)
            }
            else if(geom_plot > 1){
              data_clust <- data[which(data$group_numeric==(geom_plot-1)),]
              data_points_clust <- data_points[which(data_points$group_numeric==(geom_plot-1)),]
              group_name <- data_points_clust[[group_divider]][1]
              x11()
              j <- ggplot(data = data_clust) + 
                geom_sf(fill= "antiquewhite") + 
                geom_text(data = data_points_clust, aes(x=X, y=Y, label=data_clust[[labeldata]]), 
                          color = "darkblue", fontface = "bold", check_overlap = FALSE) + 
                coord_sf(xlim = c(min(data_points_clust$X) - map_paddings, max(data_points_clust$X) + map_paddings), 
                         ylim = c(min(data_points_clust$Y) - map_paddings, max(data_points_clust$Y) + map_paddings), 
                         expand = FALSE) + 
                xlab("Longitude") + 
                ylab("Latitude") + 
                ggtitle(paste0(group_divider," of ",group_name," Whole Map")) +
                theme(panel.grid.major = element_line(color = gray(.5), linetype = "dashed", size = 0.5), 
                      panel.background = element_rect(fill = "aliceblue"))
              print(j)
            }
          }
        }
        else if(divide_by == "cluster"){
          data_points <- cbind(data, st_coordinates(st_centroid(data$geometry)))
          coord <- data.frame(data_points[,c("X","Y")])
          b <- kmeans(coord[,c(1,2)], divide_polygon_map_kmeans)
          data_points$clusters <- b$cluster
          data$clusters <- b$cluster
          sym_label <- sym(labeldata)
          for(geom_plot in 1:(divide_polygon_map_kmeans + 1)){
            if(geom_plot == 1){
              x11()
              j <- ggplot(data = data) + 
                geom_sf(fill= "antiquewhite") + 
                geom_text(data = data_points, aes(x=X, y=Y, label=data[[labeldata]]), 
                          color = "darkblue", fontface = "bold", check_overlap = FALSE) + 
                coord_sf(expand = FALSE) + 
                xlab("Longitude") + 
                ylab("Latitude") + 
                ggtitle("Whole Map") +
                theme(panel.grid.major = element_line(color = gray(.5), linetype = "dashed", size = 0.5), 
                      panel.background = element_rect(fill = "aliceblue"))
              print(j)
            }
            else if(geom_plot > 1){
              data_clust <- data[which(data$clusters==(geom_plot-1)),]
              data_points_clust <- data_points[which(data_points$clusters==(geom_plot-1)),]
              x11()
              j <- ggplot(data = data_clust) + 
                geom_sf(fill= "antiquewhite") + 
                geom_text(data = data_points_clust, aes(x=X, y=Y, label=data_clust[[labeldata]]), 
                          color = "darkblue", fontface = "bold", check_overlap = FALSE) + 
                coord_sf(xlim = c(min(data_points_clust$X) - map_paddings, max(data_points_clust$X) + map_paddings), 
                         ylim = c(min(data_points_clust$Y) - map_paddings, max(data_points_clust$Y) + map_paddings), 
                         expand = FALSE) + 
                xlab("Longitude") + 
                ylab("Latitude") + 
                ggtitle(paste0("Cluster ",(geom_plot-1)," Whole Map")) +
                theme(panel.grid.major = element_line(color = gray(.5), linetype = "dashed", size = 0.5), 
                      panel.background = element_rect(fill = "aliceblue"))
              print(j)
            }
          }
        }
      }
    }
  }
}

#library("rnaturalearth")
#library("rnaturalearthdata")
#fast_spatial_plot(world, labeldata="name", use="notmapview", divide_by="group", group_divider="subregion")

#--------------------------------------------------------------------------------------------------------------------------
######################### 34) Text Data/Sentiment Analysis ################################################################
#--------------------------------------------------------------------------------------------------------------------------
#focus either sentiment_analysis or word_token
sentiment_word_tokenizer <- function(df, text_col, use_spark=FALSE, add_sentiment_to_df=TRUE, 
                           focus="sentiment_analysis"){
  library(tidyverse)      # data manipulation & plotting
  library(stringr)        # text cleaning and regular expressions
  library(tidytext) 
  library(sparklyr)
  library(textdata)
  library(rlang)
  
  if(use_spark){
    #df is treated as spark df
    sym_text <- sym(text_col)
    df <- df %>% mutate(no_comment = 1:n())
    clean <- df %>% select(no_comment, !!sym_text) %>%
      unnest_tokens(word, !!sym_text) %>%
      select(everything()) %>%
      left_join(get_sentiments("afinn")) %>%
      rename(afinn_sentiment = value) %>%
      left_join(get_sentiments("bing")) %>%
      rename(bing_sentiment = sentiment) %>%
      left_join(get_sentiments("nrc")) %>%
      rename(nrc_sentiment = sentiment)
    
    clean_sentiment_value <- clean %>% 
      group_by(no_comment) %>% 
      summarise(afinn_sentiment_value=sum(afinn_sentiment, na.rm=TRUE))
    
    clean_sentiment_logic <- clean %>% 
      group_by(no_comment, bing_sentiment) %>% 
      summarise(bing_sentiment_count=n()) %>%
      spread(bing_sentiment, bing_sentiment_count, fill = 0) %>%
      rename(bing_negative = negative) %>%
      rename(bing_positive = positive) %>%
      rename(bing_unclassified = `<NA>`) 
    
    clean_sentiment_category <- clean %>% 
      group_by(no_comment, nrc_sentiment) %>% 
      summarise(nrc_sentiment_count=n()) %>%
      spread(nrc_sentiment, nrc_sentiment_count, fill = 0) %>%
      rename(nrc_anger = anger) %>%
      rename(nrc_anticipation = anticipation) %>%
      rename(nrc_disgust = disgust) %>%
      rename(nrc_fear = fear) %>%
      rename(nrc_joy = joy) %>%
      rename(nrc_negative = negative) %>%
      rename(nrc_positive = positive) %>%
      rename(nrc_sadness = sadness) %>%
      rename(nrc_surprise = surprise) %>%
      rename(nrc_trust = trust) %>%
      rename(nrc_unclassified = `<NA>`)
    
    print(clean_sentiment_value)
    print(clean_sentiment_logic)
    print(clean_sentiment_category)
    
    word_summarise <- clean %>% group_by(no_comment) %>% 
      summarise(count_word = n()) %>% 
      arrange(no_comment)
    
    writeLines("Minimal Length of Text")
    print(word_summarise[which(word_summarise$count_word == min(word_summarise$count_word)),])
    writeLines("Maximal Length of Text")
    print(word_summarise[which(word_summarise$count_word == max(word_summarise$count_word)),])
    
    if(add_sentiment_to_df){
      df <- df %>%
        left_join(clean_sentiment_value, by=c("no_comment")) %>%
        left_join(clean_sentiment_logic, by=c("no_comment")) %>%
        left_join(clean_sentiment_category, by=c("no_comment"))
    }
    if(focus == "sentiment_analysis"){
      return(df)
    }else if(focus == "word_token"){
      return(list(clean, word_summarise))
    }
    
  }
  else{
    df$no_comment <- as.numeric(row.names(df))
    clean <- tibble(no_comment = df$no_comment,
                    text = df[[text_col]]) %>%
      unnest_tokens(word, text) %>%
      select(everything()) %>%
      left_join(get_sentiments("afinn")) %>%
      rename(afinn_sentiment = value) %>%
      left_join(get_sentiments("bing")) %>%
      rename(bing_sentiment = sentiment) %>%
      left_join(get_sentiments("nrc")) %>%
      rename(nrc_sentiment = sentiment)
    
    clean_sentiment_value <- clean %>% 
      group_by(no_comment) %>% 
      summarise(afinn_sentiment_value=sum(afinn_sentiment, na.rm=TRUE))
    
    clean_sentiment_logic <- clean %>% 
      group_by(no_comment, bing_sentiment) %>% 
      summarise(bing_sentiment_count=n()) %>%
      spread(bing_sentiment, bing_sentiment_count, fill = 0) %>%
      rename(bing_negative = negative) %>%
      rename(bing_positive = positive) %>%
      rename(bing_unclassified = `<NA>`) 
    
    clean_sentiment_category <- clean %>% 
      group_by(no_comment, nrc_sentiment) %>% 
      summarise(nrc_sentiment_count=n()) %>%
      spread(nrc_sentiment, nrc_sentiment_count, fill = 0) %>%
      rename(nrc_anger = anger) %>%
      rename(nrc_anticipation = anticipation) %>%
      rename(nrc_disgust = disgust) %>%
      rename(nrc_fear = fear) %>%
      rename(nrc_joy = joy) %>%
      rename(nrc_negative = negative) %>%
      rename(nrc_positive = positive) %>%
      rename(nrc_sadness = sadness) %>%
      rename(nrc_surprise = surprise) %>%
      rename(nrc_trust = trust) %>%
      rename(nrc_unclassified = `<NA>`)
    
    print(clean_sentiment_value)
    print(clean_sentiment_logic)
    print(clean_sentiment_category)
    
    word_summarise <- clean %>% group_by(no_comment) %>% 
      summarise(count_word = n()) %>% 
      arrange(no_comment)
    
    writeLines("Minimal Length of Text")
    print(word_summarise[which(word_summarise$count_word == min(word_summarise$count_word)),])
    writeLines("Maximal Length of Text")
    print(word_summarise[which(word_summarise$count_word == max(word_summarise$count_word)),])
    
    if(add_sentiment_to_df){
      df <- df %>%
        left_join(clean_sentiment_value, by=c("no_comment")) %>%
        left_join(clean_sentiment_logic, by=c("no_comment")) %>%
        left_join(clean_sentiment_category, by=c("no_comment"))
    }
    if(focus == "sentiment_analysis"){
      return(df)
    }else if(focus == "word_token"){
      return(list(clean, word_summarise))
    }
  }
}

#--------------------------------------------------------------------------------------------------------------------------
######################### 35) Event Log Data Analysis with bupaR ##########################################################
#--------------------------------------------------------------------------------------------------------------------------

eventlog_data_transform <- function(df, subject_col="", activity_col="", stage_lifecycle_col="",
                                    resource_col="",resource_prefix="worker-", timestamp_col=""){
  
  data <- df[,which(colnames(df) %in% c(subject_col, activity_col, resource_col, timestamp_col)]
  data[,which(colnames(df)==subject_col)] <- as.character(data[,which(colnames(df)==subject_col)])
  data[,which(colnames(df)==activity_col)] <- as.character(data[,which(colnames(df)==activity_col)])
  data[,which(colnames(df)==resource_col)] <- paste0(resource_prefix, 
                                                     as.character(data[,which(colnames(df)==resource_col)]))
  data[,which(colnames(df)==stage_lifecycle_col)] <- paste0(resource_prefix, 
                                                            as.character(data[,which(colnames(df)==stage_lifecycle_col)]))
  
  if(!(class(data[[timestamp_col]]) == "Date" || class(data[[timestamp_col]]) %in% c("POSIXct"))){
    stop("Time Stamp need to be a type of Date or POSIXct")
  }
  data$activity_instance_id <- 1:nrow(data)
  
  
  #http://ceur-ws.org/Vol-1527/paper18.pdf
  #https://stackoverflow.com/questions/48897542/trouble-converting-data-frame-to-eventlog-in-bupar-r-package
  writeLines("case id The case to which the event belongs.")
  writeLines("activity id The activity the event refers to.")
  writeLines("activity instance id The activity instance the event belongs to.")
  writeLines("lifecycle id The stage in the transactional life cylce.")
  writeLines("timestamp The timestamp of the event.")
  
  log_data <- eventlog(data, case_id = subject_col,
                       activity_id = activity_col,
                       activity_instance_id = "activity_instance_id",
                       lifecycle_id = stage_lifecycle_col,
                       timestamp = timestamp_col,
                       resource_id = resource_col)
  return(log_data)
}

precedent_each_subject <- function(df, subject_var="", lifecycle_var="", 
                                   lifecycle_target="complete", 
                                   activity_var="", time_var=""){
  library(dplyr)
  library(rlang)
  library(data.table)
  
  df <- as.data.frame(df)
  
  if(lifecycle_var != ""){
    if(length(unique(df[[lifecycle_var]])) > 1){
      sym_lifecycle <- sym(lifecycle_var)
      df <- df %>% filter(!!sym_lifecycle == lifecycle_target)
    }
  }
  
  sym_subject <- NULL
  sym_activity <- NULL
  sym_time <- NULL
  if(subject_var == "" || activity_var == ""){
    stop("Subject and Activity Variable must be defined!")
  }
  else{
    sym_subject <- sym(subject_var)
    sym_activity <- sym(activity_var)
  }
  
  if(time_var != ""){
    sym_time <- sym(time_var) 
  }
  
  df <- df[!duplicated(df, by=c(activity_var, subject_var, time_var)),]
  
  if(time_var != ""){
    sym_time <- sym(time_var) 
    df <- df %>% arrange(!!sym_subject, !!sym_time)
  }
  else{
    df <- df %>% arrange(!!sym_subject)
  }
  
  activity_length <- length(levels(df[,which(colnames(df) == activity_var)]))
  subject_length <- length(unique(df[,which(colnames(df) == subject_var)]))
  subject_uniques <- unique(df[,which(colnames(df) == subject_var)])
  prec_each_subject <- c()
  length_prec <- c()
  for(x in 1:subject_length){
    df_per_subject <- df %>% filter(!!sym_subject == subject_uniques[x])
    order_activity <- as.character(df_per_subject[[activity_var]])
    prec_each_subject <- c(prec_each_subject, paste0(order_activity, collapse=" -> "))
    length_prec <- c(length_prec, length(order_activity))
  }
  prec_subject_df <- data.frame(pattern=prec_each_subject, length_pattern=length_prec)
  return(prec_subject_df)
}

precedent_n_sequences_analysis <- function(df, subject_var="", lifecycle_var="", 
                                           lifecycle_target="complete",
                                           activity_var="", time_var="", 
                                           check_sequences=2, classify_seq_to_others=FALSE,
                                           classify_others_threshold=5, 
                                           order_sequence_names=FALSE){
  library(dplyr)
  library(rlang)
  library(data.table)
  
  df <- as.data.frame(df)
  
  if(lifecycle_var != ""){
    if(length(unique(df[[lifecycle_var]])) > 1){
      sym_lifecycle <- sym(lifecycle_var)
      df <- df %>% filter(!!sym_lifecycle == lifecycle_target)
    }
  }
  
  sym_subject <- NULL
  sym_activity <- NULL
  sym_time <- NULL
  if(subject_var == "" || activity_var == ""){
    stop("Subject and Activity Variable must be defined!")
  }
  else{
    sym_subject <- sym(subject_var)
    sym_activity <- sym(activity_var)
  }
  
  if(time_var != ""){
    sym_time <- sym(time_var) 
  }
  
  writeLines("Since Lifecycle variable is not defined, Focus on Activity Pattern")
  df <- df[!duplicated(df, by=c(activity_var, subject_var, time_var)),]
  subject_order <- c()
  activity_order <- c()
  activity_list <- list()
  start_vector <- c()
  finish_vector <- c()
  subject_exception <- c()
  
  if(time_var != ""){
    sym_time <- sym(time_var) 
    df <- df %>% arrange(!!sym_subject, !!sym_time)
  }
  else{
    df <- df %>% arrange(!!sym_subject)
  }
  
  activity_length <- length(levels(df[,which(colnames(df) == activity_var)]))
  subject_length <- length(unique(df[,which(colnames(df) == subject_var)]))
  subject_uniques <- unique(df[,which(colnames(df) == subject_var)])
  for(x in 1:subject_length){
    df_per_subject <- df %>% filter(!!sym_subject == subject_uniques[x])
    order_activity <- as.character(df_per_subject[[activity_var]])
    if(length(order_activity) < check_sequences){
      message(paste0("Subject ",subject_uniques[x], " Only have ",length(order_activity), " Activity! Cannot make sequences of ", check_sequences, " So it is Skipped!"))
      subject_exception <- c(subject_exception, subject_uniques[x])
    }
    else{
      if(check_sequences == 2){
        start_vector <- c(start_vector, order_activity[1])
        finish_vector <- c(finish_vector, order_activity[length(order_activity)])
      }
      else if(check_sequences > 2){
        start_vector <- c(start_vector, paste0(order_activity[1:(check_sequences-1)], collapse=" -> "))
        finish_vector <- c(finish_vector, paste0(order_activity[(length(order_activity)-(check_sequences-2)):length(order_activity)], collapse=" -> ")) 
      }
      sequence_activity_length <- length(order_activity) - (check_sequences - 1)
      for(y in 1:sequence_activity_length){
        subject_order <- c(subject_order, subject_uniques[x])
        activity_order <- c(activity_order, paste0(order_activity[y:(check_sequences + (y-1))], collapse=" -> "))
        activity_list <- c(activity_list, list(order_activity[y:(check_sequences + (y-1))]))
      }
    }
  }
  
  length_list <- length(activity_list)
  pattern_extract <- data.frame(matrix(unlist(activity_list), 
                                       nrow=length_list, byrow=TRUE), stringsAsFactors=FALSE)
  colnames(pattern_extract) <- paste0("Pattern-",seq(1:length(colnames(pattern_extract))))
  pattern_extract$subject_no <- subject_order
  pattern_extract$sequences <- activity_order
  
  if(length(subject_exception) == 0){
    subject_start_finish <- data.frame(subject=subject_uniques, start=start_vector, 
                                       finish=finish_vector)
    subject_start_finish$start_pattern <- paste0("start -> ", subject_start_finish$start)
    subject_start_finish$finish_pattern <- paste0(subject_start_finish$finish, " -> finish")
  }
  else{
    subject_start_finish <- data.frame(subject=subject_uniques[-which(subject_uniques %in% subject_exception)], 
                                       start=start_vector, 
                                       finish=finish_vector)
    subject_start_finish$start_pattern <- paste0("start -> ", subject_start_finish$start)
    subject_start_finish$finish_pattern <- paste0(subject_start_finish$finish, " -> finish")
  }
  
  activity_pattern <- sort(table(pattern_extract$sequences),decreasing=TRUE)
  starting_pattern <- sort(table(subject_start_finish$start_pattern),decreasing=TRUE)
  ending_pattern <- sort(table(subject_start_finish$finish_pattern),decreasing=TRUE)
  
  sequences_frequency <- list(activity_pattern, starting_pattern, ending_pattern)
  
  if(classify_seq_to_others){
    for(lists in 1:length(sequences_frequency)){
      sequences_reclassify <- sequences_frequency[[lists]][sequences_frequency[[lists]] <= classify_others_threshold]
      sequences_remain <- sequences_frequency[[lists]][sequences_frequency[[lists]] > classify_others_threshold]
      names(sequences_reclassify)[1:length(sequences_reclassify)] <- "Others"
      sequence_final_reclassify <- c()
      for(idx_rename in 1:classify_others_threshold){
        sequence_final_reclassify <- c(sequence_final_reclassify,
                                       length(sequences_reclassify[which(sequences_reclassify == idx_rename)]))
        names(sequence_final_reclassify)[idx_rename] <- paste0("Others_freq_",idx_rename)
      }
      sequences_frequency[[lists]] <- c(sequences_remain, sequence_final_reclassify) 
    }
  }
  if(order_sequence_names){
    for(ordering in 1:length(sequences_frequency)){
      sequences_frequency[[ordering]] <- sequences_frequency[[ordering]][order(names(sequences_frequency[[ordering]]))]
    }
  }
  
  activity_pattern <- sequences_frequency[[1]]
  starting_pattern <- sequences_frequency[[2]]
  ending_pattern <- sequences_frequency[[3]]
  
  writeLines("----------------------- Activity Pattern ------------------------")
  print(activity_pattern)
  writeLines("-----------------------------------------------------------------")
  writeLines("----------------------- Starting Pattern ------------------------")
  print(starting_pattern)
  writeLines("-----------------------------------------------------------------")
  writeLines("----------------------- Ending Pattern --------------------------")
  print(ending_pattern)
  writeLines("-----------------------------------------------------------------")
  
  if(check_sequences == 2){
    collected_sequences <- c(pattern_extract$sequences, 
                             subject_start_finish$start_pattern, 
                             subject_start_finish$finish_pattern)
    split_sequences <- strsplit(collected_sequences, split=" -> ")
    length_split <- length(split_sequences)
    collected_pattern <- data.frame(matrix(unlist(split_sequences), 
                                           nrow=length_split, byrow=TRUE), stringsAsFactors=FALSE)
    colnames(collected_pattern) <- c("Antecedent","Consequent")
    print(table(collected_pattern$Antecedent, collected_pattern$Consequent))
    
    return_list <- list(pattern_extract, subject_start_finish, 
                        activity_pattern, starting_pattern, ending_pattern, collected_pattern,
                        table(collected_pattern$Antecedent, collected_pattern$Consequent))
    names(return_list) <- c("Pattern Extract", "Start and Finish Pattern", 
                            "Activity Pattern", "Starting Pattern", "Ending Pattern",
                            "Precedence Dataframe","Precedence Tables")
    return(return_list)
  }
  else{
    return_list <- list(pattern_extract, subject_start_finish, 
                        activity_pattern, starting_pattern, ending_pattern)
    names(return_list) <- c("Pattern Extract", "Start and Finish Pattern", 
                            "Activity Pattern", "Starting Pattern", "Ending Pattern")
    return(return_list)
  }
}

visualize_event_data <- function(log_data, warn_size=1048576, interval_filter=c(), guide_dot_chart=FALSE){
  options(viewer = function(url, height = NULL)
  {
    if (!is.character(url) || (length(url) != 1))
      stop("url must be a single element character vector.", call. = FALSE)
    
    if (identical(height, "maximize"))
      height <- -1
    
    if (!is.null(height) && (!is.numeric(height) || (length(height) != 1)))
      stop("height must be a single element numeric vector or 'maximize'.", call. = FALSE)
    
    invisible(.Call("rs_showPageViewer", url, title = "RStudio", self_contained = FALSE))  
  })
  
  writeLines('1) Create Process Map')
  proc_map1 <- log_data %>% process_map(type = frequency("relative", color_scale = "Blues"))
  proc_map2 <- log_data %>% process_map(type = frequency("absolute", color_scale = "Reds"))
  proc_map3 <- log_data %>% process_map(performance(mean, "hours", color_scale = "Purples"))
  if(as.numeric(object.size(proc_map1)) > warn_size){
    writeLines("Created Relative Process Maps Exceed Threshold!")
    writeLines(paste0("Process Map Size: ",round(as.numeric(object.size(proc_map1))/1048576,1), " MB"))
  }
  if(as.numeric(object.size(proc_map2)) > warn_size){
    writeLines("Created Absolute Process Maps Exceed Threshold!")
    writeLines(paste0("Process Map Size: ",round(as.numeric(object.size(proc_map2))/1048576,1), " MB"))
  }
  if(as.numeric(object.size(proc_map3)) > warn_size){
    writeLines("Created Performance Process Maps Exceed Threshold!")
    writeLines(paste0("Process Map Size: ",round(as.numeric(object.size(proc_map3))/1048576,1), " MB"))
  }
  
  writeLines('2) Create Trace Explorer')
  trac_exp <- log_data %>% trace_explorer(coverage = 1)
  if(as.numeric(object.size(trac_exp)) > warn_size){
    writeLines("Created Trace Explorer Exceed Threshold!")
    writeLines(paste0("Trace Explorer Size: ",round(as.numeric(object.size(trac_exp))/1048576,1), " MB"))
  }
  
  writeLines('3) Create Resource Map')
  res_map <- log_data %>% resource_map()
  if(as.numeric(object.size(res_map)) > warn_size){
    writeLines("Created Resource Maps Exceed Threshold!")
    writeLines(paste0("Resource Map Size: ",as.numeric(object.size(res_map))/1048576, " MB"))
  }
  
  writeLines('4) Create Relative Duration Chart in Hours based by Activity')
  
  if(guide_dot_chart){
    writeLines("With Filter Dates, there are 4 types of filter that is available to view activity timeline in different perspective")
    writeLines("1) Filter = start, view activity timeline start from the input dates into the most updated timeline")
    writeLines("which related to the ranges of input dates")
    writeLines("2) Filter = complete, view activity timeline start from the most prior dates of activity into the end of input dates")
    writeLines("which related to the ranges of input dates")
    writeLines("3) Filter = contained, view activity timeline that is inside only of input date ranges")
    writeLines("4) Filter = intersect, view entire activity timeline, which activity is related to the input dates ranges")
  }
  
  dot_map_rel <- NULL
  dot_map_rel_filstart <- NULL
  dot_map_rel_filcomp <- NULL
  dot_map_rel_filcont <- NULL
  dot_map_rel_filint <- NULL
  
  if(length(filter_date) == 0){
    dot_map_rel <- log_data %>% 
      dotted_chart(log_data, x = "relative", y ="duration", color = NULL, units = "hours")
  }
  else if(length(filter_date) == 2){
    dot_map_rel_filstart <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "start") %>%
      dotted_chart(x = "relative", y ="duration", color = NULL, units = "hours")
    
    dot_map_rel_filcomp <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "complete") %>%
      dotted_chart(x = "relative", y ="duration", color = NULL, units = "hours")
    
    dot_map_rel_filcont <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "contained") %>%
      dotted_chart(x = "relative", y ="duration", color = NULL, units = "hours")
    
    dot_map_rel_filint <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "intersecting") %>%
      dotted_chart(x = "relative", y ="duration", color = NULL, units = "hours")
  }

  writeLines('5) Create Absolute Duration Chart in Hours based by Activity')
  dot_map_abs <- NULL
  dot_map_abs_filstart <- NULL
  dot_map_abs_filcomp <- NULL
  dot_map_abs_filcont <- NULL
  dot_map_abs_filint <- NULL
  
  if(length(filter_date) == 0){
    dot_map_abs <- log_data %>% 
      dotted_chart(log_data, x = "absolute", y ="duration", color = NULL, units = "hours")
  }
  else if(length(filter_date) == 2){
    dot_map_abs_filstart <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "start") %>%
      dotted_chart(x = "absolute", y ="duration", color = NULL, units = "hours")

    dot_map_abs_filcomp <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "complete") %>%
      dotted_chart(x = "absolute", y ="duration", color = NULL, units = "hours")
    
    dot_map_rel_filcont <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "contained") %>%
      dotted_chart(x = "absolute", y ="duration", color = NULL, units = "hours")
    
    dot_map_rel_filint <- log_data %>% 
      filter_time_period(interval = filter_date, filter_method = "intersecting") %>%
      dotted_chart(x = "absolute", y ="duration", color = NULL, units = "hours")
  }
  
  if(length(filter_date) == 0){
    return(list(proc_map1, proc_map2, proc_map3,
                trac_exp, res_map, dot_map_rel, dot_map_abs))
  }
  else if(length(filter_date) == 2){
    return(list(proc_map1, proc_map2, proc_map3,
                trac_exp, res_map, 
                dot_map_rel_filstart, dot_map_rel_filcomp, dot_map_rel_filcont, dot_map_rel_filint, 
                dot_map_abs_filstart, dot_map_abs_filcomp, dot_map_abs_filcont, dot_map_abs_filint))
  }
}

anomaly_activity <- function(log_data, activity_order=c(), 
                             threshold_sec_multiactivity=30,
                             default_activity_occurence=1,
                             label_similiar_distance=4,
                             check_user_defined=c(""),
                             check_user_bottom_top=c()){
  library(bupaR)
  library(daqapo)
  writeLines("1) Activity Repeated Multiple Times")
  evaluate_rep <- "log_data %>% detect_activity_frequency_violations("
  for(a in 1:length(activity_order)){
    if(a == length(activity_order)){
      evaluate_rep <- paste0(evaluate_rep, '"', activity_order[a], '" = ', default_activity_occurence,")")
    }
    else{
      evaluate_rep <- paste0(evaluate_rep, '"', activity_order[a], '" = ', default_activity_occurence,",")
    }
  }
  eval(parse(text=evaluate_rep))
  
  log_data %>%
    detect_multiregistration(threshold_in_seconds = threshold_sec_multiactivity)
  
  writeLines("2) Activity with Uncommon Order")
  log_data %>%
    detect_activity_order_violations(activity_order = activity_order)
  
  writeLines("3) Missing Case ID in log_data")
  log_data %>%
    detect_case_id_sequence_gaps()
  
  writeLines("4) Anomalies Activity that are not in Activity Order")
  log_data %>%
    detect_incorrect_activity_names(allowed_activities = c("Registration","Triage","Clinical exam",
                                                           "Treatment","Treatment evaluation"))
  
  writeLines("5) Spot All Missing NA values")
  log_data %>%
    detect_missing_values()
  
  writeLines("6) Anomaly based by user-defined values of data")
  evaluate_user_defined <- "hospital_actlog %>% "
  
  if(length(check_user_defined) != 2*length(check_user_bottom_top)){
    stop("Error, the User Defined values of bottom top and columns didn't match the right length")
  }
  else{
    for(b in 1:length(check_user_defined)){
      if(b == 1)){
        if(b == length(check_user_defined){
          evaluate_user_defined  <- paste0(evaluate_user_defined, "detect_value_range_violations(",check_user_defined[b], ' = domain_numeric(from = ', 
                                 check_user_bottom_top[2*b - 1], ",to = ", check_user_bottom_top[2*b],"))")
        }
        else{
          evaluate_user_defined  <- paste0(evaluate_user_defined, "detect_value_range_violations(",check_user_defined[b], ' = domain_numeric(from = ', 
                                 check_user_bottom_top[2*b - 1], ",to = ", check_user_bottom_top[2*b],"),")
        }
      }
      else if(b == length(check_user_defined)){
        evaluate_user_defined  <- paste0(evaluate_user_defined, check_user_defined[b],' = domain_numeric(from = ', 
                               check_user_bottom_top[2*b - 1], ",to = ", check_user_bottom_top[2*b],"))")
      }
      else{
        evaluate_user_defined  <- paste0(evaluate_user_defined, check_user_defined[b],' = domain_numeric(from = ', 
                               check_user_bottom_top[2*b - 1], ",to = ", check_user_bottom_top[2*b],"),")
      }
    }
  }
  
  eval(parse(text = evaluate_user_defined))
}

pattern_over_pattern <- function(subject_pattern, pattern1, pattern2){
  library(stringr)
  str_a_to_b <- paste0(pattern1, " -> ", pattern2)
  str_b_to_a <- paste0(pattern2, " -> ", pattern1)
  a_location <- str_locate(subject_pattern, pattern1)
  b_location <- str_locate(subject_pattern, pattern2)
  start_a <- as.numeric(a_location[,1])
  start_b <- as.numeric(b_location[,1])
  contains_a <- grepl(pattern1, subject_pattern)
  contains_b <- grepl(pattern2, subject_pattern)
  straight_a_to_b <- grepl(str_a_to_b, subject_pattern)
  straight_b_to_a <- grepl(str_b_to_a, subject_pattern)
  indirect_a_to_b <- ifelse(!is.na(start_a) & !is.na(start_b) & start_a < start_b, TRUE, FALSE)
  indirect_b_to_a <- ifelse(!is.na(start_a) & !is.na(start_b) & start_b < start_a, TRUE, FALSE)
  clean_indirect_a_to_b <- ifelse(straight_a_to_b == FALSE & indirect_a_to_b == TRUE, TRUE, FALSE)
  clean_indirect_b_to_a <- ifelse(straight_b_to_a == FALSE & indirect_b_to_a == TRUE, TRUE, FALSE)
  writeLines(paste0("Pattern that Contains ", pattern1))
  print(sum(contains_a))
  writeLines(paste0("Pattern that Contains ", pattern2))
  print(sum(contains_b))
  writeLines(paste0("Pattern that Have direct relationship from ", pattern1, " over ", pattern2))
  print(sum(straight_a_to_b))
  writeLines(paste0("Pattern that Have direct relationship from ", pattern2, " over ", pattern1))
  print(sum(straight_b_to_a))
  writeLines(paste0("Pattern that Have indirect relationship from ", pattern1, " over ", pattern2))
  print(sum(indirect_a_to_b))
  writeLines(paste0("Pattern that Have indirect relationship from ", pattern2, " over ", pattern1))
  print(sum(indirect_b_to_a))
  writeLines(paste0("Pattern that are pure indirect (Not bound with direct) from ", pattern1, " over ", pattern2))
  print(sum(clean_indirect_a_to_b))
  writeLines(paste0("Pattern that are pure indirect (Not bound with direct) from ", pattern2, " over ", pattern1))
  print(sum(clean_indirect_b_to_a))
  p2p_df <- data.frame(subject_pattern, contains_a, contains_b, straight_a_to_b, straight_b_to_a, 
                       indirect_a_to_b, indirect_b_to_a, clean_indirect_a_to_b, clean_indirect_b_to_a)
  return(p2p_df)
}

multiactivity_check <- function(subject_pattern){
  subject_length <- length(subject_pattern)
  all_activity <- unique(unlist(strsplit(x=subject_pattern, split=" -> ")))
  activity_freq <- list()
  activity_listed <- strsplit(x=subject_pattern, split=" -> ")
  for(act in 1:length(all_activity)){
    activity_freq <- c(activity_freq, lapply(activity_listed, FUN = function(x) sum(grepl(all_activity[act],x))))
  }
  df <- data.frame(matrix(unlist(activity_freq), nrow=subject_length))
  colnames(df) <- all_activity
  df$contains_multiactivity <- apply(df, FUN = function(x) any(x > 1), MARGIN = 1)
  return(df)
}

#library(eventdataR)
#data("sepsis")
#sepsis_pattern <- precedent_each_subject(sepsis, subject_var="case_id", 
#                                         activity_var="activity", time_var="timestamp")
#sepsis_prec3 <- precedent_n_sequences_analysis(sepsis, subject_var="case_id", 
#                                           activity_var="activity", time_var="timestamp",
#                                           check_sequences=3, classify_seq_to_others=FALSE,
#                                           classify_others_threshold=5, 
#                                           order_sequence_names=FALSE)
#sepsis_prec10 <- precedent_n_sequences_analysis(sepsis, subject_var="case_id", 
#                                           activity_var="activity", time_var="timestamp",
#                                           check_sequences=10, classify_seq_to_others=FALSE,
#                                           classify_others_threshold=5, 
#                                           order_sequence_names=FALSE)
#sepsis_ab <- pattern_over_pattern(sepsis_pattern$pattern, "ER Sepsis Triage", "CRP")
#sepsis_mult <- multiactivity_check(sepsis_pattern$pattern)

#--------------------------------------------------------------------------------------------------------------------------
################## 36) Variable Selection for Best Regression and Classification Model ################################################
#--------------------------------------------------------------------------------------------------------------------------

moving_value_statistics <- function(vec, vec_name="", size=c(10,20), 
                                    include_metric_range = FALSE,
                                    set_as_prediction_var = TRUE){
  library(data.table)
  library(dplyr)
  variable_naming <- c()
  getmode <- function(v) {
     uniqv <- unique(v)
     return(uniqv[which.max(tabulate(match(v, uniqv)))])
  }
  getmode_density <- function(v){
    tablev <- table(v)
    return(as.numeric(sort(tablev, decreasing = TRUE)[1]))
  }
  mov_df <- data.frame(vec)
  colnames(mov_df)[1] <- vec_name 
  for(f in 1:length(size)){
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Min"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Q1"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Median"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Mean"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Mode"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_ModeN"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Q3"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Max"))
    variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_Sum"))
    if(include_metric_range){
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_Q1Min"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_MedianQ1"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_ModeQ1"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_MeanQ1"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_MedianMode"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_MeanMode"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_MeanMedian"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_Q3Median"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_Q3Mode"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_Q3Mean"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_MaxQ3"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_MaxMin"))
      variable_naming <- c(variable_naming, paste0(vec_name,"_MovS",size[f],"_R_InterQ"))
    }
  }
  for(g in 1:length(size)){
    if(include_metric_range){
      mov_df[[variable_naming[1 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) min(x), fill=NA, align="right")
      mov_df[[variable_naming[2 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) quantile(x, 0.25), fill=NA, align="right")
      mov_df[[variable_naming[3 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) median(x), fill=NA, align="right")
      mov_df[[variable_naming[4 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) mean(x), fill=NA, align="right")
      mov_df[[variable_naming[5 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) getmode(round(x)), fill=NA, align="right")
      mov_df[[variable_naming[6 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) getmode_density(round(x)), fill=NA, align="right")
      mov_df[[variable_naming[7 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) quantile(x, 0.75), fill=NA, align="right")
      mov_df[[variable_naming[8 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) max(x), fill=NA, align="right")
      mov_df[[variable_naming[9 + (22 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) sum(x), fill=NA, align="right")
      mov_df[[variable_naming[10 + (22 * (g-1))]]] <- mov_df[[variable_naming[2 + (22 * (g-1))]]] - mov_df[[variable_naming[1 + (22 * (g-1))]]]
      mov_df[[variable_naming[11 + (22 * (g-1))]]] <- mov_df[[variable_naming[3 + (22 * (g-1))]]] - mov_df[[variable_naming[2 + (22 * (g-1))]]]
      mov_df[[variable_naming[12 + (22 * (g-1))]]] <- mov_df[[variable_naming[5 + (22 * (g-1))]]] - mov_df[[variable_naming[2 + (22 * (g-1))]]]
      mov_df[[variable_naming[13 + (22 * (g-1))]]] <- mov_df[[variable_naming[4 + (22 * (g-1))]]] - mov_df[[variable_naming[2 + (22 * (g-1))]]]
      mov_df[[variable_naming[14 + (22 * (g-1))]]] <- mov_df[[variable_naming[3 + (22 * (g-1))]]] - mov_df[[variable_naming[5 + (22 * (g-1))]]]
      mov_df[[variable_naming[15 + (22 * (g-1))]]] <- mov_df[[variable_naming[4 + (22 * (g-1))]]] - mov_df[[variable_naming[5 + (22 * (g-1))]]]
      mov_df[[variable_naming[16 + (22 * (g-1))]]] <- mov_df[[variable_naming[4 + (22 * (g-1))]]] - mov_df[[variable_naming[3 + (22 * (g-1))]]]
      mov_df[[variable_naming[17 + (22 * (g-1))]]] <- mov_df[[variable_naming[7 + (22 * (g-1))]]] - mov_df[[variable_naming[3 + (22 * (g-1))]]]
      mov_df[[variable_naming[18 + (22 * (g-1))]]] <- mov_df[[variable_naming[7 + (22 * (g-1))]]] - mov_df[[variable_naming[5 + (22 * (g-1))]]]
      mov_df[[variable_naming[19 + (22 * (g-1))]]] <- mov_df[[variable_naming[7 + (22 * (g-1))]]] - mov_df[[variable_naming[4 + (22 * (g-1))]]]
      mov_df[[variable_naming[20 + (22 * (g-1))]]] <- mov_df[[variable_naming[8 + (22 * (g-1))]]] - mov_df[[variable_naming[7 + (22 * (g-1))]]]
      mov_df[[variable_naming[21 + (22 * (g-1))]]] <- mov_df[[variable_naming[8 + (22 * (g-1))]]] - mov_df[[variable_naming[1 + (22 * (g-1))]]]
      mov_df[[variable_naming[22 + (22 * (g-1))]]] <- mov_df[[variable_naming[7 + (22 * (g-1))]]] - mov_df[[variable_naming[2 + (22 * (g-1))]]]
    }else{
      mov_df[[variable_naming[1 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) min(x), fill=NA, align="right")
      mov_df[[variable_naming[2 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) quantile(x, 0.25), fill=NA, align="right")
      mov_df[[variable_naming[3 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) median(x), fill=NA, align="right")
      mov_df[[variable_naming[4 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) mean(x), fill=NA, align="right")
      mov_df[[variable_naming[5 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) getmode(round(x)), fill=NA, align="right")
      mov_df[[variable_naming[6 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) getmode_density(round(x)), fill=NA, align="right")
      mov_df[[variable_naming[7 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) quantile(x, 0.75), fill=NA, align="right")
      mov_df[[variable_naming[8 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) max(x), fill=NA, align="right")
      mov_df[[variable_naming[9 + (9 * (g-1))]]] <- frollapply(vec, n=size[g], FUN= function(x) sum(x), fill=NA, align="right")
    }
  }
  
  mov_df <- na.omit(mov_df)
  
  if(set_as_prediction_var){
    #set window - 1 because the last observation of not NA value is still included in Moving Value Statistics  
    mov_df[[vec_name]][1:(nrow(mov_df) - 1)] <- mov_df[[vec_name]][2:nrow(mov_df)]
    mov_df <- mov_df[-nrow(mov_df),]
  }
  
  rownames(mov_df) <- NULL
  mov_df <- mapply(mov_df, FUN = function(x) round(x,3)) %>% as.data.frame()
  return(mov_df)
}

drop_neighbour_variable_stats <- function(data, dependent_col){
  all_lm <- lm(as.formula(paste0(dependent_col," ~ .")), data = data)
  current_r2 <- summary(all_lm)$r.squared
  current_r2adj <- summary(all_lm)$adj.r.squared
  all_coef_names <- names(all_lm$coefficients)
  mov_unique <- unique(unlist(lapply(strsplit(names(all_lm$coefficients), "_"), FUN = function(x) x[2])))[-1]
  stats_unique <- unique(unlist(lapply(strsplit(names(all_lm$coefficients), mov_unique[1]), FUN = function(x) x[2])))[-1]
  data_dropped <- data
  dropped_r2 <- 0
  dropped_r2adj <- 0
  lm_dropped <- all_lm
  while(1){
    writeLines("All Stats Currently Included in Model")
    print(stats_unique)
    temp_r2 <- 0
    temp_adj_r2_square <- 0
    for(a in 1:length(stats_unique)){
      writeLines(paste0("Models Performance When ", stats_unique[a], " is dropped"))
      filter_vars_choose <- NULL
      if(stats_unique[a] %in% c("_Min","_Q1","_Mode","_Median","_Mean","_Max","_Q3")){
        filter_vars_choose <- which(colnames(data_dropped) %like% stats_unique[a] & !colnames(data_dropped) %like% "_R_")
      }else{
        filter_vars_choose <- which(colnames(data_dropped) %like% stats_unique[a])
      }
      datatemp <- data_dropped[,-filter_vars_choose]
      temp_lm <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
      temp_r2 <- summary(temp_lm)$r.squared
      temp_adj_r2_square <- summary(temp_lm)$adj.r.squared
      writeLines(paste0("R2 Achieved: ", round(temp_r2, 3)))
      writeLines(paste0("Adj R2 Achieved: ", round(temp_adj_r2_square, 3)))
      writeLines("")
    }
    
    writeLines("Available Option")
    print(stats_unique)
    while(1){
      var_drop <- readline(prompt=paste0("Which Variable do you want to drop? (NO MORE to exit):  "))
      if(var_drop == "NO MORE"){
        writeLines("Reached the End of Results!")
        return(list(first_data = data,
                    first_model = all_lm,
                    first_r2 = current_r2,
                    first_r2adj = current_r2adj,
                    last_data = data_dropped, 
                    last_model = lm_dropped, 
                    last_r2 = dropped_r2, 
                    last_r2adj = dropped_r2adj))
      }else if(var_drop %in% stats_unique){
        filter_vars_choose <- NULL
        if(var_drop %in% c("_Min","_Q1","_Mode","_Median","_Mean","_Max","_Q3")){
          filter_vars_choose <- which(colnames(data_dropped) %like% var_drop & !colnames(data_dropped) %like% "_R_")
        }else{
          filter_vars_choose <- which(colnames(data_dropped) %like% var_drop)
        }
        data_dropped <- data_dropped[,-filter_vars_choose]
        stats_unique <- stats_unique[-which(stats_unique == var_drop)]
        lm_dropped <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_dropped)
        dropped_r2 <- summary(lm_dropped)$r.squared
        dropped_r2adj <- summary(lm_dropped)$adj.r.squared
        writeLines('Drop Successfully! Do Next Check Model Performance !')
        writeLines("")
        break
      }else{
        message("Please Enter a valid option!")
      }
    }
    
  }
}

#use this if theres a large formula that needed to be used for transforming data
formula_decode_pick_transform <- function(data, built_in_formula){
  new_data <- data
  library(stringr)
  clean_formula <- str_replace_all(built_in_formula, "\n", "")
  clean_dep <- unlist(strsplit(clean_formula, " ~ "))[1]
  clean_indep <-  unlist(strsplit(unlist(strsplit(clean_formula, " ~ "))[2], " \\+ "))
  history_pick <- c()
  history_transform <- c()
  for(a in 1:length(clean_indep)){
    contains_bracket <- grepl("\\(", clean_indep[a])
    if(contains_bracket){
      transform_in_contains <- unlist(strsplit(clean_indep[a], "\\("))[1]
      var_in_contains <- str_replace(unlist(strsplit(clean_indep[a], "\\("))[2], "\\)","")
      history_pick <- c(history_pick, var_in_contains)
      history_transform <- c(history_transform, transform_in_contains)
      eval_transform <- paste0("new_data[[var_in_contains]] <- ",transform_in_contains, "(new_data[[var_in_contains]])")
      identity_vec <- new_data[[var_in_contains]] / abs(new_data[[var_in_contains]])
      identity_vec[which(is.na(identity_vec))] <- 0 
      new_data[[var_in_contains]] <- abs(new_data[[var_in_contains]])
      eval(parse(text = eval_transform))
      new_data[[var_in_contains]][which(new_data[[var_in_contains]] == -Inf)] <- 0
      new_data[[var_in_contains]] <- new_data[[var_in_contains]] * identity_vec
    }else{
      history_pick <- c(history_pick, clean_indep[a])
      history_transform <- c(history_transform, "None")
    }
  }
  clean_formula <- paste0(clean_dep, " ~ ",paste0(history_pick, collapse = " + "))
  history_pick <- str_replace_all(history_pick, " ", "")
  history_transform <- str_replace_all(history_transform, " ", "")
  return(list(transformed_data = new_data,
              history_pick = history_pick,
              history_transform = history_transform,
              dependent_variable = clean_dep,
              clean_formula = clean_formula))
}

#use this to transform data immediately with known pick and transform variable
transform_data_from_vector <- function(data, pick_vector, transform_vector){
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  for(a in 1:length(pick_vector)){
    variable_target <- pick_vector[a]
    transformation <- tolower(transform_vector[a])
    if(!grepl("none",transformation)){
      eval_transformation <- paste0('data[[variable_target]] <- ',transformation,'(data[[variable_target]])')
      identity_vec <- data[[variable_target]] / abs(data[[variable_target]])
      if(any(is.na(identity_vec))){
        identity_vec[which(is.na(identity_vec))] <- 0
      }
      data[[variable_target]] <- abs(data[[variable_target]])
      eval(parse(text = eval_transformation))
      if(any(data[[variable_target]] %in% c(-Inf, Inf))){
        data[[variable_target]][which(data[[variable_target]] == Inf)] <- 0
        data[[variable_target]][which(data[[variable_target]] == -Inf)] <- 0
      }
      if(any(is.na(data[[variable_target]]))){
        data[[variable_target]][which(is.na(data[[variable_target]]))] <- 0
      }
      data[[variable_target]] <- data[[variable_target]] * identity_vec
    }
  }
  return(data)
}

formula_operation <- function(obj_dep=c(), obj_vector_indep=c(), obj_formula=c()){
  library(stringr)
  if(length(obj_vector_indep) > 0){
    new_formula <- paste0(obj_dep, " ~ ", paste0(obj_vector_indep, collapse = " + "))
    return(new_formula)
  }else if(length(obj_formula) > 0){
    new_obj_dep <- str_replace(unlist(strsplit(obj_formula, "~"))[1], " ", "")
    new_obj_indep <- str_replace(unlist(strsplit(unlist(strsplit(obj_formula, "~"))[2], " \\+ ")), " ", "")
    
    return(list(new_obj_dep, new_obj_indep))
  }
}

tree_performance <- function(df, dependent_col, tree_model, 
                             font_size=0.4, detailed=FALSE){
  x11()
  library(rpart)
  library(rpart.plot)
  library(party)
  library(PPtree)
  library(PPtreeViz)
  prp(tree_model, faclen = 0, cex = font_size, extra = 1, box.palette="auto")
  metrics_info <- caret::confusionMatrix(predict(tree_model,type="class"), df[,dependent_col], mode = "everything")
  metrics_info$byClass <- round(metrics_info$byClass, 4) * 100
  metrics_info$overall <- round(metrics_info$overall, 4) * 100
  if(detailed){
    print(metrics_info)
  }else{
    writeLines("Confusion Matrix")
    print(metrics_info$table)
    writeLines("Classification Metrics")
    print(metrics_info$byClass)
  }
  
  return(metrics_info)
}

tree_bucket_comparison <- function(df, myformula, bucket_vec = c()){
  all_comparison <- list()
  library(rpart)
  library(rpart.plot)
  library(party)
  library(PPtree)
  library(PPtreeViz)
  
  tree_performance <- function(df, dependent_col, tree_model, 
                               font_size=0.4, detailed=FALSE){
    x11()
    library(rpart)
    library(rpart.plot)
    library(party)
    library(PPtree)
    library(PPtreeViz)
    prp(tree_model, faclen = 0, cex = font_size, extra = 1, box.palette="auto")
    metrics_info <- caret::confusionMatrix(predict(tree_model,type="class"), df[,dependent_col], mode = "everything")
    metrics_info$byClass <- round(metrics_info$byClass, 4) * 100
    metrics_info$overall <- round(metrics_info$overall, 4) * 100
    if(detailed){
      print(metrics_info)
    }else{
      writeLines("Confusion Matrix")
      print(metrics_info$table)
      writeLines("Classification Metrics")
      print(metrics_info$byClass)
    }
    
    return(metrics_info)
  }
  
  for(g in 1:length(bucket_vec)){
    tree_in_comparison <- rpart(as.formula(myformula), data = df,
                                control = rpart.control(cp = 0.00001, minbucket = bucket_vec[g]))
    dependent_col <- unlist(strsplit(myformula, " ~ "))[1]
    writeLines("")
    writeLines("---------------------------------------------------------------------------------------")
    writeLines(paste0("------------------------------------- Bucket ",g," ----------------------------------------"))
    writeLines("---------------------------------------------------------------------------------------")
    writeLines("")
    compare_tree <- tree_performance(df, dependent_col, tree_in_comparison, detailed=FALSE)
    all_comparison <- append(all_comparison, list(compare_tree))
  }
  return(all_comparison)
}

#---------------------------------- Regression Variable Feature Selection & Search Tools Numeric and Ordinal  ----------------------------------------
#---------------------------------- #Exhaustive Searching for Variable also included in a library(ExhaustiveSearch) -------------

pick_starting_variable <- function(data, dependent_col, auto_best = FALSE, target_r2 = 90, 
                                   r2_increase_threshold = 0.1, show_model_comparison = FALSE){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_pick <- data
  best_formula <- ""
  history_pick <- c()
  current_r2 <- NA
  current_adj_r2 <- NA
  
  datm <- as.matrix(test_dat)
  obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
  print(class(obj_lm_fit))
  
  while(1){
    writeLines("Beginning Picking All Possible variable . . .")
    for(x in 2:length(all_indep_col)){
      if(length(history_pick) == 0){
        model_formula <- as.formula(paste0(dependent_col, " ~ ", all_indep_col[x]))
        lm_model <- lm(model_formula, data = data)
        r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x), 
                                         pick_columns = colnames(data_pick)[x],
                                         r2 = summary(lm_model)$r.squared * 100, 
                                         adj_r2 = summary(lm_model)$adj.r.squared *100))
      }else{
        all_variable <- c(history_pick, all_indep_col[x])
        model_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(all_variable, collapse= " + ")))
        lm_model <- lm(model_formula, data = data)
        r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x), 
                                         pick_columns = colnames(data_pick)[x],
                                         r2 = summary(lm_model)$r.squared * 100, 
                                         adj_r2 = summary(lm_model)$adj.r.squared *100))
      }
    }
    r2_df <- r2_df %>% arrange(desc(r2)) #order by r2
    if(show_model_comparison){
      writeLines("")
      writeLines("Show Performances")
      writeLines("")
      print(r2_df)
    }
    writeLines("")
    increase_r2 <- 0
    if(auto_best){
      if(length(history_pick) > 0){
        old_pick <- history_pick
        new_pick <- c(history_pick, r2_df$pick_columns[1])
        old_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(old_pick, collapse = " + ")))
        new_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(new_pick, collapse = " + ")))
        old_model <- lm(old_formula, data = data)
        new_model <- lm(new_formula, data = data)
        old_r_squared <- summary(old_model)$r.square * 100
        new_r_squared <- summary(new_model)$r.square * 100
        if(new_r_squared > old_r_squared){
          writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
          writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                            " Increase (",new_r_squared - old_r_squared,"%)"))
          increase_r2 <- new_r_squared - old_r_squared
          history_pick <- c(history_pick, r2_df$pick_columns[1])
          all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
          data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          print(best_formula)
          writeLines('Pick Successfully')
          r2_df <- NULL
          writeLines("")
        }else{
          writeLines("Variable Pick has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of R Square Achieved: ", old_r_squared))
          return(list(pickup_variable_from_start = history_pick,
                      formula_pickup = best_formula))
        }
        if(!is.na(target_r2)){
          if(new_r_squared > target_r2){
            writeLines("Variable Pick has reached r2 target! Reached the end of Results")
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
        if(!is.na(r2_increase_threshold)){
          if(increase_r2 < r2_increase_threshold){
            writeLines(paste0("Variable Pick has No more Significant Improve for Threshold ", r2_increase_threshold), " Reached the end of Results")
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
        
      }
      else{
        old_r_squared <- 0
        new_pick <- r2_df$pick_columns[1]
        new_model <- lm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
        new_r_squared <- summary(new_model)$r.square * 100
        writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
        writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                          " Increase (",new_r_squared - old_r_squared,"%)"))
        history_pick <- c(history_pick, r2_df$pick_columns[1])
        all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
        data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", history_pick[1])
        print(best_formula)
        writeLines('Pick Successfully')
        r2_df <- NULL
        writeLines("")
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      formula_pickup = best_formula))
        }
        else if(var_pick %in% r2_df$pick_columns){
          history_pick <- c(history_pick, var_pick)
          all_indep_col <- all_indep_col[-which(all_indep_col == var_pick)]
          data_pick <- data_pick[,-which(colnames(data_pick) == var_pick)]
          writeLines("Formula Built Until Now")
          if(length(history_pick) > 1){
            best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          }else{
            best_formula <- paste0(dependent_col, "~", history_pick[1])
          }
          writeLines('Pick Successfully')
          r2_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

pick_starting_variable_F <- function(data, dependent_col, auto_best = FALSE, target_r2 = 90, 
                                    r2_increase_threshold = 0.1, show_model_comparison = FALSE, 
                                    limit_execution_time = NA){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_pick <- data
  best_formula <- ""
  history_pick <- c()
  current_r2 <- NA
  current_adj_r2 <- NA
  current_execution_time <- NA
  
  #Use .lm.fit for faster operation
  fitted.list <- function(object, X) {
    X %*% object$coefficients
  }
  resid.list <- function(object, X, y) {
    y_fitted <- fitted(object, X)
    y - y_fitted
  }
  rsquared <- function(x, ...) UseMethod("rsquared")
  rsquared.default <- function(x, ...) {
    summary(x)$r.squared
  }
  rsquared.list <- function(object, X, y) {
    e <- resid.list(object, X, y)
    1 - sum(e^2)/sum( (y - mean(y))^2 )
  }
  adj_rsquared_list <- function(object, X, y){
    r2 <- rsquared.list(object, X, y)
    k <- ncol(X) - 1
    n <- nrow(X)  
    rate_of_error <- (1 - r2) * (n - 1) / (n - k - 1)
    adj_r2 <- 1 - rate_of_error
    return(adj_r2)
  }
  
  while(1){
    tic()
    writeLines("Beginning Picking All Possible variable . . .")
    for(x in 2:length(all_indep_col)){
      if(length(history_pick) == 0){
        var_index <- which(colnames(data) %in% c(dependent_col, all_indep_col[x]))
        subset_data <- data[,var_index]
        datm <- as.matrix(subset_data)
        obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
        r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x), 
                                         pick_columns = colnames(data_pick)[x],
                                         r2 = obj_lm_fit_r2 * 100, 
                                         adj_r2 = obj_lm_fit_r2adj * 100))
      }else{
        all_variable <- c(history_pick, all_indep_col[x])
        var_index <- which(colnames(data) %in% c(dependent_col, all_variable))
        subset_data <- data[,var_index]
        datm <- as.matrix(subset_data)
        obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1, datm[,-1]), datm[,1])
        r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x), 
                                         pick_columns = colnames(data_pick)[x],
                                         r2 = obj_lm_fit_r2 * 100, 
                                         adj_r2 = obj_lm_fit_r2adj * 100))
      }
    }
    current_execution_temp <- toc()
    current_execution_time <- as.numeric(current_execution_temp$toc - current_execution_temp$tic)
    r2_df <- r2_df %>% arrange(desc(r2)) #order by r2
    if(show_model_comparison){
      writeLines("")
      writeLines("Show Performances")
      writeLines("")
      print(r2_df)
    }
    writeLines("")
    increase_r2 <- 0
    if(auto_best){
      if(length(history_pick) > 0){
        old_pick <- history_pick
        new_pick <- c(history_pick, r2_df$pick_columns[1])
        old_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(old_pick, collapse = " + ")))
        new_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(new_pick, collapse = " + ")))
        old_model <- lm(old_formula, data = data)
        new_model <- lm(new_formula, data = data)
        old_r_squared <- summary(old_model)$r.square * 100
        new_r_squared <- summary(new_model)$r.square * 100
        if(new_r_squared > old_r_squared){
          writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
          writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                            " Increase (",new_r_squared - old_r_squared,"%)"))
          increase_r2 <- new_r_squared - old_r_squared
          history_pick <- c(history_pick, r2_df$pick_columns[1])
          all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
          data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          print(best_formula)
          writeLines('Pick Successfully')
          r2_df <- NULL
          writeLines("")
        }else{
          writeLines("Variable Pick has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of R Square Achieved: ", old_r_squared))
          return(list(pickup_variable_from_start = history_pick,
                      formula_pickup = best_formula))
        }
        if(!is.na(target_r2)){
          if(new_r_squared > target_r2){
            writeLines("Variable Pick has reached r2 target! Reached the end of Results")
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
        if(!is.na(r2_increase_threshold)){
          if(increase_r2 < r2_increase_threshold){
            writeLines(paste0("Variable Pick has No more Significant Improve for Threshold ", r2_increase_threshold), " Reached the end of Results")
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
        if(!is.na(limit_execution_time)){
          if(current_execution_time > limit_execution_time){
            writeLines(paste0("Variable Pick Has Begin too Slow and reached the execution limit ",limit_execution_time, " Reached the end of Results"))
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
      }
      else{
        old_r_squared <- 0
        new_pick <- r2_df$pick_columns[1]
        new_model <- lm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
        new_r_squared <- summary(new_model)$r.square * 100
        writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
        writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                          " Increase (",new_r_squared - old_r_squared,"%)"))
        history_pick <- c(history_pick, r2_df$pick_columns[1])
        all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
        data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", history_pick[1])
        print(best_formula)
        writeLines('Pick Successfully')
        r2_df <- NULL
        writeLines("")
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      formula_pickup = best_formula))
        }
        else if(var_pick %in% r2_df$pick_columns){
          history_pick <- c(history_pick, var_pick)
          all_indep_col <- all_indep_col[-which(all_indep_col == var_pick)]
          data_pick <- data_pick[,-which(colnames(data_pick) == var_pick)]
          writeLines("Formula Built Until Now")
          if(length(history_pick) > 1){
            best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          }else{
            best_formula <- paste0(dependent_col, "~", history_pick[1])
          }
          writeLines('Pick Successfully')
          r2_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

pick_transform_starting_variable <- function(data, dependent_col, auto_best = TRUE, target_r2 = 90, r2_increase_threshold = 0.1,
                                             apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                      "power1_75","quadratic","cubic"),
                                             show_model_comparison = FALSE){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_pick <- data
  data_transformed <- data
  best_formula <- ""
  best_formula_interpretation <- ""
  history_pick <- c()
  history_transform <- c()
  current_r2 <- NA
  current_adj_r2 <- NA
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  while(1){
    tic()
    writeLines("Beginning Picking All Possible variable . . .")
    for(x in 2:length(all_indep_col)){
      if(length(history_pick) == 0){
        for(w in 1:(length(apply_transformation) + 1)){
          if(w == 1){
            tryCatch({
              model_formula <- as.formula(paste0(dependent_col, " ~ ", all_indep_col[x]))
              lm_model <- lm(model_formula, data = data)
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1), 
                                               pick_columns = colnames(data_pick)[x],
                                               var_transformation = "None",
                                               r2 = summary(lm_model)$r.squared * 100, 
                                               adj_r2 = summary(lm_model)$adj.r.squared *100))
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x," Uncomputable!"))
            })
            
          }else{
            model_formula <- as.formula(paste0(dependent_col, " ~ ", all_indep_col[x]))
            eval_transform <- paste0("data_transformed[[all_indep_col[x]]] <- ",
                                     apply_transformation[w-1],"(data_transformed[[all_indep_col[x]]])")
            identity_vec <- data_transformed[[all_indep_col[x]]] / abs(data_transformed[[all_indep_col[x]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transformed[[all_indep_col[x]]] <- abs(data_transformed[[all_indep_col[x]]])
            eval(parse(text = eval_transform))
            data_transformed[[all_indep_col[x]]][which(data_transformed[[all_indep_col[x]]] == -Inf)] <- 0
            data_transformed[[all_indep_col[x]]] <- data_transformed[[all_indep_col[x]]] * identity_vec
            tryCatch({
              lm_model <- lm(model_formula, data = data_transformed)
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1),
                                               pick_columns = colnames(data_pick)[x], 
                                               var_transformation = apply_transformation[w-1],
                                               r2 = summary(lm_model)$r.squared * 100, 
                                               adj_r2 = summary(lm_model)$adj.r.squared *100))
              data_transformed <- data
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
            })
          }
        }
      }
      else{
        for(w in 1:(length(apply_transformation) + 1)){
          if(w == 1){
            all_variable <- c(history_pick, all_indep_col[x])
            tryCatch({
              model_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(all_variable, collapse= " + ")))
              lm_model <- lm(model_formula, data = data)
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1), 
                                               pick_columns = colnames(data_pick)[x],
                                               var_transformation = "None",
                                               r2 = summary(lm_model)$r.squared * 100, 
                                               adj_r2 = summary(lm_model)$adj.r.squared *100))
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x," Uncomputable!"))
            })
          }else{
            all_variable <- c(history_pick, all_indep_col[x])
            model_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(all_variable, collapse= " + ")))
            eval_transform <- paste0("data_transformed[[all_indep_col[x]]] <- ",
                                     apply_transformation[w-1],"(data_transformed[[all_indep_col[x]]])")
            identity_vec <- data_transformed[[all_indep_col[x]]] / abs(data_transformed[[all_indep_col[x]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transformed[[all_indep_col[x]]] <- abs(data_transformed[[all_indep_col[x]]])
            eval(parse(text = eval_transform))
            data_transformed[[all_indep_col[x]]][which(data_transformed[[all_indep_col[x]]] == -Inf)] <- 0
            data_transformed[[all_indep_col[x]]] <- data_transformed[[all_indep_col[x]]] * identity_vec
            tryCatch({
              lm_model <- lm(model_formula, data = data_transformed)
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1),
                                               pick_columns = colnames(data_pick)[x], 
                                               var_transformation = apply_transformation[w-1],
                                               r2 = summary(lm_model)$r.squared * 100, 
                                               adj_r2 = summary(lm_model)$adj.r.squared *100))
              data_transformed <- data
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
            })
          }
        }
      }
    }
    toc()
    r2_df <- r2_df %>% arrange(desc(r2)) #order by r2
    if(show_model_comparison){
      writeLines("")
      writeLines("Show Performances")
      writeLines("")
      print(r2_df)
    }
    writeLines("")
    increase_r2 <- 0
    if(auto_best){
      if(length(history_pick) > 0){
        old_pick <- history_pick
        new_pick <- c(history_pick, r2_df$pick_columns[1])
        old_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(old_pick, collapse = " + ")))
        new_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(new_pick, collapse = " + ")))
        old_model <- lm(old_formula, data = data)
        new_transform <- r2_df$var_transformation[1]
        if(new_transform == "None"){
          new_model <- lm(new_formula, data = data)
        }else{
          eval_transform <- paste0("data_transformed[[r2_df$pick_columns[1]]] <- ",new_transform,"(data_transformed[[r2_df$pick_columns[1]]])")
          identity_vec <- data_transformed[[r2_df$pick_columns[1]]] / abs(data_transformed[[r2_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transformed[[r2_df$pick_columns[1]]] <- abs(data_transformed[[r2_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data_transformed[[r2_df$pick_columns[1]]][which(data_transformed[[r2_df$pick_columns[1]]] == -Inf)] <- 0
          data_transformed[[r2_df$pick_columns[1]]] <- data_transformed[[r2_df$pick_columns[1]]] * identity_vec
          new_model <- lm(new_formula, data = data_transformed)
          data_transformed <- data
        }
        old_r_squared <- summary(old_model)$r.square * 100
        new_r_squared <- summary(new_model)$r.square * 100
        if(new_r_squared > old_r_squared){
          writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
          writeLines(paste0("Picked Best Transform functions: ", r2_df$var_transformation[1]))
          writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                            " Increase (",new_r_squared - old_r_squared,"%)"))
          increase_r2 <- new_r_squared - old_r_squared
          history_pick <- c(history_pick, r2_df$pick_columns[1])
          history_transform <- c(history_transform, r2_df$var_transformation[1])
          all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
          data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
          if(new_transform != "None"){
            eval_transform <- paste0("data[[r2_df$pick_columns[1]]] <- ",new_transform,"(data[[r2_df$pick_columns[1]]])")
            identity_vec <- data[[r2_df$pick_columns[1]]] / abs(data[[r2_df$pick_columns[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data[[r2_df$pick_columns[1]]] <- abs(data[[r2_df$pick_columns[1]]])
            eval(parse(text = eval_transform))
            data[[r2_df$pick_columns[1]]][which(data[[r2_df$pick_columns[1]]] == -Inf)] <- 0
            data[[r2_df$pick_columns[1]]] <- data[[r2_df$pick_columns[1]]] * identity_vec
          }
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick Transform Successfully')
          r2_df <- NULL
          writeLines("")
        }else{
          writeLines("Variable Pick has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of R Square Achieved: ", old_r_squared))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
        if(!is.na(target_r2)){
          if(new_r_squared > target_r2){
            writeLines("Variable Pick Transform has reached r2 target! Reached the end of Results")
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(r2_increase_threshold)){
          if(increase_r2 < r2_increase_threshold){
            writeLines(paste0("Variable Pick Transform Has no more significant improve over threshold ",r2_increase_threshold, " Reached the end of Results"))
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        
      }
      else{
        old_r_squared <- 0
        new_pick <- r2_df$pick_columns[1]
        new_transform <- r2_df$var_transformation[1]
        if(new_transform == "None"){
          new_model <- lm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
        }else{
          eval_transform <- paste0("data_transformed[[r2_df$pick_columns[1]]] <- ",new_transform,"(data_transformed[[r2_df$pick_columns[1]]])")
          identity_vec <- data_transformed[[r2_df$pick_columns[1]]] / abs(data_transformed[[r2_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transformed[[r2_df$pick_columns[1]]] <- abs(data_transformed[[r2_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data_transformed[[r2_df$pick_columns[1]]][which(data_transformed[[r2_df$pick_columns[1]]] == -Inf)] <- 0
          data_transformed[[r2_df$pick_columns[1]]] <- data_transformed[[r2_df$pick_columns[1]]] * identity_vec
          new_model <- lm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_transformed)
          data_transformed <- data
        }
        
        new_r_squared <- summary(new_model)$r.square * 100
        writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
        writeLines(paste0("Picked Best Transform functions: ", r2_df$var_transformation[1]))
        writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                          " Increase (",new_r_squared - old_r_squared,"%)"))
        history_pick <- c(history_pick, r2_df$pick_columns[1])
        history_transform <- c(history_transform, r2_df$var_transformation[1])
        all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
        data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
        if(new_transform != "None"){
          eval_transform <- paste0("data[[r2_df$pick_columns[1]]] <- ",new_transform,"(data[[r2_df$pick_columns[1]]])")
          identity_vec <- data[[r2_df$pick_columns[1]]] / abs(data[[r2_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data[[r2_df$pick_columns[1]]] <- abs(data[[r2_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data[[r2_df$pick_columns[1]]][which(data[[r2_df$pick_columns[1]]] == -Inf)] <- 0
          data[[r2_df$pick_columns[1]]] <- data[[r2_df$pick_columns[1]]] * identity_vec
        }
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
        best_formula_interpretation <- paste0(dependent_col, " ~ ")
        for(j in 1:length(history_transform)){
          if(j < length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
          }
          else if(j < length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
          }
          else if(j == length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
          }
          else if(j == length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
          }
        }
        print(best_formula_interpretation)
        writeLines('Pick and Transformed Successfully')
        r2_df <- NULL
        writeLines("")
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        var_transform <- readline(prompt=paste0("Which Transform function do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE" && var_transform == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
        else if(var_pick %in% r2_df$pick_columns && var_transform %in% r2_df$var_transformation){
          history_pick <- c(history_pick, var_pick)
          history_transform <- c(history_transform, var_transform)
          all_indep_col <- all_indep_col[-which(all_indep_col == var_pick)]
          data_pick <- data_pick[,-which(colnames(data_pick) == var_pick)]
          if(var_transform != "None"){
            eval_transform <- paste0("data[[r2_df$pick_columns[1]]] <- ",var_transform,"(data[[r2_df$pick_columns[1]]])")
            identity_vec <- data[[r2_df$pick_columns[1]]] / abs(data[[r2_df$pick_columns[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data[[r2_df$pick_columns[1]]] <- abs(data[[r2_df$pick_columns[1]]])
            eval(parse(text = eval_transform))
            data[[r2_df$pick_columns[1]]][which(data[[r2_df$pick_columns[1]]] == -Inf)] <- 0
            data[[r2_df$pick_columns[1]]] <- data[[r2_df$pick_columns[1]]] * identity_vec
          }
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick and Transformed Successfully')
          r2_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

pick_transform_starting_variable_F <- function(data, dependent_col, auto_best = TRUE, target_r2 = 90, r2_increase_threshold = 0.1,
                                               continue_pick_vars = NA, continue_pick_transform = NA,
                                               apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                        "power1_75","quadratic","cubic"),
                                               show_model_comparison = FALSE, limit_execution_time = NA){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_pick <- data
  data_transformed <- data
  best_formula <- ""
  best_formula_interpretation <- ""
  history_pick <- c()
  history_transform <- c()
  current_r2 <- NA
  current_adj_r2 <- NA
  current_execution_time <- NA
  
  #Use .lm.fit for faster operation
  fitted.list <- function(object, X) {
    X %*% object$coefficients
  }
  resid.list <- function(object, X, y) {
    y_fitted <- fitted(object, X)
    y - y_fitted
  }
  rsquared <- function(x, ...) UseMethod("rsquared")
  rsquared.default <- function(x, ...) {
    summary(x)$r.squared
  }
  rsquared.list <- function(object, X, y) {
    e <- resid.list(object, X, y)
    1 - sum(e^2)/sum( (y - mean(y))^2 )
  }
  adj_rsquared_list <- function(object, X, y){
    r2 <- rsquared.list(object, X, y)
    k <- ncol(X) - 1
    n <- nrow(X)  
    rate_of_error <- (1 - r2) * (n - 1) / (n - k - 1)
    adj_r2 <- 1 - rate_of_error
    return(adj_r2)
  }
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  if(!is.na(continue_pick_vars)){
    history_pick <- continue_pick_vars
    history_transform <- continue_pick_transform
    if(length(history_pick) != length(unique(history_pick))){
      all_indep_col <- all_indep_col[-which(all_indep_col %in% unique(history_pick))]
      data_pick <- data[,-which(colnames(data) %in% unique(history_pick))]
      for(g in 1:length(history_pick)){
        if(history_transform[g] != "None"){
          eval_transform <- paste0("data[[history_pick[g]]] <- ", history_transform[g],"(data[[history_pick[g]]])")
          identity_vec <- data[[history_pick[g]]] / abs(data[[history_pick[g]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data[[history_pick[g]]] <- abs(data[[history_pick[g]]])
          eval(parse(text = eval_transform))
          data[[history_pick[g]]][which(data[[history_pick[g]]] == -Inf)] <- 0
          data[[history_pick[g]]] <- data[[history_pick[g]]] * identity_vec
        }
      }
      best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
      best_formula_interpretation <- paste0(dependent_col, " ~ ")
      for(j in 1:length(history_transform)){
        if(j < length(history_transform) && history_transform[j] != "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
        }
        else if(j < length(history_transform) && history_transform[j] == "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
        }
        else if(j == length(history_transform) && history_transform[j] != "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
        }
        else if(j == length(history_transform) && history_transform[j] == "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
        }
      }
      history_pick <- unique(history_pick)
      history_transform <- history_transform[1:length(history_pick)]
    }
    else{
      all_indep_col <- all_indep_col[-which(all_indep_col %in% history_pick)]
      data_pick <- data[,-which(colnames(data) %in% history_pick)]
      for(g in 1:length(history_pick)){
        if(history_transform[g] != "None"){
          eval_transform <- paste0("data[[history_pick[g]]] <- ", history_transform[g],"(data[[history_pick[g]]])")
          identity_vec <- data[[history_pick[g]]] / abs(data[[history_pick[g]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data[[history_pick[g]]] <- abs(data[[history_pick[g]]])
          eval(parse(text = eval_transform))
          data[[history_pick[g]]][which(data[[history_pick[g]]] == -Inf)] <- 0
          data[[history_pick[g]]] <- data[[history_pick[g]]] * identity_vec
        }
      }
      best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
      best_formula_interpretation <- paste0(dependent_col, " ~ ")
      for(j in 1:length(history_transform)){
        if(j < length(history_transform) && history_transform[j] != "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
        }
        else if(j < length(history_transform) && history_transform[j] == "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
        }
        else if(j == length(history_transform) && history_transform[j] != "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
        }
        else if(j == length(history_transform) && history_transform[j] == "None"){
          best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
        }
      }
    }
    writeLines("Model Continue Check")
    print(summary(lm(as.formula(paste0(dependent_col, " ~ .")), data = data[,which(colnames(data) %in% c(dependent_col,history_pick))])))
  }else{
    data_pick <- data
  }
  
  while(1){
    tic()
    writeLines("Beginning Picking All Possible variable . . .")
    for(x in 2:length(all_indep_col)){
      if(length(history_pick) == 0){
        for(w in 1:(length(apply_transformation) + 1)){
          if(w == 1){
            tryCatch({
              var_index <- which(colnames(data) %in% c(dependent_col, all_indep_col[x]))
              subset_data <- data[,var_index]
              datm <- as.matrix(subset_data)
              obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1), 
                                               pick_columns = colnames(data_pick)[x],
                                               var_transformation = "None",
                                               r2 = obj_lm_fit_r2 * 100, 
                                               adj_r2 = obj_lm_fit_r2adj *100))
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x," Uncomputable!"))
            })
            
          }else{
            eval_transform <- paste0("data_transformed[[all_indep_col[x]]] <- ",
                                     apply_transformation[w-1],"(data_transformed[[all_indep_col[x]]])")
            identity_vec <- data_transformed[[all_indep_col[x]]] / abs(data_transformed[[all_indep_col[x]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transformed[[all_indep_col[x]]] <- abs(data_transformed[[all_indep_col[x]]])
            eval(parse(text = eval_transform))
            data_transformed[[all_indep_col[x]]][which(data_transformed[[all_indep_col[x]]] == -Inf)] <- 0
            data_transformed[[all_indep_col[x]]] <- data_transformed[[all_indep_col[x]]] * identity_vec
            tryCatch({
              var_index <- which(colnames(data_transformed) %in% c(dependent_col, all_indep_col[x]))
              subset_data <- data_transformed[,var_index]
              datm <- as.matrix(subset_data)
              obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1),
                                               pick_columns = colnames(data_pick)[x], 
                                               var_transformation = apply_transformation[w-1],
                                               r2 = obj_lm_fit_r2 * 100, 
                                               adj_r2 = obj_lm_fit_r2adj *100))
              data_transformed <- data
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
            })
          }
        }
      }
      else{
        for(w in 1:(length(apply_transformation) + 1)){
          if(w == 1){
            all_variable <- c(history_pick, all_indep_col[x])
            tryCatch({
              var_index <- which(colnames(data) %in% c(dependent_col, all_variable))
              subset_data <- data[,var_index]
              datm <- as.matrix(subset_data)
              obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1), 
                                               pick_columns = colnames(data_pick)[x],
                                               var_transformation = "None",
                                               r2 = obj_lm_fit_r2 * 100, 
                                               adj_r2 = obj_lm_fit_r2adj *100))
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x," Uncomputable!"))
            })
          }else{
            all_variable <- c(history_pick, all_indep_col[x])
            eval_transform <- paste0("data_transformed[[all_indep_col[x]]] <- ",
                                     apply_transformation[w-1],"(data_transformed[[all_indep_col[x]]])")
            identity_vec <- data_transformed[[all_indep_col[x]]] / abs(data_transformed[[all_indep_col[x]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transformed[[all_indep_col[x]]] <- abs(data_transformed[[all_indep_col[x]]])
            eval(parse(text = eval_transform))
            data_transformed[[all_indep_col[x]]][which(data_transformed[[all_indep_col[x]]] == -Inf)] <- 0
            data_transformed[[all_indep_col[x]]] <- data_transformed[[all_indep_col[x]]] * identity_vec
            tryCatch({
              var_index <- which(colnames(data_transformed) %in% c(dependent_col, all_variable))
              subset_data <- data_transformed[,var_index]
              datm <- as.matrix(subset_data)
              obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
              
              r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",x-1),
                                               pick_columns = colnames(data_pick)[x], 
                                               var_transformation = apply_transformation[w-1],
                                               r2 = obj_lm_fit_r2 * 100, 
                                               adj_r2 = obj_lm_fit_r2adj *100))
              data_transformed <- data
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
            })
          }
        }
      }
    }
    current_execution_temp <- toc()
    current_execution_time <- as.numeric(current_execution_temp$toc - current_execution_temp$tic)
    r2_df <- r2_df %>% arrange(desc(r2)) #order by r2
    if(show_model_comparison){
      writeLines("")
      writeLines("Show Performances")
      writeLines("")
      print(r2_df)
    }
    writeLines("")
    increase_r2 <- 0
    if(auto_best){
      if(length(history_pick) > 0){
        old_pick <- history_pick
        new_pick <- c(history_pick, r2_df$pick_columns[1])
        old_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(old_pick, collapse = " + ")))
        new_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(new_pick, collapse = " + ")))
        old_model <- lm(old_formula, data = data)
        new_transform <- r2_df$var_transformation[1]
        if(new_transform == "None"){
          new_model <- lm(new_formula, data = data)
        }else{
          eval_transform <- paste0("data_transformed[[r2_df$pick_columns[1]]] <- ",new_transform,"(data_transformed[[r2_df$pick_columns[1]]])")
          identity_vec <- data_transformed[[r2_df$pick_columns[1]]] / abs(data_transformed[[r2_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transformed[[r2_df$pick_columns[1]]] <- abs(data_transformed[[r2_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data_transformed[[r2_df$pick_columns[1]]][which(data_transformed[[r2_df$pick_columns[1]]] == -Inf)] <- 0
          data_transformed[[r2_df$pick_columns[1]]] <- data_transformed[[r2_df$pick_columns[1]]] * identity_vec
          new_model <- lm(new_formula, data = data_transformed)
          data_transformed <- data
        }
        old_r_squared <- summary(old_model)$r.square * 100
        new_r_squared <- summary(new_model)$r.square * 100
        if(new_r_squared > old_r_squared){
          writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
          writeLines(paste0("Picked Best Transform functions: ", r2_df$var_transformation[1]))
          writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                            " Increase (",new_r_squared - old_r_squared,"%)"))
          increase_r2 <- new_r_squared - old_r_squared
          history_pick <- c(history_pick, r2_df$pick_columns[1])
          history_transform <- c(history_transform, r2_df$var_transformation[1])
          all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
          data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
          if(new_transform != "None"){
            eval_transform <- paste0("data[[r2_df$pick_columns[1]]] <- ",new_transform,"(data[[r2_df$pick_columns[1]]])")
            identity_vec <- data[[r2_df$pick_columns[1]]] / abs(data[[r2_df$pick_columns[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data[[r2_df$pick_columns[1]]] <- abs(data[[r2_df$pick_columns[1]]])
            eval(parse(text = eval_transform))
            data[[r2_df$pick_columns[1]]][which(data[[r2_df$pick_columns[1]]] == -Inf)] <- 0
            data[[r2_df$pick_columns[1]]] <- data[[r2_df$pick_columns[1]]] * identity_vec
          }
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick Transform Successfully')
          r2_df <- NULL
          writeLines("")
        }else{
          writeLines("Variable Pick has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of R Square Achieved: ", old_r_squared))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
        if(!is.na(target_r2)){
          if(new_r_squared > target_r2){
            writeLines("Variable Pick Transform has reached r2 target! Reached the end of Results")
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(r2_increase_threshold)){
          if(increase_r2 < r2_increase_threshold){
            writeLines(paste0("Variable Pick Transform Has no more significant improve over threshold ",r2_increase_threshold, " Reached the end of Results"))
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(limit_execution_time)){
          if(current_execution_time > limit_execution_time){
            writeLines(paste0("Variable Pick Transform Has Begin too Slow and reached the execution limit ",limit_execution_time, " Reached the end of Results"))
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_r_squared))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
      }
      else{
        old_r_squared <- 0
        new_pick <- r2_df$pick_columns[1]
        new_transform <- r2_df$var_transformation[1]
        if(new_transform == "None"){
          new_model <- lm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
        }else{
          eval_transform <- paste0("data_transformed[[r2_df$pick_columns[1]]] <- ",new_transform,"(data_transformed[[r2_df$pick_columns[1]]])")
          identity_vec <- data_transformed[[r2_df$pick_columns[1]]] / abs(data_transformed[[r2_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transformed[[r2_df$pick_columns[1]]] <- abs(data_transformed[[r2_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data_transformed[[r2_df$pick_columns[1]]][which(data_transformed[[r2_df$pick_columns[1]]] == -Inf)] <- 0
          data_transformed[[r2_df$pick_columns[1]]] <- data_transformed[[r2_df$pick_columns[1]]] * identity_vec
          new_model <- lm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_transformed)
          data_transformed <- data
        }
        
        new_r_squared <- summary(new_model)$r.square * 100
        writeLines(paste0("Picked Best Model variable: ", r2_df$pick_columns[1]))
        writeLines(paste0("Picked Best Transform functions: ", r2_df$var_transformation[1]))
        writeLines(paste0("Old R Square: ", old_r_squared, " Become New R Square: ",new_r_squared, 
                          " Increase (",new_r_squared - old_r_squared,"%)"))
        history_pick <- c(history_pick, r2_df$pick_columns[1])
        history_transform <- c(history_transform, r2_df$var_transformation[1])
        all_indep_col <- all_indep_col[-which(all_indep_col == r2_df$pick_columns[1])]
        data_pick <- data_pick[,-which(colnames(data_pick) == r2_df$pick_columns[1])]
        if(new_transform != "None"){
          eval_transform <- paste0("data[[r2_df$pick_columns[1]]] <- ",new_transform,"(data[[r2_df$pick_columns[1]]])")
          identity_vec <- data[[r2_df$pick_columns[1]]] / abs(data[[r2_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data[[r2_df$pick_columns[1]]] <- abs(data[[r2_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data[[r2_df$pick_columns[1]]][which(data[[r2_df$pick_columns[1]]] == -Inf)] <- 0
          data[[r2_df$pick_columns[1]]] <- data[[r2_df$pick_columns[1]]] * identity_vec
        }
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
        best_formula_interpretation <- paste0(dependent_col, " ~ ")
        for(j in 1:length(history_transform)){
          if(j < length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
          }
          else if(j < length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
          }
          else if(j == length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
          }
          else if(j == length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
          }
        }
        print(best_formula_interpretation)
        writeLines('Pick and Transformed Successfully')
        r2_df <- NULL
        writeLines("")
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        var_transform <- readline(prompt=paste0("Which Transform function do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE" && var_transform == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
        else if(var_pick %in% r2_df$pick_columns && var_transform %in% r2_df$var_transformation){
          history_pick <- c(history_pick, var_pick)
          history_transform <- c(history_transform, var_transform)
          all_indep_col <- all_indep_col[-which(all_indep_col == var_pick)]
          data_pick <- data_pick[,-which(colnames(data_pick) == var_pick)]
          if(var_transform != "None"){
            eval_transform <- paste0("data[[r2_df$pick_columns[1]]] <- ",var_transform,"(data[[r2_df$pick_columns[1]]])")
            identity_vec <- data[[r2_df$pick_columns[1]]] / abs(data[[r2_df$pick_columns[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data[[r2_df$pick_columns[1]]] <- abs(data[[r2_df$pick_columns[1]]])
            eval(parse(text = eval_transform))
            data[[r2_df$pick_columns[1]]][which(data[[r2_df$pick_columns[1]]] == -Inf)] <- 0
            data[[r2_df$pick_columns[1]]] <- data[[r2_df$pick_columns[1]]] * identity_vec
          }
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick and Transformed Successfully')
          r2_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

selective_transformation_variable <- function(data, dependent_col, auto_best = TRUE, n_select=NA, 
                                              target_adj_r2 = 95, r2_increase_threshold = 0.1,
                                              apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                       "power1_75","quadratic","cubic"), 
                                              show_model_comparison = FALSE){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_transformed <- data
  history_transform <- c()
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  if(is.na(n_select)){
    while(1){
      tic()
      writeLines("Begin to calculate every variable transformation. .")
      for(v in 2:length(all_indep_col)){
        if(is.null(r2_df)){
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
                r2_df <- data.frame(Model_name = "Model1", transformed_columns = colnames(data_transformed)[v], 
                                    transform_function = apply_transformation[w],
                                    r2 = summary(lm_temp)$r.squared *100, adj_r2 = summary(lm_temp)$adj.r.squared *100)
              }
            },error = function(cond){
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
              
            })
          }
        }
        else{
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
                r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",v-1), 
                                                 transformed_columns = colnames(data_transformed)[v], 
                                                 transform_function = apply_transformation[w],
                                                 r2 = summary(lm_temp)$r.squared *100, 
                                                 adj_r2 = summary(lm_temp)$adj.r.squared *100))
              }
            },error = function(cond){
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
            })
            
          }
        }
      }
      toc()
      r2_df <- r2_df %>% arrange(desc(adj_r2))
      if(show_model_comparison){
        writeLines("")
        writeLines("Show the Performances")
        writeLines("")
        print(r2_df)
      }
      increase_r2 <- 0
      writeLines("")
      if(auto_best){
        old_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_transformed)
        target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
        eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
        identity_vec <- target_change / abs(target_change)
        identity_vec[which(is.na(identity_vec))] <- 0 
        target_change <- abs(target_change)
        target_change <- eval(parse(text = eval_transform))
        target_change[which(target_change == -Inf)] <- 0
        target_change <- target_change * identity_vec
        new_data <- data_transformed
        new_data[,which(colnames(new_data) == r2_df$transformed_columns[1])] <- target_change
        new_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = new_data)
        old_adj_r_squared <- summary(old_model)$adj.r.square * 100
        new_adj_r_squared <- summary(new_model)$adj.r.square * 100
        if(new_adj_r_squared > old_adj_r_squared){
          writeLines(paste0("Picked Best Model Transform: ", r2_df$transformed_columns[1]))
          writeLines(paste0("With Transform Function: ", r2_df$transform_function[1]))
          writeLines(paste0("Old Adj R Square: ", old_adj_r_squared, " Become New Adj R Square: ",new_adj_r_squared, 
                            " Increase (",new_adj_r_squared - old_adj_r_squared,"%)"))
          increase_r2 <- new_adj_r_squared - old_adj_r_squared
          history_transform <- c(history_transform, paste0(r2_df$transform_function[1], "(", r2_df$transformed_columns[1], ")"))
          target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
          eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
          identity_vec <- target_change / abs(target_change)
          identity_vec[which(is.na(identity_vec))] <- 0 
          target_change <- abs(target_change)
          target_change <- eval(parse(text = eval_transform))
          target_change[which(target_change == -Inf)] <- 0
          target_change <- target_change * identity_vec
          data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])] <- target_change
          writeLines('Transformed Successfully')
          r2_df <- NULL
          writeLines("")
        }
        else{
          writeLines("Variable Transform has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of Adj R Square Achieved: ", old_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_transformed,
                      transformed_variable_from_start = history_transform))
        }
        if(new_adj_r_squared > target_adj_r2){
          writeLines(paste0("Variable Transform has Reached Adj Target r2, Reached the end of Results"))
          writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_transformed,
                      transformed_variable_from_start = history_transform))
        }
        if(!is.na(r2_increase_threshold)){
          if(increase_r2 < r2_increase_threshold){
            writeLines(paste0("Variable Transform has No more Significant Improve for Threshold ", r2_increase_threshold, " Reached the end of Results"))
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
            return(list(first_data = data, 
                        last_data = data_transformed,
                        transformed_variable_from_start = history_transform))
          }
        }
        
      }
      else{
        while(1){
          var_transform <- readline(prompt=paste0("Which Variable do you want to transform? (NO MORE to exit):  "))
          var_transform_function <- readline(prompt=paste0("Which Transform Function to use? (NO MORE to exit):  "))
          if(var_transform == "NO MORE" || var_transform_function == "NO MORE"){
            writeLines("Reached the End of Results!")
            return(list(first_data = data, 
                        last_data = data_transformed,
                        transformed_variable_from_start = history_transform))
          }else if(var_transform %in% r2_df$transformed_columns && var_transform_function %in% r2_df$transform_function){
            var_exact_transform_r2 <- which(r2_df$transformed_columns == var_transform & 
                                              r2_df$transform_function == var_transform_function)
            history_transform <- c(history_transform, paste0(var_transform_function, "(", var_transform, ")"))
            target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])]
            eval_transform <- paste0(var_transform_function,"(target_change)")
            identity_vec <- target_change / abs(target_change)
            identity_vec[which(is.na(identity_vec))] <- 0 
            target_change <- abs(target_change)
            target_change <- eval(parse(text = eval_transform))
            target_change[which(target_change == -Inf)] <- 0
            target_change <- target_change * identity_vec
            data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])] <- target_change
            writeLines('Transformed Successfully')
            r2_df <- NULL
            writeLines("")
            break
          }else{
            message("Please Input a valid variable and transform function!")
          }
        }
      }
    }
  }
  else{
    for(f in 1:n_select){
      writeLines(paste0('Selecting ', f, " of ", n_select))
      writeLines("Begin to calculate every variable transformation. .")
      for(v in 2:length(all_indep_col)){
        if(is.null(r2_df)){
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
                r2_df <- data.frame(Model_name = "Model1", transformed_columns = colnames(data_transformed)[v], 
                                    transform_function = apply_transformation[w],
                                    r2 = summary(lm_temp)$r.squared *100, adj_r2 = summary(lm_temp)$adj.r.squared *100)
              }
            },error = function(cond){
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
              
            })
          }
        }
        else{
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
                r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",v-1), 
                                                 transformed_columns = colnames(data_transformed)[v], 
                                                 transform_function = apply_transformation[w],
                                                 r2 = summary(lm_temp)$r.squared *100, 
                                                 adj_r2 = summary(lm_temp)$adj.r.squared *100))
              }
            },error = function(cond){
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
            })
            
          }
        }
      }
      writeLines("Show the Performances")
      r2_df <- r2_df %>% arrange(desc(adj_r2))
      print(r2_df)
      writeLines("")
      if(auto_best){
        old_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_transformed)
        target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
        eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
        identity_vec <- target_change / abs(target_change)
        identity_vec[which(is.na(identity_vec))] <- 0 
        target_change <- abs(target_change)
        target_change <- eval(parse(text = eval_transform))
        target_change[which(target_change == -Inf)] <- 0
        target_change <- target_change * identity_vec
        new_data <- data_transformed
        new_data[,which(colnames(new_data) == r2_df$transformed_columns[1])] <- target_change
        new_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = new_data)
        old_adj_r_squared <- summary(old_model)$adj.r.square * 100
        new_adj_r_squared <- summary(new_model)$adj.r.square * 100
        if(new_adj_r_squared > old_adj_r_squared){
          writeLines(paste0("Picked Best Model Transform: ", r2_df$transformed_columns[1]))
          writeLines(paste0("With Transform Function: ", r2_df$transform_function[1]))
          writeLines(paste0("Old Adj R Square: ", old_adj_r_squared, " Become New Adj R Square: ",new_adj_r_squared, 
                            " Increase (",new_adj_r_squared - old_adj_r_squared,"%)"))
          history_transform <- c(history_transform, paste0(r2_df$transform_function[1], "(", r2_df$transformed_columns[1], ")"))
          target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
          eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
          identity_vec <- target_change / abs(target_change)
          identity_vec[which(is.na(identity_vec))] <- 0 
          target_change <- abs(target_change)
          target_change <- eval(parse(text = eval_transform))
          target_change[which(target_change == -Inf)] <- 0
          target_change <- target_change * identity_vec
          data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])] <- target_change
          writeLines('Transformed Successfully')
          r2_df <- NULL
          writeLines("")
        }
        else{
          writeLines("Variable Transform has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of Adj R Square Achieved: ", old_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_transformed,
                      transformed_variable_from_start = history_transform))
        }
      }
      else{
        while(1){
          var_transform <- readline(prompt=paste0("Which Variable do you want to transform? (NO MORE to exit):  "))
          var_transform_function <- readline(prompt=paste0("Which Transform Function to use? (NO MORE to exit):  "))
          if(var_transform == "NO MORE" || var_transform_function == "NO MORE"){
            writeLines("Reached the End of Results!")
            return(list(first_data = data, 
                        last_data = data_transformed,
                        transformed_variable_from_start = history_transform))
          }else if(var_transform %in% r2_df$transformed_columns && var_transform_function %in% r2_df$transform_function){
            var_exact_transform_r2 <- which(r2_df$transformed_columns == var_transform & 
                                              r2_df$transform_function == var_transform_function)
            history_transform <- c(history_transform, paste0(var_transform_function, "(", var_transform, ")"))
            target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])]
            eval_transform <- paste0(var_transform_function,"(target_change)")
            identity_vec <- target_change / abs(target_change)
            identity_vec[which(is.na(identity_vec))] <- 0 
            target_change <- abs(target_change)
            target_change <- eval(parse(text = eval_transform))
            target_change[which(target_change == -Inf)] <- 0
            target_change <- target_change * identity_vec
            data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])] <- target_change
            writeLines('Transformed Successfully')
            r2_df <- NULL
            writeLines("")
            break
          }else{
            message("Please Input a valid variable and transform function!")
          }
        }
      }
      f <- f+1
    }
    writeLines(paste0("Done Selecting After ",n_select," Times! Reached the end of Results"))
    return(list(first_data = data, 
                last_data = data_transformed,
                transformed_variable_from_start = history_transform))
  }
}

selective_transformation_variable_F <- function(data, dependent_col, auto_best = TRUE, n_select=NA, 
                                                target_adj_r2 = 95, r2_increase_threshold = 0.1,
                                                apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                       "power1_75","quadratic","cubic"), 
                                                show_model_comparison = FALSE, limit_execution_time = NA){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_transformed <- data
  history_transform <- c()
  current_execution_time <- NA
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  #Use .lm.fit for faster operation
  fitted.list <- function(object, X) {
    X %*% object$coefficients
  }
  resid.list <- function(object, X, y) {
    y_fitted <- fitted(object, X)
    y - y_fitted
  }
  rsquared <- function(x, ...) UseMethod("rsquared")
  rsquared.default <- function(x, ...) {
    summary(x)$r.squared
  }
  rsquared.list <- function(object, X, y) {
    e <- resid.list(object, X, y)
    1 - sum(e^2)/sum( (y - mean(y))^2 )
  }
  adj_rsquared_list <- function(object, X, y){
    r2 <- rsquared.list(object, X, y)
    k <- ncol(X) - 1
    n <- nrow(X)  
    rate_of_error <- (1 - r2) * (n - 1) / (n - k - 1)
    adj_r2 <- 1 - rate_of_error
    return(adj_r2)
  }
  
  
  if(is.na(n_select)){
    while(1){
      tic()
      writeLines("Begin to calculate every variable transformation. .")
      for(v in 2:length(all_indep_col)){
        if(is.null(r2_df)){
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                datm <- as.matrix(datatemp)
                obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                
                r2_df <- data.frame(Model_name = "Model1", transformed_columns = colnames(data_transformed)[v], 
                                    transform_function = apply_transformation[w],
                                    r2 = obj_lm_fit_r2 *100, adj_r2 = obj_lm_fit_r2adj *100)
              }
            },error = function(cond){
              message("Real Error Message")
              message(cond)
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
              
            })
          }
        }
        else{
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                datm <- as.matrix(datatemp)
                obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                
                r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",v-1), 
                                                 transformed_columns = colnames(data_transformed)[v], 
                                                 transform_function = apply_transformation[w],
                                                 r2 = obj_lm_fit_r2 *100, 
                                                 adj_r2 = obj_lm_fit_r2adj *100))
              }
            },error = function(cond){
              message("Real Error Message")
              message(cond)
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
            })
            
          }
        }
      }
      current_execution_temp <- toc()
      current_execution_time <- as.numeric(current_execution_temp$toc - current_execution_temp$tic)
      r2_df <- r2_df %>% arrange(desc(adj_r2))
      if(show_model_comparison){
        writeLines("")
        writeLines("Show the Performances")
        writeLines("")
        print(r2_df)
      }
      increase_r2 <- 0
      writeLines("")
      if(auto_best){
        old_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_transformed)
        target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
        eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
        identity_vec <- target_change / abs(target_change)
        identity_vec[which(is.na(identity_vec))] <- 0 
        target_change <- abs(target_change)
        target_change <- eval(parse(text = eval_transform))
        target_change[which(target_change == -Inf)] <- 0
        target_change <- target_change * identity_vec
        new_data <- data_transformed
        new_data[,which(colnames(new_data) == r2_df$transformed_columns[1])] <- target_change
        new_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = new_data)
        old_adj_r_squared <- summary(old_model)$adj.r.square * 100
        new_adj_r_squared <- summary(new_model)$adj.r.square * 100
        if(new_adj_r_squared > old_adj_r_squared){
          writeLines(paste0("Picked Best Model Transform: ", r2_df$transformed_columns[1]))
          writeLines(paste0("With Transform Function: ", r2_df$transform_function[1]))
          writeLines(paste0("Old Adj R Square: ", old_adj_r_squared, " Become New Adj R Square: ",new_adj_r_squared, 
                            " Increase (",new_adj_r_squared - old_adj_r_squared,"%)"))
          increase_r2 <- new_adj_r_squared - old_adj_r_squared
          history_transform <- c(history_transform, paste0(r2_df$transform_function[1], "(", r2_df$transformed_columns[1], ")"))
          target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
          eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
          identity_vec <- target_change / abs(target_change)
          identity_vec[which(is.na(identity_vec))] <- 0 
          target_change <- abs(target_change)
          target_change <- eval(parse(text = eval_transform))
          target_change[which(target_change == -Inf)] <- 0
          target_change <- target_change * identity_vec
          data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])] <- target_change
          writeLines('Transformed Successfully')
          r2_df <- NULL
          writeLines("")
        }
        else{
          writeLines("Variable Transform has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of Adj R Square Achieved: ", old_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_transformed,
                      transformed_variable_from_start = history_transform))
        }
        if(new_adj_r_squared > target_adj_r2){
          writeLines(paste0("Variable Transform has Reached Adj Target r2, Reached the end of Results"))
          writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_transformed,
                      transformed_variable_from_start = history_transform))
        }
        if(!is.na(r2_increase_threshold)){
          if(increase_r2 < r2_increase_threshold){
            writeLines(paste0("Variable Transform has No more Significant Improve for Threshold ", r2_increase_threshold, " Reached the end of Results"))
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
            return(list(first_data = data, 
                        last_data = data_transformed,
                        transformed_variable_from_start = history_transform))
          }
        }
        if(!is.na(limit_execution_time)){
          if(current_execution_time > limit_execution_time){
            writeLines(paste0("Variable Transform Has Begin too Slow and reached the execution limit ",limit_execution_time, " Reached the end of Results"))
            writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
            return(list(first_data = data, 
                        last_data = data_transformed,
                        transformed_variable_from_start = history_transform))
          }
        }
      }
      else{
        while(1){
          var_transform <- readline(prompt=paste0("Which Variable do you want to transform? (NO MORE to exit):  "))
          var_transform_function <- readline(prompt=paste0("Which Transform Function to use? (NO MORE to exit):  "))
          if(var_transform == "NO MORE" || var_transform_function == "NO MORE"){
            writeLines("Reached the End of Results!")
            return(list(first_data = data, 
                        last_data = data_transformed,
                        transformed_variable_from_start = history_transform))
          }else if(var_transform %in% r2_df$transformed_columns && var_transform_function %in% r2_df$transform_function){
            var_exact_transform_r2 <- which(r2_df$transformed_columns == var_transform & 
                                              r2_df$transform_function == var_transform_function)
            history_transform <- c(history_transform, paste0(var_transform_function, "(", var_transform, ")"))
            target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])]
            eval_transform <- paste0(var_transform_function,"(target_change)")
            identity_vec <- target_change / abs(target_change)
            identity_vec[which(is.na(identity_vec))] <- 0 
            target_change <- abs(target_change)
            target_change <- eval(parse(text = eval_transform))
            target_change[which(target_change == -Inf)] <- 0
            target_change <- target_change * identity_vec
            data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])] <- target_change
            writeLines('Transformed Successfully')
            r2_df <- NULL
            writeLines("")
            break
          }else{
            message("Please Input a valid variable and transform function!")
          }
        }
      }
    }
  }
  else{
    for(f in 1:n_select){
      writeLines(paste0('Selecting ', f, " of ", n_select))
      writeLines("Begin to calculate every variable transformation. .")
      for(v in 2:length(all_indep_col)){
        if(is.null(r2_df)){
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                datm <- as.matrix(datatemp)
                obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                r2_df <- data.frame(Model_name = "Model1", transformed_columns = colnames(data_transformed)[v], 
                                    transform_function = apply_transformation[w],
                                    r2 = obj_lm_fit_r2 *100, adj_r2 = obj_lm_fit_r2adj *100)
              }
            },error = function(cond){
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
              
            })
          }
        }
        else{
          for(w in 1:length(apply_transformation)){
            datatemp <- data_transformed
            eval_transform <- paste0(apply_transformation[w],"(datatemp[,v])")
            identity_vec <- datatemp[,v] / abs(datatemp[,v])
            identity_vec[which(is.na(identity_vec))] <- 0 
            datatemp[,v] <- abs(datatemp[,v])
            datatemp[,v] <- eval(parse(text = eval_transform))
            datatemp[,v][which(datatemp[,v] == -Inf)] <- 0
            datatemp[,v] <- datatemp[,v] * identity_vec
            
            tryCatch({
              if(any(is.na(datatemp[,v])) == FALSE){
                datm <- as.matrix(datatemp)
                obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
                r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",v-1), 
                                                 transformed_columns = colnames(data_transformed)[v], 
                                                 transform_function = apply_transformation[w],
                                                 r2 = obj_lm_fit_r2 *100, 
                                                 adj_r2 = obj_lm_fit_r2adj *100))
              }
            },error = function(cond){
              message("Error Happened lets inspect the datatemp")
              writeLines(paste0(eval_transform, " to variable ", colnames(datatemp)[v]))
              print(datatemp[,v])
            })
            
          }
        }
      }
      writeLines("Show the Performances")
      r2_df <- r2_df %>% arrange(desc(adj_r2))
      print(r2_df)
      writeLines("")
      if(auto_best){
        old_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_transformed)
        target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
        eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
        identity_vec <- target_change / abs(target_change)
        identity_vec[which(is.na(identity_vec))] <- 0 
        target_change <- abs(target_change)
        target_change <- eval(parse(text = eval_transform))
        target_change[which(target_change == -Inf)] <- 0
        target_change <- target_change * identity_vec
        new_data <- data_transformed
        new_data[,which(colnames(new_data) == r2_df$transformed_columns[1])] <- target_change
        new_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = new_data)
        old_adj_r_squared <- summary(old_model)$adj.r.square * 100
        new_adj_r_squared <- summary(new_model)$adj.r.square * 100
        if(new_adj_r_squared > old_adj_r_squared){
          writeLines(paste0("Picked Best Model Transform: ", r2_df$transformed_columns[1]))
          writeLines(paste0("With Transform Function: ", r2_df$transform_function[1]))
          writeLines(paste0("Old Adj R Square: ", old_adj_r_squared, " Become New Adj R Square: ",new_adj_r_squared, 
                            " Increase (",new_adj_r_squared - old_adj_r_squared,"%)"))
          history_transform <- c(history_transform, paste0(r2_df$transform_function[1], "(", r2_df$transformed_columns[1], ")"))
          target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])]
          eval_transform <- paste0(r2_df$transform_function[1],"(target_change)")
          identity_vec <- target_change / abs(target_change)
          identity_vec[which(is.na(identity_vec))] <- 0 
          target_change <- abs(target_change)
          target_change <- eval(parse(text = eval_transform))
          target_change[which(target_change == -Inf)] <- 0
          target_change <- target_change * identity_vec
          data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[1])] <- target_change
          writeLines('Transformed Successfully')
          r2_df <- NULL
          writeLines("")
        }
        else{
          writeLines("Variable Transform has reached its peak! Reached the end of Results")
          writeLines(paste0("Peak of Adj R Square Achieved: ", old_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_transformed,
                      transformed_variable_from_start = history_transform))
        }
      }
      else{
        while(1){
          var_transform <- readline(prompt=paste0("Which Variable do you want to transform? (NO MORE to exit):  "))
          var_transform_function <- readline(prompt=paste0("Which Transform Function to use? (NO MORE to exit):  "))
          if(var_transform == "NO MORE" || var_transform_function == "NO MORE"){
            writeLines("Reached the End of Results!")
            return(list(first_data = data, 
                        last_data = data_transformed,
                        transformed_variable_from_start = history_transform))
          }else if(var_transform %in% r2_df$transformed_columns && var_transform_function %in% r2_df$transform_function){
            var_exact_transform_r2 <- which(r2_df$transformed_columns == var_transform & 
                                              r2_df$transform_function == var_transform_function)
            history_transform <- c(history_transform, paste0(var_transform_function, "(", var_transform, ")"))
            target_change <- data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])]
            eval_transform <- paste0(var_transform_function,"(target_change)")
            identity_vec <- target_change / abs(target_change)
            identity_vec[which(is.na(identity_vec))] <- 0 
            target_change <- abs(target_change)
            target_change <- eval(parse(text = eval_transform))
            target_change[which(target_change == -Inf)] <- 0
            target_change <- target_change * identity_vec
            data_transformed[,which(colnames(data_transformed) == r2_df$transformed_columns[var_exact_transform_r2])] <- target_change
            writeLines('Transformed Successfully')
            r2_df <- NULL
            writeLines("")
            break
          }else{
            message("Please Input a valid variable and transform function!")
          }
        }
      }
      f <- f+1
    }
    writeLines(paste0("Done Selecting After ",n_select," Times! Reached the end of Results"))
    return(list(first_data = data, 
                last_data = data_transformed,
                transformed_variable_from_start = history_transform))
  }
}

drop_all_possible_variable <- function(data, dependent_col, auto_best = TRUE, r2_increase_threshold = 0.1, 
                                       show_model_comparison = FALSE){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_dropped <- data
  history_drop <- c()
  while(1){
    tic()
    writeLines("Begin to calculate every variable drops. .")
    for(v in 2:length(all_indep_col)){
      datatemp <- data_dropped[,-v]
      if(is.null(r2_df)){
        lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
        r2_df <- data.frame(Model_name = "Model1", column_to_drop = colnames(data_dropped)[v], 
                            r2 = summary(lm_temp)$r.squared *100, adj_r2 = summary(lm_temp)$adj.r.squared *100)
      }else{
        lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
        r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",v-1), column_to_drop = colnames(data_dropped)[v], 
                                         r2 = summary(lm_temp)$r.squared *100, adj_r2 = summary(lm_temp)$adj.r.squared *100))
      }
    }
    toc()
    r2_df <- r2_df %>% arrange(desc(adj_r2))
    if(show_model_comparison){
      writeLines("")
      writeLines("Show the Performances")
      writeLines("")
      print(r2_df)
    }
    writeLines("")
    increase_r2 <- 0
    if(auto_best){
      old_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_dropped)
      new_data <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[1])]
      new_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = new_data)
      old_adj_r_squared <- summary(old_model)$adj.r.square * 100
      new_adj_r_squared <- summary(new_model)$adj.r.square * 100
      if(new_adj_r_squared > old_adj_r_squared){
        writeLines(paste0("Picked Best Model drop: ", r2_df$column_to_drop[1]))
        writeLines(paste0("Old Adj R Square: ", old_adj_r_squared, " Become New Adj R Square: ",new_adj_r_squared, 
                          " Increase (",new_adj_r_squared - old_adj_r_squared,"%)"))
        increase_r2 <- new_adj_r_squared - old_adj_r_squared
        var_exact_indep <- which(all_indep_col == r2_df$column_to_drop[1])
        history_drop <- c(history_drop, r2_df$column_to_drop[1])
        data_dropped <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[1])]
        all_indep_col <- all_indep_col[-var_exact_indep]
        writeLines('Drop Successfully')
        r2_df <- NULL
        writeLines("")
      }else{
        writeLines("Variable Drop has reached its peak! Reached the end of Results")
        writeLines(paste0("Peak of Adj R Square Achieved: ", old_adj_r_squared))
        return(list(first_data = data, 
                    last_data = data_dropped,
                    drop_variable_from_start = history_drop))
      }
      if(!is.na(r2_increase_threshold)){
        if(increase_r2 < r2_increase_threshold){
          writeLines(paste0("Variable Drop has no more significant improve over threshold ", r2_increase_threshold, "! Reached the end of Results"))
          writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_dropped,
                      drop_variable_from_start = history_drop))
        }
      }
      
    }
    else{
      while(1){
        var_drop <- readline(prompt=paste0("Which Variable do you want to drop? (NO MORE to exit):  "))
        if(var_drop == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(first_data = data, 
                      last_data = data_dropped,
                      drop_variable_from_start = history_drop))
        }else if(var_drop %in% r2_df$column_to_drop){
          var_exact_drop_r2 <- which(r2_df$column_to_drop == var_drop)
          var_exact_indep <- which(all_indep_col == var_drop)
          history_drop <- c(history_drop, var_drop)
          data_dropped <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[var_exact_drop_r2])]
          all_indep_col <- all_indep_col[-var_exact_indep]
          writeLines('Drop Successfully')
          r2_df <- NULL
          writeLines("")
          break
        }else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

drop_all_possible_variable_F <- function(data, dependent_col, auto_best = TRUE, r2_increase_threshold = 0.1, 
                                         show_model_comparison = FALSE, limit_execution_time = NA){
  library(dplyr)
  library(tictoc)
  r2_df <- NULL
  all_indep_col <- colnames(data)
  data_dropped <- data
  history_drop <- c()
  current_execution_time <- NA
  
  #Use .lm.fit for faster operation
  fitted.list <- function(object, X) {
    X %*% object$coefficients
  }
  resid.list <- function(object, X, y) {
    y_fitted <- fitted(object, X)
    y - y_fitted
  }
  rsquared <- function(x, ...) UseMethod("rsquared")
  rsquared.default <- function(x, ...) {
    summary(x)$r.squared
  }
  rsquared.list <- function(object, X, y) {
    e <- resid.list(object, X, y)
    1 - sum(e^2)/sum( (y - mean(y))^2 )
  }
  adj_rsquared_list <- function(object, X, y){
    r2 <- rsquared.list(object, X, y)
    k <- ncol(X) - 1
    n <- nrow(X)  
    rate_of_error <- (1 - r2) * (n - 1) / (n - k - 1)
    adj_r2 <- 1 - rate_of_error
    return(adj_r2)
  }
  
  while(1){
    tic()
    writeLines("Begin to calculate every variable drops. .")
    for(v in 2:length(all_indep_col)){
      datatemp <- data_dropped[,-v]
      if(is.null(r2_df)){
        datm <- as.matrix(datatemp)
        obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
        r2_df <- data.frame(Model_name = "Model1", column_to_drop = colnames(data_dropped)[v], 
                            r2 = obj_lm_fit_r2 *100, adj_r2 = obj_lm_fit_r2adj *100)
      }else{
        datm <- as.matrix(datatemp)
        obj_lm_fit <- .lm.fit(cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2 <- rsquared(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
        obj_lm_fit_r2adj <- adj_rsquared_list(obj_lm_fit, cbind(1,datm[,-1]), datm[,1])
        r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",v-1), column_to_drop = colnames(data_dropped)[v], 
                                         r2 = obj_lm_fit_r2 *100, adj_r2 = obj_lm_fit_r2adj *100))
      }
    }
    current_execution_temp <- toc()
    current_execution_time <- as.numeric(current_execution_temp$toc - current_execution_temp$tic)
    r2_df <- r2_df %>% arrange(desc(adj_r2))
    if(show_model_comparison){
      writeLines("")
      writeLines("Show the Performances")
      writeLines("")
      print(r2_df)
    }
    writeLines("")
    increase_r2 <- 0
    if(auto_best){
      old_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_dropped)
      new_data <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[1])]
      new_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = new_data)
      old_adj_r_squared <- summary(old_model)$adj.r.square * 100
      new_adj_r_squared <- summary(new_model)$adj.r.square * 100
      if(new_adj_r_squared > old_adj_r_squared){
        writeLines(paste0("Picked Best Model drop: ", r2_df$column_to_drop[1]))
        writeLines(paste0("Old Adj R Square: ", old_adj_r_squared, " Become New Adj R Square: ",new_adj_r_squared, 
                          " Increase (",new_adj_r_squared - old_adj_r_squared,"%)"))
        increase_r2 <- new_adj_r_squared - old_adj_r_squared
        var_exact_indep <- which(all_indep_col == r2_df$column_to_drop[1])
        history_drop <- c(history_drop, r2_df$column_to_drop[1])
        data_dropped <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[1])]
        all_indep_col <- all_indep_col[-var_exact_indep]
        writeLines('Drop Successfully')
        r2_df <- NULL
        writeLines("")
      }else{
        writeLines("Variable Drop has reached its peak! Reached the end of Results")
        writeLines(paste0("Peak of Adj R Square Achieved: ", old_adj_r_squared))
        return(list(first_data = data, 
                    last_data = data_dropped,
                    drop_variable_from_start = history_drop))
      }
      if(!is.na(r2_increase_threshold)){
        if(increase_r2 < r2_increase_threshold){
          writeLines(paste0("Variable Drop has no more significant improve over threshold ", r2_increase_threshold, "! Reached the end of Results"))
          writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
          return(list(first_data = data, 
                      last_data = data_dropped,
                      drop_variable_from_start = history_drop))
        }
      }
      if(!is.na(limit_execution_time)){
        if(current_execution_time > limit_execution_time){
          writeLines(paste0("Variable Drop Has Begin too Slow and reached the execution limit ",limit_execution_time, " Reached the end of Results"))
          writeLines(paste0("Peak of Adj R Square Achieved: ", new_adj_r_squared))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
      }
    }
    else{
      while(1){
        var_drop <- readline(prompt=paste0("Which Variable do you want to drop? (NO MORE to exit):  "))
        if(var_drop == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(first_data = data, 
                      last_data = data_dropped,
                      drop_variable_from_start = history_drop))
        }else if(var_drop %in% r2_df$column_to_drop){
          var_exact_drop_r2 <- which(r2_df$column_to_drop == var_drop)
          var_exact_indep <- which(all_indep_col == var_drop)
          history_drop <- c(history_drop, var_drop)
          data_dropped <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[var_exact_drop_r2])]
          all_indep_col <- all_indep_col[-var_exact_indep]
          writeLines('Drop Successfully')
          r2_df <- NULL
          writeLines("")
          break
        }else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

pick_starting_variable_ordinal <- function(data, dependent_col, auto_best = FALSE, 
                                           target_aic = NA, target_bic = NA, target_loglik = NA,
                                           metric_decrease_threshold = NA,
                                           show_model_comparison = FALSE, 
                                           evaluation_use="aic"){
  library(dplyr)
  library(tictoc)
  ord_df <- NULL
  all_indep_col <- colnames(data)
  data_pick <- data
  best_formula <- ""
  history_pick <- c()
  current_r2 <- NA
  current_adj_r2 <- NA
  
  while(1){
    writeLines("Beginning Picking All Possible variable . . .")
    for(x in 2:length(all_indep_col)){
      if(length(history_pick) == 0){
        model_formula <- as.formula(paste0(dependent_col, " ~ ", all_indep_col[x]))
        lm_model <- lm(model_formula, data = data)
        ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x), 
                                           pick_columns = colnames(data_pick)[x],
                                           aic = stats::AIC(lm_model), 
                                           bic = stats::BIC(lm_model),
                                           neg_loglikelihood = as.numeric(stats::logLik(lm_model)))
      }else{
        all_variable <- c(history_pick, all_indep_col[x])
        model_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(all_variable, collapse= " + ")))
        lm_model <- lm(model_formula, data = data)
        ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x), 
                                           pick_columns = colnames(data_pick)[x],
                                           aic = stats::AIC(lm_model), 
                                           bic = stats::BIC(lm_model),
                                           neg_loglikelihood = as.numeric(stats::logLik(lm_model)))
      }
    }
    
    if(evaluation_use == "aic"){
      ord_df <- ord_df %>% arrange(aic) #order by aic
    }else if(evaluation_use == "bic"){
      ord_df <- ord_df %>% arrange(bic) #order by bic
    }else if(evaluation_use == "loglik"){
      ord_df <- ord_df %>% arrange(neg_loglikelihood) #order by loglik
    }
    
    if(show_model_comparison){
      writeLines("")
      writeLines("Show Performances")
      writeLines("")
      print(ord_df)
    }
    writeLines("")
    performance_growth <- 0
    if(auto_best){
      if(length(history_pick) > 0){
        old_pick <- history_pick
        new_pick <- c(history_pick, r2_df$pick_columns[1])
        old_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(old_pick, collapse = " + ")))
        new_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(new_pick, collapse = " + ")))
        old_model <- polr(old_formula, data = data)
        new_model <- polr(new_formula, data = data)
        if(evaluation_use == "aic"){
          old_metric <- stats::AIC(old_model)
          new_metric <- stats::AIC(new_model)
        }else if(evaluation_use == "bic"){
          old_metric <- stats::BIC(old_model)
          new_metric <- stats::BIC(new_model)
        }else if(evaluation_use == "loglik"){
          old_metric <- as.numeric(stats::logLik(old_model))
          new_metric <- as.numeric(stats::logLik(new_model))
        }
        if(new_metric < old_metric){
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          if(evaluation_use == "aic"){
            writeLines(paste0("Old AIC: ", old_metric, " Become New AIC: ",new_metric, 
                              " Decrease (",old_metric - new_metric,")"))
          }else if(evaluation_use == "bic"){
            writeLines(paste0("Old BIC: ", old_metric, " Become New BIC: ",new_metric, 
                              " Decrease (",old_metric - new_metric,")"))
          }else if(evaluation_use == "loglik"){
            writeLines(paste0("Old LogLikelihood: ", old_metric, " Become New LogLikelihood: ",new_metric, 
                              " Decrease (",old_metric - new_metric,")"))
          }
          
          performance_growth <- old_metric - new_metric
          history_pick <- c(history_pick, ord_df$pick_columns[1])
          all_indep_col <- all_indep_col[-which(all_indep_col == ord_df$pick_columns[1])]
          data_pick <- data_pick[,-which(colnames(data_pick) == ord_df$pick_columns[1])]
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          print(best_formula)
          writeLines('Pick Successfully')
          ord_df <- NULL
          writeLines("")
        }else{
          writeLines("Variable Pick has reached its peak! Reached the end of Results")
          if(evaluation_use == "aic"){
            writeLines(paste0("Peak of AIC Achieved: ", old_metric))
          }else if(evaluation_use == "bic"){
            writeLines(paste0("Peak of BIC Achieved: ", old_metric))
          }else if(evaluation_use == "loglik"){
            writeLines(paste0("Peak of LogLikelihood Achieved: ", old_metric))
          }
          return(list(pickup_variable_from_start = history_pick,
                      formula_pickup = best_formula))
        }
        if(!is.na(target_aic)){
          if(new_metric < target_aic){
            writeLines("Variable Pick has reached AIC target! Reached the end of Results")
            writeLines(paste0("Peak of AIC Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
        if(!is.na(target_bic)){
          if(new_metric < target_bic){
            writeLines("Variable Pick has reached BIC target! Reached the end of Results")
            writeLines(paste0("Peak of BIC Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
        if(!is.na(target_loglik)){
          if(new_metric < target_loglik){
            writeLines("Variable Pick has reached neg Log Likelihood target! Reached the end of Results")
            writeLines(paste0("Peak of Negative Log Likelihood Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
        if(!is.na(metric_decrease_threshold)){
          if(performance_growth < metric_decrease_threshold){
            writeLines("Variable Pick has reached peak of metrics! Reached the end of Results")
            if(evaluation_use == "aic"){
              writeLines(paste0("Peak of AIC Achieved: ", old_metric))
            }else if(evaluation_use == "bic"){
              writeLines(paste0("Peak of BIC Achieved: ", old_metric))
            }else if(evaluation_use == "loglik"){
              writeLines(paste0("Peak of LogLikelihood Achieved: ", old_metric))
            }
            return(list(pickup_variable_from_start = history_pick,
                        formula_pickup = best_formula))
          }
        }
      }
      else{
        old_metric <- Inf
        if(evaluation_use == "aic"){
          new_metric <- ord_df$aic[1]
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          writeLines(paste0("Old AIC: ", old_metric, " Become New AIC: ",new_metric))
        }else if(evaluation_use == "bic"){
          new_metric <- ord_df$bic[1]
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          writeLines(paste0("Old BIC: ", old_metric, " Become New BIC: ",new_metric))
        }else if(evaluation_use == "loglik"){
          new_metric <- ord_df$neg_loglikelihood[1]
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          writeLines(paste0("Old Log Likelihood: ", old_metric, " Become New Log Likelihood: ",new_metric))
        }
        
        history_pick <- c(history_pick, ord_df$pick_columns[1])
        all_indep_col <- all_indep_col[-which(all_indep_col == ord_df$pick_columns[1])]
        data_pick <- data_pick[,-which(colnames(data_pick) == ord_df$pick_columns[1])]
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", history_pick[1])
        print(best_formula)
        writeLines('Pick Successfully')
        ord_df <- NULL
        writeLines("")
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      formula_pickup = best_formula))
        }
        else if(var_pick %in% ord_df$pick_columns){
          history_pick <- c(history_pick, var_pick)
          all_indep_col <- all_indep_col[-which(all_indep_col == var_pick)]
          data_pick <- data_pick[,-which(colnames(data_pick) == var_pick)]
          writeLines("Formula Built Until Now")
          if(length(history_pick) > 1){
            best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          }else{
            best_formula <- paste0(dependent_col, "~", history_pick[1])
          }
          writeLines('Pick Successfully')
          ord_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

pick_transform_starting_variable_ordinal <- function(data, dependent_col, auto_best = TRUE, use_all_method = "",
                                                     continue_pick_vars = NA, continue_pick_transform = NA,
                                                     target_aic = NA, target_bic = NA, target_loglik = NA,
                                                     metric_decrease_threshold = NA, print_delay = NA,
                                                     apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                              "power1_75","quadratic","cubic"),
                                                     show_model_comparison = FALSE, evaluation_use="aic"){
  library(dplyr)
  library(tictoc)
  library(rms)
  library(stats)
  ord_df <- NULL
  all_indep_col <- colnames(data)
  
  data_transformed <- data
  best_formula <- ""
  best_formula_interpretation <- ""
  history_pick <- c()
  history_transform <- c()
  
  if(!is.na(continue_pick_vars)){
    history_pick <- continue_pick_vars
    history_transform <- continue_pick_transform
    all_indep_col <- all_indep_col[-which(all_indep_col %in% history_pick)]
    data_pick <- data[,-which(colnames(data) %in% history_pick)]
    best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
    best_formula_interpretation <- paste0(dependent_col, " ~ ")
    for(j in 1:length(history_transform)){
      if(j < length(history_transform) && history_transform[j] != "None"){
        best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
      }
      else if(j < length(history_transform) && history_transform[j] == "None"){
        best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
      }
      else if(j == length(history_transform) && history_transform[j] != "None"){
        best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
      }
      else if(j == length(history_transform) && history_transform[j] == "None"){
        best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
      }
    }
  }else{
    data_pick <- data
  }
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  old_model_use <- ""
  new_model_use <- ""
  if(length(history_pick) < 100){
    old_model_use <- "orm" 
  }else{
    old_model_use <- "clm"
  }
  
  if(use_all_method != ""){
    if(use_all_method == "orm"){
      old_model_use <- "orm"
    }else if(use_all_method == "polr"){
      old_model_use <- "polr"
    }else if(use_all_method == "clm"){
      old_model_use <- "clm"
    }
  }
  switch_model <<- 1
  
  while(1){
    tic()
    writeLines("Beginning Picking All Possible variable . . .")
    error_try <- 0
    for(x in 2:length(all_indep_col)){
      if(length(history_pick) == 0){
        for(w in 1:(length(apply_transformation) + 1)){
          if(w == 1){
            model_formula <- as.formula(paste0(dependent_col, " ~ ", all_indep_col[x]))
            if(use_all_method != ""){
              tryCatch({
                if(use_all_method == "orm"){
                  lm_model <- orm(model_formula, data = data)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = "None",
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "orm"))
                }
                else if(use_all_method == "polr"){
                  lm_model <- polr(model_formula, data = data)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = "None",
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "polr"))
                }
                else if(use_all_method == "clm"){
                  lm_model <- suppressWarnings(clm(model_formula, data = data))
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = "None",
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "clm"))
                }
              },error = function(cond){
                message(paste0("Failed Modeling Model ",x," Uncomputable!"))
                message(cond)
              })
              
            }
            else{
              while(1){
                tryCatch({
                  if(length(all_variable) < 100 && error_try == 0){
                    lm_model <- orm(model_formula, data = data)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = "None",
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "orm"))
                  }
                  else if(switch_model == 1){
                    lm_model <- polr(model_formula, data = data)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = "None",
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "polr"))
                  }
                  else if(switch_model == 2){
                    lm_model <- suppressWarnings(clm(model_formula, data = data))
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = "None",
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "clm"))
                  }
                  switch_model <<- 1
                  error_try <<- 0
                  break
                  
                },error = function(cond){
                  message(paste0("Failed Modeling Model ",x," Uncomputable!"))
                  message(paste0("Column Culprit: ", colnames(data_pick)[x]))
                  message(cond)
                  error_try <<- error_try + 1
                  if(length(all_variable) >= 100 || error_try == 2){
                    switch_model <<- 2
                  }else if(error_try > 2){
                    break
                  }
                })
              }
            }
          }
          else{
            model_formula <- as.formula(paste0(dependent_col, " ~ ", all_indep_col[x]))
            eval_transform <- paste0("data_transformed[[all_indep_col[x]]] <- ",
                                     apply_transformation[w-1],"(data_transformed[[all_indep_col[x]]])")
            identity_vec <- data_transformed[[all_indep_col[x]]] / abs(data_transformed[[all_indep_col[x]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transformed[[all_indep_col[x]]] <- abs(data_transformed[[all_indep_col[x]]])
            eval(parse(text = eval_transform))
            data_transformed[[all_indep_col[x]]][which(data_transformed[[all_indep_col[x]]] == -Inf)] <- 0
            data_transformed[[all_indep_col[x]]] <- data_transformed[[all_indep_col[x]]] * identity_vec
            
            if(use_all_method != ""){
              tryCatch({
                if(use_all_method == "orm"){
                  lm_model <- orm(model_formula, data = data_transformed)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "orm"))
                  data_transformed <- data
                }
                else if(use_all_method == "polr"){
                  lm_model <- polr(model_formula, data = data_transformed)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "polr"))
                  data_transformed <- data
                }
                else if(use_all_method == "clm"){
                  lm_model <- clm(model_formula, data = data_transformed)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "clm"))
                  data_transformed <- data
                }
              }, error = function(cond){
                message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
                message(cond)
              })
              
            }
            else{
              while(1){
                tryCatch({
                  if(length(all_variable) < 100 && error_try == 0){
                    lm_model <- orm(model_formula, data = data_transformed)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = apply_transformation[w-1],
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "orm"))
                  }
                  else if(switch_model == 1){
                    lm_model <- polr(model_formula, data = data_transformed)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = apply_transformation[w-1],
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "polr"))
                  }
                  else if(switch_model == 2){
                    lm_model <- clm(model_formula, data = data_transformed)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = apply_transformation[w-1],
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "clm"))
                  }
                  data_transformed <- data
                  error_try <<- 0
                  switch_model <<- 1
                  break
                },error = function(cond){
                  message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
                  message(paste0("Column Culprit: ", colnames(data_pick)[x]))
                  message(cond)
                  error_try <<- error_try + 1
                  if(length(all_variable) >= 100 || error_try == 2){
                    switch_model <<- 2
                  }else if(error_try > 2){
                    break
                  }
                })
              }
            }
          }
        }
      }
      else{
        for(w in 1:(length(apply_transformation) + 1)){
          if(w == 1){
            all_variable <- c(history_pick, all_indep_col[x])
            model_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(all_variable, collapse= " + ")))
            if(use_all_method != ""){
              tryCatch({
                if(use_all_method == "orm"){
                  lm_model <- orm(model_formula, data = data)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = "None",
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "orm"))
                }
                else if(use_all_method == "polr"){
                  lm_model <- polr(model_formula, data = data)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = "None",
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "polr"))
                }
                else if(use_all_method == "clm"){
                  lm_model <- suppressWarnings(clm(model_formula, data = data))
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = "None",
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "clm"))
                }
              },error = function(cond){
                message(paste0("Failed Modeling Model ",x," Uncomputable!"))
                message(paste0("Column Culprit: ", colnames(data_pick)[x]))
                message(cond)
              })
            }
            else{
              while(1){
                tryCatch({
                  if(length(all_variable) < 100 && error_try == 0){
                    lm_model <- orm(model_formula, data = data)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = "None",
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "orm"))
                  }
                  else if(switch_model == 1){
                    lm_model <- polr(model_formula, data = data)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = "None",
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "polr"))
                  }
                  else if(switch_model == 2){
                    lm_model <- suppressWarnings(clm(model_formula, data = data))
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = "None",
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "clm"))
                  }
                  
                  switch_model <<- 1
                  error_try <<- 0
                  break
                  
                },error = function(cond){
                  message(paste0("Failed Modeling Model ",x," Uncomputable!"))
                  message(paste0("Column Culprit: ", colnames(data_pick)[x]))
                  message(cond)
                  error_try <<- error_try + 1
                  if(length(all_variable) >= 100 || error_try == 2){
                    switch_model <<- 2
                  }else if(error_try > 2){
                    break
                  }
                })
              }
            }
          }
          else{
            all_variable <- c(history_pick, all_indep_col[x])
            model_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(all_variable, collapse= " + ")))
            
            eval_transform <- paste0("data_transformed[[all_indep_col[x]]] <- ",
                                     apply_transformation[w-1],"(data_transformed[[all_indep_col[x]]])")
            identity_vec <- data_transformed[[all_indep_col[x]]] / abs(data_transformed[[all_indep_col[x]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transformed[[all_indep_col[x]]] <- abs(data_transformed[[all_indep_col[x]]])
            eval(parse(text = eval_transform))
            data_transformed[[all_indep_col[x]]][which(data_transformed[[all_indep_col[x]]] == -Inf)] <- 0
            data_transformed[[all_indep_col[x]]] <- data_transformed[[all_indep_col[x]]] * identity_vec
            
            model_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(all_variable, collapse= " + ")))
            if(use_all_method != ""){
              tryCatch({
                if(use_all_method == "orm"){
                  lm_model <- orm(model_formula, data = data_transformed)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "orm"))
                }
                else if(use_all_method == "polr"){
                  lm_model <- polr(model_formula, data = data_transformed)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "polr"))
                }
                else if(use_all_method == "clm"){
                  lm_model <- clm(model_formula, data = data_transformed)
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_pick)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "clm"))
                }
              },error = function(cond){
                message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
                message(paste0("Column Culprit: ", colnames(data_pick)[x]))
                message(cond)
              })
            }
            else{
              while(1){
                tryCatch({
                  if(length(all_variable) < 100 && error_try == 0){
                    lm_model <- orm(model_formula, data = data_transformed)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = apply_transformation[w-1],
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "orm"))
                  }
                  else if(switch_model == 1){
                    lm_model <- polr(model_formula, data = data_transformed)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = apply_transformation[w-1],
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "polr"))
                  }
                  else if(switch_model == 2){
                    lm_model <- clm(model_formula, data = data_transformed)
                    ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                       pick_columns = colnames(data_pick)[x],
                                                       var_transformation = apply_transformation[w-1],
                                                       aic = stats::AIC(lm_model), 
                                                       bic = stats::BIC(lm_model),
                                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                       model_success = "clm"))
                  }
                  data_transformed <- data
                  error_try <<- 0
                  switch_model <<- 1
                  break
                },error = function(cond){
                  message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
                  message(paste0("Column Culprit: ", colnames(data_pick)[x]))
                  message(cond)
                  error_try <<- error_try + 1
                  if(length(all_variable) >= 100 || error_try == 2){
                    switch_model <<- 2
                  }else if(error_try > 2){
                    break
                  }
                  
                })
              }
            }
          }
        }
      }
    }
    toc()
    
    if(evaluation_use == "aic"){
      ord_df <- ord_df %>% arrange(aic) #order by aic
    }else if(evaluation_use == "bic"){
      ord_df <- ord_df %>% arrange(bic) #order by bic
    }else if(evaluation_use == "loglik"){
      ord_df <- ord_df %>% arrange(neg_loglikelihood) #order by loglik
    }
    
    if(show_model_comparison){
      writeLines("")
      writeLines("Show Performances")
      writeLines("")
      print(ord_df)
      if(!is.na(print_delay)){
        Sys.sleep(print_delay)
      }
      
    }
    writeLines("")
    performance_growth <- 0
    
    if(auto_best){
      new_model_use <- ord_df$model_success[1]
      if(length(history_pick) > 0){
        old_pick <- history_pick
        new_pick <- c(history_pick, ord_df$pick_columns[1])
        old_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(old_pick, collapse = " + ")))
        new_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(new_pick, collapse = " + ")))
        old_model <- NULL
        new_model <- NULL
        
        if(old_model_use == "orm"){
          old_model <- orm(old_formula, data = data)
        }else if(old_model_use == "clm"){
          old_model <- suppressWarnings(clm(old_formula, data = data))
        }else if(old_model_use == "polr"){
          old_model <- polr(old_formula, data = data)
        }
        
        new_transform <- ord_df$var_transformation[1]
        
        if(new_transform == "None"){
          if(new_model_use == "polr"){
            new_model <- polr(new_formula, data = data)
          }else if(new_model_use == "orm"){
            new_model <- orm(new_formula, data = data)
          }else if(new_model_use == "clm"){
            new_model <- suppressWarnings(clm(new_formula, data = data))
          }
        }
        else{
          eval_transform <- paste0("data_transformed[[ord_df$pick_columns[1]]] <- ",new_transform,"(data_transformed[[ord_df$pick_columns[1]]])")
          identity_vec <- data_transformed[[ord_df$pick_columns[1]]] / abs(data_transformed[[ord_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transformed[[ord_df$pick_columns[1]]] <- abs(data_transformed[[ord_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data_transformed[[ord_df$pick_columns[1]]][which(data_transformed[[ord_df$pick_columns[1]]] == -Inf)] <- 0
          data_transformed[[ord_df$pick_columns[1]]] <- data_transformed[[ord_df$pick_columns[1]]] * identity_vec
          if(new_model_use == "polr"){
            new_model <- polr(new_formula, data = data_transformed)
          }else if(new_model_use == "orm"){
            new_model <- orm(new_formula, data = data_transformed)
          }else if(new_model_use == "clm"){
            new_model <- suppressWarnings(clm(new_formula, data = data_transformed))
          }
          data_transformed <- data
        }
        
        if(evaluation_use == "aic"){
          old_metric <- stats::AIC(old_model)
          new_metric <- stats::AIC(new_model)
        }else if(evaluation_use == "bic"){
          old_metric <- stats::BIC(old_model)
          new_metric <- stats::BIC(new_model)
        }else if(evaluation_use == "loglik"){
          old_metric <- as.numeric(stats::logLik(old_model))
          new_metric <- as.numeric(stats::logLik(new_model))
        }
        
        if(new_metric < old_metric){
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          writeLines(paste0("Picked Best Transform functions: ", ord_df$var_transformation[1]))
          if(evaluation_use == "aic"){
            writeLines(paste0("Old AIC: ", old_metric, " Become New AIC: ",new_metric, 
                              " Decrease (",old_metric - new_metric,")"))
          }else if(evaluation_use == "bic"){
            writeLines(paste0("Old BIC: ", old_metric, " Become New BIC: ",new_metric, 
                              " Decrease (",old_metric - new_metric,")"))
          }else if(evaluation_use == "loglik"){
            writeLines(paste0("Old LogLikelihood: ", old_metric, " Become New LogLikelihood: ",new_metric, 
                              " Decrease (",old_metric - new_metric,")"))
          }
          performance_growth <- old_metric - new_metric
          history_pick <- c(history_pick, ord_df$pick_columns[1])
          history_transform <- c(history_transform, ord_df$var_transformation[1])
          prior_parameter <- c(new_model$coefficients, 0, new_model$zeta)
          all_indep_col <- all_indep_col[-which(all_indep_col == ord_df$pick_columns[1])]
          data_pick <- data_pick[,-which(colnames(data_pick) == ord_df$pick_columns[1])]
          old_model_use <- ord_df$model_success[1]
          if(new_transform != "None"){
            eval_transform <- paste0("data[[ord_df$pick_columns[1]]] <- ",new_transform,"(data[[ord_df$pick_columns[1]]])")
            identity_vec <- data[[ord_df$pick_columns[1]]] / abs(data[[ord_df$pick_columns[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data[[ord_df$pick_columns[1]]] <- abs(data[[ord_df$pick_columns[1]]])
            eval(parse(text = eval_transform))
            data[[ord_df$pick_columns[1]]][which(data[[ord_df$pick_columns[1]]] == -Inf)] <- 0
            data[[ord_df$pick_columns[1]]] <- data[[ord_df$pick_columns[1]]] * identity_vec
          }
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick Transform Successfully')
          ord_df <- NULL
          writeLines("")
        }
        else{
          writeLines("Variable Pick has reached its peak! Reached the end of Results")
          if(evaluation_use == "aic"){
            writeLines(paste0("Peak of AIC Achieved: ", old_metric))
          }else if(evaluation_use == "bic"){
            writeLines(paste0("Peak of BIC Achieved: ", old_metric))
          }else if(evaluation_use == "loglik"){
            writeLines(paste0("Peak of LogLikelihood Achieved: ", old_metric))
          }
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
        if(!is.na(target_aic)){
          if(new_metric < target_aic){
            writeLines("Variable Pick has reached AIC target! Reached the end of Results")
            writeLines(paste0("Peak of AIC Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(target_bic)){
          if(new_metric < target_bic){
            writeLines("Variable Pick has reached BIC target! Reached the end of Results")
            writeLines(paste0("Peak of BIC Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(target_loglik)){
          if(new_metric < target_loglik){
            writeLines("Variable Pick has reached neg Log Likelihood target! Reached the end of Results")
            writeLines(paste0("Peak of Negative Log Likelihood Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(metric_decrease_threshold)){
          if(performance_growth < metric_decrease_threshold){
            writeLines("Variable Pick has reached peak of metrics! Reached the end of Results")
            if(evaluation_use == "aic"){
              writeLines(paste0("Peak of AIC Achieved: ", old_metric))
            }else if(evaluation_use == "bic"){
              writeLines(paste0("Peak of BIC Achieved: ", old_metric))
            }else if(evaluation_use == "loglik"){
              writeLines(paste0("Peak of LogLikelihood Achieved: ", old_metric))
            }
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        
      }
      else{
        old_metric <- Inf
        new_pick <- ord_df$pick_columns[1]
        new_transform <- ord_df$var_transformation[1]
        if(new_transform == "None"){
          if(new_model_use == "orm"){
            new_model <- orm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
          }else if(new_model_use == "polr"){
            new_model <- polr(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
          }else if(new_model_use == "clm"){
            new_model <- suppressWarnings(clm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick))
          }
          
        }else{
          eval_transform <- paste0("data_transformed[[ord_df$pick_columns[1]]] <- ",new_transform,"(data_transformed[[ord_df$pick_columns[1]]])")
          identity_vec <- data_transformed[[ord_df$pick_columns[1]]] / abs(data_transformed[[ord_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transformed[[ord_df$pick_columns[1]]] <- abs(data_transformed[[ord_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data_transformed[[ord_df$pick_columns[1]]][which(data_transformed[[ord_df$pick_columns[1]]] == -Inf)] <- 0
          data_transformed[[ord_df$pick_columns[1]]] <- data_transformed[[ord_df$pick_columns[1]]] * identity_vec
          if(new_model_use == "orm"){
            new_model <- orm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_transformed)
          }else if(new_model_use == "polr"){
            new_model <- polr(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_transformed)
          }else if(new_model_use == "clm"){
            new_model <- suppressWarnings(clm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_transformed))
          }
          data_transformed <- data
        }
        
        if(evaluation_use == "aic"){
          new_metric <- ord_df$aic[1]
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          writeLines(paste0("Old AIC: ", old_metric, " Become New AIC: ",new_metric))
        }else if(evaluation_use == "bic"){
          new_metric <- ord_df$bic[1]
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          writeLines(paste0("Old BIC: ", old_metric, " Become New BIC: ",new_metric))
        }else if(evaluation_use == "loglik"){
          new_metric <- ord_df$neg_loglikelihood[1]
          writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
          writeLines(paste0("Old Log Likelihood: ", old_metric, " Become New Log Likelihood: ",new_metric))
        }
        
        history_pick <- c(history_pick, ord_df$pick_columns[1])
        history_transform <- c(history_transform, ord_df$var_transformation[1])
        all_indep_col <- all_indep_col[-which(all_indep_col == ord_df$pick_columns[1])]
        data_pick <- data_pick[,-which(colnames(data_pick) == ord_df$pick_columns[1])]
        
        if(new_transform != "None"){
          eval_transform <- paste0("data[[ord_df$pick_columns[1]]] <- ",new_transform,"(data[[ord_df$pick_columns[1]]])")
          identity_vec <- data[[ord_df$pick_columns[1]]] / abs(data[[ord_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data[[ord_df$pick_columns[1]]] <- abs(data[[ord_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data[[ord_df$pick_columns[1]]][which(data[[ord_df$pick_columns[1]]] == -Inf)] <- 0
          data[[ord_df$pick_columns[1]]] <- data[[ord_df$pick_columns[1]]] * identity_vec
        }
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
        best_formula_interpretation <- paste0(dependent_col, " ~ ")
        for(j in 1:length(history_transform)){
          if(j < length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
          }
          else if(j < length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
          }
          else if(j == length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
          }
          else if(j == length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
          }
        }
        print(best_formula_interpretation)
        writeLines('Pick and Transformed Successfully')
        
        if(!is.na(target_aic)){
          if(new_metric < target_aic){
            writeLines("Variable Pick has reached AIC target! Reached the end of Results")
            writeLines(paste0("Peak of AIC Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(target_bic)){
          if(new_metric < target_bic){
            writeLines("Variable Pick has reached BIC target! Reached the end of Results")
            writeLines(paste0("Peak of BIC Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        if(!is.na(target_loglik)){
          if(new_metric < target_loglik){
            writeLines("Variable Pick has reached neg Log Likelihood target! Reached the end of Results")
            writeLines(paste0("Peak of Negative Log Likelihood Achieved: ", new_metric))
            return(list(pickup_variable_from_start = history_pick,
                        transform_variable_from_start = history_transform,
                        formula_pickup = best_formula,
                        formula_interpretation = best_formula_interpretation,
                        data_transformed = data))
          }
        }
        
        ord_df <- NULL
        writeLines("")
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        var_transform <- readline(prompt=paste0("Which Transform function do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE" && var_transform == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
        else if(var_pick %in% ord_df$pick_columns && var_transform %in% ord_df$var_transformation){
          history_pick <- c(history_pick, var_pick)
          history_transform <- c(history_transform, var_transform)
          all_indep_col <- all_indep_col[-which(all_indep_col == var_pick)]
          data_pick <- data_pick[,-which(colnames(data_pick) == var_pick)]
          new_model_use <- ord_df$model_success[which(ord_df$pick_columns == var_pick & 
                                                        ord_df$var_transformation == var_transform)]
          
          if(length(history_pick) > 0){
            new_pick <- c(history_pick, ord_df$pick_columns[1])
            new_formula <- as.formula(paste0(dependent_col, " ~ ", paste0(new_pick, collapse = " + ")))
            if(new_model_use == "orm"){
              new_model <- orm(new_formula, data = data_pick)
            }else if(new_model_use == "polr"){
              new_model <- polr(new_formula, data = data_pick)
            }else if(new_model_use == "clm"){
              new_model <- suppressWarnings(clm(new_formula, data = data_pick))
            }
          }else{
            new_pick <- ord_df$pick_columns[1]
            if(new_model_use == "orm"){
              new_model <- orm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
            }else if(new_model_use == "polr"){
              new_model <- polr(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick)
            }else if(new_model_use == "clm"){
              new_model <- suppressWarnings(clm(as.formula(paste0(dependent_col, " ~ ", new_pick)), data = data_pick))
            }
          }
          
          if(var_transform != "None"){
            eval_transform <- paste0("data[[ord_df$pick_columns[1]]] <- ",var_transform,"(data[[ord_df$pick_columns[1]]])")
            identity_vec <- data[[ord_df$pick_columns[1]]] / abs(data[[ord_df$pick_columns[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data[[ord_df$pick_columns[1]]] <- abs(data[[ord_df$pick_columns[1]]])
            eval(parse(text = eval_transform))
            data[[ord_df$pick_columns[1]]][which(data[[ord_df$pick_columns[1]]] == -Inf)] <- 0
            data[[ord_df$pick_columns[1]]] <- data[[ord_df$pick_columns[1]]] * identity_vec
          }
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick and Transformed Successfully')
          ord_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

selective_transform_variable_ordinal <- function(data, dependent_col, auto_best = TRUE, use_all_method = "",
                                                 target_aic = NA, target_bic = NA, target_loglik = NA,
                                                 metric_decrease_threshold = NA, print_delay = NA,
                                                 apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                          "power1_75","quadratic","cubic"),
                                                 show_model_comparison = FALSE, evaluation_use="aic"){
  library(dplyr)
  library(tictoc)
  library(rms)
  library(stats)
  ord_df <- NULL
  all_indep_col <- colnames(data)
  
  data_transformed <- data
  data_transformed <- data
  best_formula <- ""
  best_formula_interpretation <- ""
  history_pick <- c()
  history_transform <- c()
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  old_model_use <- ""
  new_model_use <- ""
  if(length(history_pick) < 100){
    old_model_use <- "orm" 
  }else{
    old_model_use <- "clm"
  }
  
  if(use_all_method != ""){
    if(use_all_method == "orm"){
      old_model_use <- "orm"
    }else if(use_all_method == "polr"){
      old_model_use <- "polr"
    }else if(use_all_method == "clm"){
      old_model_use <- "clm"
    }
  }
  switch_model <<- 1
  
  while(1){
    tic()
    writeLines("Beginning Transform All Possible variable . . .")
    error_try <- 0
    for(x in 2:length(all_indep_col)){
      for(w in 1:length(apply_transformation)){
        model_formula <- as.formula(paste0(dependent_col, " ~ ."))
        eval_transform <- paste0("data_transformed[[all_indep_col[x]]] <- ",
                                 apply_transformation[w-1],"(data_transformed[[all_indep_col[x]]])")
        identity_vec <- data_transformed[[all_indep_col[x]]] / abs(data_transformed[[all_indep_col[x]]])
        identity_vec[which(is.na(identity_vec))] <- 0 
        data_transformed[[all_indep_col[x]]] <- abs(data_transformed[[all_indep_col[x]]])
        eval(parse(text = eval_transform))
        data_transformed[[all_indep_col[x]]][which(data_transformed[[all_indep_col[x]]] == -Inf)] <- 0
        data_transformed[[all_indep_col[x]]] <- data_transformed[[all_indep_col[x]]] * identity_vec
        
        if(use_all_method != ""){
          tryCatch({
            if(use_all_method == "orm"){
              lm_model <- orm(model_formula, data = data_transformed)
              if(is.null(ord_df)){
                ord_df <- data.frame(Model_name = paste0("Model",x-1), 
                                     pick_columns = colnames(data_transformed)[x],
                                     var_transformation = apply_transformation[w],
                                     aic = stats::AIC(lm_model), 
                                     bic = stats::BIC(lm_model),
                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                     model_success = "orm")
              }else{
                ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                   pick_columns = colnames(data_transformed)[x],
                                                   var_transformation = apply_transformation[w],
                                                   aic = stats::AIC(lm_model), 
                                                   bic = stats::BIC(lm_model),
                                                   neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                   model_success = "orm"))
              }
              data_transformed <- data
            }
            else if(use_all_method == "polr"){
              lm_model <- polr(model_formula, data = data_transformed)
              if(is.null(ord_df)){
                ord_df <- data.frame(Model_name = paste0("Model",x-1), 
                                     pick_columns = colnames(data_transformed)[x],
                                     var_transformation = apply_transformation[w-1],
                                     aic = stats::AIC(lm_model), 
                                     bic = stats::BIC(lm_model),
                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                     model_success = "polr")
              }else{
                ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                   pick_columns = colnames(data_transformed)[x],
                                                   var_transformation = apply_transformation[w-1],
                                                   aic = stats::AIC(lm_model), 
                                                   bic = stats::BIC(lm_model),
                                                   neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                   model_success = "polr"))
              }
              
              data_transformed <- data
            }
            else if(use_all_method == "clm"){
              lm_model <- clm(model_formula, data = data_transformed)
              if(is.null(ord_df)){
                ord_df <- data.frame(Model_name = paste0("Model",x-1), 
                                     pick_columns = colnames(data_transformed)[x],
                                     var_transformation = apply_transformation[w-1],
                                     aic = stats::AIC(lm_model), 
                                     bic = stats::BIC(lm_model),
                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                     model_success = "clm")
              }else{
                ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                   pick_columns = colnames(data_transformed)[x],
                                                   var_transformation = apply_transformation[w-1],
                                                   aic = stats::AIC(lm_model), 
                                                   bic = stats::BIC(lm_model),
                                                   neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                   model_success = "clm"))
              }
              data_transformed <- data
            }
          }, error = function(cond){
            message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w], " Uncomputable!"))
            message(cond)
          })
          
        }
        else{
          while(1){
            tryCatch({
              if(length(all_variable) < 100 && error_try == 0){
                lm_model <- orm(model_formula, data = data_transformed)
                if(is.null(ord_df)){
                  ord_df <- data.frame(Model_name = paste0("Model",x-1), 
                                       pick_columns = colnames(data_transformed)[x],
                                       var_transformation = apply_transformation[w-1],
                                       aic = stats::AIC(lm_model), 
                                       bic = stats::BIC(lm_model),
                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                       model_success = "orm")
                }else{
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_transformed)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "orm"))
                }
              }
              else if(switch_model == 1){
                lm_model <- polr(model_formula, data = data_transformed)
                if(is.null(ord_df)){
                  ord_df <- data.frame(Model_name = paste0("Model",x-1), 
                                       pick_columns = colnames(data_transformed)[x],
                                       var_transformation = apply_transformation[w-1],
                                       aic = stats::AIC(lm_model), 
                                       bic = stats::BIC(lm_model),
                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                       model_success = "polr")
                }else{
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_transformed)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "polr"))
                }
              }
              else if(switch_model == 2){
                lm_model <- clm(model_formula, data = data_transformed)
                if(is.null(ord_df)){
                  ord_df <- data.frame(Model_name = paste0("Model",x-1), 
                                       pick_columns = colnames(data_transformed)[x],
                                       var_transformation = apply_transformation[w-1],
                                       aic = stats::AIC(lm_model), 
                                       bic = stats::BIC(lm_model),
                                       neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                       model_success = "clm")
                }else{
                  ord_df <- rbind(ord_df, data.frame(Model_name = paste0("Model",x-1), 
                                                     pick_columns = colnames(data_transformed)[x],
                                                     var_transformation = apply_transformation[w-1],
                                                     aic = stats::AIC(lm_model), 
                                                     bic = stats::BIC(lm_model),
                                                     neg_loglikelihood = as.numeric(stats::logLik(lm_model)),
                                                     model_success = "clm"))
                }
              }
              data_transformed <- data
              error_try <<- 0
              switch_model <<- 1
              break
            },error = function(cond){
              message(paste0("Failed Modeling Model ",x, " with transform function ", apply_transformation[w-1], " Uncomputable!"))
              message(paste0("Column Culprit: ", colnames(data_transformed)[x]))
              message(cond)
              error_try <<- error_try + 1
              if(length(all_variable) >= 100 || error_try == 2){
                switch_model <<- 2
              }else if(error_try > 2){
                break
              }
            })
          }
        }
        
      }
      
    }
    toc()
    
    if(evaluation_use == "aic"){
      ord_df <- ord_df %>% arrange(aic) #order by aic
    }else if(evaluation_use == "bic"){
      ord_df <- ord_df %>% arrange(bic) #order by bic
    }else if(evaluation_use == "loglik"){
      ord_df <- ord_df %>% arrange(neg_loglikelihood) #order by loglik
    }
    
    if(show_model_comparison){
      writeLines("")
      writeLines("Show Performances")
      writeLines("")
      print(ord_df)
      if(!is.na(print_delay)){
        Sys.sleep(print_delay)
      }
      
    }
    writeLines("")
    performance_growth <- 0
    
    if(auto_best){
      new_model_use <- ord_df$model_success[1]
      old_formula <- as.formula(paste0(dependent_col, " ~ ."))
      new_formula <- as.formula(paste0(dependent_col, " ~ ."))
      old_model <- NULL
      new_model <- NULL
      
      if(old_model_use == "orm"){
        old_model <- orm(old_formula, data = data)
      }else if(old_model_use == "clm"){
        old_model <- suppressWarnings(clm(old_formula, data = data))
      }else if(old_model_use == "polr"){
        old_model <- polr(old_formula, data = data)
      }
      
      new_transform <- ord_df$var_transformation[1]
      eval_transform <- paste0("data_transformed[[ord_df$pick_columns[1]]] <- ",new_transform,"(data_transformed[[ord_df$pick_columns[1]]])")
      identity_vec <- data_transformed[[ord_df$pick_columns[1]]] / abs(data_transformed[[ord_df$pick_columns[1]]])
      identity_vec[which(is.na(identity_vec))] <- 0 
      data_transformed[[ord_df$pick_columns[1]]] <- abs(data_transformed[[ord_df$pick_columns[1]]])
      eval(parse(text = eval_transform))
      data_transformed[[ord_df$pick_columns[1]]][which(data_transformed[[ord_df$pick_columns[1]]] == -Inf)] <- 0
      data_transformed[[ord_df$pick_columns[1]]] <- data_transformed[[ord_df$pick_columns[1]]] * identity_vec
      
      if(new_model_use == "polr"){
        new_model <- polr(new_formula, data = data_transformed)
      }else if(new_model_use == "orm"){
        new_model <- orm(new_formula, data = data_transformed)
      }else if(new_model_use == "clm"){
        new_model <- suppressWarnings(clm(new_formula, data = data_transformed))
      }
      
      data_transformed <- data
      
      if(evaluation_use == "aic"){
        old_metric <- stats::AIC(old_model)
        new_metric <- stats::AIC(new_model)
      }else if(evaluation_use == "bic"){
        old_metric <- stats::BIC(old_model)
        new_metric <- stats::BIC(new_model)
      }else if(evaluation_use == "loglik"){
        old_metric <- as.numeric(stats::logLik(old_model))
        new_metric <- as.numeric(stats::logLik(new_model))
      }
      
      if(new_metric < old_metric){
        writeLines(paste0("Picked Best Model variable: ", ord_df$pick_columns[1]))
        writeLines(paste0("Picked Best Transform functions: ", ord_df$var_transformation[1]))
        if(evaluation_use == "aic"){
          writeLines(paste0("Old AIC: ", old_metric, " Become New AIC: ",new_metric, 
                            " Decrease (",old_metric - new_metric,")"))
        }else if(evaluation_use == "bic"){
          writeLines(paste0("Old BIC: ", old_metric, " Become New BIC: ",new_metric, 
                            " Decrease (",old_metric - new_metric,")"))
        }else if(evaluation_use == "loglik"){
          writeLines(paste0("Old LogLikelihood: ", old_metric, " Become New LogLikelihood: ",new_metric, 
                            " Decrease (",old_metric - new_metric,")"))
        }
        performance_growth <- old_metric - new_metric
        history_pick <- c(history_pick, ord_df$pick_columns[1])
        history_transform <- c(history_transform, ord_df$var_transformation[1])
        
        old_model_use <- ord_df$model_success[1]
        eval_transform <- paste0("data[[ord_df$pick_columns[1]]] <- ",new_transform,"(data[[ord_df$pick_columns[1]]])")
        identity_vec <- data[[ord_df$pick_columns[1]]] / abs(data[[ord_df$pick_columns[1]]])
        identity_vec[which(is.na(identity_vec))] <- 0 
        data[[ord_df$pick_columns[1]]] <- abs(data[[ord_df$pick_columns[1]]])
        eval(parse(text = eval_transform))
        data[[ord_df$pick_columns[1]]][which(data[[ord_df$pick_columns[1]]] == -Inf)] <- 0
        data[[ord_df$pick_columns[1]]] <- data[[ord_df$pick_columns[1]]] * identity_vec
        
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
        best_formula_interpretation <- paste0(dependent_col, " ~ ")
        for(j in 1:length(history_transform)){
          if(j < length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
          }
          else if(j < length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
          }
          else if(j == length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
          }
          else if(j == length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
          }
        }
        print(best_formula_interpretation)
        writeLines('Pick Transform Successfully')
        ord_df <- NULL
        writeLines("")
      }
      else{
        writeLines("Variable Pick has reached its peak! Reached the end of Results")
        if(evaluation_use == "aic"){
          writeLines(paste0("Peak of AIC Achieved: ", old_metric))
        }else if(evaluation_use == "bic"){
          writeLines(paste0("Peak of BIC Achieved: ", old_metric))
        }else if(evaluation_use == "loglik"){
          writeLines(paste0("Peak of LogLikelihood Achieved: ", old_metric))
        }
        return(list(pickup_variable_from_start = history_pick,
                    transform_variable_from_start = history_transform,
                    formula_pickup = best_formula,
                    formula_interpretation = best_formula_interpretation,
                    data_transformed = data))
      }
      if(!is.na(target_aic)){
        if(new_metric < target_aic){
          writeLines("Variable Pick has reached AIC target! Reached the end of Results")
          writeLines(paste0("Peak of AIC Achieved: ", new_metric))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
      }
      if(!is.na(target_bic)){
        if(new_metric < target_bic){
          writeLines("Variable Pick has reached BIC target! Reached the end of Results")
          writeLines(paste0("Peak of BIC Achieved: ", new_metric))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
      }
      if(!is.na(target_loglik)){
        if(new_metric < target_loglik){
          writeLines("Variable Pick has reached neg Log Likelihood target! Reached the end of Results")
          writeLines(paste0("Peak of Negative Log Likelihood Achieved: ", new_metric))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
      }
      if(!is.na(metric_decrease_threshold)){
        if(performance_growth < metric_decrease_threshold){
          writeLines("Variable Pick has reached peak of metrics! Reached the end of Results")
          if(evaluation_use == "aic"){
            writeLines(paste0("Peak of AIC Achieved: ", old_metric))
          }else if(evaluation_use == "bic"){
            writeLines(paste0("Peak of BIC Achieved: ", old_metric))
          }else if(evaluation_use == "loglik"){
            writeLines(paste0("Peak of LogLikelihood Achieved: ", old_metric))
          }
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
      }
      
      
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        var_transform <- readline(prompt=paste0("Which Transform function do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE" && var_transform == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data))
        }
        else if(var_pick %in% ord_df$pick_columns && var_transform %in% ord_df$var_transformation){
          history_pick <- c(history_pick, var_pick)
          history_transform <- c(history_transform, var_transform)
          new_model_use <- ord_df$model_success[which(ord_df$pick_columns == var_pick & 
                                                        ord_df$var_transformation == var_transform)]
          eval_transform <- paste0("data[[ord_df$pick_columns[1]]] <- ",var_transform,"(data[[ord_df$pick_columns[1]]])")
          identity_vec <- data[[ord_df$pick_columns[1]]] / abs(data[[ord_df$pick_columns[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data[[ord_df$pick_columns[1]]] <- abs(data[[ord_df$pick_columns[1]]])
          eval(parse(text = eval_transform))
          data[[ord_df$pick_columns[1]]][which(data[[ord_df$pick_columns[1]]] == -Inf)] <- 0
          data[[ord_df$pick_columns[1]]] <- data[[ord_df$pick_columns[1]]] * identity_vec
          
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "None"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick and Transformed Successfully')
          ord_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

#---------------------------------- Class Overlap Reduction Tools for Classification ------------------------------
check_overlap_reduction <- function(data, dependent_col, initial_pick, dimred_technique_use = "pca", show_plot = F){
  library(tictoc)
  plot_cluster <- function(df, clust, individual_label="", method="k-means", 
                           clust_make = "make", dimred_technique = "all", 
                           clust_var_name="", show_plot=T){
    library(ggpubr)
    library(ggrepel)
    library(factoextra)
    library(skmeans)
    library(cluster)
    
    map_list <- list()
    
    dimension_reduction_by_dimred <- function(data, my_formula, knn_num=2, method="LaE",
                                              legend_pos="topright", n_dimension=2, no_message=TRUE,
                                              parameter_guide=FALSE){
      library(dimRed)
      library(lle)
      data_label <- data[[unlist(strsplit(my_formula, " ~ "))[1]]]
      my_formula <- as.formula(my_formula)
      dat <- as.dimRedData(my_formula, data)
      technic_name <- ""
      dimen <- NULL
      dimension_params <- NULL
      if(method == "LaE"){
        technic_name <- "Laplacian Eigenmaps"
        dimen <- dimRed::LaplacianEigenmaps()
        if(!no_message){
          writeLines("Laplacian Eigenmaps use a kernel and were originally developed to separate non-convex clusters under the name spectral clustering.")
          writeLines("Laplacian Eigenmaps Parameters")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        dimension_params$knn <- knn_num
        if(!no_message){
          print(dimension_params)
        }
        
        if(parameter_guide){
          writeLines("ndim -> the number of output dimensions.")
          writeLines("sparse -> A character vector specifying hot to make the graph sparse")
          writeLines("knn -> that a K-nearest neighbor graph is constructed")
          writeLines("eps -> an epsilon neighborhood graph is constructed, else a dense distance matrix is used.")
          writeLines("t -> Parameter for the transformation of the distance matrix by w=exp(-d^2/t), 
                 larger values give less weight to differences in distance, t == Inf treats all distances != 0 equally.")
          writeLines("norm -> logical, should the normed laplacian be used?")
        }
      }
      else if(method == "Hessian"){
        technic_name <- "Hessian LLE (Locally Linear Embedding)"
        dimen <- dimRed::HLLE()
        if(!no_message){
          writeLines("HLLE uses local hessians to approximate the curvines and is an extension to non-convex subsets in lowdimensional space.")
          writeLines("Hessian LLE (Local Linear Embeddin) Parameters")
        }
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        dimension_params$knn <- knn_num
        if(!no_message){
          print(dimension_params)
        }
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions")
          writeLines("knn -> that a K-nearest neighbor graph is constructed")
        }
      }
      else if(method == "ICA"){
        technic_name <- "ICA (Independent Component Analysis)"
        dimen <- dimRed::FastICA()
        if(!no_message){
          writeLines("ICA is used for blind signal separation of different sources. It is a linear Projection.")
          writeLines("ICA (Independent Component Analysis) Parameters")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions")
        }
      }
      else if(method == "Isomap"){
        technic_name <- "Isomap (Isometric Mapping)"
        dimen <- dimRed::Isomap()
        if(!no_message){
          writeLines("The Isomap algorithm approximates a manifold using geodesic distances on a k nearest neighbor graph.")
          writeLines("Then classical scaling is performed on the resulting distance matrix.")
          writeLines("Isomap Parameters")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$knn <- knn_num
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions")
          writeLines("knn -> that a K-nearest neighbor graph is constructed")
          writeLines("get_geod -> Should the geodesic distance matrix be kept, if TRUE, access it as getOtherData(x)$geod")
        }
      }
      else if(method == "DrL"){
        technic_name <- "Distributed Recursive Layout"
        dimen <- dimRed::DrL()
        if(!no_message){
          writeLines("DrL uses a complex algorithm to avoid local minima in the graph embedding which uses several steps.")
          writeLines("DrL (Distributed Recursive Graph Layout) Parameters")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        dimension_params$knn <- knn_num
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
          writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
          writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
        }
      }
      else if(method == "DM"){
        technic_name <- "Diffusion Maps"
        dimen <- dimRed:DiffusionMaps()
        if(!no_message){
          writeLines("Diffusion Maps uses a diffusion probability matrix to robustly approximate a manifold.")
          writeLines("Diffusion Maps (Local Linear Embeddin) Parameters")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$eps <- "auto"
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions")
          writeLines("d -> a function transforming a matrix row wise into a distance matrix or dist object, e.g. dist.")
          writeLines("eps -> The epsilon parameter that determines the diffusion weight matrix from a distance matrix d, exp(-d^2/eps), 
                 if set to 'auto' it will be set to the median distance to the 0.01*n nearest neighbor.")
          writeLines("t -> Time-scale parameter. The recommended value, 0, uses multiscale geometry.")
          writeLines("delta -> Sparsity cut-off for the symmetric graph Laplacian, a higher value results in more sparsity and faster calculation. The predefined value is 10^-5.")
        }
      }
      else if(method == "FR"){
        technic_name <- "Fruchterman Reingold"
        dimen <- dimRed::FruchtermanReingold()
        if(!no_message){
          writeLines("Fruchterman Reingold Graph Layout algorithm. puts the data into a circle and puts connected points close to each other.")
          writeLines("Fruchterman Reingold Graph Layout Parameters.")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
          writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
          writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
        }
      }
      else if(method == "DRR"){
        technic_name <- "PCA with Ridge Regression"
        dimen <- dimRed::DRR()
        if(!no_message) writeLines('DRR is a non-linear extension of PCA that uses Kernel Ridge regression.')
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions")
          writeLines("lambda -> regularization parameter for the ridge regression.")
          writeLines("kernel -> The kernel to use for KRR, defaults to rbfdot")
          writeLines("kernel.pars -> A list with kernel parameters, elements depend on the kernel used, rbfdot uses sigma.")
          writeLines("pca -> logical, should an initial pca step be performed, defaults to TRUE.")
          writeLines("pca.center -> logical, should the data be centered before the pca step. Defaults to TRUE.")
          writeLines("pca.scale -> logical, should the data be scaled before the pca ste. Defaults to FALSE.")
          writeLines("fastcv -> logical, should fastCV from the CVST package be used instead of normal cross-validation.")
          writeLines("fastcv.test -> If fastcv = TRUE, separate test data set for fastcv.")
          writeLines("cv.folds -> if fastcv = FALSE, specifies the number of folds for crossvalidation.")
          writeLines("fastkrr.nblocks -> integer, higher values sacrifice numerical accuracy for speed and less memory, see below for details")
          writeLines("verbose -> logical, should the cross-validation results be printed out.")
        }
      }
      else if(method == "KK"){
        technic_name <- "Kamada Kawai"
        dimen <- dimRed::KamadaKawai()
        if(!no_message){
          writeLines("Kamada Kawai is Graph embedding algorithms se the data as a graph. 
               Between the nodes of the graph exist attracting and repelling forces 
               which can be modeled as electrical fields or springs connecting the nodes.")
          writeLines("The graph is then forced into a lower dimensional representation")
          writeLines("That tries to represent the forces between the nodes accurately")
          writeLines('by minimizing the total energy of the attracting and repelling forces.')
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        dimension_params$knn <- knn_num
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
          writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
          writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
        }
      }
      else if(method == "LLE"){
        technic_name <- "Locally Linear Embedding"
        dimen <- dimRed::LLE()
        if(!no_message){
          writeLines('LLE approximates the points in the manifold by linear combination of its neighbors.')
          writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
        }
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        dimension_params$knn <- knn_num
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions, default=2")
          writeLines("knn -> that a K-nearest neighbor graph is constructed, default=50")
        }
      }
      else if(method == "MMDS"){
        technic_name <- "Metric MDS (Multi Dimensional Scaling)"
        dimen <- dimRed::MDS()
        if(!no_message){
          writeLines('Metric MDS tries to maintain distances in high- and low-dimensional space,')
          writeLines('it has the advantage over PCA that arbitrary distance functions can be used, but it is high computational')
          writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        dimension_params$d <- function(x) exp(stats::dist(x))
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions, default=2")
          writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
        }
      }
      else if(method == "NMDS"){
        technic_name <- "Non Metric Multi Dimensional Scaling"
        dimen <- dimRed::nMDS()
        if(!no_message){
          writeLines('Non Metric MDS is a non-linear extension of MDS using monotonic regression')
          writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
        }
        
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        dimension_params$d <- function(x) exp(stats::dist(x))
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> number of output dimensions, default=2")
          writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
        }
      }
      else if(method == "NNMF"){
        technic_name <- "Non Negative Matrix Factorization"
        dimen <- dimRed::NNMF()
        if(!no_message){
          writeLines("NNMF is a method for decomposing a matrix into a smaller dimension such that the constraint that the data (and the projection) are not negative is taken into account.")
        }
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> The number of output dimensions.")
          writeLines("method -> character, which algorithm should be used. See nmf for possible values. Defaults to brunet")
          writeLines("nrun -> integer, the number of times the computations are conducted. See nmf")
          writeLines("seed -> integer, a value to control the random numbers used.")
          writeLines("options -> named list, other options to pass to nmf")
        }
      }
      else if(method == "AE"){
        library(tensorflow)
        technic_name <- "Auto Encoder"
        dimen <- dimRed::AutoEncoder()
        if(!no_message){
          writeLines('Autoencoders are neural networks that try to reproduce their input. Consider this method unstable, as the internals may still be changed.')
        }
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> The number of dimensions for reduction.")
          writeLines("n_hidden -> The number of neurons in the hidden layers, the length specifies the number of layers, 
                 the length must be impair, the middle number must be the same as ndim.")
          writeLines("activation -> The activation functions for the layers, one of tanh, sigmoid, relu, elu")
          writeLines("Everything else will silently be ignored and there will be no activation function for the layer.")
          writeLines("weight_decay -> the coefficient for weight decay, set to 0 if no weight decay desired.")
          writeLines("learning_rate -> The learning rate for gradient descend")
          writeLines("graph -> Optional: A list of bits and pieces that define the autoencoder in tensorflow, see details.")
          writeLines("keras_graph -> Optional: A list of keras layers that define the encoder and decoder, specifying this, will ignore all other topology related variables, see details.")
          writeLines("batchsize -> If NA, all data will be used for training, else only a random subset of size batchsize will be used")
          writeLines("n_steps -> the number of training steps.")
        }
      }
      else if(method == "KPCA"){
        technic_name <- "Kernel PCA"
        dimen <- dimRed::AutoEncoder()
        if(!no_message){
          writeLines('Kernel PCA is a nonlinear extension of PCA using kernel methods.')
        }
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> the number of output dimensions, defaults to 2")
          writeLines("kpar -> A list with the parameters for the kernel function, defaults to list(sigma = 0.1)")
          writeLines("kernel -> The kernel function, either as a function or a character vector with the name of the kernel. Defaults to rbfdot")
        }
      }
      else if(method == "PCA_L1"){
        technic_name <- "Regularization PCA L1"
        dimen <- dimRed::PCA_L1()
        if(!no_message){
          writeLines("PCA transforms the data so that the L2 reconstruction error is minimized or the variance of the projected data is maximized.")
          writeLines("This is sensitive to outliers, L1 PCA minimizes the L1 reconstruction error or maximizes the sum of the L1 norm of the projected observations.")
        }
        dimension_params <- dimen@stdpars
        dimension_params$ndim <- n_dimension
        if(!no_message) print(dimension_params)
        if(parameter_guide){
          writeLines("ndim -> The number of output dimensions.")
          writeLines("center -> logical, should the data be centered, defaults to TRUE.")
          writeLines("scale -> logical, should the data be scaled, defaults to FALSE.")
          writeLines("fun -> character or function, the method to apply, see the pcaL1 package")
        }
      }
      emb <- dimen@fun(dat, dimension_params) 
      dimensional_data <- data.frame(emb@data@data)
      return(dimensional_data)
    }
    
    uniform_manifold_approximation_projection <- function(data){
      library(umap)
      umap_result <- umap(data)
      return(umap_result)
    }
    
    tSNE <- function(data_train, perplex_param=15, 
                     check_duplicate=TRUE, use_initial_pca=TRUE,
                     partial_pca_enable=FALSE){
      library(Rtsne)
      library(irlba)
      
      data_train <- as.matrix(data_train)
      
      # You can change the value of perplexity and see how the plot changes
      tsne_results <- Rtsne(data_train, perplexity=perplex_param, 
                            check_duplicates = check_duplicate,
                            pca = use_initial_pca,
                            partial_pca = partial_pca_enable)
      return(tsne_results)
    }
    
    if(dimred_technique == "pca" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          pca <- stats::prcomp(df, scale = FALSE, center = FALSE)
          ind <- facto_summarize(pca, element = "ind", result = "coord", axes = c(1,2))
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- as.factor(clust)
          }
          eig <- get_eigenvalue(pca)[axes, 2]
          xlab = paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
          ylab = paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
          
          map_list <- append(map_list, list(ind[,which(colnames(ind) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "PCA_DIMENSION"
          
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with PCA! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "ica" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          ica <- NULL
          if(clust_make == "make"){
            data_with_factor <- as.data.frame(cbind(df, clust$cluster))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            ica <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="ICA")
            ica <- as.data.frame(ica)
            ica$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            data_with_factor <- as.data.frame(cbind(df, clust))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            ica <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="ICA")
            ica <- as.data.frame(ica)
            ica$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(ica[,which(colnames(ica) %in% c("ICA1","ICA2","cluster"))]))
          names(map_list)[length(map_list)] <- "ICA_DIMENSION"
          xlab = paste0("ICA1")
          ylab = paste0("ICA2")
          
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ica, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "ICA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ica, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "ICA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ica, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "ICA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ica, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "ICA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "ICA1", "ICA2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with ICA! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "umap" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          umap <- uniform_manifold_approximation_projection(df)
          umap <- as.data.frame(umap$layout)
          colnames(umap) <- c("Dim.1","Dim.2")
          if(clust_make == "make"){
            umap$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            umap$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(umap[,which(colnames(umap) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "UMAP_DIMENSION"
          
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with UMAP! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "tsne" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          tsne <- tSNE(df)
          tsne <- as.data.frame(tsne$Y)
          colnames(tsne) <- c('Dim.1',"Dim.2")
          if(clust_make == "make"){
            tsne$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            tsne$cluster <- as.factor(clust)
          }
          map_list <- append(map_list, list(tsne[,which(colnames(tsne) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "TSNE_DIMENSION"
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if(ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with TSNE! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "lae" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          lae <- NULL
          if(clust_make == "make"){
            data_with_factor <- as.data.frame(cbind(df, clust$cluster))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            data_with_factor <- as.data.frame(cbind(df, clust))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(lae[,which(colnames(lae) %in% c("LEIM1","LEIM2","cluster"))]))
          names(map_list)[length(map_list)] <- "LAE_DIMENSION"
          xlab = paste0("LEIM1")
          ylab = paste0("LEIM2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      },error = function(cond){
        message("Unable to do dimensionaly reduction with LaE! Here is Original Message")
        message(cond)
      })
    }
    return(map_list)
  }
  
  cluster_overlapping_measure <- function(xy_mapping, view_plot=T, dimred_name = ""){
    unique_cluster <- unique(xy_mapping$cluster)
    color_collection <- c("#ff584d","#ffff00","#3bff4e","#8a7dff","#d614e0",
                          "#1fa61f","#59dae3","#bc13f0","#ff1cce","#ff709b",
                          "#f4ff26","#81b584","#ff29d4","#b30009","#ff4f1f")
    
    if(view_plot){
      x11()
      plot(xy_mapping[,1],xy_mapping[,2],
           xlim = c(min(xy_mapping[,1]), max(xy_mapping[,1])), 
           ylim = c(min(xy_mapping[,2]), max(xy_mapping[,2])), col = xy_mapping[,3])
      for(y in 1:length(unique_cluster)){
        polygon(x = xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[y])],
                y = xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[y])],
                col = color_collection[y])
      }
    }
    
    library(sp)
    library(dplyr)
    overlapping_df <- data.frame()
    dimred_technique <- c()
    cluster_vector <- c()
    point_from_cluster <- c()
    overlapping_vector <- c()
    point_total <- c()
    for(v in 1:length(unique_cluster)){
      p <- Polygon(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]), c(1,2)])
      ps <- Polygons(list(p),1)
      sps <- SpatialPolygons(list(ps))
      polygon_coords <- ps@Polygons[[1]]@coords
      #check data point for every clusters
      for(x in 1:length(unique_cluster)){
        point_cluster <- nrow(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]),])
        point_location <- point.in.polygon(xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[x])], 
                                           xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[x])], 
                                           polygon_coords[,1], polygon_coords[,2], mode.checked=FALSE)
        overlap_measure <- length(point_location[point_location > 0])
        cluster_vector <- c(cluster_vector, paste0("Cluster-",v,"-",unique_cluster[v]))
        point_from_cluster <- c(point_from_cluster, paste0("Cluster-",x,"-",unique_cluster[x]))
        overlapping_vector <- c(overlapping_vector, overlap_measure)
        point_total <- c(point_total, point_cluster)
      }
    }
    
    overlapping_df <- data.frame(cluster_vector, point_from_cluster, overlapping_vector, point_total)
    overlapping_pct <- overlapping_df %>% filter(cluster_vector != point_from_cluster) %>% 
      group_by(cluster_vector) %>% 
      summarise(overlap_sum = sum(overlapping_vector),
                point_total = point_total) %>%
      mutate(overlap_pct = round(overlap_sum / point_total * 100,2)) %>% as.data.frame()
    overlapping_pct <- overlapping_pct[!duplicated(overlapping_pct),]
    overlapping_pct$dimred_technique <- dimred_name
    return(list(overlapping_df, overlapping_pct))
  }
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  library(dplyr)
  library(stringr)
  library(tictoc)
  library(data.table)
  
  options(dplyr.summarise.inform = FALSE)
  
  #transform first from any transformation in initial pick
  data_transform <- data
  transformed_pick <- initial_pick
  for(a in 1:length(initial_pick)){
    transform_target <- unlist(lapply(strsplit(initial_pick[a], "_"), FUN = function(x) x[1]))
    var_target <- paste0(unlist(lapply(strsplit(initial_pick[a], "_"), FUN = function(x) x[-1])), collapse = "_")
    if(transform_target %in% apply_transformation){
      idx <- which(colnames(data_transform) == var_target)
      eval_transform <- paste0("data_transform[,idx] <- ",transform_target,"(data_transform[,idx])")
      identity_vec <- data_transform[,idx] / abs(data_transform[,idx])
      identity_vec[which(is.na(identity_vec))] <- 0 
      data_transform[,idx] <- abs(data_transform[,idx])
      eval(parse(text = eval_transform))
      data_transform[,idx][which(data_transform[,idx] == -Inf)] <- 0
      data_transform[,idx] <- data_transform[,idx] * identity_vec
      initial_pick[a] <- var_target
    }
  }
  
  data_pick <- data_transform[,which(colnames(data_transform) %in% c(dependent_col, initial_pick))]
  cluster_simulation <- plot_cluster(data_pick[,-which(colnames(data_pick) == dependent_col)], 
                                     data_pick[[dependent_col]], clust_make = "input", 
                                     dimred_technique = dimred_technique_use, clust_var_name = dependent_col, show_plot = show_plot)
  overlap_measure_list <- list()
  all_overlap <- NULL
  for(x in 1:length(cluster_simulation)){
    overlap_measure_list <- append(overlap_measure_list, 
                                   list(cluster_overlapping_measure(cluster_simulation[[1]], view_plot = F, 
                                                                    dimred_name = names(cluster_simulation)[x])))
    all_overlap <- rbind(all_overlap, overlap_measure_list[[x]][[2]])
  }
  
  all_overlap <- all_overlap %>% arrange(cluster_vector)
  all_overlap <- all_overlap %>% group_by(cluster_vector) %>% 
    summarise(point_total = point_total,
              min_overlap = min(overlap_sum), 
              min_overlap_pct = min(overlap_pct), 
              max_overlap = max(overlap_sum),
              max_overlap_pct = max(overlap_pct)) %>% as.data.frame()
  
  all_overlap <- all_overlap[!duplicated(all_overlap),]
  return(list(all_overlap, data_pick))
}

pick_transform_overlap_reduction <- function(data, dependent_col, auto_best = TRUE, var_continue = NA, var_exit = NA,
                                             start_by_nvar = 3, target_min_overlap_sum = 3, 
                                             apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                      "power1_75","quadratic","cubic"),
                                             show_model_comparison = FALSE, limit_execution_time = NA, 
                                             dimred_technique_use = "pca"){
  
  library(tictoc)
  library(stringr)
  
  dimension_reduction_by_dimred <- function(data, my_formula, knn_num=2, method="LaE",
                                            legend_pos="topright", n_dimension=2, no_message=TRUE,
                                            parameter_guide=FALSE){
    library(dimRed)
    library(lle)
    data_label <- data[[unlist(strsplit(my_formula, " ~ "))[1]]]
    my_formula <- as.formula(my_formula)
    dat <- as.dimRedData(my_formula, data)
    technic_name <- ""
    dimen <- NULL
    dimension_params <- NULL
    if(method == "LaE"){
      technic_name <- "Laplacian Eigenmaps"
      dimen <- dimRed::LaplacianEigenmaps()
      if(!no_message){
        writeLines("Laplacian Eigenmaps use a kernel and were originally developed to separate non-convex clusters under the name spectral clustering.")
        writeLines("Laplacian Eigenmaps Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message){
        print(dimension_params)
      }
      
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions.")
        writeLines("sparse -> A character vector specifying hot to make the graph sparse")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("eps -> an epsilon neighborhood graph is constructed, else a dense distance matrix is used.")
        writeLines("t -> Parameter for the transformation of the distance matrix by w=exp(-d^2/t), 
                 larger values give less weight to differences in distance, t == Inf treats all distances != 0 equally.")
        writeLines("norm -> logical, should the normed laplacian be used?")
      }
    }
    else if(method == "Hessian"){
      technic_name <- "Hessian LLE (Locally Linear Embedding)"
      dimen <- dimRed::HLLE()
      if(!no_message){
        writeLines("HLLE uses local hessians to approximate the curvines and is an extension to non-convex subsets in lowdimensional space.")
        writeLines("Hessian LLE (Local Linear Embeddin) Parameters")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message){
        print(dimension_params)
      }
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
      }
    }
    else if(method == "ICA"){
      technic_name <- "ICA (Independent Component Analysis)"
      dimen <- dimRed::FastICA()
      if(!no_message){
        writeLines("ICA is used for blind signal separation of different sources. It is a linear Projection.")
        writeLines("ICA (Independent Component Analysis) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
      }
    }
    else if(method == "Isomap"){
      technic_name <- "Isomap (Isometric Mapping)"
      dimen <- dimRed::Isomap()
      if(!no_message){
        writeLines("The Isomap algorithm approximates a manifold using geodesic distances on a k nearest neighbor graph.")
        writeLines("Then classical scaling is performed on the resulting distance matrix.")
        writeLines("Isomap Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$knn <- knn_num
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("get_geod -> Should the geodesic distance matrix be kept, if TRUE, access it as getOtherData(x)$geod")
      }
    }
    else if(method == "DrL"){
      technic_name <- "Distributed Recursive Layout"
      dimen <- dimRed::DrL()
      if(!no_message){
        writeLines("DrL uses a complex algorithm to avoid local minima in the graph embedding which uses several steps.")
        writeLines("DrL (Distributed Recursive Graph Layout) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DM"){
      technic_name <- "Diffusion Maps"
      dimen <- dimRed:DiffusionMaps()
      if(!no_message){
        writeLines("Diffusion Maps uses a diffusion probability matrix to robustly approximate a manifold.")
        writeLines("Diffusion Maps (Local Linear Embeddin) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$eps <- "auto"
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("d -> a function transforming a matrix row wise into a distance matrix or dist object, e.g. dist.")
        writeLines("eps -> The epsilon parameter that determines the diffusion weight matrix from a distance matrix d, exp(-d^2/eps), 
                 if set to 'auto' it will be set to the median distance to the 0.01*n nearest neighbor.")
        writeLines("t -> Time-scale parameter. The recommended value, 0, uses multiscale geometry.")
        writeLines("delta -> Sparsity cut-off for the symmetric graph Laplacian, a higher value results in more sparsity and faster calculation. The predefined value is 10^-5.")
      }
    }
    else if(method == "FR"){
      technic_name <- "Fruchterman Reingold"
      dimen <- dimRed::FruchtermanReingold()
      if(!no_message){
        writeLines("Fruchterman Reingold Graph Layout algorithm. puts the data into a circle and puts connected points close to each other.")
        writeLines("Fruchterman Reingold Graph Layout Parameters.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DRR"){
      technic_name <- "PCA with Ridge Regression"
      dimen <- dimRed::DRR()
      if(!no_message) writeLines('DRR is a non-linear extension of PCA that uses Kernel Ridge regression.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("lambda -> regularization parameter for the ridge regression.")
        writeLines("kernel -> The kernel to use for KRR, defaults to rbfdot")
        writeLines("kernel.pars -> A list with kernel parameters, elements depend on the kernel used, rbfdot uses sigma.")
        writeLines("pca -> logical, should an initial pca step be performed, defaults to TRUE.")
        writeLines("pca.center -> logical, should the data be centered before the pca step. Defaults to TRUE.")
        writeLines("pca.scale -> logical, should the data be scaled before the pca ste. Defaults to FALSE.")
        writeLines("fastcv -> logical, should fastCV from the CVST package be used instead of normal cross-validation.")
        writeLines("fastcv.test -> If fastcv = TRUE, separate test data set for fastcv.")
        writeLines("cv.folds -> if fastcv = FALSE, specifies the number of folds for crossvalidation.")
        writeLines("fastkrr.nblocks -> integer, higher values sacrifice numerical accuracy for speed and less memory, see below for details")
        writeLines("verbose -> logical, should the cross-validation results be printed out.")
      }
    }
    else if(method == "KK"){
      technic_name <- "Kamada Kawai"
      dimen <- dimRed::KamadaKawai()
      if(!no_message){
        writeLines("Kamada Kawai is Graph embedding algorithms se the data as a graph. 
               Between the nodes of the graph exist attracting and repelling forces 
               which can be modeled as electrical fields or springs connecting the nodes.")
        writeLines("The graph is then forced into a lower dimensional representation")
        writeLines("That tries to represent the forces between the nodes accurately")
        writeLines('by minimizing the total energy of the attracting and repelling forces.')
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "LLE"){
      technic_name <- "Locally Linear Embedding"
      dimen <- dimRed::LLE()
      if(!no_message){
        writeLines('LLE approximates the points in the manifold by linear combination of its neighbors.')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=50")
      }
    }
    else if(method == "MMDS"){
      technic_name <- "Metric MDS (Multi Dimensional Scaling)"
      dimen <- dimRed::MDS()
      if(!no_message){
        writeLines('Metric MDS tries to maintain distances in high- and low-dimensional space,')
        writeLines('it has the advantage over PCA that arbitrary distance functions can be used, but it is high computational')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NMDS"){
      technic_name <- "Non Metric Multi Dimensional Scaling"
      dimen <- dimRed::nMDS()
      if(!no_message){
        writeLines('Non Metric MDS is a non-linear extension of MDS using monotonic regression')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NNMF"){
      technic_name <- "Non Negative Matrix Factorization"
      dimen <- dimRed::NNMF()
      if(!no_message){
        writeLines("NNMF is a method for decomposing a matrix into a smaller dimension such that the constraint that the data (and the projection) are not negative is taken into account.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("method -> character, which algorithm should be used. See nmf for possible values. Defaults to brunet")
        writeLines("nrun -> integer, the number of times the computations are conducted. See nmf")
        writeLines("seed -> integer, a value to control the random numbers used.")
        writeLines("options -> named list, other options to pass to nmf")
      }
    }
    else if(method == "AE"){
      library(tensorflow)
      technic_name <- "Auto Encoder"
      dimen <- dimRed::AutoEncoder()
      if(!no_message){
        writeLines('Autoencoders are neural networks that try to reproduce their input. Consider this method unstable, as the internals may still be changed.')
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of dimensions for reduction.")
        writeLines("n_hidden -> The number of neurons in the hidden layers, the length specifies the number of layers, 
                 the length must be impair, the middle number must be the same as ndim.")
        writeLines("activation -> The activation functions for the layers, one of tanh, sigmoid, relu, elu")
        writeLines("Everything else will silently be ignored and there will be no activation function for the layer.")
        writeLines("weight_decay -> the coefficient for weight decay, set to 0 if no weight decay desired.")
        writeLines("learning_rate -> The learning rate for gradient descend")
        writeLines("graph -> Optional: A list of bits and pieces that define the autoencoder in tensorflow, see details.")
        writeLines("keras_graph -> Optional: A list of keras layers that define the encoder and decoder, specifying this, will ignore all other topology related variables, see details.")
        writeLines("batchsize -> If NA, all data will be used for training, else only a random subset of size batchsize will be used")
        writeLines("n_steps -> the number of training steps.")
      }
    }
    else if(method == "KPCA"){
      technic_name <- "Kernel PCA"
      dimen <- dimRed::AutoEncoder()
      if(!no_message){
        writeLines('Kernel PCA is a nonlinear extension of PCA using kernel methods.')
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions, defaults to 2")
        writeLines("kpar -> A list with the parameters for the kernel function, defaults to list(sigma = 0.1)")
        writeLines("kernel -> The kernel function, either as a function or a character vector with the name of the kernel. Defaults to rbfdot")
      }
    }
    else if(method == "PCA_L1"){
      technic_name <- "Regularization PCA L1"
      dimen <- dimRed::PCA_L1()
      if(!no_message){
        writeLines("PCA transforms the data so that the L2 reconstruction error is minimized or the variance of the projected data is maximized.")
        writeLines("This is sensitive to outliers, L1 PCA minimizes the L1 reconstruction error or maximizes the sum of the L1 norm of the projected observations.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("center -> logical, should the data be centered, defaults to TRUE.")
        writeLines("scale -> logical, should the data be scaled, defaults to FALSE.")
        writeLines("fun -> character or function, the method to apply, see the pcaL1 package")
      }
    }
    emb <- dimen@fun(dat, dimension_params) 
    dimensional_data <- data.frame(emb@data@data)
    return(dimensional_data)
  }
  
  uniform_manifold_approximation_projection <- function(data){
    library(umap)
    umap_result <- umap(data)
    return(umap_result)
  }
  
  tSNE <- function(data_train, perplex_param=15, 
                   check_duplicate=TRUE, use_initial_pca=TRUE,
                   partial_pca_enable=FALSE){
    library(Rtsne)
    library(irlba)
    
    data_train <- as.matrix(data_train)
    
    # You can change the value of perplexity and see how the plot changes
    tsne_results <- Rtsne(data_train, perplexity=perplex_param, 
                          check_duplicates = check_duplicate,
                          pca = use_initial_pca,
                          partial_pca = partial_pca_enable)
    return(tsne_results)
  }
  
  plot_cluster <- function(df, clust, individual_label="", method="k-means", 
                           clust_make = "make", dimred_technique = "all", 
                           clust_var_name="", show_plot=T){
    library(ggpubr)
    library(ggrepel)
    library(factoextra)
    library(skmeans)
    library(cluster)
    
    map_list <- list()
    
    if(dimred_technique == "pca" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          pca <- stats::prcomp(df, scale = FALSE, center = FALSE)
          ind <- facto_summarize(pca, element = "ind", result = "coord", axes = c(1,2))
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- as.factor(clust)
          }
          eig <- get_eigenvalue(pca)[axes, 2]
          xlab = paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
          ylab = paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
          
          map_list <- append(map_list, list(ind[,which(colnames(ind) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "PCA_DIMENSION"
          
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with PCA! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "umap" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          umap <- uniform_manifold_approximation_projection(df)
          umap <- as.data.frame(umap$layout)
          colnames(umap) <- c("Dim.1","Dim.2")
          if(clust_make == "make"){
            umap$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            umap$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(umap[,which(colnames(umap) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "UMAP_DIMENSION"
          
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with UMAP! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "tsne" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          tsne <- tSNE(df)
          tsne <- as.data.frame(tsne$Y)
          colnames(tsne) <- c('Dim.1',"Dim.2")
          if(clust_make == "make"){
            tsne$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            tsne$cluster <- as.factor(clust)
          }
          map_list <- append(map_list, list(tsne[,which(colnames(tsne) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "TSNE_DIMENSION"
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if(ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with TSNE! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "lae" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          lae <- NULL
          if(clust_make == "make"){
            data_with_factor <- as.data.frame(cbind(df, clust$cluster))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            data_with_factor <- as.data.frame(cbind(df, clust))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(lae[,which(colnames(lae) %in% c("LEIM1","LEIM2","cluster"))]))
          names(map_list)[length(map_list)] <- "LAE_DIMENSION"
          xlab = paste0("LEIM1")
          ylab = paste0("LEIM2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      },error = function(cond){
        message("Unable to do dimensionaly reduction with LaE! Here is Original Message")
        message(cond)
      })
    }
    return(map_list)
  }
  
  cluster_overlapping_measure <- function(xy_mapping, view_plot=T, dimred_name = ""){
    unique_cluster <- unique(xy_mapping$cluster)
    color_collection <- c("#ff584d","#ffff00","#3bff4e","#8a7dff","#d614e0",
                          "#1fa61f","#59dae3","#bc13f0","#ff1cce","#ff709b",
                          "#f4ff26","#81b584","#ff29d4","#b30009","#ff4f1f")
    
    if(view_plot){
      x11()
      plot(xy_mapping[,1],xy_mapping[,2],
           xlim = c(min(xy_mapping[,1]), max(xy_mapping[,1])), 
           ylim = c(min(xy_mapping[,2]), max(xy_mapping[,2])), col = xy_mapping[,3])
      for(y in 1:length(unique_cluster)){
        polygon(x = xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[y])],
                y = xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[y])],
                col = color_collection[y])
      }
    }
    
    library(sp)
    library(dplyr)
    overlapping_df <- data.frame()
    dimred_technique <- c()
    cluster_vector <- c()
    point_from_cluster <- c()
    overlapping_vector <- c()
    point_total <- c()
    for(v in 1:length(unique_cluster)){
      p <- Polygon(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]), c(1,2)])
      ps <- Polygons(list(p),1)
      sps <- SpatialPolygons(list(ps))
      polygon_coords <- ps@Polygons[[1]]@coords
      #check data point for every clusters
      for(x in 1:length(unique_cluster)){
        point_cluster <- nrow(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]),])
        point_location <- point.in.polygon(xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[x])], 
                                           xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[x])], 
                                           polygon_coords[,1], polygon_coords[,2], mode.checked=FALSE)
        overlap_measure <- length(point_location[point_location > 0])
        cluster_vector <- c(cluster_vector, paste0("Cluster-",v,"-",unique_cluster[v]))
        point_from_cluster <- c(point_from_cluster, paste0("Cluster-",x,"-",unique_cluster[x]))
        overlapping_vector <- c(overlapping_vector, overlap_measure)
        point_total <- c(point_total, point_cluster)
      }
    }
    
    overlapping_df <- data.frame(cluster_vector, point_from_cluster, overlapping_vector, point_total)
    overlapping_pct <- overlapping_df %>% filter(cluster_vector != point_from_cluster) %>% 
      group_by(cluster_vector) %>% 
      summarise(overlap_sum = sum(overlapping_vector),
                point_total = point_total) %>%
      mutate(overlap_pct = round(overlap_sum / point_total * 100,2)) %>% as.data.frame()
    overlapping_pct <- overlapping_pct[!duplicated(overlapping_pct),]
    overlapping_pct$dimred_technique <- dimred_name
    return(list(overlapping_df, overlapping_pct))
  }
  
  combination_element <- function(elements, pick_r){
    element_df <- data.frame(t(combn(elements, pick_r)))
    colnames(element_df) <- paste0("elements_", 1:pick_r)
    return(element_df)
  }
  
  permutation_element <- function(elements, pick_r, no_repeat=TRUE){
    eval_string <- "expand.grid("
    sort_string <- "element_df %>% arrange("
    for(a in 1:pick_r){
      if(a < pick_r){
        eval_string <- paste0(eval_string, "elements,")
        sort_string <- paste0(sort_string, "elements_",a,",")
      }
      else if(a == pick_r){
        eval_string <- paste0(eval_string, "elements)")
        sort_string <- paste0(sort_string, "elements_",a,")")
      }
    }
    element_df <- eval(parse(text = eval_string))
    colnames(element_df) <- paste0("elements_", 1:pick_r)
    
    if(no_repeat){
      library(dplyr)
      element_df$unique_labeler <- apply(element_df, FUN = function(x) length(unique(x)), MARGIN = 1)
      element_df <- element_df[which(element_df$unique_labeler == pick_r),]
      element_df <- element_df[,-ncol(element_df)]
      rownames(element_df) <- NULL
      element_df <- eval(parse(text = sort_string))
    }
    
    return(element_df)
  }
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  library(dplyr)
  library(tictoc)
  options(dplyr.summarise.inform = FALSE)
  
  overlap_reduction_df <- NULL
  data_pick <- data
  data_transformed <- data
  current_execution_time <- NA
  
  var_used <- colnames(data)[-which(colnames(data) == dependent_col)]
  transform_used <- c("none", apply_transformation)
  var_starts_combi <- combination_element(var_used, start_by_nvar)
  transform_starts_combi <- permutation_element(transform_used, start_by_nvar, no_repeat = FALSE)
  
  writeLines(paste0("There are exist ", nrow(var_starts_combi), " Overall Variable Combination to be tried"))
  
  if(is.na(var_continue)) var_continue <- 1
  if(is.na(var_exit)) var_exit <- var_continue + 7
  
  while(1){
    pick_temp <- c()
    transform_temp <- c()
    for(x in var_continue:var_exit){
      var_to_select <- as.character(unlist(var_starts_combi[x,]))
      writeLines("--------------------------- Beginning with combination variable of -------------------------------")
      print(var_to_select)
      for(y in 1:nrow(transform_starts_combi)){
        tic()
        writeLines(paste0("Current Var Combination (",x,")"))
        print(var_to_select)
        writeLines(paste0("Operation Transform ",y, " of ", nrow(transform_starts_combi)))
        transform_to_select <- as.character(unlist(transform_starts_combi[y,]))
        data_pick <- data[,which(colnames(data) %in% c(dependent_col, var_to_select))]
        for(z in 2:ncol(data_pick)){
          if(transform_to_select[z-1] != "none"){
            eval_transform <- paste0("data_pick[,z] <- ",transform_to_select[z-1],"(data_pick[,z])")
            identity_vec <- data_pick[,z] / abs(data_pick[,z])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_pick[,z] <- abs(data_pick[,z])
            data_pick[,z] <- eval(parse(text = eval_transform))
            data_pick[,z][which(data_pick[,z] == -Inf)] <- 0
            data_pick[,z] <- data_pick[,z] * identity_vec
          }
        }
        
        data_pick <- data_pick[!duplicated(data_pick),]
        cluster_simulation <- plot_cluster(data_pick[,-which(colnames(data_pick) == dependent_col)], 
                                           data_pick[[dependent_col]], clust_make = "input", 
                                           dimred_technique = dimred_technique_use, 
                                           clust_var_name = dependent_col, show_plot = F)
        overlap_measure_list <- list()
        all_overlap <- NULL
        for(x in 1:length(cluster_simulation)){
          overlap_measure_list <- append(overlap_measure_list, 
                                         list(cluster_overlapping_measure(cluster_simulation[[x]], view_plot = F, 
                                                                          dimred_name = names(cluster_simulation)[x])))
          all_overlap <- rbind(all_overlap,overlap_measure_list[[x]][[2]])
        }
        
        all_overlap <- all_overlap %>% arrange(cluster_vector)
        all_overlap <- all_overlap %>% group_by(cluster_vector) %>% 
          summarise(point_total = point_total,
                    min_overlap = min(overlap_sum), 
                    min_overlap_pct = min(overlap_pct), 
                    max_overlap = max(overlap_sum),
                    max_overlap_pct = max(overlap_pct)) %>% as.data.frame()
        
        all_overlap <- all_overlap[!duplicated(all_overlap),]
        if(is.null(overlap_reduction_df)){
          prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
          names(prefix_name) <- c(1:length(prefix_name))
          prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
          prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
          
          for(vars in 1:length(var_to_select)){
            if(vars == 1){
              overlap_reduction_df <- data.frame(all_overlap$point_total[vars], 
                                                 all_overlap$min_overlap[vars],
                                                 all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df) <- c(paste0(prefix_name[vars], "_pts"), 
                                                  paste0(prefix_name[vars], "_overlap_min"), 
                                                  paste0(prefix_name[vars], "_overlap_max"))
            }else{
              overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                     all_overlap$min_overlap[vars],
                                                     all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                      paste0(prefix_name[vars], "_overlap_min"), 
                                                      paste0(prefix_name[vars], "_overlap_max"))
              overlap_reduction_df <- as.data.frame(cbind(overlap_reduction_df, overlap_reduction_df_add))
              colnames(overlap_reduction_df)[(ncol(overlap_reduction_df) - 2):ncol(overlap_reduction_df)] <- colnames(overlap_reduction_df_add)
            }
          }
          
          for(vars in 1:length(var_to_select)){
            add_colname <- paste0("Var",vars)
            if(transform_to_select[vars] != "none"){
              overlap_reduction_df[[add_colname]] <- paste0(transform_to_select[vars],"_",var_to_select[vars])
            }else{
              overlap_reduction_df[[add_colname]] <- var_to_select[vars]
            }
          }
          
        }
        else{
          prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
          names(prefix_name) <- c(1:length(prefix_name))
          prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
          prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
          
          for(vars in 1:length(var_to_select)){
            if(vars == 1){
              overlap_reduction_df_temp <- data.frame(all_overlap$point_total[vars], 
                                                      all_overlap$min_overlap[vars],
                                                      all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df_temp) <- c(paste0(prefix_name[vars], "_pts"), 
                                                       paste0(prefix_name[vars], "_overlap_min"), 
                                                       paste0(prefix_name[vars], "_overlap_max"))
            }else{
              overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                     all_overlap$min_overlap[vars],
                                                     all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                      paste0(prefix_name[vars], "_overlap_min"), 
                                                      paste0(prefix_name[vars], "_overlap_max"))
              overlap_reduction_df_temp <- as.data.frame(cbind(overlap_reduction_df_temp, overlap_reduction_df_add))
              colnames(overlap_reduction_df_temp)[(ncol(overlap_reduction_df_temp) - 2):ncol(overlap_reduction_df_temp)] <- colnames(overlap_reduction_df_add)
            }
          }
          
          for(vars in 1:length(var_to_select)){
            add_colname <- paste0("Var",vars)
            if(transform_to_select[vars] != "none"){
              overlap_reduction_df_temp[[add_colname]] <- paste0(transform_to_select[vars],"_",var_to_select[vars])
            }else{
              overlap_reduction_df_temp[[add_colname]] <- var_to_select[vars]
            }
          }
          overlap_reduction_df <- rbind(overlap_reduction_df, overlap_reduction_df_temp)
        }
        toc()
      }
    }
    overlap_reduction_df$observation_included <- paste0((overlap_reduction_df[,1] + overlap_reduction_df[,4] + overlap_reduction_df[,7]),"/",nrow(data))
    overlap_reduction_df$redudancy_score <- overlap_reduction_df[,2] + overlap_reduction_df[,3] + overlap_reduction_df[,5] + overlap_reduction_df[,6] + overlap_reduction_df[,8] + overlap_reduction_df[,9]
    overlap_reduction_df <- overlap_reduction_df %>% arrange(redudancy_score)
    return(overlap_reduction_df)
  }
}

continue_pick_transform_overlap_reduction <- function(data, dependent_col, auto_best = TRUE, initial_pick=c(), initial_obs = 0,
                                                      initial_class_points = c(), initial_class_min_overlap = c(), 
                                                      initial_class_max_overlap = c(), initial_redudancy_score = 0,
                                                      minimize_redudancy_threshold = 5, lose_observation_threshold = 10, 
                                                      apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                               "power1_75","quadratic","cubic"), 
                                                      dimred_technique_use = "pca"){
  data_transform <- data
  all_unique_class <- unique(data[[dependent_col]])
  
  dimension_reduction_by_dimred <- function(data, my_formula, knn_num=2, method="LaE",
                                            legend_pos="topright", n_dimension=2, no_message=TRUE,
                                            parameter_guide=FALSE){
    library(dimRed)
    library(lle)
    data_label <- data[[unlist(strsplit(my_formula, " ~ "))[1]]]
    my_formula <- as.formula(my_formula)
    dat <- as.dimRedData(my_formula, data)
    technic_name <- ""
    dimen <- NULL
    dimension_params <- NULL
    if(method == "LaE"){
      technic_name <- "Laplacian Eigenmaps"
      dimen <- dimRed::LaplacianEigenmaps()
      if(!no_message){
        writeLines("Laplacian Eigenmaps use a kernel and were originally developed to separate non-convex clusters under the name spectral clustering.")
        writeLines("Laplacian Eigenmaps Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message){
        print(dimension_params)
      }
      
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions.")
        writeLines("sparse -> A character vector specifying hot to make the graph sparse")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("eps -> an epsilon neighborhood graph is constructed, else a dense distance matrix is used.")
        writeLines("t -> Parameter for the transformation of the distance matrix by w=exp(-d^2/t), 
                 larger values give less weight to differences in distance, t == Inf treats all distances != 0 equally.")
        writeLines("norm -> logical, should the normed laplacian be used?")
      }
    }
    else if(method == "Hessian"){
      technic_name <- "Hessian LLE (Locally Linear Embedding)"
      dimen <- dimRed::HLLE()
      if(!no_message){
        writeLines("HLLE uses local hessians to approximate the curvines and is an extension to non-convex subsets in lowdimensional space.")
        writeLines("Hessian LLE (Local Linear Embeddin) Parameters")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message){
        print(dimension_params)
      }
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
      }
    }
    else if(method == "ICA"){
      technic_name <- "ICA (Independent Component Analysis)"
      dimen <- dimRed::FastICA()
      if(!no_message){
        writeLines("ICA is used for blind signal separation of different sources. It is a linear Projection.")
        writeLines("ICA (Independent Component Analysis) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
      }
    }
    else if(method == "Isomap"){
      technic_name <- "Isomap (Isometric Mapping)"
      dimen <- dimRed::Isomap()
      if(!no_message){
        writeLines("The Isomap algorithm approximates a manifold using geodesic distances on a k nearest neighbor graph.")
        writeLines("Then classical scaling is performed on the resulting distance matrix.")
        writeLines("Isomap Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$knn <- knn_num
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("get_geod -> Should the geodesic distance matrix be kept, if TRUE, access it as getOtherData(x)$geod")
      }
    }
    else if(method == "DrL"){
      technic_name <- "Distributed Recursive Layout"
      dimen <- dimRed::DrL()
      if(!no_message){
        writeLines("DrL uses a complex algorithm to avoid local minima in the graph embedding which uses several steps.")
        writeLines("DrL (Distributed Recursive Graph Layout) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DM"){
      technic_name <- "Diffusion Maps"
      dimen <- dimRed:DiffusionMaps()
      if(!no_message){
        writeLines("Diffusion Maps uses a diffusion probability matrix to robustly approximate a manifold.")
        writeLines("Diffusion Maps (Local Linear Embeddin) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$eps <- "auto"
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("d -> a function transforming a matrix row wise into a distance matrix or dist object, e.g. dist.")
        writeLines("eps -> The epsilon parameter that determines the diffusion weight matrix from a distance matrix d, exp(-d^2/eps), 
                 if set to 'auto' it will be set to the median distance to the 0.01*n nearest neighbor.")
        writeLines("t -> Time-scale parameter. The recommended value, 0, uses multiscale geometry.")
        writeLines("delta -> Sparsity cut-off for the symmetric graph Laplacian, a higher value results in more sparsity and faster calculation. The predefined value is 10^-5.")
      }
    }
    else if(method == "FR"){
      technic_name <- "Fruchterman Reingold"
      dimen <- dimRed::FruchtermanReingold()
      if(!no_message){
        writeLines("Fruchterman Reingold Graph Layout algorithm. puts the data into a circle and puts connected points close to each other.")
        writeLines("Fruchterman Reingold Graph Layout Parameters.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DRR"){
      technic_name <- "PCA with Ridge Regression"
      dimen <- dimRed::DRR()
      if(!no_message) writeLines('DRR is a non-linear extension of PCA that uses Kernel Ridge regression.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("lambda -> regularization parameter for the ridge regression.")
        writeLines("kernel -> The kernel to use for KRR, defaults to rbfdot")
        writeLines("kernel.pars -> A list with kernel parameters, elements depend on the kernel used, rbfdot uses sigma.")
        writeLines("pca -> logical, should an initial pca step be performed, defaults to TRUE.")
        writeLines("pca.center -> logical, should the data be centered before the pca step. Defaults to TRUE.")
        writeLines("pca.scale -> logical, should the data be scaled before the pca ste. Defaults to FALSE.")
        writeLines("fastcv -> logical, should fastCV from the CVST package be used instead of normal cross-validation.")
        writeLines("fastcv.test -> If fastcv = TRUE, separate test data set for fastcv.")
        writeLines("cv.folds -> if fastcv = FALSE, specifies the number of folds for crossvalidation.")
        writeLines("fastkrr.nblocks -> integer, higher values sacrifice numerical accuracy for speed and less memory, see below for details")
        writeLines("verbose -> logical, should the cross-validation results be printed out.")
      }
    }
    else if(method == "KK"){
      technic_name <- "Kamada Kawai"
      dimen <- dimRed::KamadaKawai()
      if(!no_message){
        writeLines("Kamada Kawai is Graph embedding algorithms se the data as a graph. 
               Between the nodes of the graph exist attracting and repelling forces 
               which can be modeled as electrical fields or springs connecting the nodes.")
        writeLines("The graph is then forced into a lower dimensional representation")
        writeLines("That tries to represent the forces between the nodes accurately")
        writeLines('by minimizing the total energy of the attracting and repelling forces.')
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "LLE"){
      technic_name <- "Locally Linear Embedding"
      dimen <- dimRed::LLE()
      if(!no_message){
        writeLines('LLE approximates the points in the manifold by linear combination of its neighbors.')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=50")
      }
    }
    else if(method == "MMDS"){
      technic_name <- "Metric MDS (Multi Dimensional Scaling)"
      dimen <- dimRed::MDS()
      if(!no_message){
        writeLines('Metric MDS tries to maintain distances in high- and low-dimensional space,')
        writeLines('it has the advantage over PCA that arbitrary distance functions can be used, but it is high computational')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NMDS"){
      technic_name <- "Non Metric Multi Dimensional Scaling"
      dimen <- dimRed::nMDS()
      if(!no_message){
        writeLines('Non Metric MDS is a non-linear extension of MDS using monotonic regression')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NNMF"){
      technic_name <- "Non Negative Matrix Factorization"
      dimen <- dimRed::NNMF()
      if(!no_message){
        writeLines("NNMF is a method for decomposing a matrix into a smaller dimension such that the constraint that the data (and the projection) are not negative is taken into account.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("method -> character, which algorithm should be used. See nmf for possible values. Defaults to brunet")
        writeLines("nrun -> integer, the number of times the computations are conducted. See nmf")
        writeLines("seed -> integer, a value to control the random numbers used.")
        writeLines("options -> named list, other options to pass to nmf")
      }
    }
    else if(method == "AE"){
      library(tensorflow)
      technic_name <- "Auto Encoder"
      dimen <- dimRed::AutoEncoder()
      if(!no_message){
        writeLines('Autoencoders are neural networks that try to reproduce their input. Consider this method unstable, as the internals may still be changed.')
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of dimensions for reduction.")
        writeLines("n_hidden -> The number of neurons in the hidden layers, the length specifies the number of layers, 
                 the length must be impair, the middle number must be the same as ndim.")
        writeLines("activation -> The activation functions for the layers, one of tanh, sigmoid, relu, elu")
        writeLines("Everything else will silently be ignored and there will be no activation function for the layer.")
        writeLines("weight_decay -> the coefficient for weight decay, set to 0 if no weight decay desired.")
        writeLines("learning_rate -> The learning rate for gradient descend")
        writeLines("graph -> Optional: A list of bits and pieces that define the autoencoder in tensorflow, see details.")
        writeLines("keras_graph -> Optional: A list of keras layers that define the encoder and decoder, specifying this, will ignore all other topology related variables, see details.")
        writeLines("batchsize -> If NA, all data will be used for training, else only a random subset of size batchsize will be used")
        writeLines("n_steps -> the number of training steps.")
      }
    }
    else if(method == "KPCA"){
      technic_name <- "Kernel PCA"
      dimen <- dimRed::AutoEncoder()
      if(!no_message){
        writeLines('Kernel PCA is a nonlinear extension of PCA using kernel methods.')
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions, defaults to 2")
        writeLines("kpar -> A list with the parameters for the kernel function, defaults to list(sigma = 0.1)")
        writeLines("kernel -> The kernel function, either as a function or a character vector with the name of the kernel. Defaults to rbfdot")
      }
    }
    else if(method == "PCA_L1"){
      technic_name <- "Regularization PCA L1"
      dimen <- dimRed::PCA_L1()
      if(!no_message){
        writeLines("PCA transforms the data so that the L2 reconstruction error is minimized or the variance of the projected data is maximized.")
        writeLines("This is sensitive to outliers, L1 PCA minimizes the L1 reconstruction error or maximizes the sum of the L1 norm of the projected observations.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("center -> logical, should the data be centered, defaults to TRUE.")
        writeLines("scale -> logical, should the data be scaled, defaults to FALSE.")
        writeLines("fun -> character or function, the method to apply, see the pcaL1 package")
      }
    }
    emb <- dimen@fun(dat, dimension_params) 
    dimensional_data <- data.frame(emb@data@data)
    return(dimensional_data)
  }
  
  uniform_manifold_approximation_projection <- function(data){
    library(umap)
    umap_result <- umap(data)
    return(umap_result)
  }
  
  tSNE <- function(data_train, perplex_param=15, 
                   check_duplicate=TRUE, use_initial_pca=TRUE,
                   partial_pca_enable=FALSE){
    library(Rtsne)
    library(irlba)
    
    data_train <- as.matrix(data_train)
    
    # You can change the value of perplexity and see how the plot changes
    tsne_results <- Rtsne(data_train, perplexity=perplex_param, 
                          check_duplicates = check_duplicate,
                          pca = use_initial_pca,
                          partial_pca = partial_pca_enable)
    return(tsne_results)
  }
  
  plot_cluster <- function(df, clust, individual_label="", method="k-means", 
                           clust_make = "make", dimred_technique = "all", 
                           clust_var_name="", show_plot=T){
    library(ggpubr)
    library(ggrepel)
    library(factoextra)
    library(skmeans)
    library(cluster)
    
    map_list <- list()
    
    if(dimred_technique == "pca" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          pca <- stats::prcomp(df, scale = FALSE, center = FALSE)
          ind <- facto_summarize(pca, element = "ind", result = "coord", axes = c(1,2))
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- as.factor(clust)
          }
          eig <- get_eigenvalue(pca)[axes, 2]
          xlab = paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
          ylab = paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
          
          map_list <- append(map_list, list(ind[,which(colnames(ind) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "PCA_DIMENSION"
          
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with PCA! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "umap" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          umap <- uniform_manifold_approximation_projection(df)
          umap <- as.data.frame(umap$layout)
          colnames(umap) <- c("Dim.1","Dim.2")
          if(clust_make == "make"){
            umap$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            umap$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(umap[,which(colnames(umap) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "UMAP_DIMENSION"
          
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with UMAP! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "tsne" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          tsne <- tSNE(df)
          tsne <- as.data.frame(tsne$Y)
          colnames(tsne) <- c('Dim.1',"Dim.2")
          if(clust_make == "make"){
            tsne$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            tsne$cluster <- as.factor(clust)
          }
          map_list <- append(map_list, list(tsne[,which(colnames(tsne) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "TSNE_DIMENSION"
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if(ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with TSNE! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "lae" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          lae <- NULL
          if(clust_make == "make"){
            data_with_factor <- as.data.frame(cbind(df, clust$cluster))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            data_with_factor <- as.data.frame(cbind(df, clust))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(lae[,which(colnames(lae) %in% c("LEIM1","LEIM2","cluster"))]))
          names(map_list)[length(map_list)] <- "LAE_DIMENSION"
          xlab = paste0("LEIM1")
          ylab = paste0("LEIM2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      },error = function(cond){
        message("Unable to do dimensionaly reduction with LaE! Here is Original Message")
        message(cond)
      })
    }
    return(map_list)
  }
  
  cluster_overlapping_measure <- function(xy_mapping, view_plot=T, dimred_name = ""){
    unique_cluster <- unique(xy_mapping$cluster)
    color_collection <- c("#ff584d","#ffff00","#3bff4e","#8a7dff","#d614e0",
                          "#1fa61f","#59dae3","#bc13f0","#ff1cce","#ff709b",
                          "#f4ff26","#81b584","#ff29d4","#b30009","#ff4f1f")
    
    if(view_plot){
      x11()
      plot(xy_mapping[,1],xy_mapping[,2],
           xlim = c(min(xy_mapping[,1]), max(xy_mapping[,1])), 
           ylim = c(min(xy_mapping[,2]), max(xy_mapping[,2])), col = xy_mapping[,3])
      for(y in 1:length(unique_cluster)){
        polygon(x = xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[y])],
                y = xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[y])],
                col = color_collection[y])
      }
    }
    
    library(sp)
    library(dplyr)
    overlapping_df <- data.frame()
    dimred_technique <- c()
    cluster_vector <- c()
    point_from_cluster <- c()
    overlapping_vector <- c()
    point_total <- c()
    for(v in 1:length(unique_cluster)){
      p <- Polygon(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]), c(1,2)])
      ps <- Polygons(list(p),1)
      sps <- SpatialPolygons(list(ps))
      polygon_coords <- ps@Polygons[[1]]@coords
      #check data point for every clusters
      for(x in 1:length(unique_cluster)){
        point_cluster <- nrow(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]),])
        point_location <- point.in.polygon(xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[x])], 
                                           xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[x])], 
                                           polygon_coords[,1], polygon_coords[,2], mode.checked=FALSE)
        overlap_measure <- length(point_location[point_location > 0])
        cluster_vector <- c(cluster_vector, paste0("Cluster-",v,"-",unique_cluster[v]))
        point_from_cluster <- c(point_from_cluster, paste0("Cluster-",x,"-",unique_cluster[x]))
        overlapping_vector <- c(overlapping_vector, overlap_measure)
        point_total <- c(point_total, point_cluster)
      }
    }
    
    overlapping_df <- data.frame(cluster_vector, point_from_cluster, overlapping_vector, point_total)
    overlapping_pct <- overlapping_df %>% filter(cluster_vector != point_from_cluster) %>% 
      group_by(cluster_vector) %>% 
      summarise(overlap_sum = sum(overlapping_vector),
                point_total = point_total) %>%
      mutate(overlap_pct = round(overlap_sum / point_total * 100,2)) %>% as.data.frame()
    overlapping_pct <- overlapping_pct[!duplicated(overlapping_pct),]
    overlapping_pct$dimred_technique <- dimred_name
    return(list(overlapping_df, overlapping_pct))
  }
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  library(dplyr)
  library(stringr)
  library(tictoc)
  library(data.table)
  
  options(dplyr.summarise.inform = FALSE)
  
  #transform first from any transformation in initial pick
  transformed_pick <- initial_pick
  for(a in 1:length(initial_pick)){
    transform_target <- unlist(lapply(strsplit(initial_pick[a], "_"), FUN = function(x) x[1]))
    var_target <- paste0(unlist(lapply(strsplit(initial_pick[a], "_"), FUN = function(x) x[-1])), collapse = "_")
    if(transform_target %in% apply_transformation){
      idx <- which(colnames(data_transform) == var_target)
      eval_transform <- paste0("data_transform[,idx] <- ",transform_target,"(data_transform[,idx])")
      identity_vec <- data_transform[,idx] / abs(data_transform[,idx])
      identity_vec[which(is.na(identity_vec))] <- 0 
      data_transform[,idx] <- abs(data_transform[,idx])
      eval(parse(text = eval_transform))
      data_transform[,idx][which(data_transform[,idx] == -Inf)] <- 0
      data_transform[,idx] <- data_transform[,idx] * identity_vec
      initial_pick[a] <- var_target
    }
  }
  
  overlap_reduction_df <- NULL
  remaining_pick <- colnames(data_transform)[!(colnames(data_transform) %in% c(dependent_col, initial_pick))]
  history_pick <- c()
  history_transform <- c()
  formula_pickup <- ""
  formula_interpretation <- ""
  batch_no <- 1
  
  while(1){
    start_execution_pick <- tic()
    for(b in 1:length(remaining_pick)){
      writeLines(paste0("------------------ Start Picking Variable ", remaining_pick[b], " (",b," of ",length(remaining_pick), ") -----------------------------"))
      all_transform <- c("none", apply_transformation)
      for(c in 1:length(all_transform)){
        tic()
        if(c==1){
          all_variable_test <- NULL
          if(length(history_pick) != 0){
            all_variable_test <- c(initial_pick, history_pick, remaining_pick[b])
          }else{
            all_variable_test <- c(initial_pick, remaining_pick[b])
          }
          
          data_pick <- data_transform[,which(colnames(data_transform) %in% c(dependent_col, all_variable_test))]
          data_pick <- data_pick[!duplicated(data_pick),]
          cluster_simulation <- plot_cluster(data_pick[,-which(colnames(data_pick) == dependent_col)], 
                                             data_pick[[dependent_col]], clust_make = "input", 
                                             dimred_technique = dimred_technique_use, clust_var_name = dependent_col, show_plot = F)
          overlap_measure_list <- list()
          all_overlap <- NULL
          for(x in 1:length(cluster_simulation)){
            overlap_measure_list <- append(overlap_measure_list, 
                                           list(cluster_overlapping_measure(cluster_simulation[[x]], view_plot = F, 
                                                                            dimred_name = names(cluster_simulation)[x])))
            all_overlap <- rbind(all_overlap, overlap_measure_list[[x]][[2]])
          }
          
          all_overlap <- all_overlap %>% arrange(cluster_vector)
          all_overlap <- all_overlap %>% group_by(cluster_vector) %>% 
            summarise(point_total = point_total,
                      min_overlap = min(overlap_sum), 
                      min_overlap_pct = min(overlap_pct), 
                      max_overlap = max(overlap_sum),
                      max_overlap_pct = max(overlap_pct)) %>% as.data.frame()
          
          all_overlap <- all_overlap[!duplicated(all_overlap),]
          if(is.null(overlap_reduction_df)){
            prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
            names(prefix_name) <- c(1:length(prefix_name))
            prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
            prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
            
            for(vars in 1:length(all_unique_class)){
              if(vars == 1){
                overlap_reduction_df <- data.frame(all_overlap$point_total[vars], 
                                                   all_overlap$min_overlap[vars],
                                                   all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df) <- c(paste0(prefix_name[vars], "_pts"), 
                                                    paste0(prefix_name[vars], "_overlap_min"), 
                                                    paste0(prefix_name[vars], "_overlap_max"))
              }else{
                overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                       all_overlap$min_overlap[vars],
                                                       all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                        paste0(prefix_name[vars], "_overlap_min"), 
                                                        paste0(prefix_name[vars], "_overlap_max"))
                overlap_reduction_df <- as.data.frame(cbind(overlap_reduction_df, overlap_reduction_df_add))
                colnames(overlap_reduction_df)[(ncol(overlap_reduction_df) - 2):ncol(overlap_reduction_df)] <- colnames(overlap_reduction_df_add)
              }
            }
            
            overlap_reduction_df$add_var <- remaining_pick[b]
            overlap_reduction_df$transform_var <- "none"
            
          }
          else{
            prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
            names(prefix_name) <- c(1:length(prefix_name))
            prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
            prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
            
            for(vars in 1:length(all_unique_class)){
              if(vars == 1){
                overlap_reduction_df_temp <- data.frame(all_overlap$point_total[vars], 
                                                        all_overlap$min_overlap[vars],
                                                        all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df_temp) <- c(paste0(prefix_name[vars], "_pts"), 
                                                         paste0(prefix_name[vars], "_overlap_min"), 
                                                         paste0(prefix_name[vars], "_overlap_max"))
              }else{
                overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                       all_overlap$min_overlap[vars],
                                                       all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                        paste0(prefix_name[vars], "_overlap_min"), 
                                                        paste0(prefix_name[vars], "_overlap_max"))
                overlap_reduction_df_temp <- as.data.frame(cbind(overlap_reduction_df_temp, overlap_reduction_df_add))
                colnames(overlap_reduction_df_temp)[(ncol(overlap_reduction_df_temp) - 2):ncol(overlap_reduction_df_temp)] <- colnames(overlap_reduction_df_add)
              }
            }
            
            overlap_reduction_df_temp$add_var <- remaining_pick[b]
            overlap_reduction_df_temp$transform_var <- "none"
            overlap_reduction_df <- rbind(overlap_reduction_df, overlap_reduction_df_temp)
          }
          toc()
        }
        else{
          all_variable_test <- NULL
          if(length(history_pick) != 0){
            all_variable_test <- c(initial_pick, history_pick, remaining_pick[b])
          }else{
            all_variable_test <- c(initial_pick, remaining_pick[b])
          }
          
          data_pick <- data_transform[,which(colnames(data_transform) %in% c(dependent_col,all_variable_test))]
          idx <- which(colnames(data_pick) == remaining_pick[b])
          eval_transform <- paste0("data_pick[,idx] <- ",all_transform[c],"(data_pick[,idx])")
          identity_vec <- data_pick[,idx] / abs(data_pick[,idx])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_pick[,idx] <- abs(data_pick[,idx])
          data_pick[,idx] <- eval(parse(text = eval_transform))
          data_pick[,idx][which(data_pick[,idx] == -Inf)] <- 0
          data_pick[,idx] <- data_pick[,idx] * identity_vec
          
          data_pick <- data_pick[!duplicated(data_pick),]
          cluster_simulation <- plot_cluster(data_pick[,-which(colnames(data_pick) == dependent_col)], 
                                             data_pick[[dependent_col]], clust_make = "input", 
                                             dimred_technique = dimred_technique_use, clust_var_name = dependent_col, show_plot = F)
          overlap_measure_list <- list()
          all_overlap <- NULL
          for(x in 1:length(cluster_simulation)){
            overlap_measure_list <- append(overlap_measure_list, 
                                           list(cluster_overlapping_measure(cluster_simulation[[x]], view_plot = F, 
                                                                            dimred_name = names(cluster_simulation)[x])))
            all_overlap <- rbind(all_overlap, overlap_measure_list[[x]][[2]])
          }
          
          all_overlap <- all_overlap %>% arrange(cluster_vector)
          all_overlap <- all_overlap %>% group_by(cluster_vector) %>% 
            summarise(point_total = point_total,
                      min_overlap = min(overlap_sum), 
                      min_overlap_pct = min(overlap_pct), 
                      max_overlap = max(overlap_sum),
                      max_overlap_pct = max(overlap_pct)) %>% as.data.frame()
          
          all_overlap <- all_overlap[!duplicated(all_overlap),]
          if(is.null(overlap_reduction_df)){
            prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
            names(prefix_name) <- c(1:length(prefix_name))
            prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
            prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
            
            for(vars in 1:length(all_unique_class)){
              if(vars == 1){
                overlap_reduction_df <- data.frame(all_overlap$point_total[vars], 
                                                   all_overlap$min_overlap[vars],
                                                   all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df) <- c(paste0(prefix_name[vars], "_pts"), 
                                                    paste0(prefix_name[vars], "_overlap_min"), 
                                                    paste0(prefix_name[vars], "_overlap_max"))
              }else{
                overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                       all_overlap$min_overlap[vars],
                                                       all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                        paste0(prefix_name[vars], "_overlap_min"), 
                                                        paste0(prefix_name[vars], "_overlap_max"))
                overlap_reduction_df <- as.data.frame(cbind(overlap_reduction_df, overlap_reduction_df_add))
                colnames(overlap_reduction_df)[(ncol(overlap_reduction_df) - 2):ncol(overlap_reduction_df)] <- colnames(overlap_reduction_df_add)
              }
            }
            
            overlap_reduction_df$add_var <- remaining_pick[b]
            overlap_reduction_df$transform_var <- "none"
            
          }
          else{
            prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
            names(prefix_name) <- c(1:length(prefix_name))
            prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
            prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
            
            for(vars in 1:length(all_unique_class)){
              if(vars == 1){
                overlap_reduction_df_temp <- data.frame(all_overlap$point_total[vars], 
                                                        all_overlap$min_overlap[vars],
                                                        all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df_temp) <- c(paste0(prefix_name[vars], "_pts"), 
                                                         paste0(prefix_name[vars], "_overlap_min"), 
                                                         paste0(prefix_name[vars], "_overlap_max"))
              }else{
                overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                       all_overlap$min_overlap[vars],
                                                       all_overlap$max_overlap[vars])
                colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                        paste0(prefix_name[vars], "_overlap_min"), 
                                                        paste0(prefix_name[vars], "_overlap_max"))
                overlap_reduction_df_temp <- as.data.frame(cbind(overlap_reduction_df_temp, overlap_reduction_df_add))
                colnames(overlap_reduction_df_temp)[(ncol(overlap_reduction_df_temp) - 2):ncol(overlap_reduction_df_temp)] <- colnames(overlap_reduction_df_add)
              }
            }
            overlap_reduction_df_temp$add_var <- remaining_pick[b]
            overlap_reduction_df_temp$transform_var <- all_transform[c]
            overlap_reduction_df <- rbind(overlap_reduction_df, overlap_reduction_df_temp)
          }
          toc()
        }
      }
    }
    
    points_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "pts")]
    overlap_min_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_min")]
    overlap_max_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_max")]
    overlap_reduction_df$observation_included <- paste0(rowSums(points_df),"/",nrow(data))
    overlap_reduction_df$redudancy_score <- rowSums(overlap_min_df) + rowSums(overlap_max_df)
    overlap_reduction_df <- overlap_reduction_df %>% arrange(redudancy_score)
    
    print(overlap_reduction_df)
    
    end_execution_pick <- toc()
    current_execution_time <- as.numeric(end_execution_pick$toc - start_execution_pick)
    writeLines(paste0("Pick Transform for batch ",batch_no," take ",current_execution_time, " secs"))
    
    if(auto_best){
      writeLines("Best Model Information from Search")
      writeLines(paste0("By Adding ",overlap_reduction_df$transform_var[1],"(", overlap_reduction_df$add_var[1],")"))
      points_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "pts")]
      overlap_min_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_min")]
      overlap_max_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_max")]
      class_name <- unlist(lapply(strsplit(colnames(overlap_min_df), "_"), FUN = function(x) x[[1]]))
      for(f in 1:length(all_unique_class)){
        writeLines(paste0("Class Name: ", class_name[f]))
        writeLines(paste0("Old Captured Data: ", initial_class_points[f], " -> New Captured Data: ", points_df[1,f], " (Growth: ", points_df[1,f] - initial_class_points[f], ") "))
        writeLines(paste0("Old Overlap Min: ",initial_class_min_overlap[f], " -> New Overlap Min: ", overlap_min_df[1,f], " (Growth: ", overlap_min_df[1,f] - initial_class_min_overlap[f], ") "))
        writeLines(paste0("Old Overlap Max: ",initial_class_max_overlap[f], " -> New Overlap Max: ", overlap_max_df[1,f], " (Growth: ", overlap_max_df[1,f] - initial_class_max_overlap[f], ") "))
      }
      new_observation_include <- as.numeric(unlist(strsplit(overlap_reduction_df$observation_included[1], "/"))[1])
      writeLines(paste0("Old Observation Included: ", initial_obs, " -> New Observation Included: ", new_observation_include, " (Growth: ",new_observation_include - initial_obs, ")"))
      writeLines(paste0("Old Redudancy Score: ", initial_redudancy_score, " -> New Redudancy Score: ", overlap_reduction_df$redudancy_score[1], " (Growth: ",overlap_reduction_df$redudancy_score[1] - initial_redudancy_score, ")"))
      writeLines("")
      
      old_overlap_score <- initial_redudancy_score
      new_overlap_score <- overlap_reduction_df$redudancy_score[1]
      old_obs <- initial_obs
      new_obs <- new_observation_include
      redudancy_loss <- new_overlap_score - old_overlap_score
      observation_loss <- new_obs - old_obs
      if(new_overlap_score > old_overlap_score || new_obs > old_obs){
        for(f in 1:length(all_unique_class)){
          initial_class_points[f] <- points_df[1,f]
          initial_class_min_overlap[f] <- overlap_min_df[1,f]
          initial_class_max_overlap[f] <- overlap_max_df[1,f]
        }
        initial_obs <- new_obs
        initial_redudancy_score <- new_overlap_score
        history_pick <- c(history_pick, overlap_reduction_df$add_var[1])
        history_transform <- c(history_transform, overlap_reduction_df$transform_var[1])
        remaining_pick <- remaining_pick[-which(remaining_pick == overlap_reduction_df$add_var[1])]
        if(overlap_reduction_df$transform_var[1] != "none"){
          eval_transform <- paste0("data_transform[[overlap_reduction_df$add_var[1]]] <- ",overlap_reduction_df$transform_var[1],"(data_transform[[overlap_reduction_df$add_var[1]]])")
          identity_vec <- data_transform[[overlap_reduction_df$add_var[1]]] / abs(data_transform[[overlap_reduction_df$add_var[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transform[[overlap_reduction_df$add_var[1]]] <- abs(data_transform[[overlap_reduction_df$add_var[1]]])
          eval(parse(text = eval_transform))
          data_transform[[overlap_reduction_df$add_var[1]]][which(data_transform[[overlap_reduction_df$add_var[1]]] == -Inf)] <- 0
          data_transform[[overlap_reduction_df$add_var[1]]] <- data_transform[[overlap_reduction_df$add_var[1]]] * identity_vec
        }
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
        best_formula_interpretation <- paste0(dependent_col, " ~ ")
        for(j in 1:length(history_transform)){
          if(j < length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
          }
          else if(j < length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
          }
          else if(j == length(history_transform) && history_transform[j] != "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
          }
          else if(j == length(history_transform) && history_transform[j] == "None"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
          }
        }
        print(best_formula_interpretation)
        writeLines('Pick Transform Successfully')
        overlap_reduction_df <- NULL
        writeLines("")
      }
      else{
        writeLines("Variable Pick Transform has reached its peak! Reached the end of Results")
        for(f in 1:length(all_unique_class)){
          writeLines(paste0("Class Name: ", class_name[f]))
          writeLines(paste0("Peak Captured Data: ", initial_class_points[f]))
          writeLines(paste0("Peak Overlap Min: ",initial_class_min_overlap[f]))
          writeLines(paste0("Peak Overlap Max: ",initial_class_max_overlap[f]))
        }
        writeLines(paste0("Peak Observation Included: ", initial_obs))
        writeLines(paste0("Peak Redudancy Score: ", initial_redudancy_score))
        
        return(list(pickup_variable_from_start = history_pick,
                    transform_variable_from_start = history_transform,
                    end_class_points = initial_class_points,
                    end_class_overlap_min = initial_class_min_overlap,
                    end_class_overlap_max = initial_class_max_overlap,
                    end_captured_observation = initial_obs,
                    end_redudancy_score = initial_redudancy_score,
                    formula_pickup = best_formula,
                    formula_interpretation = best_formula_interpretation,
                    data_transformed = data,
                    overlap_collection = overlap_reduction_df))
      }
      if(!is.na(minimize_redudancy_threshold)){
        if(redudancy_loss <= -minimize_redudancy_threshold){
          writeLines("Variable Pick Transform has reached Redudancy Loss Threshold! Reached the end of Results")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      end_class_points = initial_class_points,
                      end_class_overlap_min = initial_class_min_overlap,
                      end_class_overlap_max = initial_class_max_overlap,
                      end_captured_observation = initial_obs,
                      end_redudancy_score = initial_redudancy_score,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data,
                      overlap_collection = overlap_reduction_df))
        }
      }
      if(!is.na(lose_observation_threshold)){
        if(observation_loss <= -lose_observation_threshold){
          writeLines(paste0("Variable Pick Transform Begin to Loss Observation in Searching Process over ",lose_observation_threshold, " Reached the end of Results"))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      end_class_points = initial_class_points,
                      end_class_overlap_min = initial_class_min_overlap,
                      end_class_overlap_max = initial_class_max_overlap,
                      end_captured_observation = initial_obs,
                      end_redudancy_score = initial_redudancy_score,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data,
                      overlap_collection = overlap_reduction_df))
        }
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        var_transform <- readline(prompt=paste0("Which Transform function do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE" && var_transform == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      end_class_points = initial_class_points,
                      end_class_overlap_min = initial_class_min_overlap,
                      end_class_overlap_max = initial_class_max_overlap,
                      end_captured_observation = initial_obs,
                      end_redudancy_score = initial_redudancy_score,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data,
                      overlap_collection = overlap_reduction_df))
        }
        else if(var_pick %in% overlap_reduction_df$add_var && var_transform %in% overlap_reduction_df$transform_var){
          library(data.table)
          points_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "pts")]
          overlap_min_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_min")]
          overlap_max_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_max")]
          pick_which <- which(overlap_reduction_df$add_var == var_pick & overlap_reduction_df$transform_var == var_transform)
          class_name <- unlist(lapply(strsplit(colnames(overlap_min_df), "_"), FUN = function(x) x[[1]]))
          
          for(f in 1:length(all_unique_class)){
            writeLines(paste0("Class Name: ", class_name[f]))
            writeLines(paste0("Old Captured Data: ", initial_class_points[f], " -> New Captured Data: ", points_df[pick_which,f], " (Growth: ", points_df[pick_which,f] - initial_class_points[f], ") "))
            writeLines(paste0("Old Overlap Min: ",initial_class_min_overlap[f], " -> New Overlap Min: ", overlap_min_df[pick_which,f], " (Growth: ", overlap_min_df[pick_which,f] - initial_class_min_overlap[f], ") "))
            writeLines(paste0("Old Overlap Max: ",initial_class_max_overlap[f], " -> New Overlap Max: ", overlap_max_df[pick_which,f], " (Growth: ", overlap_max_df[pick_which,f] - initial_class_max_overlap[f], ") "))
          }
          
          new_observation_include <- as.numeric(unlist(strsplit(overlap_reduction_df$observation_included[pick_which], "/"))[1])
          writeLines(paste0("Old Observation Included: ", initial_obs, " -> New Observation Included: ", new_observation_include, " (Growth: ",new_observation_include - initial_obs, ")"))
          writeLines(paste0("Old Redudancy Score: ", initial_redudancy_score, " -> New Redudancy Score: ", overlap_reduction_df$redudancy_score[pick_which], " (Growth: ",overlap_reduction_df$redudancy_score[pick_which] - initial_redudancy_score, ")"))
          writeLines("")
          
          for(g in 1:length(all_unique_class)){
            initial_class_points[g] <- points_df[pick_which,g]
            initial_class_min_overlap[g] <- overlap_min_df[pick_which,g]
            initial_class_max_overlap[g] <- overlap_max_df[pick_which,g]
          }
          initial_obs <- new_observation_include
          initial_redudancy_score <- overlap_reduction_df$redudancy_score[pick_which]
          history_pick <- c(history_pick, var_pick)
          history_transform <- c(history_transform, var_transform)
          remaining_pick <- remaining_pick[-which(remaining_pick == var_pick)]
          
          if(var_transform != "none"){
            eval_transform <- paste0("data_transform[[var_pick]] <- ",overlap_reduction_df$transform_var[1],"(data_transform[[var_pick]])")
            identity_vec <- data_transform[[overlap_reduction_df$add_var[1]]] / abs(data_transform[[overlap_reduction_df$add_var[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transform[[overlap_reduction_df$add_var[1]]] <- abs(data_transform[[overlap_reduction_df$add_var[1]]])
            eval(parse(text = eval_transform))
            data_transform[[overlap_reduction_df$add_var[1]]][which(data_transform[[overlap_reduction_df$add_var[1]]] == -Inf)] <- 0
            data_transform[[overlap_reduction_df$add_var[1]]] <- data_transform[[overlap_reduction_df$add_var[1]]] * identity_vec
          }
          
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick and Transformed Successfully')
          overlap_reduction_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

selective_transformation_overlap_reduction <- function(data, dependent_col, auto_best = TRUE, initial_pick=c(), initial_obs = 0,
                                                       initial_class_points = c(), initial_class_min_overlap = c(), 
                                                       initial_class_max_overlap = c(), initial_redudancy_score = 0,
                                                       minimize_redudancy_threshold = 5, lose_observation_threshold = 10, 
                                                       apply_transformation = c("log","sqrt","inverse","power1_25","power1_5",
                                                                                "power1_75","quadratic","cubic"), 
                                                       dimred_technique_use = "pca"){
  data_transform <- data
  all_unique_class <- unique(data[[dependent_col]])
  
  dimension_reduction_by_dimred <- function(data, my_formula, knn_num=2, method="LaE",
                                            legend_pos="topright", n_dimension=2, no_message=TRUE,
                                            parameter_guide=FALSE){
    library(dimRed)
    library(lle)
    data_label <- data[[unlist(strsplit(my_formula, " ~ "))[1]]]
    my_formula <- as.formula(my_formula)
    dat <- as.dimRedData(my_formula, data)
    technic_name <- ""
    dimen <- NULL
    dimension_params <- NULL
    if(method == "LaE"){
      technic_name <- "Laplacian Eigenmaps"
      dimen <- dimRed::LaplacianEigenmaps()
      if(!no_message){
        writeLines("Laplacian Eigenmaps use a kernel and were originally developed to separate non-convex clusters under the name spectral clustering.")
        writeLines("Laplacian Eigenmaps Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message){
        print(dimension_params)
      }
      
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions.")
        writeLines("sparse -> A character vector specifying hot to make the graph sparse")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("eps -> an epsilon neighborhood graph is constructed, else a dense distance matrix is used.")
        writeLines("t -> Parameter for the transformation of the distance matrix by w=exp(-d^2/t), 
                 larger values give less weight to differences in distance, t == Inf treats all distances != 0 equally.")
        writeLines("norm -> logical, should the normed laplacian be used?")
      }
    }
    else if(method == "Hessian"){
      technic_name <- "Hessian LLE (Locally Linear Embedding)"
      dimen <- dimRed::HLLE()
      if(!no_message){
        writeLines("HLLE uses local hessians to approximate the curvines and is an extension to non-convex subsets in lowdimensional space.")
        writeLines("Hessian LLE (Local Linear Embeddin) Parameters")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message){
        print(dimension_params)
      }
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
      }
    }
    else if(method == "ICA"){
      technic_name <- "ICA (Independent Component Analysis)"
      dimen <- dimRed::FastICA()
      if(!no_message){
        writeLines("ICA is used for blind signal separation of different sources. It is a linear Projection.")
        writeLines("ICA (Independent Component Analysis) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
      }
    }
    else if(method == "Isomap"){
      technic_name <- "Isomap (Isometric Mapping)"
      dimen <- dimRed::Isomap()
      if(!no_message){
        writeLines("The Isomap algorithm approximates a manifold using geodesic distances on a k nearest neighbor graph.")
        writeLines("Then classical scaling is performed on the resulting distance matrix.")
        writeLines("Isomap Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$knn <- knn_num
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("knn -> that a K-nearest neighbor graph is constructed")
        writeLines("get_geod -> Should the geodesic distance matrix be kept, if TRUE, access it as getOtherData(x)$geod")
      }
    }
    else if(method == "DrL"){
      technic_name <- "Distributed Recursive Layout"
      dimen <- dimRed::DrL()
      if(!no_message){
        writeLines("DrL uses a complex algorithm to avoid local minima in the graph embedding which uses several steps.")
        writeLines("DrL (Distributed Recursive Graph Layout) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DM"){
      technic_name <- "Diffusion Maps"
      dimen <- dimRed:DiffusionMaps()
      if(!no_message){
        writeLines("Diffusion Maps uses a diffusion probability matrix to robustly approximate a manifold.")
        writeLines("Diffusion Maps (Local Linear Embeddin) Parameters")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$eps <- "auto"
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("d -> a function transforming a matrix row wise into a distance matrix or dist object, e.g. dist.")
        writeLines("eps -> The epsilon parameter that determines the diffusion weight matrix from a distance matrix d, exp(-d^2/eps), 
                 if set to 'auto' it will be set to the median distance to the 0.01*n nearest neighbor.")
        writeLines("t -> Time-scale parameter. The recommended value, 0, uses multiscale geometry.")
        writeLines("delta -> Sparsity cut-off for the symmetric graph Laplacian, a higher value results in more sparsity and faster calculation. The predefined value is 10^-5.")
      }
    }
    else if(method == "FR"){
      technic_name <- "Fruchterman Reingold"
      dimen <- dimRed::FruchtermanReingold()
      if(!no_message){
        writeLines("Fruchterman Reingold Graph Layout algorithm. puts the data into a circle and puts connected points close to each other.")
        writeLines("Fruchterman Reingold Graph Layout Parameters.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "DRR"){
      technic_name <- "PCA with Ridge Regression"
      dimen <- dimRed::DRR()
      if(!no_message) writeLines('DRR is a non-linear extension of PCA that uses Kernel Ridge regression.')
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions")
        writeLines("lambda -> regularization parameter for the ridge regression.")
        writeLines("kernel -> The kernel to use for KRR, defaults to rbfdot")
        writeLines("kernel.pars -> A list with kernel parameters, elements depend on the kernel used, rbfdot uses sigma.")
        writeLines("pca -> logical, should an initial pca step be performed, defaults to TRUE.")
        writeLines("pca.center -> logical, should the data be centered before the pca step. Defaults to TRUE.")
        writeLines("pca.scale -> logical, should the data be scaled before the pca ste. Defaults to FALSE.")
        writeLines("fastcv -> logical, should fastCV from the CVST package be used instead of normal cross-validation.")
        writeLines("fastcv.test -> If fastcv = TRUE, separate test data set for fastcv.")
        writeLines("cv.folds -> if fastcv = FALSE, specifies the number of folds for crossvalidation.")
        writeLines("fastkrr.nblocks -> integer, higher values sacrifice numerical accuracy for speed and less memory, see below for details")
        writeLines("verbose -> logical, should the cross-validation results be printed out.")
      }
    }
    else if(method == "KK"){
      technic_name <- "Kamada Kawai"
      dimen <- dimRed::KamadaKawai()
      if(!no_message){
        writeLines("Kamada Kawai is Graph embedding algorithms se the data as a graph. 
               Between the nodes of the graph exist attracting and repelling forces 
               which can be modeled as electrical fields or springs connecting the nodes.")
        writeLines("The graph is then forced into a lower dimensional representation")
        writeLines("That tries to represent the forces between the nodes accurately")
        writeLines('by minimizing the total energy of the attracting and repelling forces.')
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, Can only be 2 or 3, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=100")
        writeLines("d -> The distance function to determine the weights of the graph edges. Defaults to euclidean distances.")
      }
    }
    else if(method == "LLE"){
      technic_name <- "Locally Linear Embedding"
      dimen <- dimRed::LLE()
      if(!no_message){
        writeLines('LLE approximates the points in the manifold by linear combination of its neighbors.')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$knn <- knn_num
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("knn -> that a K-nearest neighbor graph is constructed, default=50")
      }
    }
    else if(method == "MMDS"){
      technic_name <- "Metric MDS (Multi Dimensional Scaling)"
      dimen <- dimRed::MDS()
      if(!no_message){
        writeLines('Metric MDS tries to maintain distances in high- and low-dimensional space,')
        writeLines('it has the advantage over PCA that arbitrary distance functions can be used, but it is high computational')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NMDS"){
      technic_name <- "Non Metric Multi Dimensional Scaling"
      dimen <- dimRed::nMDS()
      if(!no_message){
        writeLines('Non Metric MDS is a non-linear extension of MDS using monotonic regression')
        writeLines("These linear combinations are the same inside the manifold and in highdimensional space.")
      }
      
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      dimension_params$d <- function(x) exp(stats::dist(x))
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> number of output dimensions, default=2")
        writeLines("d -> Distance Matrix function calculation, default=Euclidean Distances")
      }
    }
    else if(method == "NNMF"){
      technic_name <- "Non Negative Matrix Factorization"
      dimen <- dimRed::NNMF()
      if(!no_message){
        writeLines("NNMF is a method for decomposing a matrix into a smaller dimension such that the constraint that the data (and the projection) are not negative is taken into account.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("method -> character, which algorithm should be used. See nmf for possible values. Defaults to brunet")
        writeLines("nrun -> integer, the number of times the computations are conducted. See nmf")
        writeLines("seed -> integer, a value to control the random numbers used.")
        writeLines("options -> named list, other options to pass to nmf")
      }
    }
    else if(method == "AE"){
      library(tensorflow)
      technic_name <- "Auto Encoder"
      dimen <- dimRed::AutoEncoder()
      if(!no_message){
        writeLines('Autoencoders are neural networks that try to reproduce their input. Consider this method unstable, as the internals may still be changed.')
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of dimensions for reduction.")
        writeLines("n_hidden -> The number of neurons in the hidden layers, the length specifies the number of layers, 
                 the length must be impair, the middle number must be the same as ndim.")
        writeLines("activation -> The activation functions for the layers, one of tanh, sigmoid, relu, elu")
        writeLines("Everything else will silently be ignored and there will be no activation function for the layer.")
        writeLines("weight_decay -> the coefficient for weight decay, set to 0 if no weight decay desired.")
        writeLines("learning_rate -> The learning rate for gradient descend")
        writeLines("graph -> Optional: A list of bits and pieces that define the autoencoder in tensorflow, see details.")
        writeLines("keras_graph -> Optional: A list of keras layers that define the encoder and decoder, specifying this, will ignore all other topology related variables, see details.")
        writeLines("batchsize -> If NA, all data will be used for training, else only a random subset of size batchsize will be used")
        writeLines("n_steps -> the number of training steps.")
      }
    }
    else if(method == "KPCA"){
      technic_name <- "Kernel PCA"
      dimen <- dimRed::AutoEncoder()
      if(!no_message){
        writeLines('Kernel PCA is a nonlinear extension of PCA using kernel methods.')
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> the number of output dimensions, defaults to 2")
        writeLines("kpar -> A list with the parameters for the kernel function, defaults to list(sigma = 0.1)")
        writeLines("kernel -> The kernel function, either as a function or a character vector with the name of the kernel. Defaults to rbfdot")
      }
    }
    else if(method == "PCA_L1"){
      technic_name <- "Regularization PCA L1"
      dimen <- dimRed::PCA_L1()
      if(!no_message){
        writeLines("PCA transforms the data so that the L2 reconstruction error is minimized or the variance of the projected data is maximized.")
        writeLines("This is sensitive to outliers, L1 PCA minimizes the L1 reconstruction error or maximizes the sum of the L1 norm of the projected observations.")
      }
      dimension_params <- dimen@stdpars
      dimension_params$ndim <- n_dimension
      if(!no_message) print(dimension_params)
      if(parameter_guide){
        writeLines("ndim -> The number of output dimensions.")
        writeLines("center -> logical, should the data be centered, defaults to TRUE.")
        writeLines("scale -> logical, should the data be scaled, defaults to FALSE.")
        writeLines("fun -> character or function, the method to apply, see the pcaL1 package")
      }
    }
    emb <- dimen@fun(dat, dimension_params) 
    dimensional_data <- data.frame(emb@data@data)
    return(dimensional_data)
  }
  
  uniform_manifold_approximation_projection <- function(data){
    library(umap)
    umap_result <- umap(data)
    return(umap_result)
  }
  
  tSNE <- function(data_train, perplex_param=15, 
                   check_duplicate=TRUE, use_initial_pca=TRUE,
                   partial_pca_enable=FALSE){
    library(Rtsne)
    library(irlba)
    
    data_train <- as.matrix(data_train)
    
    # You can change the value of perplexity and see how the plot changes
    tsne_results <- Rtsne(data_train, perplexity=perplex_param, 
                          check_duplicates = check_duplicate,
                          pca = use_initial_pca,
                          partial_pca = partial_pca_enable)
    return(tsne_results)
  }
  
  plot_cluster <- function(df, clust, individual_label="", method="k-means", 
                           clust_make = "make", dimred_technique = "all", 
                           clust_var_name="", show_plot=T){
    library(ggpubr)
    library(ggrepel)
    library(factoextra)
    library(skmeans)
    library(cluster)
    
    map_list <- list()
    
    if(dimred_technique == "pca" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          pca <- stats::prcomp(df, scale = FALSE, center = FALSE)
          ind <- facto_summarize(pca, element = "ind", result = "coord", axes = c(1,2))
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- as.factor(clust)
          }
          eig <- get_eigenvalue(pca)[axes, 2]
          xlab = paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
          ylab = paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
          
          map_list <- append(map_list, list(ind[,which(colnames(ind) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "PCA_DIMENSION"
          
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "PCA Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with PCA! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "umap" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          umap <- uniform_manifold_approximation_projection(df)
          umap <- as.data.frame(umap$layout)
          colnames(umap) <- c("Dim.1","Dim.2")
          if(clust_make == "make"){
            umap$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            umap$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(umap[,which(colnames(umap) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "UMAP_DIMENSION"
          
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(umap, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "UMAP Method Cluster", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with UMAP! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "tsne" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          tsne <- tSNE(df)
          tsne <- as.data.frame(tsne$Y)
          colnames(tsne) <- c('Dim.1',"Dim.2")
          if(clust_make == "make"){
            tsne$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            tsne$cluster <- as.factor(clust)
          }
          map_list <- append(map_list, list(tsne[,which(colnames(tsne) %in% c("Dim.1","Dim.2","cluster"))]))
          names(map_list)[length(map_list)] <- "TSNE_DIMENSION"
          xlab = paste0("Dim.1")
          ylab = paste0("Dim.2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(tsne, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "TSNE Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if(ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      }, error = function(cond){
        message("Unable to do dimensionaly reduction with TSNE! Here is Original Message")
        message(cond)
      })
      
    }
    if(dimred_technique == "lae" || dimred_technique == "all"){
      tryCatch({
        if(show_plot) x11()
        if(ncol(df) > 2){
          axes=c(1,2)
          lae <- NULL
          if(clust_make == "make"){
            data_with_factor <- as.data.frame(cbind(df, clust$cluster))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            data_with_factor <- as.data.frame(cbind(df, clust))
            colnames(data_with_factor)[ncol(data_with_factor)] <- clust_var_name
            formula_dimred <- paste0(clust_var_name, " ~ .")
            lae <- dimension_reduction_by_dimred(data_with_factor, formula_dimred, method="LaE")
            lae <- as.data.frame(lae)
            lae$cluster <- as.factor(clust)
          }
          
          map_list <- append(map_list, list(lae[,which(colnames(lae) %in% c("LEIM1","LEIM2","cluster"))]))
          names(map_list)[length(map_list)] <- "LAE_DIMENSION"
          xlab = paste0("LEIM1")
          ylab = paste0("LEIM2")
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(lae, "LEIM1", "LEIM2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Laplacian Eigenmaps Cluster Method", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else if (ncol(df) == 2){
          ind <- as.data.frame(df)
          if(clust_make == "make"){
            ind$cluster <- as.factor(clust$cluster)
          }else if(clust_make == "input"){
            ind$cluster <- clust
          }
          xlab <- colnames(ind)[1]
          ylab <- colnames(ind)[2]
          if(individual_label!=""){
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=individual_label))
              if(show_plot) print(plot)
            }
          }
          else{
            if(clust_make == "input"){
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }else{
              plot <- ggpubr::ggscatter(ind, "Dim.1", "Dim.2", color = "cluster",
                                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                                        pointsize = 1.5, labelsize = 12, main = "Cluster plot", 
                                        xlab=xlab, ylab=ylab, ggtheme=theme_minimal()) + geom_text_repel(aes(label=rownames(df)))
              if(show_plot) print(plot)
            }
          }
        }
        else{
          print("You Cannot Plot Cluster with Univariate Data!")
        }
      },error = function(cond){
        message("Unable to do dimensionaly reduction with LaE! Here is Original Message")
        message(cond)
      })
    }
    return(map_list)
  }
  
  cluster_overlapping_measure <- function(xy_mapping, view_plot=T, dimred_name = ""){
    unique_cluster <- unique(xy_mapping$cluster)
    color_collection <- c("#ff584d","#ffff00","#3bff4e","#8a7dff","#d614e0",
                          "#1fa61f","#59dae3","#bc13f0","#ff1cce","#ff709b",
                          "#f4ff26","#81b584","#ff29d4","#b30009","#ff4f1f")
    
    if(view_plot){
      x11()
      plot(xy_mapping[,1],xy_mapping[,2],
           xlim = c(min(xy_mapping[,1]), max(xy_mapping[,1])), 
           ylim = c(min(xy_mapping[,2]), max(xy_mapping[,2])), col = xy_mapping[,3])
      for(y in 1:length(unique_cluster)){
        polygon(x = xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[y])],
                y = xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[y])],
                col = color_collection[y])
      }
    }
    
    library(sp)
    library(dplyr)
    overlapping_df <- data.frame()
    dimred_technique <- c()
    cluster_vector <- c()
    point_from_cluster <- c()
    overlapping_vector <- c()
    point_total <- c()
    for(v in 1:length(unique_cluster)){
      p <- Polygon(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]), c(1,2)])
      ps <- Polygons(list(p),1)
      sps <- SpatialPolygons(list(ps))
      polygon_coords <- ps@Polygons[[1]]@coords
      #check data point for every clusters
      for(x in 1:length(unique_cluster)){
        point_cluster <- nrow(xy_mapping[which(xy_mapping[,3] == unique_cluster[v]),])
        point_location <- point.in.polygon(xy_mapping[,1][which(xy_mapping[,3] == unique_cluster[x])], 
                                           xy_mapping[,2][which(xy_mapping[,3] == unique_cluster[x])], 
                                           polygon_coords[,1], polygon_coords[,2], mode.checked=FALSE)
        overlap_measure <- length(point_location[point_location > 0])
        cluster_vector <- c(cluster_vector, paste0("Cluster-",v,"-",unique_cluster[v]))
        point_from_cluster <- c(point_from_cluster, paste0("Cluster-",x,"-",unique_cluster[x]))
        overlapping_vector <- c(overlapping_vector, overlap_measure)
        point_total <- c(point_total, point_cluster)
      }
    }
    
    overlapping_df <- data.frame(cluster_vector, point_from_cluster, overlapping_vector, point_total)
    overlapping_pct <- overlapping_df %>% filter(cluster_vector != point_from_cluster) %>% 
      group_by(cluster_vector) %>% 
      summarise(overlap_sum = sum(overlapping_vector),
                point_total = point_total) %>%
      mutate(overlap_pct = round(overlap_sum / point_total * 100,2)) %>% as.data.frame()
    overlapping_pct <- overlapping_pct[!duplicated(overlapping_pct),]
    overlapping_pct$dimred_technique <- dimred_name
    return(list(overlapping_df, overlapping_pct))
  }
  
  inverse <- function(x){
    inverse_result <- x^-1
    inverse_result[which(inverse_result == Inf)] <- 0
    return(inverse_result)
  }
  power1_25 <- function(x){
    return(x^1.25)
  }
  power1_5 <- function(x){
    return(x^1.5)
  }
  power1_75 <- function(x){
    return(x^1.75)
  }
  quadratic <- function(x){
    return(x^2)
  }
  cubic <- function(x){
    return(x^3)
  }
  
  library(dplyr)
  library(stringr)
  library(tictoc)
  library(data.table)
  
  options(dplyr.summarise.inform = FALSE)
  
  #transform first from any transformation in initial pick
  transformed_pick <- initial_pick
  for(a in 1:length(initial_pick)){
    transform_target <- unlist(lapply(strsplit(initial_pick[a], "_"), FUN = function(x) x[1]))
    var_target <- paste0(unlist(lapply(strsplit(initial_pick[a], "_"), FUN = function(x) x[-1])), collapse = "_")
    if(transform_target %in% apply_transformation){
      idx <- which(colnames(data_transform) == var_target)
      eval_transform <- paste0("data_transform[,idx] <- ",transform_target,"(data_transform[,idx])")
      identity_vec <- data_transform[,idx] / abs(data_transform[,idx])
      identity_vec[which(is.na(identity_vec))] <- 0 
      data_transform[,idx] <- abs(data_transform[,idx])
      eval(parse(text = eval_transform))
      data_transform[,idx][which(data_transform[,idx] == -Inf)] <- 0
      data_transform[,idx] <- data_transform[,idx] * identity_vec
      initial_pick[a] <- var_target
    }
  }
  
  overlap_reduction_df <- NULL
  data_transform <- data_transform[colnames(data_transform) %in% c(dependent_col, initial_pick)]
  target_transform <- colnames(data_transform)
  history_pick <- c()
  history_transform <- c()
  formula_pickup <- ""
  formula_interpretation <- ""
  batch_no <- 1
  
  while(1){
    start_execution_pick <- tic()
    for(b in 2:length(target_transform)){
      writeLines(paste0("------------------ Transforming Variable ", target_transform[b], " (",b-1," of ",length(target_transform), ") -----------------------------"))
      for(c in 1:length(apply_transformation)){
        tic()
        
        data_pick <- data_transform
        idx <- which(colnames(data_pick) == target_transform[b])
        eval_transform <- paste0("data_pick[,idx] <- ",apply_transformation[c],"(data_pick[,idx])")
        identity_vec <- data_pick[,idx] / abs(data_pick[,idx])
        identity_vec[which(is.na(identity_vec))] <- 0 
        data_pick[,idx] <- abs(data_pick[,idx])
        data_pick[,idx] <- eval(parse(text = eval_transform))
        data_pick[,idx][which(data_pick[,idx] == -Inf)] <- 0
        data_pick[,idx] <- data_pick[,idx] * identity_vec
        data_pick <- data_pick[!duplicated(data_pick),]
        cluster_simulation <- plot_cluster(data_pick[,-which(colnames(data_pick) == dependent_col)], 
                                           data_pick[[dependent_col]], clust_make = "input", 
                                           dimred_technique = dimred_technique_use, 
                                           clust_var_name = dependent_col, show_plot = F)
        overlap_measure_list <- list()
        all_overlap <- NULL
        for(x in 1:length(cluster_simulation)){
          overlap_measure_list <- append(overlap_measure_list, 
                                         list(cluster_overlapping_measure(cluster_simulation[[x]], view_plot = F, 
                                                                          dimred_name = names(cluster_simulation)[x])))
          all_overlap <- rbind(all_overlap, overlap_measure_list[[x]][[2]])
        }
        
        all_overlap <- all_overlap %>% arrange(cluster_vector)
        all_overlap <- all_overlap %>% group_by(cluster_vector) %>% 
          summarise(point_total = point_total,
                    min_overlap = min(overlap_sum), 
                    min_overlap_pct = min(overlap_pct), 
                    max_overlap = max(overlap_sum),
                    max_overlap_pct = max(overlap_pct)) %>% as.data.frame()
        
        all_overlap <- all_overlap[!duplicated(all_overlap),]
        if(is.null(overlap_reduction_df)){
          prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
          names(prefix_name) <- c(1:length(prefix_name))
          prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
          prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
          
          for(vars in 1:length(all_unique_class)){
            if(vars == 1){
              overlap_reduction_df <- data.frame(all_overlap$point_total[vars], 
                                                 all_overlap$min_overlap[vars],
                                                 all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df) <- c(paste0(prefix_name[vars], "_pts"), 
                                                  paste0(prefix_name[vars], "_overlap_min"), 
                                                  paste0(prefix_name[vars], "_overlap_max"))
            }else{
              overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                     all_overlap$min_overlap[vars],
                                                     all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                      paste0(prefix_name[vars], "_overlap_min"), 
                                                      paste0(prefix_name[vars], "_overlap_max"))
              overlap_reduction_df <- as.data.frame(cbind(overlap_reduction_df, overlap_reduction_df_add))
              colnames(overlap_reduction_df)[(ncol(overlap_reduction_df) - 2):ncol(overlap_reduction_df)] <- colnames(overlap_reduction_df_add)
            }
          }
          
          overlap_reduction_df$add_var <- target_transform[b]
          overlap_reduction_df$transform_var <- apply_transformation[c]
          
        }
        else{
          prefix_name <- str_extract_all(all_overlap$cluster_vector, "[A-Z]+")
          names(prefix_name) <- c(1:length(prefix_name))
          prefix_name <- unlist(lapply(prefix_name, FUN = function(x) paste0(x[1],"-",paste0(x[2:length(x)], collapse=""))))
          prefix_name <- str_replace_all(prefix_name, "\\-", paste0(names(prefix_name), "-"))
          
          for(vars in 1:length(all_unique_class)){
            if(vars == 1){
              overlap_reduction_df_temp <- data.frame(all_overlap$point_total[vars], 
                                                      all_overlap$min_overlap[vars],
                                                      all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df_temp) <- c(paste0(prefix_name[vars], "_pts"), 
                                                       paste0(prefix_name[vars], "_overlap_min"), 
                                                       paste0(prefix_name[vars], "_overlap_max"))
            }else{
              overlap_reduction_df_add <- data.frame(all_overlap$point_total[vars], 
                                                     all_overlap$min_overlap[vars],
                                                     all_overlap$max_overlap[vars])
              colnames(overlap_reduction_df_add) <- c(paste0(prefix_name[vars], "_pts"), 
                                                      paste0(prefix_name[vars], "_overlap_min"), 
                                                      paste0(prefix_name[vars], "_overlap_max"))
              overlap_reduction_df_temp <- as.data.frame(cbind(overlap_reduction_df_temp, overlap_reduction_df_add))
              colnames(overlap_reduction_df_temp)[(ncol(overlap_reduction_df_temp) - 2):ncol(overlap_reduction_df_temp)] <- colnames(overlap_reduction_df_add)
            }
          }
          overlap_reduction_df_temp$add_var <- target_transform[b]
          overlap_reduction_df_temp$transform_var <- apply_transformation[c]
          overlap_reduction_df <- rbind(overlap_reduction_df, overlap_reduction_df_temp)
        }
        toc()
        
      }
    }
    
    points_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "pts")]
    overlap_min_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_min")]
    overlap_max_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_max")]
    overlap_reduction_df$observation_included <- paste0(rowSums(points_df),"/",nrow(data))
    overlap_reduction_df$redudancy_score <- rowSums(overlap_min_df) + rowSums(overlap_max_df)
    overlap_reduction_df <- overlap_reduction_df %>% arrange(redudancy_score)
    
    print(overlap_reduction_df)
    
    end_execution_pick <- toc()
    current_execution_time <- as.numeric(end_execution_pick$toc - start_execution_pick)
    writeLines(paste0("Pick Transform for batch ",batch_no," take ",current_execution_time, " secs"))
    
    if(auto_best){
      writeLines("Best Model Information from Search")
      writeLines(paste0("By Adding ",overlap_reduction_df$transform_var[1],"(", overlap_reduction_df$add_var[1],")"))
      points_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "pts")]
      overlap_min_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_min")]
      overlap_max_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_max")]
      class_name <- unlist(lapply(strsplit(colnames(overlap_min_df), "_"), FUN = function(x) x[[1]]))
      for(f in 1:length(all_unique_class)){
        writeLines(paste0("Class Name: ", class_name[f]))
        writeLines(paste0("Old Captured Data: ", initial_class_points[f], " -> New Captured Data: ", points_df[1,f], " (Growth: ", points_df[1,f] - initial_class_points[f], ") "))
        writeLines(paste0("Old Overlap Min: ",initial_class_min_overlap[f], " -> New Overlap Min: ", overlap_min_df[1,f], " (Growth: ", overlap_min_df[1,f] - initial_class_min_overlap[f], ") "))
        writeLines(paste0("Old Overlap Max: ",initial_class_max_overlap[f], " -> New Overlap Max: ", overlap_max_df[1,f], " (Growth: ", overlap_max_df[1,f] - initial_class_max_overlap[f], ") "))
      }
      new_observation_include <- as.numeric(unlist(strsplit(overlap_reduction_df$observation_included[1], "/"))[1])
      writeLines(paste0("Old Observation Included: ", initial_obs, " -> New Observation Included: ", new_observation_include, " (Growth: ",new_observation_include - initial_obs, ")"))
      writeLines(paste0("Old Redudancy Score: ", initial_redudancy_score, " -> New Redudancy Score: ", overlap_reduction_df$redudancy_score[1], " (Growth: ",overlap_reduction_df$redudancy_score[1] - initial_redudancy_score, ")"))
      writeLines("")
      
      old_overlap_score <- initial_redudancy_score
      new_overlap_score <- overlap_reduction_df$redudancy_score[1]
      old_obs <- initial_obs
      new_obs <- new_observation_include
      redudancy_loss <- new_overlap_score - old_overlap_score
      observation_loss <- new_obs - old_obs
      if(new_overlap_score > old_overlap_score || new_obs > old_obs){
        for(f in 1:length(all_unique_class)){
          initial_class_points[f] <- points_df[1,f]
          initial_class_min_overlap[f] <- overlap_min_df[1,f]
          initial_class_max_overlap[f] <- overlap_max_df[1,f]
        }
        initial_obs <- new_obs
        initial_redudancy_score <- new_overlap_score
        history_pick <- c(history_pick, overlap_reduction_df$add_var[1])
        history_transform <- c(history_transform, overlap_reduction_df$transform_var[1])
        if(overlap_reduction_df$transform_var[1] != "none"){
          eval_transform <- paste0("data_transform[[overlap_reduction_df$add_var[1]]] <- ",overlap_reduction_df$transform_var[1],"(data_transform[[overlap_reduction_df$add_var[1]]])")
          identity_vec <- data_transform[[overlap_reduction_df$add_var[1]]] / abs(data_transform[[overlap_reduction_df$add_var[1]]])
          identity_vec[which(is.na(identity_vec))] <- 0 
          data_transform[[overlap_reduction_df$add_var[1]]] <- abs(data_transform[[overlap_reduction_df$add_var[1]]])
          eval(parse(text = eval_transform))
          data_transform[[overlap_reduction_df$add_var[1]]][which(data_transform[[overlap_reduction_df$add_var[1]]] == -Inf)] <- 0
          data_transform[[overlap_reduction_df$add_var[1]]] <- data_transform[[overlap_reduction_df$add_var[1]]] * identity_vec
        }
        writeLines("Formula Built Until Now")
        best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
        best_formula_interpretation <- paste0(dependent_col, " ~ ")
        for(j in 1:length(history_transform)){
          if(j < length(history_transform) && history_transform[j] != "none"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
          }
          else if(j < length(history_transform) && history_transform[j] == "none"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
          }
          else if(j == length(history_transform) && history_transform[j] != "none"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
          }
          else if(j == length(history_transform) && history_transform[j] == "none"){
            best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
          }
        }
        print(best_formula_interpretation)
        writeLines('Pick Transform Successfully')
        overlap_reduction_df <- NULL
        writeLines("")
      }
      else{
        writeLines("Variable Pick Transform has reached its peak! Reached the end of Results")
        for(f in 1:length(all_unique_class)){
          writeLines(paste0("Class Name: ", class_name[f]))
          writeLines(paste0("Peak Captured Data: ", initial_class_points[f]))
          writeLines(paste0("Peak Overlap Min: ",initial_class_min_overlap[f]))
          writeLines(paste0("Peak Overlap Max: ",initial_class_max_overlap[f]))
        }
        writeLines(paste0("Peak Observation Included: ", initial_obs))
        writeLines(paste0("Peak Redudancy Score: ", initial_redudancy_score))
        
        return(list(pickup_variable_from_start = history_pick,
                    transform_variable_from_start = history_transform,
                    end_class_points = initial_class_points,
                    end_class_overlap_min = initial_class_min_overlap,
                    end_class_overlap_max = initial_class_max_overlap,
                    end_captured_observation = initial_obs,
                    end_redudancy_score = initial_redudancy_score,
                    formula_pickup = best_formula,
                    formula_interpretation = best_formula_interpretation,
                    data_transformed = data,
                    overlap_collection = overlap_reduction_df))
      }
      if(!is.na(minimize_redudancy_threshold)){
        if(redudancy_loss <= -minimize_redudancy_threshold){
          writeLines("Variable Pick Transform has reached Redudancy Loss Threshold! Reached the end of Results")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      end_class_points = initial_class_points,
                      end_class_overlap_min = initial_class_min_overlap,
                      end_class_overlap_max = initial_class_max_overlap,
                      end_captured_observation = initial_obs,
                      end_redudancy_score = initial_redudancy_score,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data,
                      overlap_collection = overlap_reduction_df))
        }
      }
      if(!is.na(lose_observation_threshold)){
        if(observation_loss <= -lose_observation_threshold){
          writeLines(paste0("Variable Pick Transform Begin to Loss Observation in Searching Process over ",lose_observation_threshold, " Reached the end of Results"))
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      end_class_points = initial_class_points,
                      end_class_overlap_min = initial_class_min_overlap,
                      end_class_overlap_max = initial_class_max_overlap,
                      end_captured_observation = initial_obs,
                      end_redudancy_score = initial_redudancy_score,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data,
                      overlap_collection = overlap_reduction_df))
        }
      }
    }
    else{
      while(1){
        var_pick <- readline(prompt=paste0("Which Variable do you want to pick? (NO MORE to exit):  "))
        var_transform <- readline(prompt=paste0("Which Transform function do you want to pick? (NO MORE to exit):  "))
        if(var_pick == "NO MORE" && var_transform == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(pickup_variable_from_start = history_pick,
                      transform_variable_from_start = history_transform,
                      end_class_points = initial_class_points,
                      end_class_overlap_min = initial_class_min_overlap,
                      end_class_overlap_max = initial_class_max_overlap,
                      end_captured_observation = initial_obs,
                      end_redudancy_score = initial_redudancy_score,
                      formula_pickup = best_formula,
                      formula_interpretation = best_formula_interpretation,
                      data_transformed = data,
                      overlap_collection = overlap_reduction_df))
        }
        else if(var_pick %in% overlap_reduction_df$add_var && var_transform %in% overlap_reduction_df$transform_var){
          library(data.table)
          points_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "pts")]
          overlap_min_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_min")]
          overlap_max_df <- overlap_reduction_df[,which(colnames(overlap_reduction_df) %like% "overlap_max")]
          pick_which <- which(overlap_reduction_df$add_var == var_pick & overlap_reduction_df$transform_var == var_transform)
          class_name <- unlist(lapply(strsplit(colnames(overlap_min_df), "_"), FUN = function(x) x[[1]]))
          
          for(f in 1:length(all_unique_class)){
            writeLines(paste0("Class Name: ", class_name[f]))
            writeLines(paste0("Old Captured Data: ", initial_class_points[f], " -> New Captured Data: ", points_df[pick_which,f], " (Growth: ", points_df[pick_which,f] - initial_class_points[f], ") "))
            writeLines(paste0("Old Overlap Min: ",initial_class_min_overlap[f], " -> New Overlap Min: ", overlap_min_df[pick_which,f], " (Growth: ", overlap_min_df[pick_which,f] - initial_class_min_overlap[f], ") "))
            writeLines(paste0("Old Overlap Max: ",initial_class_max_overlap[f], " -> New Overlap Max: ", overlap_max_df[pick_which,f], " (Growth: ", overlap_max_df[pick_which,f] - initial_class_max_overlap[f], ") "))
          }
          
          new_observation_include <- as.numeric(unlist(strsplit(overlap_reduction_df$observation_included[pick_which], "/"))[1])
          writeLines(paste0("Old Observation Included: ", initial_obs, " -> New Observation Included: ", new_observation_include, " (Growth: ",new_observation_include - initial_obs, ")"))
          writeLines(paste0("Old Redudancy Score: ", initial_redudancy_score, " -> New Redudancy Score: ", overlap_reduction_df$redudancy_score[pick_which], " (Growth: ",overlap_reduction_df$redudancy_score[pick_which] - initial_redudancy_score, ")"))
          writeLines("")
          
          for(g in 1:length(all_unique_class)){
            initial_class_points[g] <- points_df[pick_which,g]
            initial_class_min_overlap[g] <- overlap_min_df[pick_which,g]
            initial_class_max_overlap[g] <- overlap_max_df[pick_which,g]
          }
          initial_obs <- new_observation_include
          initial_redudancy_score <- overlap_reduction_df$redudancy_score[pick_which]
          history_pick <- c(history_pick, var_pick)
          history_transform <- c(history_transform, var_transform)
          
          if(var_transform != "none"){
            eval_transform <- paste0("data_transform[[var_pick]] <- ",overlap_reduction_df$transform_var[1],"(data_transform[[var_pick]])")
            identity_vec <- data_transform[[overlap_reduction_df$add_var[1]]] / abs(data_transform[[overlap_reduction_df$add_var[1]]])
            identity_vec[which(is.na(identity_vec))] <- 0 
            data_transform[[overlap_reduction_df$add_var[1]]] <- abs(data_transform[[overlap_reduction_df$add_var[1]]])
            eval(parse(text = eval_transform))
            data_transform[[overlap_reduction_df$add_var[1]]][which(data_transform[[overlap_reduction_df$add_var[1]]] == -Inf)] <- 0
            data_transform[[overlap_reduction_df$add_var[1]]] <- data_transform[[overlap_reduction_df$add_var[1]]] * identity_vec
          }
          
          writeLines("Formula Built Until Now")
          best_formula <- paste0(dependent_col, "~", paste0(history_pick, collapse = " + "))
          best_formula_interpretation <- paste0(dependent_col, " ~ ")
          for(j in 1:length(history_transform)){
            if(j < length(history_transform) && history_transform[j] != "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],") + ")
            }
            else if(j < length(history_transform) && history_transform[j] == "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j]," + ")
            }
            else if(j == length(history_transform) && history_transform[j] != "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_transform[j],"(",history_pick[j],")")
            }
            else if(j == length(history_transform) && history_transform[j] == "none"){
              best_formula_interpretation <- paste0(best_formula_interpretation, history_pick[j])
            }
          }
          print(best_formula_interpretation)
          writeLines('Pick and Transformed Successfully')
          overlap_reduction_df <- NULL
          writeLines("")
          break
        }
        else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

save_model_recipe <- function(data_source, pick_variable_recipes, pick_transform_recipes, transform_recipes, 
                              drop_recipes, recipe_dependent_var="", recipe_name = "", recipe_sources = ""){
  
  variable_transform_recipe <- NULL
  if(length(pick_transform_recipes) == 0){
    variable_transform_recipe <- data.frame(variable = pick_variable_recipes, transform_function = "None")
  }else{
    variable_transform_recipe <- data.frame(variable = pick_variable_recipes, transform_function = pick_transform_recipes)
  }
  if(length(transform_recipes) > 0){
    library(stringr)
    for(b in 1:length(transform_recipes)){
      transform_decode <- strsplit(transform_recipes[[b]], "\\(")
      transform_functions <- unlist(lapply(transform_decode, FUN = function(x) x[[1]]))
      transform_variable <- unlist(lapply(transform_decode, FUN = function(x) x[[2]]))
      transform_variable <- str_replace_all(transform_variable, "\\)","")
      add_transform <- data.frame(variable = transform_variable, transform_function = transform_functions)
      variable_transform_recipe <- rbind(variable_transform_recipe, add_transform)
    }
  }
  if(length(drop_recipes) > 0){
    for(a in 1:length(drop_recipes)){
      variable_transform_recipe <- variable_transform_recipe %>% filter(!variable %in% drop_recipes[[a]])
    }
  }
  variable_transform_recipe$used_n_variable <- length(unique(variable_transform_recipe$variable))
  variable_transform_recipe$dependent_var <- rep(recipe_dependent_var, nrow(variable_transform_recipe))
  variable_transform_recipe$sources <- rep(recipe_sources, nrow(variable_transform_recipe))
  write.csv(variable_transform_recipe, recipe_name)
}

drop_neighbour_variable_stats <- function(data, dependent_col){
  all_lm <- lm(as.formula(paste0(dependent_col," ~ .")), data = data)
  current_r2 <- summary(all_lm)$r.squared
  current_r2adj <- summary(all_lm)$adj.r.squared
  all_coef_names <- names(all_lm$coefficients)
  mov_unique <- unique(unlist(lapply(strsplit(names(all_lm$coefficients), "_"), FUN = function(x) x[2])))[-1]
  stats_unique <- unique(unlist(lapply(strsplit(names(all_lm$coefficients), mov_unique[1]), FUN = function(x) x[2])))[-1]
  data_dropped <- data
  dropped_r2 <- 0
  dropped_r2adj <- 0
  lm_dropped <- all_lm
  while(1){
    writeLines("All Stats Currently Included in Model")
    print(stats_unique)
    temp_r2 <- 0
    temp_adj_r2_square <- 0
    for(a in 1:length(stats_unique)){
      writeLines(paste0("Models Performance When ", stats_unique[a], " is dropped"))
      filter_vars_choose <- NULL
      if(stats_unique[a] %in% c("_Min","_Q1","_Mode","_Median","_Mean","_Max","_Q3")){
        filter_vars_choose <- which(colnames(data_dropped) %like% stats_unique[a] & !colnames(data_dropped) %like% "_R_")
      }else{
        filter_vars_choose <- which(colnames(data_dropped) %like% stats_unique[a])
      }
      datatemp <- data_dropped[,-filter_vars_choose]
      temp_lm <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
      temp_r2 <- summary(temp_lm)$r.squared
      temp_adj_r2_square <- summary(temp_lm)$adj.r.squared
      writeLines(paste0("R2 Achieved: ", round(temp_r2, 3)))
      writeLines(paste0("Adj R2 Achieved: ", round(temp_adj_r2_square, 3)))
      writeLines("")
    }
    
    writeLines("Available Option")
    print(stats_unique)
    while(1){
      var_drop <- readline(prompt=paste0("Which Variable do you want to drop? (NO MORE to exit):  "))
      if(var_drop == "NO MORE"){
        writeLines("Reached the End of Results!")
        return(list(first_data = data,
                    first_model = all_lm,
                    first_r2 = current_r2,
                    first_r2adj = current_r2adj,
                    last_data = data_dropped, 
                    last_model = lm_dropped, 
                    last_r2 = dropped_r2, 
                    last_r2adj = dropped_r2adj))
      }else if(var_drop %in% stats_unique){
        filter_vars_choose <- NULL
        if(var_drop %in% c("_Min","_Q1","_Mode","_Median","_Mean","_Max","_Q3")){
          filter_vars_choose <- which(colnames(data_dropped) %like% var_drop & !colnames(data_dropped) %like% "_R_")
        }else{
          filter_vars_choose <- which(colnames(data_dropped) %like% var_drop)
        }
        data_dropped <- data_dropped[,-filter_vars_choose]
        stats_unique <- stats_unique[-which(stats_unique == var_drop)]
        lm_dropped <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_dropped)
        dropped_r2 <- summary(lm_dropped)$r.squared
        dropped_r2adj <- summary(lm_dropped)$adj.r.squared
        writeLines('Drop Successfully! Do Next Check Model Performance !')
        writeLines("")
        break
      }else{
        message("Please Enter a valid option!")
      }
    }
    
  }
}


drop_all_possible_variable <- function(data, dependent_col, auto_best = TRUE){
  r2_df <- NULL
  all_indep_col <- colnames(data[,-which(colnames(data) == dependent_col)])
  data_dropped <- data
  history_drop <- c()
  while(1){
    writeLines("Begin to calculate every variable drops. .")
    for(v in 2:length(all_indep_col)){
      datatemp <- data_dropped[,-v]
      if(is.null(r2_df)){
        lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
        r2_df <- data.frame(Model_name = "Model1", column_to_drop = colnames(data_dropped)[v], 
                            r2 = summary(lm_temp)$r.squared *100, adj_r2 = summary(lm_temp)$adj.r.squared *100)
      }else{
        lm_temp <- lm(as.formula(paste0(dependent_col," ~ .")), data = datatemp)
        r2_df <- rbind(r2_df, data.frame(Model_name = paste0("Model",v-1), column_to_drop = colnames(data_dropped)[v], 
                                         r2 = summary(lm_temp)$r.squared *100, adj_r2 = summary(lm_temp)$adj.r.squared *100))
      }
    }
    writeLines("Show the Performances")
    r2_df <- r2_df %>% arrange(desc(adj_r2))
    print(r2_df)
    writeLines("")
    if(auto_best){
      old_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = data_dropped)
      new_data <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[1])]
      new_model <- lm(as.formula(paste0(dependent_col," ~ .")), data = new_data)
      old_adj_r_squared <- summary(old_model)$adj.r.square
      new_adj_r_squared <- summary(new_model)$adj.r.square
      if(new_adj_r_squared > old_adj_r_squared){
        var_exact_indep <- which(all_indep_col == var_drop)
        history_drop <- c(history_drop, var_drop)
        data_dropped <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[1])]
        all_indep_col <- all_indep_col[-var_exact_indep]
        writeLines('Drop Successfully')
        r2_df <- NULL
        writeLines("")
      }else{
        writeLines("Variable Drop has reached its peak! Reached the end of Results")
        return(list(first_data = data, 
                    last_data = data_dropped,
                    drop_variable_from_start = history_drop))
      }
    }
    else{
      while(1){
        var_drop <- readline(prompt=paste0("Which Variable do you want to drop? (NO MORE to exit):  "))
        if(var_drop == "NO MORE"){
          writeLines("Reached the End of Results!")
          return(list(first_data = data, 
                      last_data = data_dropped,
                      drop_variable_from_start = history_drop))
        }else if(var_drop %in% r2_df$column_to_drop){
          var_exact_drop_r2 <- which(r2_df$column_to_drop == var_drop)
          var_exact_indep <- which(all_indep_col == var_drop)
          history_drop <- c(history_drop, var_drop)
          data_dropped <- data_dropped[,-which(colnames(data_dropped) == r2_df$column_to_drop[var_exact_drop_r2])]
          all_indep_col <- all_indep_col[-var_exact_indep]
          writeLines('Drop Successfully')
          r2_df <- NULL
          writeLines("")
          break
        }else{
          message("Please Input a valid variable!")
        }
      }
    }
  }
}

regr_subset_model_inspect <- function(subset_model, dataset, nvar=5, dependent_var="", log=FALSE){
  library(dplyr)
  library(caret)
  library(purrr)
  
  writeLines("1) Subset Model Summary")
  res.sum <- summary(subset_model)
  if(log){
    print(res.sum)
  }
  
  writeLines("")
  writeLines("2) Model Criterion Best N Variable")
  criterion <- data.frame(
    Adj.R2 = which.max(res.sum$adjr2),
    CP = which.min(res.sum$cp),
    BIC = which.min(res.sum$bic)
  )
  print(criterion)
  
  model_formula_int <- c(criterion[1,1], criterion[1,2], criterion[1,3])
  
  get_model_formula <- function(id, object, outcome){
    models <- summary(object)$which[id,-1]
    predictors <- names(which(models == TRUE))
    predictors <- paste(predictors, collapse = "+")
    as.formula(paste0(outcome, "~", predictors))
  }
  
  get_cv_error <- function(model.formula, data){
    set.seed(1)
    train.control <- trainControl(method = "cv", number = 5)
    cv <- train(model.formula, data = data, method = "lm",
                trControl = train.control)
    cv$results$RMSE
  }
  
  writeLines("")
  writeLines("3) Prediction Error by Linear Model (by nvar)")
  model.ids <- 1:nrow(res.sum[[1]])
  cv.errors <-  map(model.ids, get_model_formula, subset_model, dependent_var) %>%
    map(get_cv_error, data = dataset) %>%
    unlist()
  
  print(cv.errors)
  writeLines("")
  writeLines(paste("Minimum CV Error: ", which.min(cv.errors)))
  cv_error_order <- order(cv.errors)
  writeLines(paste0("Variable referred by Criterion ADJ.R2 have Error rate of: ",round(cv.errors[model_formula_int[1]],2), 
                    " Performance Rank ",which(cv_error_order == model_formula_int[1])))
  writeLines(paste0("Variable referred by Criterion CP have Error rate of: ",round(cv.errors[model_formula_int[2]],2), 
                    " Performance Rank ",which(cv_error_order == model_formula_int[2])))
  writeLines(paste0("Variable referred by Criterion BIC have Error rate of: ",round(cv.errors[model_formula_int[3]],2), 
                    " Performance Rank ",which(cv_error_order == model_formula_int[3])))
  
  writeLines("")
  writeLines("4) Create Formula Based by Experiment")
  best_cv_error <- cv_error_order[1:5]
  best_idx <- unique(c(best_cv_error, model_formula_int))
  formula_set <- list()
  
  for(x in 1:length(best_idx)){
    formula_set <- c(formula_set, list(get_model_formula(best_idx[x], subset_model, dependent_var)))
    names(formula_set)[x] <- paste0("Formula-",best_idx[x])
  }
  
  return(list(subset_model = subset_model, 
              criterion = criterion, 
              cv_error = cv.errors, 
              formula_set = formula_set))
  
  #plot(regfit_full, scale = "r2")
  #plot(regfit_full, scale = "adjr2")
  #plot(regfit_full, scale = "Cp")
  #plot(regfit_full, scale = "bic")
}

regr_subset_essential_info <- function(data, dependent_col = ""){
  library(tictoc)
  library(leaps)
  library(dplyr)
  library(caret)
  library(purrr)
  
  tic()
  subset_model <-
    regsubsets(as.formula(paste0(dependent_col, " ~ .")),
               data = data,    
               nvmax = NULL,    
               force.in = NULL, 
               force.out = NULL,
               method = "exhaustive", 
               really.big = TRUE)
  time_take <- toc()
  time_take <- as.numeric(time_take$toc) - as.numeric(time_take$tic)
  
  res.sum <- summary(subset_model)
  
  criterion <- data.frame(
    Adj.R2 = which.max(res.sum$adjr2),
    CP = which.min(res.sum$cp),
    BIC = which.min(res.sum$bic)
  )
  
  model_formula_int <- c(criterion[1,1], criterion[1,2], criterion[1,3])
  
  get_model_formula <- function(id, object, outcome){
    models <- summary(object)$which[id,-1]
    predictors <- names(which(models == TRUE))
    predictors <- paste(predictors, collapse = "+")
    as.formula(paste0(outcome, "~", predictors))
  }
  
  formula_R2 <- get_model_formula(model_formula_int[1], subset_model, dependent_col)
  formula_CP <- get_model_formula(model_formula_int[2], subset_model, dependent_col)
  formula_BIC <- get_model_formula(model_formula_int[3], subset_model, dependent_col)
  
  return(list(time_compute = time_take, subset_model = subset_model, criterion = criterion, 
              formula_R2 = formula_R2, formula_CP = formula_CP, 
              formula_BIC = formula_BIC))
}

batch_regr_subset_RMSE_wise <- function(df, initial_var=c(), 
                                        initial_RMSE=NA,
                                        dependent_var="",
                                        limit_vars_batch=40){
  library(tictoc)
  library(stringr)
  library(dplyr)
  library(data.table)
  library(leaps)
  batch_no <- 1
  pointer_var <- 1
  better_var <- c()
  df_independent <- df[,-which(colnames(df) == dependent_var)]
  
  regr_subset_model_inspect <- function(subset_model, dataset, nvar=5, dependent_var="", log=FALSE){
    library(dplyr)
    library(caret)
    library(purrr)
    
    writeLines("1) Subset Model Summary")
    res.sum <- summary(subset_model)
    if(log){
      print(res.sum)
    }
    
    writeLines("")
    writeLines("2) Model Criterion Best N Variable")
    criterion <- data.frame(
      Adj.R2 = which.max(res.sum$adjr2),
      CP = which.min(res.sum$cp),
      BIC = which.min(res.sum$bic)
    )
    print(criterion)
    
    model_formula_int <- c(criterion[1,1], criterion[1,2], criterion[1,3])
    
    get_model_formula <- function(id, object, outcome){
      models <- summary(object)$which[id,-1]
      predictors <- names(which(models == TRUE))
      predictors <- paste(predictors, collapse = "+")
      as.formula(paste0(outcome, "~", predictors))
    }
    
    get_cv_error <- function(model.formula, data){
      set.seed(1)
      train.control <- trainControl(method = "cv", number = 5)
      cv <- train(model.formula, data = data, method = "lm",
                  trControl = train.control)
      cv$results$RMSE
    }
    
    writeLines("")
    writeLines("3) Prediction Error by Linear Model (by nvar)")
    model.ids <- 1:nrow(res.sum[[1]])
    cv.errors <-  map(model.ids, get_model_formula, subset_model, dependent_var) %>%
      map(get_cv_error, data = dataset) %>%
      unlist()
    
    print(cv.errors)
    writeLines("")
    writeLines(paste("Minimum CV Error: ", which.min(cv.errors)))
    cv_error_order <- order(cv.errors)
    writeLines(paste0("Variable referred by Criterion ADJ.R2 have Error rate of: ",round(cv.errors[model_formula_int[1]],2), 
                      " Performance Rank ",which(cv_error_order == model_formula_int[1])))
    writeLines(paste0("Variable referred by Criterion CP have Error rate of: ",round(cv.errors[model_formula_int[2]],2), 
                      " Performance Rank ",which(cv_error_order == model_formula_int[2])))
    writeLines(paste0("Variable referred by Criterion BIC have Error rate of: ",round(cv.errors[model_formula_int[3]],2), 
                      " Performance Rank ",which(cv_error_order == model_formula_int[3])))
    
    writeLines("")
    writeLines("4) Create Formula Based by Experiment")
    best_cv_error <- cv_error_order[1:5]
    best_idx <- unique(c(best_cv_error, model_formula_int))
    formula_set <- list()
    
    for(x in 1:length(best_idx)){
      formula_set <- c(formula_set, list(get_model_formula(best_idx[x], subset_model, dependent_var)))
      names(formula_set)[x] <- paste0("Formula-",best_idx[x])
    }
    
    return(list(subset_model = subset_model, 
                criterion = criterion, 
                cv_error = cv.errors, 
                formula_set = formula_set))
    
    #plot(regfit_full, scale = "r2")
    #plot(regfit_full, scale = "adjr2")
    #plot(regfit_full, scale = "Cp")
    #plot(regfit_full, scale = "bic")
  }
  
  if(length(initial_var) != 0){
    df_independent <- df_independent[,-which(colnames(df_independent) %in% initial_var)]
    while(pointer_var < ncol(df_independent)){
      writeLines(paste0("Begin Batch No ",batch_no))
      formula_all <- as.formula(paste0(dependent_var, " ~ ."))
      take_var_batch <- limit_vars_batch - length(initial_var)
      to_idx <- (take_var_batch + pointer_var - 1)
      if(to_idx < ncol(df_independent)){
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        df_subset <- c(which(colnames(df) == dependent_var), 
                       which(colnames(df) %in% initial_var),
                       which(colnames(df) %in% add_subset))
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        tic()
        Best_Subset_X <-
          regsubsets(formula_all,
                     data = df[,df_subset],    
                     nvmax = NULL,    
                     force.in = NULL, 
                     force.out = NULL,
                     method = "exhaustive", 
                     really.big = TRUE)
        
        length_var <- limit_vars_batch
        low_high_summary <- summary(Best_Subset_X)
        subset_model_inspect_X <- regr_subset_model_inspect(Best_Subset_X, df, 
                                                            length_var, dependent_var = dependent_var)
        toc()
        
        writeLines(paste0("Minimum RMSE from this Batch Subset: ", min(subset_model_inspect_X[[3]])))
        if(initial_RMSE > min(subset_model_inspect_X[[3]])){
          writeLines("Add new Record to Initial Variables")
          writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with RMSE: ",initial_RMSE))
          print(initial_var)
          formula_to_continue <- as.character(subset_model_inspect13[[4]][[1]])
          initial_var <- str_replace_all(unlist(strsplit(formula_to_continue[3], "\\+"))," ","")
          initial_RMSE <- min(subset_model_inspect_X[[3]])
          writeLines(paste0("Initial Variable After length: ", length(initial_var), " with RMSE: ",initial_RMSE))
          print(initial_var)
          bool_search <- readline(prompt=paste0("Found Better RMSE, Continue Searching? (YES(or any Answer)/NO) "))
          if(bool_search == "NO"){
            return(subset_model_inspect_X)
          }
          pointer_var <- to_idx + 1
          batch_no <- batch_no + 1
        }else{
          writeLines("Not Found better RMSE in this batch compared from Initial RMSE!")
          pointer_var <- to_idx + 1
          batch_no <- batch_no + 1
        }
      }else if(to_idx >= ncol(df_independent)){
        to_idx <- ncol(df_independent)
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        df_subset <- c(which(colnames(df) == dependent_var), 
                       which(colnames(df) %in% initial_var),
                       which(colnames(df) %in% add_subset))
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        tic()
        Best_Subset_X <-
          regsubsets(formula_all,
                     data = df[,df_subset],    
                     nvmax = NULL,    
                     force.in = NULL, 
                     force.out = NULL,
                     method = "exhaustive", 
                     really.big = TRUE)
        
        length_var <- limit_vars_batch
        low_high_summary <- summary(Best_Subset_X)
        subset_model_inspect_X <- regr_subset_model_inspect(Best_Subset_X, df, 
                                                            length_var, dependent_var = dependent_var)
        toc()
        
        writeLines(paste0("Minimum RMSE from this Batch Subset: ", min(subset_model_inspect_X[[3]])))
        if(initial_RMSE > min(subset_model_inspect_X[[3]])){
          writeLines("Add new Record to Initial Variables")
          writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with RMSE: ",initial_RMSE))
          print(initial_var)
          formula_to_continue <- as.character(subset_model_inspect_X[[4]][[1]])
          initial_var <- str_replace_all(unlist(strsplit(formula_to_continue[3], "\\+"))," ","")
          initial_RMSE <- min(subset_model_inspect_X[[3]])
          writeLines(paste0("Initial Variable After length: ", length(initial_var), " with RMSE: ",initial_RMSE))
          print(initial_var)
          writeLines("Found Better RMSE at the end of Batch!")
          return(subset_model_inspect_X)
        }else{
          writeLines("Not Found better RMSE in the end of Batch!")
          return(subset_model_inspect_X)
        }
        break
      }
      
    }
  }
  else{
    while(pointer_var < ncol(df_independent)){
      writeLines(paste0("Begin Batch No ",batch_no))
      formula_all <- as.formula(paste0(dependent_var, "~ ."))
      take_var_batch <- 0
      if(length(initial_var) == 0){
        take_var_batch <- limit_vars_batch
      }else{
        take_var_batch <- limit_vars_batch - length(initial_var)
      }
      
      to_idx <- (take_var_batch + pointer_var - 1)
      if(to_idx < ncol(df_independent)){
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        if(length(initial_var) != 0){
          df_subset <- c(which(colnames(df) == dependent_var), 
                         which(colnames(df) %in% add_subset))
        }else{
          df_subset <- c(which(colnames(df) == dependent_var), 
                         which(colnames(df) %in% initial_var),
                         which(colnames(df) %in% add_subset))
        }
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        tic()
        Best_Subset_X <-
          regsubsets(formula_all,
                     data = df[,df_subset],    
                     nvmax = NULL,    
                     force.in = NULL, 
                     force.out = NULL,
                     method = "exhaustive", 
                     really.big = TRUE)
        
        length_var <- limit_vars_batch
        low_high_summary <- summary(Best_Subset_X)
        subset_model_inspect_X <- regr_subset_model_inspect(Best_Subset_X, df, 
                                                            length_var, dependent_var = dependent_var)
        toc()
        
        writeLines(paste0("Minimum RMSE from this Batch Subset: ", min(subset_model_inspect_X[[3]])))
        if(is.na(initial_RMSE)){
          initial_RMSE <- min(subset_model_inspect_X[[3]])
        }
        if(initial_RMSE > min(subset_model_inspect_X[[3]])){
          writeLines("Add new Record to Initial Variables")
          if(length(initial_var) != 0){
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with RMSE: ",initial_RMSE))
            print(initial_var)
          }
          formula_to_continue <- as.character(subset_model_inspect13[[4]][[1]])
          initial_var <- str_replace_all(unlist(strsplit(formula_to_continue[3], "\\+"))," ","")
          initial_RMSE <- min(subset_model_inspect_X[[3]])
          writeLines(paste0("Initial Variable After length: ", length(initial_var), " with RMSE: ",initial_RMSE))
          print(initial_var)
          
          bool_search <- readline(prompt=paste0("Found Better RMSE, Continue Searching? (YES(or any Answer)/NO) "))
          if(bool_search == "NO"){
            return(subset_model_inspect_X)
          }
          pointer_var <- to_idx + 1
          batch_no <- batch_no + 1
        }else{
          writeLines("Not Found better RMSE in this batch compared from Initial RMSE!")
          pointer_var <- to_idx + 1
          batch_no <- batch_no + 1
        }
      }
      else if(to_idx >= ncol(df_independent)){
        to_idx <- ncol(df_independent)
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        df_subset <- c(which(colnames(df) == dependent_var), 
                       which(colnames(df) %in% initial_var),
                       which(colnames(df) %in% add_subset))
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        tic()
        Best_Subset_X <-
          regsubsets(formula_all,
                     data = df[,df_subset],    
                     nvmax = NULL,    
                     force.in = NULL, 
                     force.out = NULL,
                     method = "exhaustive", 
                     really.big = TRUE)
        
        length_var <- limit_vars_batch
        low_high_summary <- summary(Best_Subset_X)
        subset_model_inspect_X <- regr_subset_model_inspect(Best_Subset_X, df, 
                                                            length_var, dependent_var = dependent_var)
        toc()
        writeLines(paste0("Minimum RMSE from this Batch Subset: ", min(subset_model_inspect_X[[3]])))
        if(initial_RMSE > min(subset_model_inspect_X[[3]])){
          writeLines("Add new Record to Initial Variables")
          writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with RMSE: ",initial_RMSE))
          print(initial_var)
          formula_to_continue <- as.character(subset_model_inspect_X[[4]][[1]])
          initial_var <- str_replace_all(unlist(strsplit(formula_to_continue[3], "\\+"))," ","")
          initial_RMSE <- min(subset_model_inspect_X[[3]])
          writeLines(paste0("Initial Variable After length: ", length(initial_var), " with RMSE: ",initial_RMSE))
          print(initial_var)
          writeLines("Found Better RMSE at the end of Batch!")
          return(subset_model_inspect_X)
        }else{
          writeLines("Not Found better RMSE in the end of Batch!")
          return(subset_model_inspect_X)
        }
      }
    }
  }
}

batch_regr_subset_criterion_wise <- function(df, initial_var=c(), 
                                             initial_adjr2=NA,
                                             initial_cp=NA,
                                             initial_bic=NA,
                                             dependent_var="",
                                             limit_vars_batch=40,
                                             criterion_choose = "adjr2"){
  library(tictoc)
  library(stringr)
  library(dplyr)
  library(data.table)
  library(leaps)
  batch_no <- 1
  pointer_var <- 1
  better_var <- c()
  df_independent <- df[,-which(colnames(df) == dependent_var)]
  
  regr_subset_essential_info <- function(data, dependent_col = ""){
    library(tictoc)
    library(leaps)
    library(dplyr)
    library(caret)
    library(purrr)
    
    tic()
    subset_model <-
      regsubsets(as.formula(paste0(dependent_col, " ~ .")),
                 data = data,    
                 nvmax = NULL,    
                 force.in = NULL, 
                 force.out = NULL,
                 method = "exhaustive", 
                 really.big = TRUE)
    time_take <- toc()
    time_take <- as.numeric(time_take$toc) - as.numeric(time_take$tic)
    
    res.sum <- summary(subset_model)
    
    criterion <- data.frame(
      Adj.R2 = which.max(res.sum$adjr2),
      CP = which.min(res.sum$cp),
      BIC = which.min(res.sum$bic)
    )
    
    model_formula_int <- c(criterion[1,1], criterion[1,2], criterion[1,3])
    
    get_model_formula <- function(id, object, outcome){
      models <- summary(object)$which[id,-1]
      predictors <- names(which(models == TRUE))
      predictors <- paste(predictors, collapse = "+")
      as.formula(paste0(outcome, "~", predictors))
    }
    
    formula_R2 <- get_model_formula(model_formula_int[1], subset_model, dependent_col)
    formula_CP <- get_model_formula(model_formula_int[2], subset_model, dependent_col)
    formula_BIC <- get_model_formula(model_formula_int[3], subset_model, dependent_col)
    
    return(list(time_compute = time_take, subset_model = subset_model, criterion = criterion, 
                formula_R2 = formula_R2, formula_CP = formula_CP, 
                formula_BIC = formula_BIC))
  }
  
  if(length(initial_var) != 0){
    df_independent <- df_independent[,-which(colnames(df_independent) %in% initial_var)]
    while(pointer_var < ncol(df_independent)){
      writeLines(paste0("Begin Batch No ",batch_no))
      formula_all <- as.formula(paste0(dependent_var, " ~ ."))
      take_var_batch <- limit_vars_batch - length(initial_var)
      to_idx <- (take_var_batch + pointer_var - 1)
      if(to_idx < ncol(df_independent)){
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        df_subset <- c(which(colnames(df) == dependent_var), 
                       which(colnames(df) %in% initial_var),
                       which(colnames(df) %in% add_subset))
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        batch_results <- regr_subset_essential_info(df[,df_subset], dependent_var)
        current_adjr2 <- summary(batch_results$subset_model)$adjr2[as.numeric(batch_results$criterion[1])]
        current_cp <- summary(batch_results$subset_model)$cp[as.numeric(batch_results$criterion[2])]
        current_bic <- summary(batch_results$subset_model)$bic[as.numeric(batch_results$criterion[3])]
        formula_r2 <- batch_results$formula_R2
        formula_cp <- batch_results$formula_CP
        formula_bic <- batch_results$formula_BIC
        current_adjr2_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_r2), " ~ "))[3], "\\+"))," ","")
        current_cp_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_cp), " ~ "))[3], "\\+"))," ","")
        current_bic_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_bic), " ~ "))[3], "\\+"))," ","")
        
        if(criterion_choose == "adjr2"){
          writeLines(paste0("Maximum Adjusted R2 Found on this Batch: ", current_adjr2))
          if(initial_adjr2 < current_adjr2){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
            print(initial_var)
            initial_var <- current_adjr2_var
            initial_adjr2 <- current_adjr2
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
            print(initial_var)
            bool_search <- readline(prompt=paste0("Found Better Adjusted R2, Continue Searching? (YES(or any Answer)/NO) "))
            if(bool_search == "NO"){
              return(list(initial_adjr2, initial_var))
            }
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }else{
            writeLines("Not Found better Adjusted R2 in this batch compared from Initial R2!")
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }
        }
        else if(criterion_choose == "cp"){
          writeLines(paste0("Minimum CP Found on this Batch: ", current_cp))
          if(initial_cp > current_cp){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with CP: ",initial_cp))
            print(initial_var)
            initial_var <- current_cp_var
            initial_cp <- current_cp
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with CP: ",initial_cp))
            print(initial_var)
            bool_search <- readline(prompt=paste0("Found Better CP, Continue Searching? (YES(or any Answer)/NO) "))
            if(bool_search == "NO"){
              return(list(initial_cp, initial_var))
            }
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }else{
            writeLines("Not Found better CP in this batch compared from Initial CP!")
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }
        }
        else if(criterion_choose == "bic"){
          writeLines(paste0("Minimum BIC Found on this Batch: ", current_bic))
          if(initial_bic > current_bic){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with BIC: ",initial_bic))
            print(initial_var)
            initial_var <- current_bic_var
            initial_bic <- current_bic
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with BIC: ",initial_bic))
            print(initial_var)
            bool_search <- readline(prompt=paste0("Found Better BIC, Continue Searching? (YES(or any Answer)/NO) "))
            if(bool_search == "NO"){
              return(list(initial_bic, initial_var))
            }
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }else{
            writeLines("Not Found better BIC in this batch compared from Initial BIC!")
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }
        }
        
      }
      else if(to_idx >= ncol(df_independent)){
        to_idx <- ncol(df_independent)
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        df_subset <- c(which(colnames(df) == dependent_var), 
                       which(colnames(df) %in% initial_var),
                       which(colnames(df) %in% add_subset))
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        batch_results <- regr_subset_essential_info(df[,df_subset], dependent_var)
        current_adjr2 <- summary(batch_results$subset_model)$adjr2[as.numeric(batch_results$criterion[1])]
        current_cp <- summary(batch_results$subset_model)$cp[as.numeric(batch_results$criterion[2])]
        current_bic <- summary(batch_results$subset_model)$bic[as.numeric(batch_results$criterion[3])]
        formula_r2 <- batch_results$formula_R2
        formula_cp <- batch_results$formula_CP
        formula_bic <- batch_results$formula_BIC
        current_adjr2_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_r2), " ~ "))[3], "\\+"))," ","")
        current_cp_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_cp), " ~ "))[3], "\\+"))," ","")
        current_bic_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_bic), " ~ "))[3], "\\+"))," ","")
        
        if(criterion_choose == "adjr2"){
          writeLines(paste0("Maximum Adjusted R2 Found on this Batch: ", current_adjr2))
          if(initial_adjr2 < current_adjr2){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
            print(initial_var)
            initial_var <- current_adjr2_var
            initial_adjr2 <- current_adjr2
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
            print(initial_var)
            writeLines("Found Better Adjusted R2 at the End of Batch")
            return(list(initial_adjr2, initial_var))
          }else{
            writeLines("Not Found better Adjusted R2 at the End of Batch")
            return(list(initial_adjr2, initial_var))
          }
        }
        else if(criterion_choose == "cp"){
          writeLines(paste0("Minimum CP Found on this Batch: ", current_cp))
          if(initial_cp > current_cp){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with CP: ",initial_cp))
            print(initial_var)
            initial_var <- current_cp_var
            initial_cp <- current_cp
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with CP: ",initial_cp))
            print(initial_var)
            writeLines("Found Better CP at the End of Batch")
            return(list(initial_cp, initial_var))
          }else{
            writeLines("Not Found better CP at the End of Batch")
            return(list(initial_cp, initial_var))
          }
        }
        else if(criterion_choose == "bic"){
          writeLines(paste0("Minimum BIC Found on this Batch: ", current_bic))
          if(initial_bic > current_bic){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with BIC: ",initial_bic))
            print(initial_var)
            initial_var <- current_bic_var
            initial_bic <- current_bic
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with BIC: ",initial_bic))
            print(initial_var)
            writeLines("Found Better BIC at the End of Batch")
            return(list(initial_bic, initial_var))
          }else{
            writeLines("Not Found Better BIC at the End of Batch")
            return(list(initial_bic, initial_var))
          }
        }
      }
    }
  }
  else{
    while(pointer_var < ncol(df_independent)){
      writeLines(paste0("Begin Batch No ",batch_no))
      formula_all <- as.formula(paste0(dependent_var, " ~ ."))
      take_var_batch <- 0
      if(length(initial_var) == 0){
        take_var_batch <- limit_vars_batch
      }else{
        take_var_batch <- limit_vars_batch - length(initial_var)
      }
      to_idx <- (take_var_batch + pointer_var - 1)
      if(to_idx < ncol(df_independent)){
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        if(length(initial_var) == 0){
          df_subset <- c(which(colnames(df) == dependent_var), 
                         which(colnames(df) %in% add_subset))
        }else{
          df_subset <- c(which(colnames(df) == dependent_var), 
                         which(colnames(df) %in% initial_var),
                         which(colnames(df) %in% add_subset))
        }
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        batch_results <- regr_subset_essential_info(df[,df_subset], dependent_var)
        current_adjr2 <- summary(batch_results$subset_model)$adjr2[as.numeric(batch_results$criterion[1])]
        current_cp <- summary(batch_results$subset_model)$cp[as.numeric(batch_results$criterion[2])]
        current_bic <- summary(batch_results$subset_model)$bic[as.numeric(batch_results$criterion[3])]
        formula_r2 <- batch_results$formula_R2
        formula_cp <- batch_results$formula_CP
        formula_bic <- batch_results$formula_BIC
        current_adjr2_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_r2), " ~ "))[3], "\\+"))," ","")
        current_cp_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_cp), " ~ "))[3], "\\+"))," ","")
        current_bic_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_bic), " ~ "))[3], "\\+"))," ","")
        
        if(criterion_choose == "adjr2"){
          writeLines(paste0("Maximum Adjusted R2 Found on this Batch: ", current_adjr2))
          if(is.na(initial_adjr2)){
            writeLines("Add Record to Initial Variables")
            initial_var <- current_adjr2_var
            initial_adjr2 <- current_adjr2
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }else{
            if(initial_adjr2 < current_adjr2){
              writeLines("Add new Record to Initial Variables")
              writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
              print(initial_var)
              initial_var <- current_adjr2_var
              initial_adjr2 <- current_adjr2
              writeLines(paste0("Initial Variable After length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
              print(initial_var)
              bool_search <- readline(prompt=paste0("Found Better Adjusted R2, Continue Searching? (YES(or any Answer)/NO) "))
              if(bool_search == "NO"){
                return(list(initial_adjr2, initial_var))
              }
              pointer_var <- to_idx + 1
              batch_no <- batch_no + 1
            }else{
              writeLines("Not Found better Adjusted R2 in this batch compared from Initial R2!")
              pointer_var <- to_idx + 1
              batch_no <- batch_no + 1
            }
          }
          
        }
        else if(criterion_choose == "cp"){
          writeLines(paste0("Minimum CP Found on this Batch: ", current_cp))
          if(is.na(initial_cp)){
            writeLines("Add Record to Initial Variables")
            initial_var <- current_cp_var
            initial_cp <- current_cp
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }else{
            if(initial_cp > current_cp){
              writeLines("Add new Record to Initial Variables")
              writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with CP: ",initial_cp))
              print(initial_var)
              initial_var <- current_cp_var
              initial_cp <- current_cp
              writeLines(paste0("Initial Variable After length: ", length(initial_var), " with CP: ",initial_cp))
              print(initial_var)
              bool_search <- readline(prompt=paste0("Found Better CP, Continue Searching? (YES(or any Answer)/NO) "))
              if(bool_search == "NO"){
                return(list(initial_cp, initial_var))
              }
              pointer_var <- to_idx + 1
              batch_no <- batch_no + 1
            }else{
              writeLines("Not Found better CP in this batch compared from Initial CP!")
              pointer_var <- to_idx + 1
              batch_no <- batch_no + 1
            }
          }
          
        }
        else if(criterion_choose == "bic"){
          writeLines(paste0("Minimum BIC Found on this Batch: ", current_bic))
          if(is.na(initial_bic)){
            writeLines("Add Record to Initial Variables")
            initial_var <- current_bic_var
            initial_bic <- current_bic
            pointer_var <- to_idx + 1
            batch_no <- batch_no + 1
          }else{
            if(initial_bic > current_bic){
              writeLines("Add new Record to Initial Variables")
              writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with BIC: ",initial_bic))
              print(initial_var)
              initial_var <- current_bic_var
              initial_bic <- current_bic
              writeLines(paste0("Initial Variable After length: ", length(initial_var), " with BIC: ",initial_bic))
              print(initial_var)
              bool_search <- readline(prompt=paste0("Found Better BIC, Continue Searching? (YES(or any Answer)/NO) "))
              if(bool_search == "NO"){
                return(list(initial_bic, initial_var))
              }
              pointer_var <- to_idx + 1
              batch_no <- batch_no + 1
            }else{
              writeLines("Not Found better BIC in this batch compared from Initial BIC!")
              pointer_var <- to_idx + 1
              batch_no <- batch_no + 1
            }
          }
          
        }
        
      }
      else if(to_idx >= ncol(df_independent)){
        to_idx <- ncol(df_independent)
        add_subset <- colnames(df_independent)[pointer_var:to_idx]
        
        df_subset <- c(which(colnames(df) == dependent_var), 
                       which(colnames(df) %in% initial_var),
                       which(colnames(df) %in% add_subset))
        
        writeLines("Use Variables at idx")
        print(df_subset)
        
        batch_results <- regr_subset_essential_info(df[,df_subset], dependent_var)
        current_adjr2 <- summary(batch_results$subset_model)$adjr2[as.numeric(batch_results$criterion[1])]
        current_cp <- summary(batch_results$subset_model)$cp[as.numeric(batch_results$criterion[2])]
        current_bic <- summary(batch_results$subset_model)$bic[as.numeric(batch_results$criterion[3])]
        formula_r2 <- batch_results$formula_R2
        formula_cp <- batch_results$formula_CP
        formula_bic <- batch_results$formula_BIC
        current_adjr2_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_r2), " ~ "))[3], "\\+"))," ","")
        current_cp_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_cp), " ~ "))[3], "\\+"))," ","")
        current_bic_var <- str_replace_all(unlist(strsplit(unlist(strsplit(as.character(formula_bic), " ~ "))[3], "\\+"))," ","")
        
        if(criterion_choose == "adjr2"){
          writeLines(paste0("Maximum Adjusted R2 Found on this Batch: ", current_adjr2))
          if(initial_adjr2 < current_adjr2){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
            print(initial_var)
            initial_var <- current_adjr2_var
            initial_adjr2 <- current_adjr2
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with AdjR2: ",initial_adjr2))
            print(initial_var)
            writeLines("Found Better Adjusted R2 at the End of Batch")
            return(list(initial_adjr2, initial_var))
          }else{
            writeLines("Not Found better Adjusted R2 at the End of Batch")
            return(list(initial_adjr2, initial_var))
          }
        }
        else if(criterion_choose == "cp"){
          writeLines(paste0("Minimum CP Found on this Batch: ", current_cp))
          if(initial_cp > current_cp){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with CP: ",initial_cp))
            print(initial_var)
            initial_var <- current_cp_var
            initial_cp <- current_cp
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with CP: ",initial_cp))
            print(initial_var)
            writeLines("Found Better CP at the End of Batch")
            return(list(initial_cp, initial_var))
          }else{
            writeLines("Not Found better CP at the End of Batch")
            return(list(initial_cp, initial_var))
          }
        }
        else if(criterion_choose == "bic"){
          writeLines(paste0("Minimum BIC Found on this Batch: ", current_bic))
          if(initial_bic > current_bic){
            writeLines("Add new Record to Initial Variables")
            writeLines(paste0("Initial Variable Before length: ", length(initial_var), " with BIC: ",initial_bic))
            print(initial_var)
            initial_var <- current_bic_var
            initial_bic <- current_bic
            writeLines(paste0("Initial Variable After length: ", length(initial_var), " with BIC: ",initial_bic))
            print(initial_var)
            writeLines("Found Better BIC at the End of Batch")
            return(list(initial_bic, initial_var))
          }else{
            writeLines("Not Found Better BIC at the End of Batch")
            return(list(initial_bic, initial_var))
          }
        }
      }
    }
  }
}

calculate_nvar_select_combination <- function(n_var, limit_n_combination=NA){
  options(scipen=99)
  calc_combination <- function(n,r){
    return(factorial(n) / (factorial(r) * factorial(n-r)))
  }
  combination_vec <- c()
  if(is.na(limit_n_combination)){
    for(a in 1:n_var){
      combination_vec <- c(combination_vec, calc_combination(n_var, a))
      names(combination_vec)[a] <- paste0("var_combination_",a)
    }
    writeLines(paste0("Combination by ", n_var, " Total Variable Resulting a possibility of ", 
                      format(sum(combination_vec), big.mark=","), " Combination"))
    return(combination_vec)
  }else{
    for(a in 1:limit_n_combination){
      combination_vec <- c(combination_vec, calc_combination(n_var, a))
      names(combination_vec)[a] <- paste0("var_combination_",a)
    }
    writeLines(paste0("Combination by ", n_var, " Total Variable, Limited by ",limit_n_combination,
                      " Streak Combination, Resulting a possibility of ", format(sum(combination_vec), big.mark=","), " Combination"))
    return(combination_vec)
  }
}

#check independent variable contribution
#https://www.researchgate.net/post/How_can_I_determine_the_relative_contribution_of_predictors_in_multiple_regression_models
#https://towardsdatascience.com/relative-importance-analysis-a-better-way-to-communicate-multiple-regression-results-d70a6fbbaf9c
sequential_lm_relative_contribution <- function(data, dep_var, indep_var, log=FALSE){
  indep_length <- length(indep_var)
  deviance_vec <- c()
  STD_coef_list <- list()
  PRE_vec <- c()
  for(x in 2:indep_length){
    if(x - 1 == 1){
      model1_formula <- paste0(dep_var, " ~ ", indep_var[x-1])
      model1 <- lm(as.formula(model1_formula), data = data)
      model2_formula <- paste0(dep_var, " ~ ", paste0(indep_var[1:x], collapse = " + "))
      model2 <- lm(as.formula(model2_formula), data = data)
      deviance_model1 <- deviance(model1)
      deviance_model2 <- deviance(model2)
      USTD_model1 <- na.omit(model1$coefficients)
      USTD_model2 <- na.omit(model2$coefficients)
      current_PRE <- (deviance_model2 - deviance_model1) / deviance_model2
      STD_coef_list <- append(STD_coef_list, list(sort(USTD_model1 / sum(USTD_model1, na.rm=TRUE), decreasing = TRUE)))
      STD_coef_list <- append(STD_coef_list, list(sort(USTD_model2 / sum(USTD_model2, na.rm=TRUE), decreasing = TRUE)))
      PRE_vec <- c(PRE_vec, current_PRE)
      deviance_vec <- c(deviance_vec, deviance_model1, deviance_model2)
      names(PRE_vec)[x-1] <- paste0("V",x," (", indep_var[x],")")
      names(deviance_vec) <- c("Model 1", "Model 2")
      names(STD_coef_list) <- c("Model 1", "Model 2")
      if(log){
        writeLines(paste0("Model 1 Formula Deviance: ",deviance_model1))
        print(model1_formula)
        writeLines(paste0("Model 2 Formula Deviance: ",deviance_model2))
        print(model2_formula)
        writeLines(paste0("PRE ",x, "(",indep_var[x],"): ",current_PRE))
      }
    }else{
      #writeLines(paste0("Adding Variable ", indep_var[x]))
      model1_formula <- paste0(dep_var, " ~ ", paste0(indep_var[1:(x-1)], collapse = " + "))
      model1 <- lm(as.formula(model1_formula), data = data)
      model2_formula <- paste0(dep_var, " ~ ", paste0(indep_var[1:x], collapse = " + "))
      model2 <- lm(as.formula(model2_formula), data = data)
      deviance_model1 <- deviance(model1)
      deviance_model2 <- deviance(model2)
      USTD_model1 <- na.omit(model1$coefficients)
      USTD_model2 <- na.omit(model2$coefficients)
      current_PRE <- (deviance_model2 - deviance_model1) / deviance_model2
      PRE_vec <- c(PRE_vec, current_PRE)
      deviance_vec <- c(deviance_vec, deviance_model2)
      STD_coef_list <- append(STD_coef_list, list(sort(USTD_model2 / sum(USTD_model2, na.rm=TRUE), decreasing = TRUE)))
      names(PRE_vec)[x-1] <- paste0("V",x," (", indep_var[x],")")
      names(deviance_vec)[x] <- paste0("Model ",x) 
      names(STD_coef_list)[x] <- paste0("Model ",x) 
      if(log){
        writeLines(paste0("Model 1 Formula Deviance: ",deviance_model1))
        print(model1_formula)
        writeLines(paste0("Model 2 Formula Deviance: ",deviance_model2))
        print(model2_formula)
        writeLines(paste0("PRE ",x, "(",indep_var[x],"): ",current_PRE))
      }
    }
  }
  return(list(deviance_record = deviance_vec, 
              relative_contribution = PRE_vec, 
              standardized_coef = STD_coef_list))
}

tree_bucket_comparison <- function(df, myformula, bucket_vec = c()){
  all_comparison <- list()
  library(rpart)
  library(rpart.plot)
  library(party)
  library(PPtree)
  library(PPtreeViz)
  
  tree_performance <- function(df, dependent_col, tree_model, 
                               font_size=0.4, detailed=FALSE){
    x11()
    library(rpart)
    library(rpart.plot)
    library(party)
    library(PPtree)
    library(PPtreeViz)
    prp(tree_model, faclen = 0, cex = font_size, extra = 1, box.palette="auto")
    metrics_info <- caret::confusionMatrix(predict(tree_model,type="class"), df[,dependent_col], mode = "everything")
    metrics_info$byClass <- round(metrics_info$byClass, 4) * 100
    metrics_info$overall <- round(metrics_info$overall, 4) * 100
    if(detailed){
      print(metrics_info)
    }else{
      writeLines("Confusion Matrix")
      print(metrics_info$table)
      writeLines("Classification Metrics")
      print(metrics_info$byClass)
    }
    
    return(metrics_info)
  }
  
  for(g in 1:length(bucket_vec)){
    tree_in_comparison <- rpart(as.formula(myformula), data = df,
                                control = rpart.control(cp = 0.00001, minbucket = bucket_vec[g]))
    dependent_col <- unlist(strsplit(myformula, " ~ "))[1]
    writeLines("")
    writeLines("---------------------------------------------------------------------------------------")
    writeLines(paste0("------------------------------------- Bucket ",g," ----------------------------------------"))
    writeLines("---------------------------------------------------------------------------------------")
    writeLines("")
    compare_tree <- tree_performance(df, dependent_col, tree_in_comparison, detailed=FALSE)
    all_comparison <- append(all_comparison, list(compare_tree))
  }
  return(all_comparison)
}

#--------------------------------------------------------------------------------------------------------------------------
################## 37) Maximum Likelihood Estimation for various distributions ############################################
#--------------------------------------------------------------------------------------------------------------------------

#----------------------------- Continous Distribution ----------------------------------------#
#ranges: near zero - Inf both mean and sd
normal_dist_ranges <- function(mean, sd, print_simplify=F){
  norm_ranges <- qnorm(seq(0.001,1,0.001),mean,sd)
  names(norm_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    norm_ranges <- norm_ranges[which(norm_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of normal distribution: ",min(norm_ranges), " - ", max(norm_ranges)))
  }
  return(norm_ranges)
}

find_mle_norm <- function(data_vec){
  library(bbmle)
  mle_norm <- mle2(x~dnorm(mean=mu,sd=sd),start=list(mu=0.1,sd=0.1),data=data.frame(data_vec))
  return(mle_norm)
}

#ranges: near zero - Inf, mean of 1 is equal to 10 of normal mean (maybe)
lognormal_dist_ranges <- function(meanlog, sdlog, print_simplify=F){
  lnorm_ranges <- qlnorm(seq(0.001,1,0.001),meanlog,sdlog)
  names(lnorm_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    lnorm_ranges <- lnorm_ranges[which(lnorm_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of log normal distribution: ",min(lnorm_ranges), " - ", max(lnorm_ranges)))
  }
  return(lnorm_ranges)
}

find_mle_lognorm <- function(data_vec){
  library(bbmle)
  mle_lnorm <- mle2(x~dlnorm(meanlog=mu,sdlog=sd),start=list(mu=0.1,sd=0.1),data=data.frame(data_vec))
  return(mle_lnorm)
}

beta_dist_ranges <- function(shape1, shape2, print_simplify=F){
  beta_ranges <- qbeta(seq(0.001,1,0.001), shape1 = shape1, shape2 = shape2)
  names(beta_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    beta_ranges <- beta_ranges[which(beta_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of beta distribution: ",min(beta_ranges), " - ", max(beta_ranges)))
  }
  return(beta_ranges)
}

find_mle_beta <- function(data_vec){
  library(bbmle)
  mle_beta <- mle2(x~dbeta(shape1 = shape1, shape2 = shape2),
                   start=list(shape1=1, shape2=1),data=data.frame(data_vec))
  return(mle_beta)
}

exp_dist_ranges <- function(rate, print_simplify=F){
  exp_ranges <- qexp(seq(0.001,1,0.001),rate)
  names(exp_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    exp_ranges <- exp_ranges[which(exp_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of expson distribution: ",min(exp_ranges), " - ", max(exp_ranges)))
  }
  return(exp_ranges)
}

find_mle_exp <- function(data_vec){
  library(bbmle)
  mle_exp <- mle2(x~dexp(rate = rate),
                  start=list(rate = 0.1),data=data.frame(data_vec))
  return(mle_exp)
}

gamma_dist_ranges <- function(shape, print_simplify=F){
  gamma_ranges <- qgamma(seq(0.001,1,0.001), shape = shape)
  names(gamma_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    gamma_ranges <- gamma_ranges[which(gamma_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of gamma distribution: ",min(gamma_ranges), " - ", max(gamma_ranges)))
  }
  return(gamma_ranges)
}

find_mle_gamma <- function(data_vec){
  library(bbmle)
  mle_exp <- mle2(x~dgamma(scale = scale),
                  start=list(scale = 10),data=data.frame(data_vec))
  return(mle_exp)
}

weibull_dist_ranges <- function(shape, print_simplify=F){
  weibull_ranges <- qweibull(seq(0.001,1,0.001), shape = shape)
  names(weibull_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    weibull_ranges <- weibull_ranges[which(weibull_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of weibull distribution: ",min(weibull_ranges), " - ", max(weibull_ranges)))
  }
  return(weibull_ranges)
}

find_mle_weibull <- function(data_vec){
  library(bbmle)
  mle_exp <- mle2(x~dweibull(scale = scale),
                  start=list(scale = 20),data=data.frame(data_vec))
  return(mle_exp)
}

uniform_dist_ranges <- function(min, max, print_simplify=F){
  unif_ranges <- qunif(seq(0.001,1,0.001), min = min, max = max)
  names(unif_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    unif_ranges <- unif_ranges[which(unif_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of uniform distribution: ",min(unif_ranges), " - ", max(unif_ranges)))
  }
  return(unif_ranges)
}

find_mle_uniform <- function(data_vec){
  library(bbmle)
  mle_unif <- mle2(x~dunif(min = min, max = max),
                     start=list(m = 10, n = 10),data=data.frame(data_vec))
  return(mle_unif)
}

cauchy_dist_ranges <- function(location, scale, print_simplify=F){
  cauchy_ranges <- qcauchy(seq(0.001,1,0.001), location = location, scale = scale)
  names(cauchy_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    cauchy_ranges <- cauchy_ranges[which(cauchy_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of cauchy distribution: ",min(cauchy_ranges), " - ", max(cauchy_ranges)))
  }
  return(cauchy_ranges)
}

find_mle_cauchy <- function(data_vec){
  library(bbmle)
  mle_cauchy <- mle2(x~dcauchy(location = location, scale = scale),
                   start=list(location = 5, scale = 4),data=data.frame(data_vec))
  return(mle_cauchy)
}

chi_square_dist_ranges <- function(df1, df2, print_simplify=F){
  chisq_ranges <- qchisq(seq(0.001,1,0.001), df = df, ncp = ncp)
  names(chisq_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    chisq_ranges <- chisq_ranges[which(chisq_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of chisq distribution: ",min(chisq_ranges), " - ", max(chisq_ranges)))
  }
  return(chisq_ranges)
}

find_mle_chisq <- function(data_vec){
  library(bbmle)
  mle_chisq <- mle2(x~dchisq(df = df, ncp = ncp),
                     start=list(df = 20, ncp = 8),data=data.frame(data_vec))
  return(mle_chisq)
}

f_dist_ranges <- function(df1, df2, print_simplify=F){
  f_ranges <- qf(seq(0.001,1,0.001), df1 = df1, df2 = df2)
  names(f_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    f_ranges <- f_ranges[which(f_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of f distribution: ",min(f_ranges), " - ", max(f_ranges)))
  }
  return(f_ranges)
}

find_mle_f <- function(data_vec){
  library(bbmle)
  mle_f <- mle2(x~stats::df(df1 = df1, df2 = df2),
                start=list(df = 21, ncp = 12),data=data.frame(data_vec))
  return(mle_f)
}

logistic_dist_ranges <- function(location, scale, print_simplify=F){
  logis_ranges <- qlogis(seq(0.001,1,0.001), location = location, scale = scale)
  names(logis_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    logis_ranges <- logis_ranges[which(logis_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of logistic distribution: ",min(logis_ranges), " - ", max(logis_ranges)))
  }
  return(logis_ranges)
}

find_mle_logistic <- function(data_vec){
  library(bbmle)
  mle_logistic <- mle2(x~dlogis(location = location, scale = scale),
                       start=list(location = 10, scale = 5), data=data.frame(data_vec))
  return(mle_logistic)
}

#------------------------------- Discrete Distribution -----------------------------------#
poisson_dist_ranges <- function(lambda, print_simplify=F){
  pois_ranges <- qpois(seq(0.001,1,0.001),lambda)
  names(pois_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    pois_ranges <- pois_ranges[which(pois_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of poisson distribution: ",min(pois_ranges), " - ", max(pois_ranges)))
  }
  return(pois_ranges)
}

find_mle_pois <- function(data_vec){
  library(bbmle)
  mle_pois <- mle2(x~dpois(lambda=lambda),start=list(lambda=0.1),data=data.frame(data_vec))
  return(mle_pois)
}

geometric_dist_ranges <- function(probs, print_simplify=F){
  geom_ranges <- qgeom(seq(0.001,1,0.001),probs)
  names(geom_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    geom_ranges <- geom_ranges[which(geom_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of geometic distribution: ",min(geom_ranges), " - ", max(geom_ranges)))
  }
  return(geom_ranges)
}

find_mle_geom <- function(data_vec){
  library(bbmle)
  mle_geom <- mle2(x~dgeom(prob=prob),start=list(prob=0.01),data=data.frame(data_vec))
  return(mle_geom)
}

binom_dist_ranges <- function(probs, size_vector=c(), print_simplify=F){
  if(length(size_vector) == 1){
    binom_ranges <- c()
    binom_ranges <- qbinom(seq(0.001,1,0.001), size=size_vector, prob = probs)
    names(binom_ranges) <- paste0("Q",seq(0.1,100,0.1))
    if(print_simplify){
      binom_ranges <- binom_ranges[which(binom_ranges != Inf)]
      writeLines(paste0("Possible Ranges of a given parameter of binomial distribution: ",min(binom_ranges), " - ", max(binom_ranges)))
    }
    return(binom_ranges)
  }else{
    binom_ranges <- list()
    quantile_ranges <- seq(0.001,1,0.001)
    for(v in 1:length(quantile_ranges)){
      binom_ranges <- append(binom_ranges, list(qbinom(quantile_ranges[v], size=size_vector, prob = probs)))
      names(binom_ranges) <- paste0("Q",quantile_ranges[v] * 100)
    }
    if(print_simplify){
      binom_ranges <- lapply(binom_ranges, FUN = function(x) x[which(x != Inf)])
      min_binom <- min(unlist(lapply(binom_ranges, FUN = function(x) min(x))))
      max_binom <- max(unlist(lapply(binom_ranges, FUN = function(x) max(x))))
      writeLines(paste0("Possible Ranges of a given parameter of binomial distribution: ",min_binom, " - ", max_binom))
    }
    return(binom_ranges)
  }
}

find_mle_binom <- function(data_vec){
  library(bbmle)
  mle_binom <- mle2(x~dbinom(size=size, prob=prob),
                   start=list(size=1, prob=0.1),data=data.frame(data_vec))
  return(mle_binom)
}

negative_binom_dist_ranges <- function(probs, size_vector=c(), print_simplify=F){
  if(length(size_vector) == 1){
    nbinom_ranges <- c()
    nbinom_ranges <- qnbinom(seq(0.001,1,0.001), size=size_vector, prob = probs)
    names(nbinom_ranges) <- paste0("Q",seq(0.1,100,0.1))
    if(print_simplify){
      nbinom_ranges <- nbinom_ranges[which(nbinom_ranges != Inf)]
      writeLines(paste0("Possible Ranges of a given parameter of negative binomial distribution: ",min(nbinom_ranges), " - ", max(nbinom_ranges)))
    }
    return(nbinom_ranges)
  }else{
    nbinom_ranges <- list()
    quantile_ranges <- seq(0.001,1,0.001)
    for(v in 1:length(quantile_ranges)){
      nbinom_ranges <- append(nbinom_ranges, list(qnbinom(quantile_ranges[v], size=size_vector, prob = probs)))
      names(nbinom_ranges) <- paste0("Q",quantile_ranges[v] * 100)
    }
    if(print_simplify){
      nbinom_ranges <- lapply(nbinom_ranges, FUN = function(x) x[which(x != Inf)])
      min_nbinom <- min(unlist(lapply(nbinom_ranges, FUN = function(x) min(x))))
      max_nbinom <- max(unlist(lapply(nbinom_ranges, FUN = function(x) max(x))))
      writeLines(paste0("Possible Ranges of a given parameter of negative binomial distribution: ",min_nbinom, " - ", max_nbinom))
    }
    return(nbinom_ranges)
  }
}

find_mle_neg_binom <- function(data_vec){
  library(bbmle)
  mle_nbinom <- mle2(x~dnbinom(size=size, prob=prob),
                    start=list(size=1, prob=0.1),data=data.frame(data_vec))
  return(mle_nbinom)
}

hypergeometric_dist_ranges <- function(m, n, k, print_simplify=F){
  if(k > (m+n)){
    stop("k cannot be higher than m+n")
  }
  hypergeom_ranges <- qhyper(seq(0.001,1,0.001), m = m, n = n, k = k)
  names(hypergeom_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    hypergeom_ranges <- hypergeom_ranges[which(hypergeom_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of hypergeom distribution: ",min(hypergeom_ranges), " - ", max(hypergeom_ranges)))
  }
  return(hypergeom_ranges)
}

find_mle_hypergeometric <- function(data_vec){
  library(bbmle)
  mle_exp <- mle2(x~dhyper(m = m, n = n, k = k),
                  start=list(m = 10, n = 10, k=15),data=data.frame(data_vec))
  return(mle_exp)
}

wilcoxon_dist_ranges <- function(m, n, print_simplify=F){
  wilcox_ranges <- qwilcox(seq(0.001,1,0.001), m = m, n = n)
  names(wilcox_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    wilcox_ranges <- wilcox_ranges[which(wilcox_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of wilcoxon rank sum statistic distribution: ",min(wilcox_ranges), " - ", max(wilcox_ranges)))
  }
  return(wilcox_ranges)
}

find_mle_wilcoxon <- function(data_vec){
  library(bbmle)
  mle_wilcox <- mle2(x~dwilcox(m = m, n = n),
                  start=list(m = 10, n = 10),data=data.frame(data_vec))
  return(mle_wilcox)
}

signrank_dist_ranges <- function(n, print_simplify=F){
  signrank_ranges <- qsignrank(seq(0.001,1,0.001), n = n)
  names(signrank_ranges) <- paste0("Q",seq(0.1,100,0.1))
  if(print_simplify){
    signrank_ranges <- signrank_ranges[which(signrank_ranges != Inf)]
    writeLines(paste0("Possible Ranges of a given parameter of wilcoxon signrank distribution: ",min(signrank_ranges), " - ", max(signrank_ranges)))
  }
  return(signrank_ranges)
}

find_mle_signrank <- function(data_vec){
  library(bbmle)
  mle_signrank <- mle2(x~dsignrank(n = n),
                    start=list(n = 10), data=data.frame(data_vec))
  return(mle_signrank)
}

#--------------------------------------------------------------------------------------------------------------------------
################################### 38) ZigZag Computing with Economic/Forex Data #########################################
#--------------------------------------------------------------------------------------------------------------------------

#default and custom difference: default using automated quantile gap based by maximum variances, custom can make quantile gap flexible
zigzag_quantile_insights <- function(asset_tf, limit_zz_output = 3.5, 
                                     asset_name_display = "", display_process=F){
  
  library(TTR)
  library(lubridate)
  library(dplyr)
  library(stringr)
  
  local_minima_maxima_extraction <- function(df, time_col="", value_col="", visualize=FALSE){
    inflect <- function(x, threshold = 1){
      up   <- sapply(1:threshold, function(n) c(x[-(seq(n))], rep(NA, n)))
      down <-  sapply(-1:-threshold, function(n) c(rep(NA,abs(n)), x[-seq(length(x), length(x) - abs(n) + 1)]))
      a    <- cbind(x,up,down)
      list(minima = which(apply(a, 1, min) == a[,1]), maxima = which(apply(a, 1, max) == a[,1]))
    }
    library(dplyr)
    
    n <- 2
    data_to_check <- df[[value_col]]
    time_to_check <- df[[time_col]]
    bottoms <- lapply(1:n, function(x) inflect(data_to_check, threshold = x)$minima)
    tops <- lapply(1:n, function(x) inflect(data_to_check, threshold = x)$maxima)
    
    if(visualize){
      x11()
      # Color functions
      cf.1 <- grDevices::colorRampPalette(c("pink","red"))
      cf.2 <- grDevices::colorRampPalette(c("cyan","blue"))
      plot(data_to_check, type = 'l', main = "Minima & Maxima\nVariable Thresholds")
      for(i in 1:n){
        points(bottoms[[i]], data_to_check[bottoms[[i]]], pch = 16, col = cf.1(n)[i], cex = i/1.5)
      }
      for(i in 1:n){
        points(tops[[i]], data_to_check[tops[[i]]], pch = 16, col = cf.2(n)[i], cex = i/1.5)
      }
      legend("topleft", legend = c("Minima",1:n,"Maxima",1:n), 
             pch = rep(c(NA, rep(16,n)), 2), col = c(1, cf.1(n),1, cf.2(n)), 
             pt.cex =  c(rep(c(1, c(1:n) / 1.5), 2)), cex = .75, ncol = 2)
    }
    
    extract_bottom <- bottoms[[1]]
    extract_top <- tops[[1]]
    
    if(extract_bottom[1] < extract_top[1]) extract_top <- c(1, extract_top)
    else if(extract_bottom[1] > extract_top[1]) extract_bottom <- c(1, extract_bottom)
    
    df_bottom <- data.frame(idx = extract_bottom, sign="bottom", 
                            val = df[extract_bottom,][[value_col]],
                            dates = df[extract_bottom,][[time_col]])
    df_top <- data.frame(idx = extract_top, sign="bottom", 
                         val = df[extract_top,][[value_col]],
                         dates = df[extract_top,][[time_col]])
    all_df <- rbind(df_bottom, df_top)
    all_df <- all_df %>% arrange(idx)
    
    from_idx <- all_df[1:(nrow(all_df)-1),]$idx
    to_idx <- all_df[2:nrow(all_df),]$idx
    from_date <- all_df[1:(nrow(all_df)-1),]$dates
    to_date <- all_df[2:nrow(all_df),]$dates
    from_value <- all_df[1:(nrow(all_df)-1),]$val
    to_value <- all_df[2:nrow(all_df),]$val
    diff_value <- to_value - from_value
    diff_day <- as.numeric(difftime(to_date, from_date, units = "days"))
    local_minima_maxima_df <- NULL
    local_minima_maxima_df <- data.frame(from = from_value, from_date = from_date,
                                         to = to_value, to_date = to_date,
                                         value_diff = diff_value, day_diff = diff_day)
    local_minima_maxima_df$trend <- ifelse(local_minima_maxima_df$value_diff >= 0, 
                                           paste0("Bullish for ",local_minima_maxima_df$day_diff," Days"), 
                                           paste0("Bearish for ",local_minima_maxima_df$day_diff," Days"))
    local_minima_maxima_df$growth_lev <- round(local_minima_maxima_df$value_diff / local_minima_maxima_df$day_diff,3)
    return(local_minima_maxima_df)
  }
  
  if(display_process) writeLines("----------------- 1) Create ZigZag data by local minima maxima from imported data ---------------------------------")
  asset_tf$zz10 <- ZigZag(asset_tf[,c("High", "Low")], change=10)
  asset_tf$zz5 <- ZigZag(asset_tf[,c("High", "Low")], change=5)
  asset_tf$zz3.5 <- ZigZag(asset_tf[,c("High", "Low")], change=3.5)
  asset_tf$zz2 <- ZigZag(asset_tf[,c("High", "Low")], change=2)
  asset_tf$zz1 <- ZigZag(asset_tf[,c("High", "Low")], change=1)
  asset_LMM10 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz10", visualize=FALSE)
  asset_LMM5 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz5", visualize=FALSE)
  asset_LMM3_5 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz3.5", visualize=FALSE)
  asset_LMM2 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz2", visualize=FALSE)
  asset_LMM1 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz1", visualize=FALSE)
  
  if(display_process) writeLines("----------------- 2) Create Point From and Point to by the numerical ZigZag data ----------------------------------")
  zz1_df <- asset_LMM1[,-c(7,8)]
  colnames(zz1_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz1_df$datetime_change <- zz1_df$day_change * 24 #Hours
  zz1_df$day_string <- paste0(floor(zz1_df$day_change), " Days ", round((zz1_df$day_change - floor(zz1_df$day_change)) * 24), "Hours")
  
  zz2_df <- asset_LMM2[,-c(7,8)]
  colnames(zz2_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz2_df$datetime_change <- zz2_df$day_change * 24 #Hours
  zz2_df$day_string <- paste0(floor(zz2_df$day_change), " Days ", round((zz2_df$day_change - floor(zz2_df$day_change)) * 24), "Hours")
  
  zz3_5_df <- asset_LMM3_5[,-c(7,8)]
  colnames(zz3_5_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz3_5_df$datetime_change <- zz3_5_df$day_change * 24 #Hours
  zz3_5_df$day_string <- paste0(floor(zz3_5_df$day_change), " Days ", round((zz3_5_df$day_change - floor(zz3_5_df$day_change)) * 24), "Hours")
  
  zz5_df <- asset_LMM5[,-c(7,8)]
  colnames(zz5_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz5_df$datetime_change <- zz5_df$day_change * 24 #Hours
  zz5_df$day_string <- paste0(floor(zz5_df$day_change), " Days ", round((zz5_df$day_change - floor(zz5_df$day_change)) * 24), "Hours")
  
  zz10_df <- asset_LMM10[,-c(7,8)]
  colnames(zz10_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz10_df$datetime_change <- zz10_df$day_change * 24 #Hours
  zz10_df$day_string <- paste0(floor(zz10_df$day_change), " Days ", round((zz10_df$day_change - floor(zz10_df$day_change)) * 24), "Hours")
  
  if(display_process) writeLines("----------------- 3) Find Quantile Ranges from ZigZag Point data which has the most high variances -----------------")
  quantile_distribution <- function(value_vec, quantile_gap=0.1, 
                                    use_custom=F, custom_quantile=c(), 
                                    asset_name = ""){
    library(dplyr)
    rounding_number <- 0
    if(asset_name == "XAUUSD" || grepl("JPY",asset_name)){
      rounding_number <- 2  
    }else{
      rounding_number <- 5
    }
     
    if(use_custom){
      quantile_value <- quantile(abs(value_vec), custom_quantile)
    }else{
      quantile_set <- seq(0, 1, quantile_gap)
      quantile_value <- quantile(abs(value_vec), quantile_set)
    }
    
    quantile_ranges <- c()
    quantile_dist <- c()
    for(b in 1:(length(quantile_value) - 1)){
      quantile_ranges <- c(quantile_ranges, paste0(round(quantile_value[b],rounding_number), "-",round(quantile_value[b+1],rounding_number)))
      quantile_dist <- c(quantile_dist, length(value_vec[which(value_vec >= quantile_value[b] & value_vec <= quantile_value[b+1])]))
      names(quantile_dist)[length(quantile_dist)] <- paste0(names(quantile_value[b]),"-",names(quantile_value[b+1]))
    }
    quantile_df <- data.frame(quantile_pct = names(quantile_dist),
                              quantile_value = quantile_ranges,
                              quantile_dist = as.numeric(quantile_dist))
    quantile_df <- quantile_df %>% arrange(desc(quantile_dist))
    return(quantile_df)
  }
  quantile_find_minmax_std_deviance <- function(value_vec, 
                                                try_gap = c(0.33/seq(1,10,1),
                                                            0.25/seq(1,10,1),
                                                            0.2/seq(1,10,1),
                                                            0.1428/seq(1,10,1),
                                                            0.1111/seq(1,10,1)),
                                                priority = "max",
                                                asset_name = ""){
    rounding_number <- 0
    if(asset_name == "XAUUSD" || grepl("JPY",asset_name)){
      rounding_number <- 2  
    }else{
      rounding_number <- 5
    }
    try_gap <- unique(try_gap)
    try_gap <- sort(try_gap, decreasing = T)
    std_value <- c()
    quantile_distribution <- function(value_vec, quantile_gap=0.1, 
                                      use_custom=F, custom_quantile=c()){
      library(dplyr)
      if(use_custom){
        quantile_value <- quantile(abs(value_vec), custom_quantile)
      }else{
        quantile_set <- seq(0, 1, quantile_gap)
        quantile_value <- quantile(abs(value_vec), quantile_set)
      }
      
      quantile_ranges <- c()
      quantile_dist <- c()
      for(b in 1:(length(quantile_value) - 1)){
        quantile_ranges <- c(quantile_ranges, paste0(round(quantile_value[b],rounding_number), "-",round(quantile_value[b+1],rounding_number)))
        quantile_dist <- c(quantile_dist, length(value_vec[which(value_vec >= quantile_value[b] & value_vec <= quantile_value[b+1])]))
        names(quantile_dist)[length(quantile_dist)] <- paste0(names(quantile_value[b]),"-",names(quantile_value[b+1]))
      }
      quantile_df <- data.frame(quantile_pct = names(quantile_dist),
                                quantile_value = quantile_ranges,
                                quantile_dist = as.numeric(quantile_dist))
      quantile_df <- quantile_df %>% arrange(desc(quantile_dist))
      return(quantile_df)
    }
    for(a in 1:length(try_gap)){
      x <- quantile_distribution(value_vec, try_gap[a])
      std_value <- c(std_value, sqrt(var(x$quantile_dist)))
    }
    
    quantile_dev_df <- data.frame(try_gap, std_value)
    
    if(priority == "max"){
      quantile_dev_df <- quantile_dev_df %>% arrange(desc(std_value))
    }else if(priority == "min"){
      quantile_dev_df <- quantile_dev_df %>% arrange(std_value)
    }
    return(quantile_dev_df)
  }
  
  max_std_value_quantile10 <- quantile_find_minmax_std_deviance(zz10_df$value_change, priority = "max", asset_name = asset_name_display)
  max_std_value_quantile5 <- quantile_find_minmax_std_deviance(zz5_df$value_change, priority = "max", asset_name = asset_name_display)
  max_std_value_quantile3_5 <- quantile_find_minmax_std_deviance(zz3_5_df$value_change, priority = "max", asset_name = asset_name_display)
  max_std_value_quantile2 <- quantile_find_minmax_std_deviance(zz2_df$value_change, priority = "max", asset_name = asset_name_display)
  max_std_value_quantile1 <- quantile_find_minmax_std_deviance(zz1_df$value_change, priority = "max", asset_name = asset_name_display)
  max_std_time_quantile10 <- quantile_find_minmax_std_deviance(zz10_df$datetime_change, priority = "max", asset_name = asset_name_display)
  max_std_time_quantile5 <- quantile_find_minmax_std_deviance(zz5_df$datetime_change, priority = "max", asset_name = asset_name_display)
  max_std_time_quantile3_5 <- quantile_find_minmax_std_deviance(zz3_5_df$datetime_change, priority = "max", asset_name = asset_name_display)
  max_std_time_quantile2 <- quantile_find_minmax_std_deviance(zz2_df$datetime_change, priority = "max", asset_name = asset_name_display)
  max_std_time_quantile1 <- quantile_find_minmax_std_deviance(zz1_df$datetime_change, priority = "max", asset_name = asset_name_display)
  
  Value_QDist1 <- quantile_distribution(zz10_df$value_change, max_std_value_quantile10$try_gap[1], asset_name = asset_name_display)
  Value_QDist2 <- quantile_distribution(zz5_df$value_change, max_std_value_quantile5$try_gap[1], asset_name = asset_name_display)
  Value_QDist3 <- quantile_distribution(zz3_5_df$value_change, max_std_value_quantile3_5$try_gap[1], asset_name = asset_name_display)
  Value_QDist4 <- quantile_distribution(zz2_df$value_change, max_std_value_quantile2$try_gap[1], asset_name = asset_name_display)
  Value_QDist5 <- quantile_distribution(zz1_df$value_change, max_std_value_quantile1$try_gap[1], asset_name = asset_name_display)
  Time_QDist1 <- quantile_distribution(zz10_df$datetime_change, max_std_time_quantile10$try_gap[1], asset_name = asset_name_display)
  Time_QDist2 <- quantile_distribution(zz5_df$datetime_change, max_std_time_quantile5$try_gap[1], asset_name = asset_name_display)
  Time_QDist3 <- quantile_distribution(zz3_5_df$datetime_change, max_std_time_quantile3_5$try_gap[1], asset_name = asset_name_display)
  Time_QDist4 <- quantile_distribution(zz2_df$datetime_change, max_std_time_quantile2$try_gap[1], asset_name = asset_name_display)
  Time_QDist5 <- quantile_distribution(zz1_df$datetime_change, max_std_time_quantile1$try_gap[1], asset_name = asset_name_display)
  
  if(display_process) writeLines("----------------- 4) Make Combination of Value and Time Quantile Ranges to find Max ZigZag Occurences -------------")
  zz10_combn_quantile <- expand.grid(Value_QDist1$quantile_value, Time_QDist1$quantile_value)
  colnames(zz10_combn_quantile) <- c("ZZ10_value_change","ZZ10_datetime_change")
  zz5_combn_quantile <- expand.grid(Value_QDist2$quantile_value, Time_QDist2$quantile_value)
  colnames(zz5_combn_quantile) <- c("ZZ5_value_change","ZZ5_datetime_change")
  zz3_5_combn_quantile <- expand.grid(Value_QDist3$quantile_value, Time_QDist3$quantile_value)
  colnames(zz3_5_combn_quantile) <- c("ZZ3_5_value_change","ZZ3_5_datetime_change")
  zz2_combn_quantile <- expand.grid(Value_QDist4$quantile_value, Time_QDist4$quantile_value)
  colnames(zz2_combn_quantile) <- c("ZZ2_value_change","ZZ2_datetime_change")
  zz1_combn_quantile <- expand.grid(Value_QDist5$quantile_value, Time_QDist5$quantile_value)
  colnames(zz1_combn_quantile) <- c("ZZ1_value_change","ZZ1_datetime_change")
  
  if(display_process) writeLines("------------------ 5) Add Value and Time Ranges data to ZigZag Point From and To dataframes --------------------------")
  
  value_to_quantile_ranges <- function(df, value_col, unique_ranges=c(), asset_name=""){
    rounding_number <- 0
    if(asset_name == "XAUUSD" || grepl("JPY",asset_name)){
      rounding_number <- 2
    }else{
      rounding_number <- 5
    }
    
    from_range <- as.numeric(unlist(lapply(strsplit(unique_ranges,"-"), FUN = function(x) x[[1]])))
    to_range <- as.numeric(unlist(lapply(strsplit(unique_ranges,"-"), FUN = function(x) x[[2]])))
    which_max_range <- which(to_range == max(to_range))
    new_value_col <- paste0(value_col, "_rng")
    df[[new_value_col]] <- "-"
    for(g in 1:length(from_range)){
      df[[new_value_col]] <- ifelse(round(abs(df[[value_col]]),rounding_number) >= from_range[g] & round(abs(df[[value_col]]),rounding_number) <= to_range[g], 
                                    unique_ranges[g], df[[new_value_col]])
    }
    # watch out for any max percentile data which is not 100%, use which_max_range to fill it
    df[[new_value_col]] <- ifelse(df[[new_value_col]] == "-" & round(abs(df[[value_col]]),rounding_number) >= to_range[which_max_range], 
                                  unique_ranges[which_max_range], df[[new_value_col]])
    return(df)
  }
  zz10_format <- value_to_quantile_ranges(zz10_df, "value_change", as.character(unique(zz10_combn_quantile$ZZ10_value_change)), asset_name = asset_name_display)
  zz10_format <- value_to_quantile_ranges(zz10_format, "datetime_change", as.character(unique(zz10_combn_quantile$ZZ10_datetime_change)), asset_name = asset_name_display)
  zz5_format <- value_to_quantile_ranges(zz5_df, "value_change", as.character(unique(zz5_combn_quantile$ZZ5_value_change)), asset_name = asset_name_display)
  zz5_format <- value_to_quantile_ranges(zz5_format, "datetime_change", as.character(unique(zz5_combn_quantile$ZZ5_datetime_change)), asset_name = asset_name_display)
  zz3_5_format <- value_to_quantile_ranges(zz3_5_df, "value_change", as.character(unique(zz3_5_combn_quantile$ZZ3_5_value_change)), asset_name = asset_name_display)
  zz3_5_format <- value_to_quantile_ranges(zz3_5_format, "datetime_change", as.character(unique(zz3_5_combn_quantile$ZZ3_5_datetime_change)), asset_name = asset_name_display)
  zz2_format <- value_to_quantile_ranges(zz2_df, "value_change", as.character(unique(zz2_combn_quantile$ZZ2_value_change)), asset_name = asset_name_display)
  zz2_format <- value_to_quantile_ranges(zz2_format, "datetime_change", as.character(unique(zz2_combn_quantile$ZZ2_datetime_change)), asset_name = asset_name_display)
  zz1_format <- value_to_quantile_ranges(zz1_df, "value_change", as.character(unique(zz1_combn_quantile$ZZ1_value_change)), asset_name = asset_name_display)
  zz1_format <- value_to_quantile_ranges(zz1_format, "datetime_change", as.character(unique(zz1_combn_quantile$ZZ1_datetime_change)), asset_name = asset_name_display)
  
  if(display_process) writeLines("-------------------- 6) Summarise Count from the Value and Time Ranges data added ---------------------------------------")
  value_time_sum_zz10 <- zz10_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz5 <- zz5_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz3_5 <- zz3_5_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz2 <- zz2_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz1 <- zz1_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  
  if(display_process) writeLines("-------------------- 7) Use all the information to form the Next most likely ZigZag pattern with Quantile -----------------")
  next_zigzag_quantile <- function(zigzag_name, zigzag_from_to, 
                                   quantile_dist_value, quantile_dist_time, 
                                   value_time_combination, return_result = F, 
                                   simplify_next=T, hide_quantile_info = T,
                                   asset_name = ""){
    last_zigzag <- zigzag_from_to[nrow(zigzag_from_to),]
    current_value_change <- last_zigzag$value_change
    current_time_change <- last_zigzag$datetime_change
    writeLines("")
    quantile_dist_value <- quantile_dist_value %>% arrange(quantile_pct)
    quantile_dist_time <- quantile_dist_time %>% arrange(quantile_pct)
    from_qvalue <- as.numeric(unlist(lapply(strsplit(quantile_dist_value$quantile_value,"-"), FUN = function(x) x[[1]])))
    to_qvalue <- as.numeric(unlist(lapply(strsplit(quantile_dist_value$quantile_value,"-"), FUN = function(x) x[[2]])))
    from_qtime <- round(as.numeric(unlist(lapply(strsplit(quantile_dist_time$quantile_value,"-"), FUN = function(x) x[[1]])))) #Hours from pct value must be rounded
    to_qtime <- round(as.numeric(unlist(lapply(strsplit(quantile_dist_time$quantile_value,"-"), FUN = function(x) x[[2]])))) #Hours from pct value must be rounded
    min_qvalue <- min(from_qvalue)
    max_qvalue <- max(to_qvalue)
    min_qtime <- min(from_qtime)
    max_qtime <- max(to_qtime)
    
    quantile_dist_value <- quantile_dist_value %>%
      mutate(max_quantile_dist = max(quantile_dist))
    quantile_dist_time <- quantile_dist_time %>%
      mutate(max_quantile_dist = max(quantile_dist))
    
    library(crayon)
    bullish_zigzag <- make_style("#92ff78")
    bearish_zigzag <- make_style("#ff8457")
    rounding_number <- 0
    if(asset_name == "XAUUSD" || grepl("JPY",asset_name)){
      rounding_number <- 2
    }else{
      rounding_number <- 5
    }
    
    if(current_value_change < 0){
      cat(bearish_zigzag("------------------------------- Current ZigZag Status (",zigzag_name,") Bearish Move ------------------------"))
      writeLines("")
      print(last_zigzag)
    }else if(current_value_change >= 0){
      cat(bullish_zigzag("------------------------------- Current ZigZag Status (",zigzag_name,") Bullish Move ------------------------"))
      writeLines("")
      print(last_zigzag)
    }
    
    if(current_value_change < 0){
      current_value_change_sub <- abs(current_value_change)
      for(a in 1:length(from_qvalue)){
        if(current_value_change_sub >= from_qvalue[a] && current_value_change_sub <= to_qvalue[a]){
          if(hide_quantile_info == F){
            writeLines("")
            writeLines("Quantile Value Information")
            writeLines("")
            print(quantile_dist_value)
          }
          writeLines("")
          writeLines("Value Oriented ZigZag resides in Quantile")
          writeLines("")
          print(quantile_dist_value[a,])
          writeLines("")
          if(current_value_change_sub < to_qvalue[a]){
            diff_to <- to_qvalue[a] - current_value_change_sub
            potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to - diff_to, rounding_number))
            writeLines(paste0("Current Bearish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
          }
        }
        if(current_value_change_sub < from_qvalue[a]){
          diff_to <- to_qvalue[a] - current_value_change_sub
          potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to - diff_to, rounding_number))
          writeLines(paste0("Current Bearish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
        }
      }
    }
    else if(current_value_change >= 0){
      for(a in 1:length(to_qvalue)){
        if(current_value_change >= from_qvalue[a] && current_value_change <= to_qvalue[a]){
          if(hide_quantile_info == F){
            writeLines("")
            writeLines("Quantile Value Information")
            writeLines("")
            print(quantile_dist_value)
          }
          writeLines("")
          writeLines("Value Oriented ZigZag resides in Quantile")
          writeLines("")
          print(quantile_dist_value[a,])
          writeLines("")
          if(current_value_change < to_qvalue[a]){
            diff_to <- to_qvalue[a] - current_value_change
            potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to + diff_to,rounding_number))
            writeLines(paste0("Current Bullish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
          }
        }
        if(current_value_change < from_qvalue[a]){
          diff_to <- to_qvalue[a] - current_value_change
          potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to + diff_to,rounding_number))
          writeLines(paste0("Current Bullish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
        }
      }
    }
    library(lubridate)
    for(a in 1:length(to_qtime)){
      if(current_time_change >= from_qtime[a] && current_time_change <= to_qtime[a]){
        if(hide_quantile_info == F){
          writeLines("")
          writeLines("Quantile Time Information")
          writeLines("")
          print(quantile_dist_time)
        }
        writeLines("")
        writeLines("Time Oriented ZigZag resides in Quantile")
        writeLines("")
        print(quantile_dist_time[a,])
        writeLines("")
        if(current_time_change < to_qtime[a]){
          diff_to <- to_qtime[a] - current_time_change
          potential_to <- paste0(last_zigzag$datetime_to, " -> ",(last_zigzag$datetime_to + hours(diff_to)))
          writeLines(paste0("Current ZigZag length can potentially last at: ",potential_to, " Reference from Quantile ", quantile_dist_time$quantile_pct[a]))
        }
      }
      if(current_time_change < from_qtime[a]){
        diff_to <- to_qtime[a] - current_time_change
        potential_to <- paste0(last_zigzag$datetime_to, " -> ",(last_zigzag$datetime_to + hours(diff_to)))
        writeLines(paste0("Current ZigZag length can potentially last at: ",potential_to, " Reference from Quantile ", quantile_dist_time$quantile_pct[a]))
      }
    }
    
    writeLines("")
    if(current_value_change < 0){
      cat(bullish_zigzag("----------------------- Next ZigZag Status (",zigzag_name,") Bullish Move ------------------------"))
      writeLines("")
    }else if(current_value_change >= 0){
      cat(bearish_zigzag("----------------------- Next ZigZag Status (",zigzag_name,") Bearish Move ------------------------"))
      writeLines("")
    }
    
    new_datetime_from <- rep(last_zigzag$datetime_to, nrow(value_time_combination))
    new_value_from <- rep(last_zigzag$value_to, nrow(value_time_combination))
    new_datetime_to <- c()
    new_value_to <- c()
    new_datetime_change <- c()
    new_day_change <- c()
    new_day_string <- c()
    new_value_change <- c()
    combination_occurence <- c()
    
    for(b in 1:nrow(value_time_combination)){
      value_rng <- as.numeric(unlist(strsplit(value_time_combination$value_change_rng[b],"-")))
      datetime_rng <- round(as.numeric(unlist(strsplit(value_time_combination$datetime_change_rng[b],"-")))) #Hours from pct value must be rounded
      dt_to <- new_datetime_from[b] + hours(datetime_rng)
      val_to <- c()
      
      if(current_value_change < 0){
        val_to <- new_value_from[b] + value_rng
      }
      else if(current_value_change >= 0){
        val_to <- new_value_from[b] - value_rng
      }
      
      new_datetime_to <- c(new_datetime_to, paste0(dt_to, collapse = " - "))
      new_value_to <- c(new_value_to, paste0(val_to, collapse=" - "))
      new_datetime_change <- c(new_datetime_change, paste0(datetime_rng, " Hours", collapse = " - "))
      day_value <- round(datetime_rng/24,2)
      new_day_change <- c(new_day_change, paste0(day_value, collapse = " - "))
      new_day_string <- c(new_day_string, paste0(floor(day_value)," Day ",
                                                 round((day_value - floor(day_value)) * 24), " Hours ", collapse = " - "))
      new_value_change <- c(new_value_change, paste0(value_rng, collapse="-"))
      combination_occurence <- c(combination_occurence, value_time_combination$total_count[b])
    }
    
    new_datetime_from <- as.character(as.POSIXct(new_datetime_from, origin="1970-01-01"))
    next_zigzag_df <- as.data.frame(cbind(new_datetime_from, new_datetime_to, new_value_from, 
                                          new_value_to, new_datetime_change, new_day_change,
                                          new_day_string, new_value_change, combination_occurence))
    if(simplify_next){
      next_zigzag_df <- next_zigzag_df %>% select(new_datetime_from, new_datetime_to, new_value_from, 
                                                  new_value_to, combination_occurence)
    }
    next_zigzag_df <- next_zigzag_df %>% arrange(desc(new_datetime_to), desc(new_value_to))
    print(next_zigzag_df)
    
    if(return_result){
      return(next_zigzag_df)
    }
  }
  
  sink(file="C:/Users/User/Desktop/Trading Decision Make File/Assets ZigZag Output.txt", append = TRUE)
  writeLines("")
  writeLines("============================================================================================================")
  writeLines(paste0("=========================================== ",asset_name_display," ========================================================="))
  writeLines("============================================================================================================")
  writeLines("")
  
  if(limit_zz_output == 5){
    zz5_next <- next_zigzag_quantile("zz5",zz5_df, Value_QDist2,Time_QDist2, value_time_sum_zz5, return_result = T)
    zz3_5_next <- next_zigzag_quantile("zz3_5",zz3_5_df, Value_QDist3,Time_QDist3, value_time_sum_zz3_5, return_result = T)
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz5_FT = zz5_format, zz5_N = zz5_next, VQD5 = Value_QDist2, VQT5 = Time_QDist2, RNGSUM5 = value_time_sum_zz5, QCOMB5 = zz5_combn_quantile,
                zz3_5_FT = zz3_5_format, zz3_5_N = zz3_5_next, VQD3_5 = Value_QDist3, VQT3_5 = Time_QDist3, RNGSUM3_5 = value_time_sum_zz3_5, QCOMB3_5 = zz3_5_combn_quantile,
                zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else if(limit_zz_output == 3.5){
    zz3_5_next <- next_zigzag_quantile("zz3_5",zz3_5_df, Value_QDist3,Time_QDist3, value_time_sum_zz3_5, return_result = T)
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz3_5_FT = zz3_5_format, zz3_5_N = zz3_5_next, VQD3_5 = Value_QDist3, VQT3_5 = Time_QDist3, RNGSUM3_5 = value_time_sum_zz3_5, QCOMB3_5 = zz3_5_combn_quantile,
                zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else if(limit_zz_output == 2){
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else if(limit_zz_output == 1){
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else{
    zz10_next <- next_zigzag_quantile("zz10",zz10_df, Value_QDist1,Time_QDist1, value_time_sum_zz10, return_result = T)
    zz5_next <- next_zigzag_quantile("zz5",zz5_df, Value_QDist2,Time_QDist2, value_time_sum_zz5, return_result = T)
    zz3_5_next <- next_zigzag_quantile("zz3_5",zz3_5_df, Value_QDist3,Time_QDist3, value_time_sum_zz3_5, return_result = T)
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz10_FT = zz10_format, zz10_N = zz10_next, VQD10 = Value_QDist1, VQT10 = Time_QDist1, RNGSUM10 = value_time_sum_zz10, QCOMB10 = zz10_combn_quantile,
                zz5_FT = zz5_format, zz5_N = zz5_next, VQD5 = Value_QDist2, VQT5 = Time_QDist2, RNGSUM5 = value_time_sum_zz5, QCOMB5 = zz5_combn_quantile,
                zz3_5_FT = zz3_5_format, zz3_5_N = zz3_5_next, VQD3_5 = Value_QDist3, VQT3_5 = Time_QDist3, RNGSUM3_5 = value_time_sum_zz3_5, QCOMB3_5 = zz3_5_combn_quantile,
                zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  sink()
  closeAllConnections()
}

zigzag_quantile_custom_setting <- function(asset_tf, limit_zz_output = 3.5, quantile_gap = 0.01,
                                           asset_name_display = "", display_process=F){
  
  library(TTR)
  library(lubridate)
  library(dplyr)
  library(stringr)
  
  local_minima_maxima_extraction <- function(df, time_col="", value_col="", visualize=FALSE){
    inflect <- function(x, threshold = 1){
      up   <- sapply(1:threshold, function(n) c(x[-(seq(n))], rep(NA, n)))
      down <-  sapply(-1:-threshold, function(n) c(rep(NA,abs(n)), x[-seq(length(x), length(x) - abs(n) + 1)]))
      a    <- cbind(x,up,down)
      list(minima = which(apply(a, 1, min) == a[,1]), maxima = which(apply(a, 1, max) == a[,1]))
    }
    library(dplyr)
    
    n <- 2
    data_to_check <- df[[value_col]]
    time_to_check <- df[[time_col]]
    bottoms <- lapply(1:n, function(x) inflect(data_to_check, threshold = x)$minima)
    tops <- lapply(1:n, function(x) inflect(data_to_check, threshold = x)$maxima)
    
    if(visualize){
      x11()
      # Color functions
      cf.1 <- grDevices::colorRampPalette(c("pink","red"))
      cf.2 <- grDevices::colorRampPalette(c("cyan","blue"))
      plot(data_to_check, type = 'l', main = "Minima & Maxima\nVariable Thresholds")
      for(i in 1:n){
        points(bottoms[[i]], data_to_check[bottoms[[i]]], pch = 16, col = cf.1(n)[i], cex = i/1.5)
      }
      for(i in 1:n){
        points(tops[[i]], data_to_check[tops[[i]]], pch = 16, col = cf.2(n)[i], cex = i/1.5)
      }
      legend("topleft", legend = c("Minima",1:n,"Maxima",1:n), 
             pch = rep(c(NA, rep(16,n)), 2), col = c(1, cf.1(n),1, cf.2(n)), 
             pt.cex =  c(rep(c(1, c(1:n) / 1.5), 2)), cex = .75, ncol = 2)
    }
    
    extract_bottom <- bottoms[[1]]
    extract_top <- tops[[1]]
    
    if(extract_bottom[1] < extract_top[1]) extract_top <- c(1, extract_top)
    else if(extract_bottom[1] > extract_top[1]) extract_bottom <- c(1, extract_bottom)
    
    df_bottom <- data.frame(idx = extract_bottom, sign="bottom", 
                            val = df[extract_bottom,][[value_col]],
                            dates = df[extract_bottom,][[time_col]])
    df_top <- data.frame(idx = extract_top, sign="bottom", 
                         val = df[extract_top,][[value_col]],
                         dates = df[extract_top,][[time_col]])
    all_df <- rbind(df_bottom, df_top)
    all_df <- all_df %>% arrange(idx)
    
    from_idx <- all_df[1:(nrow(all_df)-1),]$idx
    to_idx <- all_df[2:nrow(all_df),]$idx
    from_date <- all_df[1:(nrow(all_df)-1),]$dates
    to_date <- all_df[2:nrow(all_df),]$dates
    from_value <- all_df[1:(nrow(all_df)-1),]$val
    to_value <- all_df[2:nrow(all_df),]$val
    diff_value <- to_value - from_value
    diff_day <- as.numeric(difftime(to_date, from_date, units = "days"))
    local_minima_maxima_df <- NULL
    local_minima_maxima_df <- data.frame(from = from_value, from_date = from_date,
                                         to = to_value, to_date = to_date,
                                         value_diff = diff_value, day_diff = diff_day)
    local_minima_maxima_df$trend <- ifelse(local_minima_maxima_df$value_diff >= 0, 
                                           paste0("Bullish for ",local_minima_maxima_df$day_diff," Days"), 
                                           paste0("Bearish for ",local_minima_maxima_df$day_diff," Days"))
    local_minima_maxima_df$growth_lev <- round(local_minima_maxima_df$value_diff / local_minima_maxima_df$day_diff,3)
    return(local_minima_maxima_df)
  }
  
  if(display_process) writeLines("----------------- 1) Create ZigZag data by local minima maxima from imported data ---------------------------------")
  asset_tf$zz10 <- ZigZag(asset_tf[,c("High", "Low")], change=10)
  asset_tf$zz5 <- ZigZag(asset_tf[,c("High", "Low")], change=5)
  asset_tf$zz3.5 <- ZigZag(asset_tf[,c("High", "Low")], change=3.5)
  asset_tf$zz2 <- ZigZag(asset_tf[,c("High", "Low")], change=2)
  asset_tf$zz1 <- ZigZag(asset_tf[,c("High", "Low")], change=1)
  asset_LMM10 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz10", visualize=FALSE)
  asset_LMM5 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz5", visualize=FALSE)
  asset_LMM3_5 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz3.5", visualize=FALSE)
  asset_LMM2 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz2", visualize=FALSE)
  asset_LMM1 <- local_minima_maxima_extraction(asset_tf, "Datetime", "zz1", visualize=FALSE)
  
  if(display_process) writeLines("----------------- 2) Create Point From and Point to by the numerical ZigZag data ----------------------------------")
  zz1_df <- asset_LMM1[,-c(7,8)]
  colnames(zz1_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz1_df$datetime_change <- zz1_df$day_change * 24 #Hours
  zz1_df$day_string <- paste0(floor(zz1_df$day_change), " Days ", round((zz1_df$day_change - floor(zz1_df$day_change)) * 24), "Hours")
  
  zz2_df <- asset_LMM2[,-c(7,8)]
  colnames(zz2_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz2_df$datetime_change <- zz2_df$day_change * 24 #Hours
  zz2_df$day_string <- paste0(floor(zz2_df$day_change), " Days ", round((zz2_df$day_change - floor(zz2_df$day_change)) * 24), "Hours")
  
  zz3_5_df <- asset_LMM3_5[,-c(7,8)]
  colnames(zz3_5_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz3_5_df$datetime_change <- zz3_5_df$day_change * 24 #Hours
  zz3_5_df$day_string <- paste0(floor(zz3_5_df$day_change), " Days ", round((zz3_5_df$day_change - floor(zz3_5_df$day_change)) * 24), "Hours")
  
  zz5_df <- asset_LMM5[,-c(7,8)]
  colnames(zz5_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz5_df$datetime_change <- zz5_df$day_change * 24 #Hours
  zz5_df$day_string <- paste0(floor(zz5_df$day_change), " Days ", round((zz5_df$day_change - floor(zz5_df$day_change)) * 24), "Hours")
  
  zz10_df <- asset_LMM10[,-c(7,8)]
  colnames(zz10_df) <- c("value_from","datetime_from","value_to","datetime_to","value_change","day_change")
  zz10_df$datetime_change <- zz10_df$day_change * 24 #Hours
  zz10_df$day_string <- paste0(floor(zz10_df$day_change), " Days ", round((zz10_df$day_change - floor(zz10_df$day_change)) * 24), "Hours")
  
  if(display_process) writeLines("----------------- 3) Find Quantile Ranges from ZigZag Point data which has the most high variances -----------------")
  quantile_distribution <- function(value_vec, quantile_gap=0.1, 
                                    use_custom=F, custom_quantile=c(), 
                                    asset_name = ""){
    library(dplyr)
    rounding_number <- 0
    if(asset_name == "XAUUSD" || grepl("JPY",asset_name)){
      rounding_number <- 2  
    }else{
      rounding_number <- 5
    }
    
    if(use_custom){
      quantile_value <- quantile(abs(value_vec), custom_quantile)
    }else{
      quantile_set <- seq(0, 1, quantile_gap)
      quantile_value <- quantile(abs(value_vec), quantile_set)
    }
    
    quantile_ranges <- c()
    quantile_dist <- c()
    for(b in 1:(length(quantile_value) - 1)){
      quantile_ranges <- c(quantile_ranges, paste0(round(quantile_value[b],rounding_number), "-",round(quantile_value[b+1],rounding_number)))
      quantile_dist <- c(quantile_dist, length(value_vec[which(value_vec >= quantile_value[b] & value_vec <= quantile_value[b+1])]))
      names(quantile_dist)[length(quantile_dist)] <- paste0(names(quantile_value[b]),"-",names(quantile_value[b+1]))
    }
    quantile_df <- data.frame(quantile_pct = names(quantile_dist),
                              quantile_value = quantile_ranges,
                              quantile_dist = as.numeric(quantile_dist))
    quantile_df <- quantile_df %>% arrange(desc(quantile_dist))
    return(quantile_df)
  }

  Value_QDist1 <- quantile_distribution(zz10_df$value_change, quantile_gap, asset_name = asset_name_display)
  Value_QDist2 <- quantile_distribution(zz5_df$value_change, quantile_gap, asset_name = asset_name_display)
  Value_QDist3 <- quantile_distribution(zz3_5_df$value_change, quantile_gap, asset_name = asset_name_display)
  Value_QDist4 <- quantile_distribution(zz2_df$value_change, quantile_gap, asset_name = asset_name_display)
  Value_QDist5 <- quantile_distribution(zz1_df$value_change, quantile_gap, asset_name = asset_name_display)
  Time_QDist1 <- quantile_distribution(zz10_df$datetime_change, quantile_gap, asset_name = asset_name_display)
  Time_QDist2 <- quantile_distribution(zz5_df$datetime_change, quantile_gap, asset_name = asset_name_display)
  Time_QDist3 <- quantile_distribution(zz3_5_df$datetime_change, quantile_gap, asset_name = asset_name_display)
  Time_QDist4 <- quantile_distribution(zz2_df$datetime_change, quantile_gap, asset_name = asset_name_display)
  Time_QDist5 <- quantile_distribution(zz1_df$datetime_change, quantile_gap, asset_name = asset_name_display)
  
  if(display_process) writeLines("----------------- 4) Make Combination of Value and Time Quantile Ranges to find Max ZigZag Occurences -------------")
  zz10_combn_quantile <- expand.grid(Value_QDist1$quantile_value, Time_QDist1$quantile_value)
  colnames(zz10_combn_quantile) <- c("ZZ10_value_change","ZZ10_datetime_change")
  zz5_combn_quantile <- expand.grid(Value_QDist2$quantile_value, Time_QDist2$quantile_value)
  colnames(zz5_combn_quantile) <- c("ZZ5_value_change","ZZ5_datetime_change")
  zz3_5_combn_quantile <- expand.grid(Value_QDist3$quantile_value, Time_QDist3$quantile_value)
  colnames(zz3_5_combn_quantile) <- c("ZZ3_5_value_change","ZZ3_5_datetime_change")
  zz2_combn_quantile <- expand.grid(Value_QDist4$quantile_value, Time_QDist4$quantile_value)
  colnames(zz2_combn_quantile) <- c("ZZ2_value_change","ZZ2_datetime_change")
  zz1_combn_quantile <- expand.grid(Value_QDist5$quantile_value, Time_QDist5$quantile_value)
  colnames(zz1_combn_quantile) <- c("ZZ1_value_change","ZZ1_datetime_change")
  
  if(display_process) writeLines("------------------ 5) Add Value and Time Ranges data to ZigZag Point From and To dataframes --------------------------")
  
  value_to_quantile_ranges <- function(df, value_col, unique_ranges=c(), asset_name=""){
    rounding_number <- 0
    if(asset_name == "XAUUSD" || grepl("JPY",asset_name)){
      rounding_number <- 2
    }else{
      rounding_number <- 5
    }
    
    from_range <- as.numeric(unlist(lapply(strsplit(unique_ranges,"-"), FUN = function(x) x[[1]])))
    to_range <- as.numeric(unlist(lapply(strsplit(unique_ranges,"-"), FUN = function(x) x[[2]])))
    which_max_range <- which(to_range == max(to_range))
    new_value_col <- paste0(value_col, "_rng")
    df[[new_value_col]] <- "-"
    for(g in 1:length(from_range)){
      df[[new_value_col]] <- ifelse(round(abs(df[[value_col]]),rounding_number) >= from_range[g] & round(abs(df[[value_col]]),rounding_number) <= to_range[g], 
                                    unique_ranges[g], df[[new_value_col]])
    }
    # watch out for any max percentile data which is not 100%, use which_max_range to fill it
    df[[new_value_col]] <- ifelse(df[[new_value_col]] == "-" & round(abs(df[[value_col]]),rounding_number) >= to_range[which_max_range], 
                                  unique_ranges[which_max_range], df[[new_value_col]])
    return(df)
  }
  zz10_format <- value_to_quantile_ranges(zz10_df, "value_change", as.character(unique(zz10_combn_quantile$ZZ10_value_change)), asset_name = asset_name_display)
  zz10_format <- value_to_quantile_ranges(zz10_format, "datetime_change", as.character(unique(zz10_combn_quantile$ZZ10_datetime_change)), asset_name = asset_name_display)
  zz5_format <- value_to_quantile_ranges(zz5_df, "value_change", as.character(unique(zz5_combn_quantile$ZZ5_value_change)), asset_name = asset_name_display)
  zz5_format <- value_to_quantile_ranges(zz5_format, "datetime_change", as.character(unique(zz5_combn_quantile$ZZ5_datetime_change)), asset_name = asset_name_display)
  zz3_5_format <- value_to_quantile_ranges(zz3_5_df, "value_change", as.character(unique(zz3_5_combn_quantile$ZZ3_5_value_change)), asset_name = asset_name_display)
  zz3_5_format <- value_to_quantile_ranges(zz3_5_format, "datetime_change", as.character(unique(zz3_5_combn_quantile$ZZ3_5_datetime_change)), asset_name = asset_name_display)
  zz2_format <- value_to_quantile_ranges(zz2_df, "value_change", as.character(unique(zz2_combn_quantile$ZZ2_value_change)), asset_name = asset_name_display)
  zz2_format <- value_to_quantile_ranges(zz2_format, "datetime_change", as.character(unique(zz2_combn_quantile$ZZ2_datetime_change)), asset_name = asset_name_display)
  zz1_format <- value_to_quantile_ranges(zz1_df, "value_change", as.character(unique(zz1_combn_quantile$ZZ1_value_change)), asset_name = asset_name_display)
  zz1_format <- value_to_quantile_ranges(zz1_format, "datetime_change", as.character(unique(zz1_combn_quantile$ZZ1_datetime_change)), asset_name = asset_name_display)
  
  if(display_process) writeLines("-------------------- 6) Summarise Count from the Value and Time Ranges data added ---------------------------------------")
  value_time_sum_zz10 <- zz10_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz5 <- zz5_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz3_5 <- zz3_5_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz2 <- zz2_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  value_time_sum_zz1 <- zz1_format %>% group_by(value_change_rng, datetime_change_rng) %>% 
    summarise(total_count = n()) %>% arrange(desc(total_count)) %>% as.data.frame()
  
  if(display_process) writeLines("-------------------- 7) Use all the information to form the Next most likely ZigZag pattern with Quantile -----------------")
  next_zigzag_quantile <- function(zigzag_name, zigzag_from_to, 
                                   quantile_dist_value, quantile_dist_time, 
                                   value_time_combination, return_result = F, 
                                   simplify_next=T, hide_quantile_info = T,
                                   asset_name = ""){
    last_zigzag <- zigzag_from_to[nrow(zigzag_from_to),]
    current_value_change <- last_zigzag$value_change
    current_time_change <- last_zigzag$datetime_change
    writeLines("")
    quantile_dist_value <- quantile_dist_value %>% arrange(quantile_pct)
    quantile_dist_time <- quantile_dist_time %>% arrange(quantile_pct)
    from_qvalue <- as.numeric(unlist(lapply(strsplit(quantile_dist_value$quantile_value,"-"), FUN = function(x) x[[1]])))
    to_qvalue <- as.numeric(unlist(lapply(strsplit(quantile_dist_value$quantile_value,"-"), FUN = function(x) x[[2]])))
    from_qtime <- round(as.numeric(unlist(lapply(strsplit(quantile_dist_time$quantile_value,"-"), FUN = function(x) x[[1]])))) #Hours from pct value must be rounded
    to_qtime <- round(as.numeric(unlist(lapply(strsplit(quantile_dist_time$quantile_value,"-"), FUN = function(x) x[[2]])))) #Hours from pct value must be rounded
    min_qvalue <- min(from_qvalue)
    max_qvalue <- max(to_qvalue)
    min_qtime <- min(from_qtime)
    max_qtime <- max(to_qtime)
    
    quantile_dist_value <- quantile_dist_value %>%
      mutate(max_quantile_dist = max(quantile_dist))
    quantile_dist_time <- quantile_dist_time %>%
      mutate(max_quantile_dist = max(quantile_dist))
    
    library(crayon)
    bullish_zigzag <- make_style("#92ff78")
    bearish_zigzag <- make_style("#ff8457")
    rounding_number <- 0
    if(asset_name == "XAUUSD" || grepl("JPY",asset_name)){
      rounding_number <- 2
    }else{
      rounding_number <- 5
    }
    
    if(current_value_change < 0){
      cat(bearish_zigzag("------------------------------- Current ZigZag Status (",zigzag_name,") Bearish Move ------------------------"))
      writeLines("")
      print(last_zigzag)
    }else if(current_value_change >= 0){
      cat(bullish_zigzag("------------------------------- Current ZigZag Status (",zigzag_name,") Bullish Move ------------------------"))
      writeLines("")
      print(last_zigzag)
    }
    
    if(current_value_change < 0){
      current_value_change_sub <- abs(current_value_change)
      for(a in 1:length(from_qvalue)){
        if(current_value_change_sub >= from_qvalue[a] && current_value_change_sub <= to_qvalue[a]){
          if(hide_quantile_info == F){
            writeLines("")
            writeLines("Quantile Value Information")
            writeLines("")
            print(quantile_dist_value)
          }
          writeLines("")
          writeLines("Value Oriented ZigZag resides in Quantile")
          writeLines("")
          print(quantile_dist_value[a,])
          writeLines("")
          if(current_value_change_sub < to_qvalue[a]){
            diff_to <- to_qvalue[a] - current_value_change_sub
            potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to - diff_to, rounding_number))
            writeLines(paste0("Current Bearish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
          }
        }
        if(current_value_change_sub < from_qvalue[a]){
          diff_to <- to_qvalue[a] - current_value_change_sub
          potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to - diff_to, rounding_number))
          writeLines(paste0("Current Bearish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
        }
      }
    }
    else if(current_value_change >= 0){
      for(a in 1:length(to_qvalue)){
        if(current_value_change >= from_qvalue[a] && current_value_change <= to_qvalue[a]){
          if(hide_quantile_info == F){
            writeLines("")
            writeLines("Quantile Value Information")
            writeLines("")
            print(quantile_dist_value)
          }
          writeLines("")
          writeLines("Value Oriented ZigZag resides in Quantile")
          writeLines("")
          print(quantile_dist_value[a,])
          writeLines("")
          if(current_value_change < to_qvalue[a]){
            diff_to <- to_qvalue[a] - current_value_change
            potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to + diff_to,rounding_number))
            writeLines(paste0("Current Bullish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
          }
        }
        if(current_value_change < from_qvalue[a]){
          diff_to <- to_qvalue[a] - current_value_change
          potential_to <- paste0(round(last_zigzag$value_to,rounding_number), " -> ",round(last_zigzag$value_to + diff_to,rounding_number))
          writeLines(paste0("Current Bullish ZigZag can potentially expand ", potential_to, " Reference from Quantile ", quantile_dist_value$quantile_pct[a]))
        }
      }
    }
    library(lubridate)
    for(a in 1:length(to_qtime)){
      if(current_time_change >= from_qtime[a] && current_time_change <= to_qtime[a]){
        if(hide_quantile_info == F){
          writeLines("")
          writeLines("Quantile Time Information")
          writeLines("")
          print(quantile_dist_time)
        }
        writeLines("")
        writeLines("Time Oriented ZigZag resides in Quantile")
        writeLines("")
        print(quantile_dist_time[a,])
        writeLines("")
        if(current_time_change < to_qtime[a]){
          diff_to <- to_qtime[a] - current_time_change
          potential_to <- paste0(last_zigzag$datetime_to, " -> ",(last_zigzag$datetime_to + hours(diff_to)))
          writeLines(paste0("Current ZigZag length can potentially last at: ",potential_to, " Reference from Quantile ", quantile_dist_time$quantile_pct[a]))
        }
      }
      if(current_time_change < from_qtime[a]){
        diff_to <- to_qtime[a] - current_time_change
        potential_to <- paste0(last_zigzag$datetime_to, " -> ",(last_zigzag$datetime_to + hours(diff_to)))
        writeLines(paste0("Current ZigZag length can potentially last at: ",potential_to, " Reference from Quantile ", quantile_dist_time$quantile_pct[a]))
      }
    }
    
    writeLines("")
    if(current_value_change < 0){
      cat(bullish_zigzag("----------------------- Next ZigZag Status (",zigzag_name,") Bullish Move ------------------------"))
      writeLines("")
    }else if(current_value_change >= 0){
      cat(bearish_zigzag("----------------------- Next ZigZag Status (",zigzag_name,") Bearish Move ------------------------"))
      writeLines("")
    }
    
    new_datetime_from <- rep(last_zigzag$datetime_to, nrow(value_time_combination))
    new_value_from <- rep(last_zigzag$value_to, nrow(value_time_combination))
    new_datetime_to <- c()
    new_value_to <- c()
    new_datetime_change <- c()
    new_day_change <- c()
    new_day_string <- c()
    new_value_change <- c()
    combination_occurence <- c()
    
    for(b in 1:nrow(value_time_combination)){
      value_rng <- as.numeric(unlist(strsplit(value_time_combination$value_change_rng[b],"-")))
      datetime_rng <- round(as.numeric(unlist(strsplit(value_time_combination$datetime_change_rng[b],"-")))) #Hours from pct value must be rounded
      dt_to <- new_datetime_from[b] + hours(datetime_rng)
      val_to <- c()
      
      if(current_value_change < 0){
        val_to <- new_value_from[b] + value_rng
      }
      else if(current_value_change >= 0){
        val_to <- new_value_from[b] - value_rng
      }
      
      new_datetime_to <- c(new_datetime_to, paste0(dt_to, collapse = " - "))
      new_value_to <- c(new_value_to, paste0(val_to, collapse=" - "))
      new_datetime_change <- c(new_datetime_change, paste0(datetime_rng, " Hours", collapse = " - "))
      day_value <- round(datetime_rng/24,2)
      new_day_change <- c(new_day_change, paste0(day_value, collapse = " - "))
      new_day_string <- c(new_day_string, paste0(floor(day_value)," Day ",
                                                 round((day_value - floor(day_value)) * 24), " Hours ", collapse = " - "))
      new_value_change <- c(new_value_change, paste0(value_rng, collapse="-"))
      combination_occurence <- c(combination_occurence, value_time_combination$total_count[b])
    }
    
    new_datetime_from <- as.character(as.POSIXct(new_datetime_from, origin="1970-01-01"))
    next_zigzag_df <- as.data.frame(cbind(new_datetime_from, new_datetime_to, new_value_from, 
                                          new_value_to, new_datetime_change, new_day_change,
                                          new_day_string, new_value_change, combination_occurence))
    if(simplify_next){
      next_zigzag_df <- next_zigzag_df %>% select(new_datetime_from, new_datetime_to, new_value_from, 
                                                  new_value_to, combination_occurence)
    }
    print(next_zigzag_df)
    
    if(return_result){
      return(next_zigzag_df)
    }
  }
  
  sink(file="C:/Users/User/Desktop/Trading Decision Make File/Assets ZigZag Custom Output.txt", append = TRUE)
  writeLines("")
  writeLines("============================================================================================================")
  writeLines(paste0("=========================================== ",asset_name_display," ========================================================="))
  writeLines("============================================================================================================")
  writeLines("")
  
  if(limit_zz_output == 5){
    zz5_next <- next_zigzag_quantile("zz5",zz5_df, Value_QDist2,Time_QDist2, value_time_sum_zz5, return_result = T)
    zz3_5_next <- next_zigzag_quantile("zz3_5",zz3_5_df, Value_QDist3,Time_QDist3, value_time_sum_zz3_5, return_result = T)
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz5_FT = zz5_format, zz5_N = zz5_next, VQD5 = Value_QDist2, VQT5 = Time_QDist2, RNGSUM5 = value_time_sum_zz5, QCOMB5 = zz5_combn_quantile,
                zz3_5_FT = zz3_5_format, zz3_5_N = zz3_5_next, VQD3_5 = Value_QDist3, VQT3_5 = Time_QDist3, RNGSUM3_5 = value_time_sum_zz3_5, QCOMB3_5 = zz3_5_combn_quantile,
                zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else if(limit_zz_output == 3.5){
    zz3_5_next <- next_zigzag_quantile("zz3_5",zz3_5_df, Value_QDist3,Time_QDist3, value_time_sum_zz3_5, return_result = T)
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz3_5_FT = zz3_5_format, zz3_5_N = zz3_5_next, VQD3_5 = Value_QDist3, VQT3_5 = Time_QDist3, RNGSUM3_5 = value_time_sum_zz3_5, QCOMB3_5 = zz3_5_combn_quantile,
                zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else if(limit_zz_output == 2){
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else if(limit_zz_output == 1){
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  else{
    zz10_next <- next_zigzag_quantile("zz10",zz10_df, Value_QDist1,Time_QDist1, value_time_sum_zz10, return_result = T)
    zz5_next <- next_zigzag_quantile("zz5",zz5_df, Value_QDist2,Time_QDist2, value_time_sum_zz5, return_result = T)
    zz3_5_next <- next_zigzag_quantile("zz3_5",zz3_5_df, Value_QDist3,Time_QDist3, value_time_sum_zz3_5, return_result = T)
    zz2_next <- next_zigzag_quantile("zz2",zz2_df, Value_QDist4,Time_QDist4, value_time_sum_zz2, return_result = T)
    zz1_next <- next_zigzag_quantile("zz1",zz1_df, Value_QDist5,Time_QDist5, value_time_sum_zz1, return_result = T)
    return(list(zz10_FT = zz10_format, zz10_N = zz10_next, VQD10 = Value_QDist1, VQT10 = Time_QDist1, RNGSUM10 = value_time_sum_zz10, QCOMB10 = zz10_combn_quantile,
                zz5_FT = zz5_format, zz5_N = zz5_next, VQD5 = Value_QDist2, VQT5 = Time_QDist2, RNGSUM5 = value_time_sum_zz5, QCOMB5 = zz5_combn_quantile,
                zz3_5_FT = zz3_5_format, zz3_5_N = zz3_5_next, VQD3_5 = Value_QDist3, VQT3_5 = Time_QDist3, RNGSUM3_5 = value_time_sum_zz3_5, QCOMB3_5 = zz3_5_combn_quantile,
                zz2_FT = zz2_format, zz2_N = zz2_next, VQD2 = Value_QDist4, VQT2 = Time_QDist4, RNGSUM2 = value_time_sum_zz2, QCOMB2 = zz2_combn_quantile,
                zz1_FT = zz1_format, zz1_N = zz1_next, VQD1 = Value_QDist5, VQT1 = Time_QDist5, RNGSUM1 = value_time_sum_zz1, QCOMB1 = zz1_combn_quantile))
  }
  sink()
  closeAllConnections()
}

#Use this if there is error on sink() stack to reset it back to console output
#sink()
#closeAllConnections()
summarise_next_zigzag <- function(zz_ins, asset_name="", write_possible_value = T, write_possible_datetime = F, 
  file_destination = "C:/Users/User/Desktop/Trading Decision Make File/Simplified_Assets_ZigZag.txt"){
  sink(file_destination, append=TRUE)
  all_next_zigzag <- which(grepl("_N",names(zz_ins)) == T)
  all_value_qdist <- which(grepl("_VQD", names(zz_ins)) == T)
  for(summarise in 1:length(all_next_zigzag)){
    next_zigzag_df <- zz_ins[[all_next_zigzag[summarise]]]
    new_datetime_from <- unique(next_zigzag_df$new_datetime_from)
    new_datetime_to <- unique(next_zigzag_df$new_datetime_to)
    new_value_from <- as.numeric(unique(next_zigzag_df$new_value_from))
    new_value_to <- unique(next_zigzag_df$new_value_to)
    new_value_to_split1 <- as.numeric(lapply(strsplit(new_value_to," - "), FUN = function(x) x[[1]]))
    new_value_to_split2 <- as.numeric(lapply(strsplit(new_value_to," - "), FUN = function(x) x[[2]]))
    writeLines("")
    if(new_value_from > new_value_to_split1 && new_value_from > new_value_to_split2){
      sorted_new_datetime_to <- sort(new_datetime_to, decreasing = T)
      names(sorted_new_datetime_to) <- paste0("L",seq(1,length(sorted_new_datetime_to),1))
      names_datetime_idx <- c()
      for(f in 1:length(new_datetime_to)){
        names_datetime_idx <- c(names_datetime_idx, which(sorted_new_datetime_to == new_datetime_to[f]))
      }
      sorted_new_value_to <- sort(new_value_to, decreasing = T)
      names(sorted_new_value_to) <- paste0("L",seq(1,length(sorted_new_value_to),1))
      names_value_idx <- c()
      for(f in 1:length(new_datetime_to)){
        names_value_idx <- c(names_value_idx, which(sorted_new_value_to == new_value_to[f]))
      }
      writeLines("-----------------------------------------------------------------------------------------")
      writeLines(paste0(asset_name," Next Zigzag (", names(zz_ins)[all_next_zigzag[summarise]], ") Bearish Summarise"))
      writeLines("-----------------------------------------------------------------------------------------")
      if(write_possible_value){
        writeLines("")
        writeLines("Possible Value Distribution")
        writeLines("")
        value_vector <- paste0(new_value_from, " -> ", new_value_to)
        names(value_vector) <- names(sorted_new_value_to)[names_value_idx]
        order_names <- order(names(value_vector), decreasing = T)
        value_vector <- value_vector[order_names]
        print(value_vector)
      }
      if(write_possible_datetime){
        writeLines("")
        writeLines("Possible Datetime Distribution")
        writeLines("")
        datetime_vector <- paste0(new_datetime_from, " -> ", new_datetime_to)
        names(datetime_vector) <- names(sorted_new_datetime_to)[names_datetime_idx]
        order_names <- order(names(datetime_vector), decreasing = T)
        datetime_vector <- datetime_vector[order_names]
        print(datetime_vector)
      }
    }
    else if(new_value_from < new_value_to_split1 && new_value_from < new_value_to_split2){
      sorted_new_datetime_to <- sort(new_datetime_to)
      names(sorted_new_datetime_to) <- paste0("L",seq(1,length(sorted_new_datetime_to),1))
      names_datetime_idx <- c()
      for(f in 1:length(new_datetime_to)){
        names_datetime_idx <- c(names_datetime_idx, which(sorted_new_datetime_to == new_datetime_to[f]))
      }
      sorted_new_value_to <- sort(new_value_to)
      names(sorted_new_value_to) <- paste0("L",seq(1,length(sorted_new_value_to),1))
      names_value_idx <- c()
      for(f in 1:length(new_datetime_to)){
        names_value_idx <- c(names_value_idx, which(sorted_new_value_to == new_value_to[f]))
      }
      
      writeLines("-----------------------------------------------------------------------------------------")
      writeLines(paste0(asset_name," Next Zigzag (", names(zz_ins)[all_next_zigzag[summarise]], ") Bullish Summarise"))
      writeLines("-----------------------------------------------------------------------------------------")
      if(write_possible_value){
        writeLines("")
        writeLines("Possible Value Distribution")
        writeLines("")
        value_vector <- paste0(new_value_from, " -> ", new_value_to)
        names(value_vector) <- names(sorted_new_value_to)[names_value_idx]
        order_names <- order(names(value_vector), decreasing = T)
        value_vector <- value_vector[order_names]
        print(value_vector)
      }
      if(write_possible_datetime){
        writeLines("")
        writeLines("Possible Datetime Distribution")
        writeLines("")
        datetime_vector <- paste0(new_datetime_from, " -> ", new_datetime_to)
        names(datetime_vector) <- names(sorted_new_datetime_to)[names_datetime_idx]
        order_names <- order(names(datetime_vector), decreasing = T)
        datetime_vector <- datetime_vector[order_names]
        print(datetime_vector)
      }
    }
  }
  sink()
  closeAllConnections()
}

#zz_ins_named_list <- list(xauusd = xauusd_zz_ins, audcad = audcad_zz_ins, audusd = audusd_zz_ins,
#                      audchf = audchf_zz_ins, audjpy = audjpy_zz_ins, audnzd = audnzd_zz_ins, cadchf = cadchf_zz_ins,
#                      cadjpy = cadjpy_zz_ins, chfjpy = chfjpy_zz_ins, euraud = euraud_zz_ins, eurcad = eurcad_zz_ins,
#                      eurchf = eurchf_zz_ins, eurgbp = eurgbp_zz_ins, eurjpy = eurjpy_zz_ins, eurnzd = eurnzd_zz_ins,
#                      eurusd = eurusd_zz_ins, gbpchf = gbpchf_zz_ins, gbpjpy = gbpjpy_zz_ins, gbpusd = gbpusd_zz_ins,
#                      nzdjpy = nzdjpy_zz_ins, nzdusd = nzdusd_zz_ins, usdcad = usdcad_zz_ins, usdchf = usdchf_zz_ins,
#                      usdjpy = usdjpy_zz_ins)
summarise_bull_bear_current_zigzag <- function(zz_ins_named_list, color_formatting = T){
  bull_bear_df <- NULL
  for(v in 1:length(zz_ins_named_list)){
    all_current_zigzag <- which(grepl("_FT",names(zz_ins_named_list[[v]])) == T)
    bb_vector <- c()
    for(summarise in 1:length(all_current_zigzag)){
      current_zigzag_df <- zz_ins_named_list[[v]][[all_current_zigzag[summarise]]]
      datetime_from <- current_zigzag_df$datetime_from[nrow(current_zigzag_df)]
      datetime_to <- current_zigzag_df$datetime_to[nrow(current_zigzag_df)]
      value_from <- current_zigzag_df$value_from[nrow(current_zigzag_df)]
      value_to <- current_zigzag_df$value_to[nrow(current_zigzag_df)]
      if(value_from >= value_to){
        bb_vector <- c(bb_vector, "Bearish")
      }
      else if(value_from <= value_to){
        bb_vector <- c(bb_vector, "Bullish")
      }
    }
    names(bb_vector) <- names(zz_ins_named_list[[v]])[which(grepl("_FT",names(zz_ins_named_list[[v]])) == T)]
    if(is.null(bull_bear_df)){
      bull_bear_df <- as.data.frame(cbind(asset_name_FT = names(zz_ins_named_list)[v], t(bb_vector)))
    }else{
      bull_bear_df_add <- as.data.frame(cbind(asset_name_FT = names(zz_ins_named_list)[v], t(bb_vector)))
      bull_bear_df <- rbind(bull_bear_df, bull_bear_df_add)
    }
  }
  
  if(color_formatting){
    library(vctrs)
    format.vctrs_conclusion_highlight <<- function(x,...) {
      y <- gsub("Bullish",crayon::green("Bullish"),vec_data(x))
      y <- gsub("Bearish",crayon::red("Bearish"),y)
      y <- gsub("Mixed",crayon::yellow("Mixed"),y)
      return(y)
    }
    function_conclusion_highlight <<- function(x = character()) {
      vec_assert(x, character())
      new_vctr(x, class = "vctrs_conclusion_highlight")
    }
    for(v in 2:ncol(bull_bear_df)){
      bull_bear_df[,v] <- function_conclusion_highlight(bull_bear_df[,v])
    }
    
    library(tibble)
    bull_bear_df <- as.tibble(bull_bear_df)
    
    return(bull_bear_df)
  }else{
    return(bull_bear_df)
  }
  
}
summarise_bull_bear_next_zigzag <- function(zz_ins_named_list, color_formatting = T){
  bull_bear_df <- NULL
  for(v in 1:length(zz_ins_named_list)){
    all_next_zigzag <- which(grepl("_N",names(zz_ins_named_list[[v]])) == T)
    bb_vector <- c()
    for(summarise in 1:length(all_next_zigzag)){
      next_zigzag_df <- zz_ins_named_list[[v]][[all_next_zigzag[summarise]]]
      new_datetime_from <- unique(next_zigzag_df$new_datetime_from)
      new_datetime_to <- unique(next_zigzag_df$new_datetime_to)
      new_value_from <- as.numeric(unique(next_zigzag_df$new_value_from))
      new_value_to <- unique(next_zigzag_df$new_value_to)
      new_value_to_split1 <- as.numeric(lapply(strsplit(new_value_to," - "), FUN = function(x) x[[1]]))
      new_value_to_split2 <- as.numeric(lapply(strsplit(new_value_to," - "), FUN = function(x) x[[2]]))
      if(new_value_from > new_value_to_split1 && new_value_from > new_value_to_split2){
        bb_vector <- c(bb_vector, "Bearish")
      }
      else if(new_value_from < new_value_to_split1 && new_value_from < new_value_to_split2){
        bb_vector <- c(bb_vector, "Bullish")
      }
    }
    names(bb_vector) <- names(zz_ins_named_list[[v]])[which(grepl("_N",names(zz_ins_named_list[[v]])) == T)]
    if(is.null(bull_bear_df)){
      bull_bear_df <- as.data.frame(cbind(asset_name_N = names(zz_ins_named_list)[v], t(bb_vector)))
    }else{
      bull_bear_df_add <- as.data.frame(cbind(asset_name_N = names(zz_ins_named_list)[v], t(bb_vector)))
      bull_bear_df <- rbind(bull_bear_df, bull_bear_df_add)
    }
  }
  
  if(color_formatting == T){
    library(vctrs)
    format.vctrs_conclusion_highlight <<- function(x,...) {
      y <- gsub("Bullish",crayon::green("Bullish"),vec_data(x))
      y <- gsub("Bearish",crayon::red("Bearish"),y)
      y <- gsub("Mixed",crayon::yellow("Mixed"),y)
      return(y)
    }
    function_conclusion_highlight <<- function(x = character()) {
      vec_assert(x, character())
      new_vctr(x, class = "vctrs_conclusion_highlight")
    }
    for(v in 2:ncol(bull_bear_df)){
      bull_bear_df[,v] <- function_conclusion_highlight(bull_bear_df[,v])
    }
    
    library(tibble)
    bull_bear_df <- as.tibble(bull_bear_df)
    
    return(bull_bear_df)
  }else{
    return(bull_bear_df)
  }
  
}
summarise_timereach_current_zigzag <- function(zz_ins_named_list){
  time_df <- NULL
  for(v in 1:length(zz_ins_named_list)){
    all_current_zigzag <- which(grepl("_FT",names(zz_ins_named_list[[v]])) == T)
    bb_vector <- c()
    for(summarise in 1:length(all_current_zigzag)){
      current_zigzag_df <- zz_ins_named_list[[v]][[all_current_zigzag[summarise]]]
      datetime_from <- current_zigzag_df$datetime_from[nrow(current_zigzag_df)]
      datetime_to <- current_zigzag_df$datetime_to[nrow(current_zigzag_df)]
      bb_vector <- c(bb_vector, paste0(datetime_from, " - ", datetime_to))
    }
    names(bb_vector) <- names(zz_ins_named_list[[v]])[which(grepl("_FT",names(zz_ins_named_list[[v]])) == T)]
    if(is.null(time_df)){
      time_df <- as.data.frame(cbind(asset_name_FT = names(zz_ins_named_list)[v], t(bb_vector)))
    }else{
      time_df_add <- as.data.frame(cbind(asset_name_FT = names(zz_ins_named_list)[v], t(bb_vector)))
      time_df <- rbind(time_df, time_df_add)
    }
  }
  return(time_df)
}
summarise_interconnected_next_zigzag <- function(zz_ins_named_list, stream_live_data = T){
  latest_data <- NULL
  if(stream_live_data){
    library(stringr)
    library(rvest)
    writeLines("Fetching data . . ")
    latest_xauusd <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/xau-usd") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_audcad <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/aud-cad") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_audusd <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/aud-usd") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_audchf <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/aud-chf") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_audjpy <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/aud-jpy") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_audnzd <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/aud-nzd") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_cadchf <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/cad-chf") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_cadjpy <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/cad-jpy") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_chfjpy <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/chf-jpy") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_euraud <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/eur-aud") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_eurcad <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/eur-cad") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_eurchf <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/eur-chf") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_eurgbp <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/eur-gbp") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_eurjpy <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/eur-jpy") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_eurnzd <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/eur-nzd") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_eurusd <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/eur-usd") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_gbpchf <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/gbp-chf") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_gbpjpy <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/gbp-jpy") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_gbpusd <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/gbp-usd") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_nzdjpy <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/nzd-jpy") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_nzdusd <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/nzd-usd") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_usdcad <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/usd-cad") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_usdchf <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/usd-chf") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    latest_usdjpy <- as.numeric(str_replace_all(read_html("https://www.investing.com/currencies/usd-jpy") %>% html_nodes(xpath = "//span[@data-test='instrument-price-last']") %>% html_text(), ",", ""))
    for(a in 1:length(zz_ins_named_list)){
      name_idx <- names(zz_ins_named_list)[a]
      eval_latest_data <- paste0("latest_data <- c(latest_data, latest_",name_idx,")")
      eval_name_latest_data <- paste0("names(latest_data)[length(latest_data)] <- '",name_idx,"'")
      eval(parse(text = eval_latest_data))
      eval(parse(text = eval_name_latest_data))
    }
    writeLines("Done Fetching Data!")
  }
  
  list_of_unique_value <- list()
  for(v in 1:length(zz_ins_named_list)){
    all_next_zigzag <- which(grepl("_N",names(zz_ins_named_list[[v]])) == T)
    bb_vector <- c()
    for(summarise in 1:length(all_next_zigzag)){
      next_zigzag_df <- zz_ins_named_list[[v]][[all_next_zigzag[summarise]]]
      check_from <- as.numeric(unlist(lapply(strsplit(next_zigzag_df$new_value_to, " - "), FUN = function(x) x[[1]])))
      check_to <- as.numeric(unlist(lapply(strsplit(next_zigzag_df$new_value_to, " - "), FUN = function(x) x[[2]])))
      if(check_from[1] < check_to[1]){ #Bullish rank
        value_to <- sort(unique(next_zigzag_df$new_value_to),decreasing = T)
      }else if(check_from[1] > check_to[1]){ #Bearish rank
        value_to <- sort(unique(next_zigzag_df$new_value_to))
      }
      names(value_to) <- paste0("L",seq(1,length(value_to),1))
      list_of_unique_value <- c(list_of_unique_value, list(value_to))
      names(list_of_unique_value)[length(list_of_unique_value)] <- paste0(names(zz_ins_named_list)[v], "_", names(zz_ins_named_list[[v]][all_next_zigzag[summarise]]))
    }
  }
  list_names <- names(list_of_unique_value)
  unique_zz <- paste0("zz",unique(unlist(lapply(strsplit(list_names, "zz"), FUN = function(x) x[[2]]))))
  unique_zz_backup <- unique_zz
  interconnected_df <- NULL
  
  for(x in 1:length(zz_ins_named_list)){
    asset_name <- names(zz_ins_named_list)[x]
    unique_zz <- unique_zz_backup
    initial_zz_val <- c()
    names_from_to_zz_con <- c()
    from_to_zz_con <- c()
    for(zz in 1:length(unique_zz)){
      initial_zz_val <- c(initial_zz_val, unique(zz_ins_named_list[[x]][[which(names(zz_ins_named_list[[x]]) == unique_zz[zz])]]$new_value_from))
    }
    if(stream_live_data){
      unique_zz <- c(unique_zz, "current")
      initial_zz_val <- as.numeric(initial_zz_val)
      initial_zz_val <- c(initial_zz_val, latest_data[which(names(latest_data) == asset_name)])
      names(initial_zz_val) <- unique_zz
    }else{
      initial_zz_val <- as.numeric(initial_zz_val)
      names(initial_zz_val) <- unique_zz
    }
    
    for(from_con in 1:(length(unique_zz) - 1)){
      list_focus <- which(grepl(asset_name, list_names))
      for(to_con in (from_con + 1):length(unique_zz)){
        names_from_to_zz_con <- c(names_from_to_zz_con, paste0(unique_zz[from_con], " -> ", unique_zz[to_con]))
        from_con_unique_val <- list_of_unique_value[[list_focus[from_con]]]
        to_con_value <- initial_zz_val[to_con]
        compare_from <- as.numeric(unlist(lapply(strsplit(from_con_unique_val, " - "), FUN = function(x) x[[1]])))
        compare_to <- as.numeric(unlist(lapply(strsplit(from_con_unique_val, " - "), FUN = function(x) x[[2]])))
        #print(paste0(unique_zz[from_con], " -> ", unique_zz[to_con]))
        #print(asset_name)
        #print(compare_from)
        #print(compare_to)
        #print(to_con_value)
        if(compare_from[1] > compare_to[1]){
          #bearish setup
          #print("Bearish")
          which_idx <- which(compare_from > to_con_value)
          #print(which_idx)
          #print(length(which_idx))
          if(length(which_idx) == 0){
            from_to_zz_con <- c(from_to_zz_con, paste0(to_con_value, "_L0 (BEAR)"))
          }else{
            which_idx <- which_idx[1]
            from_to_zz_con <- c(from_to_zz_con, paste0(from_con_unique_val[which_idx], " (", names(from_con_unique_val)[which_idx],"/",names(from_con_unique_val)[length(from_con_unique_val)], ") (BEAR)"))
          }
        }
        else if(compare_from[1] < compare_to[1]){
          #bullish setup
          #print("Bullish")
          which_idx <- which(compare_from < to_con_value)
          #print(which_idx)
          #print(length(which_idx))
          if(length(which_idx) == 0){
            from_to_zz_con <- c(from_to_zz_con, paste0(to_con_value, "_L0 (BULL)"))
          }else{
            which_idx <- which_idx[1]
            from_to_zz_con <- c(from_to_zz_con, paste0(from_con_unique_val[which_idx], " (", names(from_con_unique_val)[which_idx],"/",names(from_con_unique_val)[length(from_con_unique_val)], ") (BULL)"))
          }
        }
      }
    }
    names(from_to_zz_con) <- names_from_to_zz_con
    if(is.null(interconnected_df)){
      interconnected_df <- as.data.frame(cbind(asset_name=asset_name, t(initial_zz_val), t(from_to_zz_con)))
    }else{
      interconnected_df_add <- as.data.frame(cbind(asset_name=asset_name, t(initial_zz_val), t(from_to_zz_con)))
      interconnected_df <- rbind(interconnected_df, interconnected_df_add)
    }
  }
  return(interconnected_df)
}

#--------------------------------------------------------------------------------------------------------------------------
################################### 39) Pivot Point/Pivot Range Computing with Economic/Forex Data ########################
#--------------------------------------------------------------------------------------------------------------------------
pivot_point_trading <- function(price_ohlc, save_passing = TRUE, save_value = FALSE, asset_name = ""){
  backup <- price_ohlc
  price_ohlc <- price_ohlc[1:(nrow(price_ohlc) - 1),]
  
  pp <- (price_ohlc[["Close"]] + price_ohlc[["High"]] + price_ohlc[["Low"]]) / 3
  tc <- (price_ohlc[["High"]] + price_ohlc[["Low"]]) / 2
  bc <- pp - tc + pp
  
  #standard pivot rules
  std_r1 <- pp + pp - price_ohlc[["Low"]]
  std_r2 <- pp + price_ohlc[["High"]] - price_ohlc[["Low"]]
  std_r3 <- price_ohlc[["High"]] + 2 * (pp - price_ohlc[["Low"]])
  std_s1 <- pp - (price_ohlc[["High"]] - pp)
  std_s2 <- pp - (price_ohlc[["High"]] - price_ohlc[["Low"]])
  std_s3 <- price_ohlc[["Low"]] - 2 * (price_ohlc[["High"]] - pp)
  
  #fibonacci pivot_rules
  fib_vec <- c(0.236, 0.382, 0.5, 0.618, 0.786)
  fib_r1 <- pp + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[1]]
  fib_r2 <- pp + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[2]]
  fib_r3 <- pp + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[3]]
  fib_r4 <- pp + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[4]]
  fib_r5 <- pp + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[5]]
  fib_s1 <- pp - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[1]]
  fib_s2 <- pp - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[2]]
  fib_s3 <- pp - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[3]]
  fib_s4 <- pp - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[4]]
  fib_s5 <- pp - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * fib_vec[[5]]
  
  #demark pivot rules
  XData <- ifelse(price_ohlc[["Close"]] < price_ohlc[["Open"]], 
                  price_ohlc[["High"]] + (price_ohlc[["Low"]] * 2) + price_ohlc[["Close"]],
                  ifelse(price_ohlc[["Close"]] > price_ohlc[["Open"]],
                         (price_ohlc[["High"]] * 2) + price_ohlc[["Low"]] + price_ohlc[["Close"]], 
                         price_ohlc[["High"]] + price_ohlc[["Low"]] + (price_ohlc[["Close"]] * 2)))
  
  #camarilla pivot rules
  cam_vec <- c(1.0833, 1.1666, 1.25, 1.5)
  cam_r1 <- price_ohlc[["Close"]] + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[1]
  cam_r2 <- price_ohlc[["Close"]] + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[2]
  cam_r3 <- price_ohlc[["Close"]] + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[3]
  cam_r4 <- price_ohlc[["Close"]] + (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[4]
  cam_s1 <- price_ohlc[["Close"]] - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[1]
  cam_s2 <- price_ohlc[["Close"]] - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[2]
  cam_s3 <- price_ohlc[["Close"]] - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[3]
  cam_s4 <- price_ohlc[["Close"]] - (price_ohlc[["High"]] - price_ohlc[["Low"]]) * cam_vec[4]
  
  pp_demark <- (XData / 4)
  r1_demark <- (XData / 2) - price_ohlc[["Low"]]
  s1_demark <- (XData / 2) - price_ohlc[["High"]]
  
  if(asset_name == "XAUUSD"){
    asset_decimal <- 2
  }else if(grepl("JPY",asset_name)){
    asset_decimal <- 2
  }else{
    asset_decimal <- 4
  }
  
  if(save_passing){
    target_data <- backup[2:nrow(backup),]
    backup[["CPR_GAP"]] <- c(NA, round(abs(bc - pp) * 10^asset_decimal,2))
    backup[["FIB_GAP"]] <- c(NA, round((((fib_r2 - fib_r1) + (fib_r3 - fib_r2) + (fib_r4 - fib_r3) + (fib_r5 - fib_r4)) / 4) * 10^asset_decimal, 2))
    backup[["P_Std_R1"]] <- c(NA, ifelse(target_data[["High"]] >= std_r1, TRUE, FALSE))
    backup[["P_Std_R2"]] <- c(NA, ifelse(target_data[["High"]] >= std_r2, TRUE, FALSE))
    backup[["P_Std_R3"]] <- c(NA, ifelse(target_data[["High"]] >= std_r3, TRUE, FALSE))
    backup[["P_Std_S1"]] <- c(NA, ifelse(target_data[["Low"]] <= std_s1, TRUE, FALSE))
    backup[["P_Std_S2"]] <- c(NA, ifelse(target_data[["Low"]] <= std_s2, TRUE, FALSE))
    backup[["P_Std_S3"]] <- c(NA, ifelse(target_data[["Low"]] <= std_s3, TRUE, FALSE))
    backup[["P_Fib_R1"]] <- c(NA, ifelse(target_data[["High"]] >= fib_r1, TRUE, FALSE))
    backup[["P_Fib_R2"]] <- c(NA, ifelse(target_data[["High"]] >= fib_r2, TRUE, FALSE))
    backup[["P_Fib_R3"]] <- c(NA, ifelse(target_data[["High"]] >= fib_r3, TRUE, FALSE))
    backup[["P_Fib_R4"]] <- c(NA, ifelse(target_data[["High"]] >= fib_r4, TRUE, FALSE))
    backup[["P_Fib_R5"]] <- c(NA, ifelse(target_data[["High"]] >= fib_r5, TRUE, FALSE))
    backup[["P_Cam_R1"]] <- c(NA, ifelse(target_data[["High"]] >= cam_r1, TRUE, FALSE))
    backup[["P_Cam_R2"]] <- c(NA, ifelse(target_data[["High"]] >= cam_r2, TRUE, FALSE))
    backup[["P_Cam_R3"]] <- c(NA, ifelse(target_data[["High"]] >= cam_r3, TRUE, FALSE))
    backup[["P_Cam_R4"]] <- c(NA, ifelse(target_data[["High"]] >= cam_r4, TRUE, FALSE))
    backup[["P_Fib_S1"]] <- c(NA, ifelse(target_data[["Low"]] <= fib_s1, TRUE, FALSE))
    backup[["P_Fib_S2"]] <- c(NA, ifelse(target_data[["Low"]] <= fib_s2, TRUE, FALSE))
    backup[["P_Fib_S3"]] <- c(NA, ifelse(target_data[["Low"]] <= fib_s3, TRUE, FALSE))
    backup[["P_Fib_S4"]] <- c(NA, ifelse(target_data[["Low"]] <= fib_s4, TRUE, FALSE))
    backup[["P_Fib_S5"]] <- c(NA, ifelse(target_data[["Low"]] <= fib_s5, TRUE, FALSE))
    backup[["P_Cam_S1"]] <- c(NA, ifelse(target_data[["Low"]] <= cam_s1, TRUE, FALSE))
    backup[["P_Cam_S2"]] <- c(NA, ifelse(target_data[["Low"]] <= cam_s2, TRUE, FALSE))
    backup[["P_Cam_S3"]] <- c(NA, ifelse(target_data[["Low"]] <= cam_s3, TRUE, FALSE))
    backup[["P_Cam_S4"]] <- c(NA, ifelse(target_data[["Low"]] <= cam_s4, TRUE, FALSE))
    backup[["P_Demark_R1"]] <- c(NA, ifelse(target_data[["High"]] >= r1_demark, TRUE, FALSE))
    backup[["P_Demark_S1"]] <- c(NA, ifelse(target_data[["Low"]] <= s1_demark, TRUE, FALSE))
  }
  
  if(save_value){
    target_data <- backup[2:nrow(backup),]
    backup[["V_PP"]] <- c(NA, pp)
    backup[["V_TC"]] <- c(NA, tc)
    backup[["V_BC"]] <- c(NA, bc)
    backup[["V_Std_R1"]] <- c(NA, std_r1)
    backup[["V_Std_R2"]] <- c(NA, std_r2)
    backup[["V_Std_R3"]] <- c(NA, std_r3)
    backup[["V_Std_S1"]] <- c(NA, std_s1)
    backup[["V_Std_S2"]] <- c(NA, std_s2)
    backup[["V_Std_S3"]] <- c(NA, std_s3)
    backup[["V_Fib_R1"]] <- c(NA, fib_r1)
    backup[["V_Fib_R2"]] <- c(NA, fib_r2)
    backup[["V_Fib_R3"]] <- c(NA, fib_r3)
    backup[["V_Fib_R4"]] <- c(NA, fib_r4)
    backup[["V_Fib_R5"]] <- c(NA, fib_r5)
    backup[["V_Cam_R1"]] <- c(NA, cam_r1)
    backup[["V_Cam_R2"]] <- c(NA, cam_r2)
    backup[["V_Cam_R3"]] <- c(NA, cam_r3)
    backup[["V_Cam_R4"]] <- c(NA, cam_r4)
    backup[["V_Fib_S1"]] <- c(NA, fib_s1)
    backup[["V_Fib_S2"]] <- c(NA, fib_s2)
    backup[["V_Fib_S3"]] <- c(NA, fib_s3)
    backup[["V_Fib_S4"]] <- c(NA, fib_s4)
    backup[["V_Fib_S5"]] <- c(NA, fib_s5)
    backup[["V_Cam_S1"]] <- c(NA, cam_s1)
    backup[["V_Cam_S2"]] <- c(NA, cam_s2)
    backup[["V_Cam_S3"]] <- c(NA, cam_s3)
    backup[["V_Cam_S4"]] <- c(NA, cam_s4)
    backup[["V_Demark_R1"]] <- c(NA, r1_demark)
    backup[["V_Demark_S1"]] <- c(NA, s1_demark)
  }
  return(backup)
}

pivot_point_vector <- function(pivot_price, vector_type = "nearest_diff", return_possible_rs = F){
  library(dplyr)
  #print("d1")
  pivot_price <- pivot_price[2:nrow(pivot_price),]
  
  diff_oh <- abs(pivot_price[["Open"]] - pivot_price[["High"]])
  diff_ol <- abs(pivot_price[["Open"]] - pivot_price[["Low"]])
  
  diff_pp_open <- abs(pivot_price[["Open"]] - pivot_price[["V_PP"]])
  diff_tc_open <- abs(pivot_price[["Open"]] - pivot_price[["V_TC"]])
  diff_bc_open <- abs(pivot_price[["Open"]] - pivot_price[["V_BC"]])
  diff_std_r1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Std_R1"]])
  diff_std_r2_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Std_R2"]])
  diff_std_r3_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Std_R3"]])
  diff_std_s1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Std_S1"]])
  diff_std_s2_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Std_S2"]])
  diff_std_s3_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Std_S3"]])
  diff_fib_r1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_R1"]])
  diff_fib_r2_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_R2"]])
  diff_fib_r3_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_R3"]])
  diff_fib_r4_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_R4"]])
  diff_fib_r5_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_R5"]])
  diff_cam_r1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_R1"]])
  diff_cam_r2_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_R2"]])
  diff_cam_r3_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_R3"]])
  diff_cam_r4_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_R4"]])
  diff_fib_s1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_S1"]])
  diff_fib_s2_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_S2"]])
  diff_fib_s3_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_S3"]])
  diff_fib_s4_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_S4"]])
  diff_fib_s5_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Fib_S5"]])
  diff_cam_s1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_S1"]])
  diff_cam_s2_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_S2"]])
  diff_cam_s3_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_S3"]])
  diff_cam_s4_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Cam_S4"]])
  diff_demark_s1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Demark_S1"]])
  diff_demark_r1_open <- abs(pivot_price[["Open"]] - pivot_price[["V_Demark_R1"]])
  
  diff_pp_high <- abs(pivot_price[["High"]] - pivot_price[["V_PP"]])
  diff_tc_high <- abs(pivot_price[["High"]] - pivot_price[["V_TC"]])
  diff_bc_high <- abs(pivot_price[["High"]] - pivot_price[["V_BC"]])
  diff_std_r1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Std_R1"]])
  diff_std_r2_high <- abs(pivot_price[["High"]] - pivot_price[["V_Std_R2"]])
  diff_std_r3_high <- abs(pivot_price[["High"]] - pivot_price[["V_Std_R3"]])
  diff_std_s1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Std_S1"]])
  diff_std_s2_high <- abs(pivot_price[["High"]] - pivot_price[["V_Std_S2"]])
  diff_std_s3_high <- abs(pivot_price[["High"]] - pivot_price[["V_Std_S3"]])
  diff_fib_r1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_R1"]])
  diff_fib_r2_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_R2"]])
  diff_fib_r3_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_R3"]])
  diff_fib_r4_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_R4"]])
  diff_fib_r5_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_R5"]])
  diff_cam_r1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_R1"]])
  diff_cam_r2_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_R2"]])
  diff_cam_r3_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_R3"]])
  diff_cam_r4_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_R4"]])
  diff_fib_s1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_S1"]])
  diff_fib_s2_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_S2"]])
  diff_fib_s3_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_S3"]])
  diff_fib_s4_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_S4"]])
  diff_fib_s5_high <- abs(pivot_price[["High"]] - pivot_price[["V_Fib_S5"]])
  diff_cam_s1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_S1"]])
  diff_cam_s2_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_S2"]])
  diff_cam_s3_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_S3"]])
  diff_cam_s4_high <- abs(pivot_price[["High"]] - pivot_price[["V_Cam_S4"]])
  diff_demark_s1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Demark_S1"]])
  diff_demark_r1_high <- abs(pivot_price[["High"]] - pivot_price[["V_Demark_R1"]])
  
  diff_pp_low <- abs(pivot_price[["Low"]] - pivot_price[["V_PP"]])
  diff_tc_low <- abs(pivot_price[["Low"]] - pivot_price[["V_TC"]])
  diff_bc_low <- abs(pivot_price[["Low"]] - pivot_price[["V_BC"]])
  diff_std_r1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Std_R1"]])
  diff_std_r2_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Std_R2"]])
  diff_std_r3_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Std_R3"]])
  diff_std_s1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Std_S1"]])
  diff_std_s2_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Std_S2"]])
  diff_std_s3_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Std_S3"]])
  diff_fib_r1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_R1"]])
  diff_fib_r2_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_R2"]])
  diff_fib_r3_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_R3"]])
  diff_fib_r4_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_R4"]])
  diff_fib_r5_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_R5"]])
  diff_cam_r1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_R1"]])
  diff_cam_r2_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_R2"]])
  diff_cam_r3_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_R3"]])
  diff_cam_r4_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_R4"]])
  diff_fib_s1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_S1"]])
  diff_fib_s2_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_S2"]])
  diff_fib_s3_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_S3"]])
  diff_fib_s4_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_S4"]])
  diff_fib_s5_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Fib_S5"]])
  diff_cam_s1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_S1"]])
  diff_cam_s2_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_S2"]])
  diff_cam_s3_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_S3"]])
  diff_cam_s4_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Cam_S4"]])
  diff_demark_s1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Demark_S1"]])
  diff_demark_r1_low <- abs(pivot_price[["Low"]] - pivot_price[["V_Demark_R1"]])
  
  
  diff_pp_resist <- pivot_price[["High"]] - pivot_price[["V_PP"]]
  diff_tc_resist <- pivot_price[["High"]] - pivot_price[["V_TC"]]
  diff_bc_resist <- pivot_price[["High"]] - pivot_price[["V_BC"]]
  diff_pp_support <- pivot_price[["V_PP"]] - pivot_price[["Low"]]
  diff_tc_support <- pivot_price[["V_TC"]] - pivot_price[["Low"]] 
  diff_bc_support <- pivot_price[["V_BC"]] - pivot_price[["Low"]]
  
  diff_pp_close <- abs(pivot_price[["Close"]] - pivot_price[["V_PP"]])
  diff_tc_close <- abs(pivot_price[["Close"]] - pivot_price[["V_TC"]])
  diff_bc_close <- abs(pivot_price[["Close"]] - pivot_price[["V_BC"]])
  diff_std_r1 <- pivot_price[["High"]] - pivot_price[["V_Std_R1"]]
  diff_std_r2 <- pivot_price[["High"]] - pivot_price[["V_Std_R2"]]
  diff_std_r3 <- pivot_price[["High"]] - pivot_price[["V_Std_R3"]]
  diff_std_s1 <- pivot_price[["V_Std_S1"]] - pivot_price[["Low"]]
  diff_std_s2 <- pivot_price[["V_Std_S2"]] - pivot_price[["Low"]]
  diff_std_s3 <- pivot_price[["V_Std_S3"]] - pivot_price[["Low"]]
  diff_fib_r1 <- pivot_price[["High"]] - pivot_price[["V_Fib_R1"]]
  diff_fib_r2 <- pivot_price[["High"]] - pivot_price[["V_Fib_R2"]]
  diff_fib_r3 <- pivot_price[["High"]] - pivot_price[["V_Fib_R3"]]
  diff_fib_r4 <- pivot_price[["High"]] - pivot_price[["V_Fib_R4"]]
  diff_fib_r5 <- pivot_price[["High"]] - pivot_price[["V_Fib_R5"]]
  diff_cam_r1 <- pivot_price[["High"]] - pivot_price[["V_Cam_R1"]]
  diff_cam_r2 <- pivot_price[["High"]] - pivot_price[["V_Cam_R2"]]
  diff_cam_r3 <- pivot_price[["High"]] - pivot_price[["V_Cam_R3"]]
  diff_cam_r4 <- pivot_price[["High"]] - pivot_price[["V_Cam_R4"]]
  diff_fib_s1 <- pivot_price[["V_Fib_S1"]] - pivot_price[["Low"]]
  diff_fib_s2 <- pivot_price[["V_Fib_S2"]] - pivot_price[["Low"]]
  diff_fib_s3 <- pivot_price[["V_Fib_S3"]] - pivot_price[["Low"]]
  diff_fib_s4 <- pivot_price[["V_Fib_S4"]] - pivot_price[["Low"]]
  diff_fib_s5 <- pivot_price[["V_Fib_S5"]] - pivot_price[["Low"]]
  diff_cam_s1 <- pivot_price[["V_Cam_S1"]] - pivot_price[["Low"]]
  diff_cam_s2 <- pivot_price[["V_Cam_S2"]] - pivot_price[["Low"]]
  diff_cam_s3 <- pivot_price[["V_Cam_S3"]] - pivot_price[["Low"]]
  diff_cam_s4 <- pivot_price[["V_Cam_S4"]] - pivot_price[["Low"]]
  diff_demark_r1 <- pivot_price[["High"]] - pivot_price[["V_Demark_R1"]]
  diff_demark_s1 <- pivot_price[["V_Demark_S1"]] - pivot_price[["Low"]]
  
  #print("d21")
  std_resist_mark <- pivot_price %>% select(P_Std_R1, P_Std_R2, P_Std_R3) %>% 
    mutate(hit = P_Std_R1 | P_Std_R2 | P_Std_R3) %>%
    mutate(farthest = ifelse(hit & P_Std_R1 & P_Std_R2 & P_Std_R3, "Std_R3",
                             ifelse(hit & P_Std_R1 & P_Std_R2, "Std_R2",
                                    ifelse(hit & P_Std_R1, "Std_R1", NA))))
  
  #print("d22")
  std_support_mark <- pivot_price %>% select(P_Std_S1, P_Std_S2, P_Std_S3) %>% 
    mutate(hit = P_Std_S1 | P_Std_S2 | P_Std_S3) %>%
    mutate(farthest = ifelse(hit & P_Std_S1 & P_Std_S2 & P_Std_S3, "Std_S3",
                             ifelse(hit & P_Std_S1 & P_Std_S2, "Std_S2",
                                    ifelse(hit & P_Std_S1, "Std_S1", NA))))
  
  #print("d23")
  fib_resist_mark <- pivot_price %>% select(P_Fib_R1, P_Fib_R2, P_Fib_R3, P_Fib_R4, P_Fib_R5) %>% 
    mutate(hit = P_Fib_R1 | P_Fib_R2 | P_Fib_R3 | P_Fib_R4 | P_Fib_R5) %>%
    mutate(farthest = ifelse(hit & P_Fib_R1 & P_Fib_R2 & P_Fib_R3 & P_Fib_R4 & P_Fib_R5, "Fib_R5",
                             ifelse(hit & P_Fib_R1 & P_Fib_R2 & P_Fib_R3 & P_Fib_R4, "Fib_R4",
                                    ifelse(hit & P_Fib_R1 & P_Fib_R2 & P_Fib_R3, "Fib_R3",
                                           ifelse(hit & P_Fib_R1 & P_Fib_R2, "Fib_R2",
                                                  ifelse(hit & P_Fib_R1, "Fib_R1", NA))))))
  
  #print("d24")
  fib_support_mark <- pivot_price %>% select(P_Fib_S1, P_Fib_S2, P_Fib_S3, P_Fib_S4, P_Fib_S5) %>% 
    mutate(hit = P_Fib_S1 | P_Fib_S2 | P_Fib_S3 | P_Fib_S4 | P_Fib_S5) %>%
    mutate(farthest = ifelse(hit & P_Fib_S1 & P_Fib_S2 & P_Fib_S3 & P_Fib_S4 & P_Fib_S5, "Fib_S5",
                             ifelse(hit & P_Fib_S1 & P_Fib_S2 & P_Fib_S3 & P_Fib_S4, "Fib_S4",
                                    ifelse(hit & P_Fib_S1 & P_Fib_S2 & P_Fib_S3, "Fib_S3",
                                           ifelse(hit & P_Fib_S1 & P_Fib_S2, "Fib_S2",
                                                  ifelse(hit & P_Fib_S1, "Fib_S1", NA))))))
  
  #print("d25")
  cam_resist_mark <- pivot_price %>% select(P_Cam_R1, P_Cam_R2, P_Cam_R3, P_Cam_R4) %>% 
    mutate(hit = P_Cam_R1 | P_Cam_R2 | P_Cam_R3 | P_Cam_R4) %>%
    mutate(farthest = ifelse(hit & P_Cam_R1 & P_Cam_R2 & P_Cam_R3 & P_Cam_R4, "Cam_R4",
                             ifelse(hit & P_Cam_R1 & P_Cam_R2 & P_Cam_R3, "Cam_R3",
                                    ifelse(hit & P_Cam_R1 & P_Cam_R2, "Cam_R2",
                                           ifelse(hit & P_Cam_R1, "Cam_R1", NA)))))
  
  #print("d26")
  cam_support_mark <- pivot_price %>% select(P_Cam_S1, P_Cam_S2, P_Cam_S3, P_Cam_S4) %>% 
    mutate(hit = P_Cam_S1 | P_Cam_S2 | P_Cam_S3 | P_Cam_S4) %>%
    mutate(farthest = ifelse(hit & P_Cam_S1 & P_Cam_S2 & P_Cam_S3 & P_Cam_S4, "Cam_S4",
                             ifelse(hit & P_Cam_S1 & P_Cam_S2 & P_Cam_S3, "Cam_S3",
                                    ifelse(hit & P_Cam_S1 & P_Cam_S2, "Cam_S2",
                                           ifelse(hit & P_Cam_S1, "Cam_S1", NA)))))
  
  #print("d27")
  demark_resist_mark <- pivot_price %>% select(hit = P_Demark_R1) %>%
    mutate(hit = ifelse(hit, "Demark_R1", NA))
  demark_support_mark <- pivot_price %>% select(hit = P_Demark_S1) %>%
    mutate(hit = ifelse(hit, "Demark_S1", NA))
  
  #--------------------------headline--------------------------------------:
  #resist false support false no vector
  #resist true support false check if close > open, then p -> resist
  #resist true support false check if close < open, then resist -> p
  #resist false support true check if close > open, then support -> p
  #resist false support true check if close < open, then p -> support
  #resist true support true, check if close > open, then support -> resist
  #resist true support true, check if close < open, then resist -> support
  
  #print("d3")
  possible_resist <- as.data.frame(cbind(std_far = std_resist_mark$farthest, fib_far = fib_resist_mark$farthest, demark_far = demark_resist_mark, cam_far = cam_resist_mark$farthest))
  possible_support <- as.data.frame(cbind(std_far = std_support_mark$farthest, fib_far = fib_support_mark$farthest, demark_far = demark_support_mark, cam_far = cam_support_mark$farthest))
  
  if(return_possible_rs){
    return(list(possible_resist, possible_support))
  }
  
  resist_vector_length <- as.vector(apply(possible_resist, FUN = function(x) length(na.omit(x)), MARGIN = 1))
  support_vector_length <- as.vector(apply(possible_support, FUN = function(x) length(na.omit(x)), MARGIN = 1))
  resist_vector_string <- as.vector(apply(possible_resist, FUN = function(x) paste0(na.omit(x), collapse="|"), MARGIN = 1))
  support_vector_string <- as.vector(apply(possible_support, FUN = function(x) paste0(na.omit(x), collapse="|"), MARGIN = 1))
  rs_df <- as.data.frame(cbind(resist = resist_vector_length, support = support_vector_length))
  vector_pivot <- c()
  
  if(vector_type == "nearest_diff"){
    for(a in 1:nrow(rs_df)){
      if(rs_df$resist[a] >= 1 && rs_df$support[a] >= 1){
        resist_name <- na.omit(as.vector(unlist(possible_resist[a,])))
        support_name <- na.omit(as.vector(unlist(possible_support[a,])))
        resist_cols <- paste0("diff_", tolower(resist_name))
        support_cols <- paste0("diff_", tolower(support_name))
        
        if(any(resist_cols == "diff_")) resist_cols <- resist_cols[-which(resist_cols == "diff_")]
        if(any(support_cols == "diff_")) support_cols <- support_cols[-which(support_cols == "diff_")]
        
        resist_value_diff <- c()
        support_value_diff <- c()
        
        for(co in 1:length(resist_cols)){
          resist_value_diff <- c(resist_value_diff, get(resist_cols[co])[a])
        }
        for(co in 1:length(support_cols)){
          support_value_diff <- c(support_value_diff, get(support_cols[co])[a])
        }
        which_min_resist <- which(resist_value_diff == min(resist_value_diff))[1]
        which_min_support <- which(support_value_diff == min(support_value_diff))[1]
        
        if(pivot_price[["Close"]][a] > pivot_price[["Open"]][a]){
          vector_pivot <- c(vector_pivot, paste0(support_name[which_min_support], " -> ", resist_name[which_min_resist]))
        }else if(pivot_price[["Close"]][a] < pivot_price[["Open"]][a]){
          vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_resist], " -> ", support_name[which_min_support]))
        }else{
          if(diff_oh[a] > diff_ol[a]){
            vector_pivot <- c(vector_pivot, paste0(support_name[which_min_support], " -> ", resist_name[which_min_resist]))
          }else if(diff_oh[a] < diff_ol[a]){
            vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_resist], " -> ", support_name[which_min_support]))
          }else{
            vector_pivot <- c(vector_pivot, paste0(support_name[which_min_support], " -> ", resist_name[which_min_resist] ," | ",resist_name[which_min_resist], " -> ", support_name[which_min_support]))
          }
        }
      }
      else if(rs_df$resist[a] >= 1 && rs_df$support[a] == 0){
        resist_name_destination <- na.omit(as.vector(unlist(possible_resist[a,])))
        resist_name <- c("PP","TC","BC",na.omit(as.vector(unlist(possible_resist[a,]))))
        if(any(resist_name == 'Std_R2')){
          resist_name <- c(resist_name, 'Std_R1')
        }
        if(any(resist_name  == 'Std_R3')){
          resist_name <- c(resist_name, 'Std_R1', 'Std_R2')
        }
        if(any(resist_name  == 'Fib_R2')){
          resist_name <- c(resist_name, 'Fib_R1')
        }
        if(any(resist_name  == 'Fib_R3')){
          resist_name <- c(resist_name, 'Fib_R1', 'Fib_R2')
        }
        if(any(resist_name  == 'Fib_R4')){
          resist_name <- c(resist_name, 'Fib_R1', 'Fib_R2', 'Fib_R3')
        }
        if(any(resist_name  == 'Fib_R5')){
          resist_name <- c(resist_name, 'Fib_R1', 'Fib_R2', 'Fib_R3', 'Fib_R4')
        }
        if(any(resist_name  == 'Cam_R2')){
          resist_name <- c(resist_name, 'Cam_R1')
        }
        if(any(resist_name  == 'Cam_R3')){
          resist_name <- c(resist_name, 'Cam_R1', 'Cam_R2')
        }
        if(any(resist_name  == 'Cam_R4')){
          resist_name <- c(resist_name, 'Cam_R1', 'Cam_R2', 'Cam_R3')
        }
        
        #Source Open
        resist_cols_source_o <- paste0("diff_", tolower(resist_name),"_open")
        if(any(resist_cols_source_o == "diff__open")) resist_cols_source_o <- resist_cols_source_o[-which(resist_cols_source_o == "diff__open")]
        resist_value_diff_source_o <- c()
        #print(resist_cols_source_o)
        
        for(co in 1:length(resist_cols_source_o)){
          resist_value_diff_source_o <- c(resist_value_diff_source_o, get(resist_cols_source_o[co])[a])
        }
        
        #Source High
        resist_cols_source_h <- paste0("diff_", tolower(resist_name),"_high")
        if(any(resist_cols_source_h == "diff__high")) resist_cols_source_h <- resist_cols_source_h[-which(resist_cols_source_h == "diff__high")]
        resist_value_diff_source_h <- c()
        #print(resist_cols_source_h)
        
        for(co in 1:length(resist_cols_source_h)){
          resist_value_diff_source_h <- c(resist_value_diff_source_h, get(resist_cols_source_h[co])[a])
        }
        
        #Source Low
        resist_cols_source_l <- paste0("diff_", tolower(resist_name),"_low")
        if(any(resist_cols_source_l == "diff__low")) resist_cols_source_l <- resist_cols_source_l[-which(resist_cols_source_l == "diff__low")]
        resist_value_diff_source_l <- c()
        #print(resist_cols_source)
        
        for(co in 1:length(resist_cols_source_l)){
          resist_value_diff_source_l <- c(resist_value_diff_source_l, get(resist_cols_source_l[co])[a])
        }
        
        #Destination
        resist_cols <- c("diff_pp_resist","diff_tc_resist","diff_bc_resist",paste0("diff_", tolower(resist_name_destination)))
        if(any(resist_cols == "diff_")) resist_cols <- resist_cols[-which(resist_cols == "diff_")]
        resist_value_diff <- c()
        resist_name_destination <- c("PP","TC","BC", resist_name_destination)
        #print(resist_cols)
        
        for(co in 1:length(resist_cols)){
          resist_value_diff <- c(resist_value_diff, get(resist_cols[co])[a])
        }
        
        which_min_open <- which(resist_value_diff_source_o == min(resist_value_diff_source_o))[1]
        which_min_high <- which(resist_value_diff_source_h == min(resist_value_diff_source_h))[1]
        which_min_low <- which(resist_value_diff_source_l == min(resist_value_diff_source_l))[1]
        which_min_resist <- which(resist_value_diff == min(resist_value_diff))[1]
        
        if(pivot_price[["Close"]][a] > pivot_price[["Open"]][a]){
          if(resist_name[which_min_open] == resist_name_destination[which_min_resist]){
            if((resist_name[which_min_high] != resist_name_destination[which_min_resist]) && (resist_name[which_min_low] == resist_name_destination[which_min_resist])) vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_high]," -> ", resist_name_destination[which_min_resist]))
            else if((resist_name[which_min_high] == resist_name_destination[which_min_resist]) && (resist_name[which_min_low] != resist_name_destination[which_min_resist])) vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_low]," -> ", resist_name_destination[which_min_resist]))
            else{
              vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_open]," -> ", resist_name_destination[which_min_resist]))
            }
          }else{
            vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_open]," -> ", resist_name_destination[which_min_resist]))
          }
        }else if(pivot_price[["Close"]][a] < pivot_price[["Open"]][a]){
          if(resist_name_destination[which_min_resist] == resist_name[which_min_open]){
            if((resist_name[which_min_high] != resist_name_destination[which_min_resist]) && (resist_name[which_min_low] == resist_name_destination[which_min_resist])) vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_high]," -> ", resist_name_destination[which_min_resist]))
            else if((resist_name[which_min_high] == resist_name_destination[which_min_resist]) && (resist_name[which_min_low] != resist_name_destination[which_min_resist])) vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_low]," -> ", resist_name_destination[which_min_resist]))
            else{
              vector_pivot <- c(vector_pivot, paste0(resist_name_destination[which_min_resist], " -> ",resist_name[which_min_open]))
            }
          }else{
            vector_pivot <- c(vector_pivot, paste0(resist_name_destination[which_min_resist], " -> ",resist_name[which_min_open]))
          }
        }else{
          if(resist_name[which_min_open] == resist_name_destination[which_min_resist]){
            if(resist_name[which_min_high] == resist_name_destination[which_min_resist] &&
               resist_name[which_min_low] != resist_name_destination[which_min_resist]){
              vector_pivot <- c(vector_pivot, paste0(resist_name_destination[which_min_resist], " -> ",resist_name[which_min_low]))
            }else if(resist_name[which_min_high] != resist_name_destination[which_min_resist] &&
                     resist_name[which_min_low] == resist_name_destination[which_min_resist]){
              vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_high]," -> ", resist_name_destination[which_min_resist]))
            }else if(resist_name[which_min_high] == resist_name_destination[which_min_resist] &&
                     resist_name[which_min_low] == resist_name_destination[which_min_resist]){
              vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_high]," -> ", resist_name_destination[which_min_resist]))
            }
          }else{
            if(diff_oh[a] > diff_ol[a]){
              vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_open]," -> ", resist_name_destination[which_min_resist]))
            }else if(diff_oh[a] < diff_ol[a]){
              vector_pivot <- c(vector_pivot, paste0(resist_name_destination[which_min_resist], " -> ",resist_name[which_min_open]))
            }else{
              vector_pivot <- c(vector_pivot, paste0(resist_name[which_min_open]," -> ", resist_name_destination[which_min_resist], " | ", resist_name_destination[which_min_resist], " -> ",resist_name[which_min_open]))
            }
          }
        }
      }
      else if(rs_df$resist[a] == 0 && rs_df$support[a] >= 1){
        support_name_destination <- na.omit(as.vector(unlist(possible_support[a,])))
        support_name <- c("PP","TC","BC",na.omit(as.vector(unlist(possible_support[a,]))))
        if(any(support_name == 'Std_S2')){
          support_name <- c(support_name, 'Std_S1')
        }
        if(any(support_name  == 'Std_S3')){
          support_name <- c(support_name, 'Std_S1', 'Std_S2')
        }
        if(any(support_name  == 'Fib_S2')){
          support_name <- c(support_name, 'Fib_S1')
        }
        if(any(support_name  == 'Fib_S3')){
          support_name <- c(support_name, 'Fib_S1', 'Fib_S2')
        }
        if(any(support_name  == 'Fib_S4')){
          support_name <- c(support_name, 'Fib_S1', 'Fib_S2', 'Fib_S3')
        }
        if(any(support_name  == 'Fib_S5')){
          support_name <- c(support_name, 'Fib_S1', 'Fib_S2', 'Fib_S3', 'Fib_S4')
        }
        if(any(support_name  == 'Cam_S2')){
          support_name <- c(support_name, 'Cam_S1')
        }
        if(any(support_name  == 'Cam_S3')){
          support_name <- c(support_name, 'Cam_S1', 'Cam_S2')
        }
        if(any(support_name  == 'Cam_S4')){
          support_name <- c(support_name, 'Cam_S1', 'Cam_S2', 'Cam_S3')
        }
        
        #Source Open
        support_cols_source_o <- paste0("diff_", tolower(support_name),"_open")
        if(any(support_cols_source_o == "diff__open")) support_cols_source_o <- support_cols_source_o[-which(support_cols_source_o == "diff__open")]
        support_value_diff_source_o <- c()
        #print(support_cols_source_o)
        
        for(co in 1:length(support_cols_source_o)){
          support_value_diff_source_o <- c(support_value_diff_source_o, get(support_cols_source_o[co])[a])
        }
        
        #Source High
        support_cols_source_h <- paste0("diff_", tolower(support_name),"_high")
        if(any(support_cols_source_h == "diff__high")) support_cols_source_h <- support_cols_source_h[-which(support_cols_source_h == "diff__high")]
        support_value_diff_source_h <- c()
        #print(support_cols_source_h)
        
        for(co in 1:length(support_cols_source_h)){
          support_value_diff_source_h <- c(support_value_diff_source_h, get(support_cols_source_h[co])[a])
        }
        
        #Source Low
        support_cols_source_l <- paste0("diff_", tolower(support_name),"_low")
        if(any(support_cols_source_l == "diff__low")) support_cols_source_l <- support_cols_source_l[-which(support_cols_source_l == "diff__low")]
        support_value_diff_source_l <- c()
        #print(support_cols_source)
        
        for(co in 1:length(support_cols_source_l)){
          support_value_diff_source_l <- c(support_value_diff_source_l, get(support_cols_source_l[co])[a])
        }
        
        #Destination
        support_cols <- c("diff_pp_support","diff_tc_support","diff_bc_support",paste0("diff_", tolower(support_name_destination)))
        if(any(support_cols == "diff_")) support_cols <- support_cols[-which(support_cols == "diff_")]
        support_value_diff <- c()
        support_name_destination <- c("PP","TC","BC", support_name_destination)
        #print(support_cols)
        
        for(co in 1:length(support_cols)){
          support_value_diff <- c(support_value_diff, get(support_cols[co])[a])
        }
        
        which_min_open <- which(support_value_diff_source_o == min(support_value_diff_source_o))[1]
        which_min_high <- which(support_value_diff_source_h == min(support_value_diff_source_h))[1]
        which_min_low <- which(support_value_diff_source_l == min(support_value_diff_source_l))[1]
        which_min_support <- which(support_value_diff == min(support_value_diff))[1]
        
        if(pivot_price[["Close"]][a] > pivot_price[["Open"]][a]){
          if(support_name_destination[which_min_support] == support_name[which_min_open]){
            if((support_name[which_min_high] != support_name_destination[which_min_support]) && (support_name[which_min_low] == support_name_destination[which_min_support])) vector_pivot <- c(vector_pivot, paste0(support_name[which_min_high]," -> ", support_name_destination[which_min_support]))
            else if((support_name[which_min_high] == support_name_destination[which_min_support]) && (support_name[which_min_low] != support_name_destination[which_min_support])) vector_pivot <- c(vector_pivot, paste0(support_name[which_min_low]," -> ", support_name_destination[which_min_support]))
            else{
              vector_pivot <- c(vector_pivot, paste0(support_name_destination[which_min_support], " -> ",support_name[which_min_open]))
            }
          }else{
            vector_pivot <- c(vector_pivot, paste0(support_name_destination[which_min_support], " -> ",support_name[which_min_open]))
          }
          
        }else if(pivot_price[["Close"]][a] < pivot_price[["Open"]][a]){
          if(support_name_destination[which_min_support] == support_name[which_min_open]){
            if((support_name[which_min_high] != support_name_destination[which_min_support]) && (support_name[which_min_low] == support_name_destination[which_min_support])) vector_pivot <- c(vector_pivot, paste0(support_name[which_min_high]," -> ", support_name_destination[which_min_support]))
            else if((support_name[which_min_high] == support_name_destination[which_min_support]) && (support_name[which_min_low] != support_name_destination[which_min_support])) vector_pivot <- c(vector_pivot, paste0(support_name[which_min_low]," -> ", support_name_destination[which_min_support]))
            else{
              vector_pivot <- c(vector_pivot, paste0(support_name[which_min_open]," -> ", support_name_destination[which_min_support]))
            }
          }else{
            vector_pivot <- c(vector_pivot, paste0(support_name[which_min_open]," -> ", support_name_destination[which_min_support]))
          }
        }else{
          if(support_name_destination[which_min_support] == support_name[which_min_open]){
            if(support_name[which_min_high] == support_name_destination[which_min_support] &&
               support_name[which_min_low] != support_name_destination[which_min_support]){
              vector_pivot <- c(vector_pivot, paste0(support_name[which_min_low] ," -> ", support_name_destination[which_min_support])) 
            }else if(support_name[which_min_high] != support_name_destination[which_min_support] &&
                     support_name[which_min_low] == support_name_destination[which_min_support]){
              vector_pivot <- c(vector_pivot, paste0(support_name_destination[which_min_support]," -> ", support_name[which_min_high])) 
            }else if(support_name[which_min_high] == support_name_destination[which_min_support] &&
                     support_name[which_min_low] == support_name_destination[which_min_support]){
              vector_pivot <- c(vector_pivot, paste0(support_name[which_min_high]," -> ", support_name_destination[which_min_support])) 
            }
            
          }else{
            if(diff_oh[a] > diff_ol[a]){
              vector_pivot <- c(vector_pivot, paste0(support_name_destination[which_min_support], " -> ",support_name[which_min_open]))
            }else if(diff_oh[a] < diff_ol[a]){
              vector_pivot <- c(vector_pivot, paste0(support_name[which_min_open]," -> ", support_name_destination[which_min_support]))
            }else{
              vector_pivot <- c(vector_pivot, paste0(support_name[which_min_open]," -> ", support_name_destination[which_min_support] ," | ", support_name_destination[which_min_support], " -> ",support_name[which_min_open]))
            }
          }
        }
      }
      else{
        pp_name <- c("PP","TC","BC")
        open_pivots <- c(diff_pp_open[a], diff_tc_open[a], diff_bc_open[a])
        close_pivots <- c(diff_pp_close[a], diff_tc_close[a], diff_bc_close[a])
        high_pivots <- c(diff_pp_high[a], diff_tc_high[a], diff_bc_high[a])
        low_pivots <- c(diff_pp_low[a], diff_tc_low[a], diff_bc_low[a])
        which_min_open <- which(open_pivots == min(open_pivots))[1]
        which_min_close <- which(close_pivots == min(close_pivots))[1]
        which_min_high <- which(high_pivots == min(high_pivots))[1]
        which_min_low <- which(low_pivots == min(low_pivots))[1]
        
        if(pivot_price[["Close"]][a] > pivot_price[["Open"]][a]){
          if(pp_name[which_min_open] == pp_name[which_min_close]){
            if((pp_name[which_min_high] != pp_name[which_min_close]) && (pp_name[which_min_low] == pp_name[which_min_close])) vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_high], " -> ", pp_name[which_min_low]))
            else if((pp_name[which_min_high] == pp_name[which_min_close]) && (pp_name[which_min_low] != pp_name[which_min_close])) vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_low], " -> ", pp_name[which_min_high]))
            else{
              vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_low], " -> ", pp_name[which_min_high]))
            }
          }else{
            vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_open], " -> ", pp_name[which_min_close]))
          }
        }
        else if(pivot_price[["Close"]][a] < pivot_price[["Open"]][a]){
          if(pp_name[which_min_open] == pp_name[which_min_close]){
            if((pp_name[which_min_high] != pp_name[which_min_close]) && (pp_name[which_min_low] == pp_name[which_min_close])) vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_high], " -> ", pp_name[which_min_low]))
            else if((pp_name[which_min_high] == pp_name[which_min_close]) && (pp_name[which_min_low] != pp_name[which_min_close])) vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_low], " -> ", pp_name[which_min_high]))
            else{
              vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_high], " -> ", pp_name[which_min_low]))
            }
          }else{
            vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_close], " -> ", pp_name[which_min_open]))
          }
        }
        else{
          if(pp_name[which_min_open] == pp_name[which_min_close]){
            vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_high], " -> ", pp_name[which_min_low], " | ", pp_name[which_min_low], " -> ", pp_name[which_min_high]))
          }else{
            vector_pivot <- c(vector_pivot, paste0(pp_name[which_min_close], " -> ", pp_name[which_min_open], " | ", pp_name[which_min_open], " -> ", pp_name[which_min_close]))
          }
        }
      }
    }
    pivot_price$vector_pivot <- vector_pivot
    return(pivot_price)
  }
  else if(vector_type == "combination"){
    for(a in 1:nrow(rs_df)){
      if(rs_df$resist[a] >= 1 && rs_df$support[a] >= 1){
        elem1 <- unlist(strsplit(resist_vector_string[a], "\\|"))
        elem2 <- unlist(strsplit(support_vector_string[a], "\\|"))
        if(pivot_price[["Close"]][a] > pivot_price[["Open"]][a]){
          elem_comb <- expand.grid(elem2, elem1)
          vector_pivot <- c(vector_pivot, paste0(apply(elem_comb, FUN = function(x) paste0(x, collapse=" -> "), MARGIN = 1), collapse = " | "))
        }else if(pivot_price[["Close"]][a] < pivot_price[["Open"]][a]){
          elem_comb <- expand.grid(elem1, elem2)
          vector_pivot <- c(vector_pivot, paste0(apply(elem_comb, FUN = function(x) paste0(x, collapse=" -> "), MARGIN = 1), collapse = " | "))
        }else{
          elem_comb1 <- expand.grid(elem2, elem1)
          elem_comb2 <- expand.grid(elem1, elem2)
          elem_allcomb <- rbind(elem_comb1, elem_comb2)
          vector_pivot <- c(vector_pivot, paste0(apply(elem_allcomb, FUN = function(x) paste0(x, collapse=" -> "), MARGIN = 1), collapse = " | "))
        }
      }
      else if(rs_df$resist[a] >= 1 && rs_df$support[a] == 0){
        elem1 <- unlist(strsplit(resist_vector_string[a], "\\|"))
        if(pivot_price[["Close"]][a] > pivot_price[["Open"]][a]){
          elem_comb <- paste0("PP -> ",elem1)
          vector_pivot <- c(vector_pivot, paste0(elem_comb, collapse = " | "))
        }else if(pivot_price[["Close"]][a] < pivot_price[["Open"]][a]){
          elem_comb <- paste0(elem1," -> PP")
          vector_pivot <- c(vector_pivot, paste0(elem_comb, collapse = " | "))
        }else{
          elem_comb1 <- paste0("PP -> ",elem1)
          elem_comb2 <- paste0(elem1," -> PP")
          elem_allcomb <- c(elem_comb1, elem_comb2)
          vector_pivot <- c(vector_pivot, paste0(elem_comb, collapse = " | "))
        }
      }
      else if(rs_df$resist[a] == 0 && rs_df$support[a] >= 1){
        elem2 <- unlist(strsplit(support_vector_string[a], "\\|"))
        if(pivot_price[["Close"]][a] > pivot_price[["Open"]][a]){
          elem_comb <- paste0(elem2," -> PP")
          vector_pivot <- c(vector_pivot, paste0(elem_comb, collapse = " | "))
        }else if(pivot_price[["Close"]][a] < pivot_price[["Open"]][a]){
          elem_comb <- paste0("PP -> ",elem2)
          vector_pivot <- c(vector_pivot, paste0(elem_comb, collapse = " | "))
        }else{
          elem_comb1 <- paste0(elem2," -> PP")
          elem_comb2 <- paste0("PP -> ",elem2)
          elem_allcomb <- c(elem_comb1, elem_comb2)
          vector_pivot <- c(vector_pivot, paste0(elem_comb, collapse = " | "))
        }
      }
      else{
        vector_pivot <- c(vector_pivot, "Trade Around PP")
      }
    }
    pivot_price$vector_pivot <- vector_pivot
    return(pivot_price)
  }
}

vector_clean_same_oc <- function(daily_pivot_vector, see_results = F){
  library(dplyr)
  library(tidyr)
  which_unclean <- grepl(" \\| ", daily_pivot_vector$vector_pivot)
  which_unclean <- which(which_unclean == T)
  backup <- daily_pivot_vector$vector_pivot[which_unclean]
  separated_pivot <- daily_pivot_vector[which_unclean,] %>% 
    select(vector_pivot) %>%
    separate(vector_pivot, into = c("pos_vec1","pos_vec2"), sep = " \\| ") %>% as.data.frame()
  diff_oh <- abs(daily_pivot_vector$Open - daily_pivot_vector$High)
  diff_ol <- abs(daily_pivot_vector$Open - daily_pivot_vector$Low)
  diff_oh <- diff_oh[which_unclean]
  diff_ol <- diff_ol[which_unclean]
  daily_pivot_vector$vector_pivot[which_unclean] <- ifelse(diff_oh >= diff_ol, separated_pivot$pos_vec1, separated_pivot$pos_vec2)
  if(see_results){
    writeLines("Before")
    print(backup)
    writeLines("After")
    print(daily_pivot_vector$vector_pivot[which_unclean])
  }
  return(daily_pivot_vector)
}

pivot_precedence_break <- function(pivot_pre_df){
  library(dplyr)
  library(tidyr)
  library(rlang)
  break_df <- NULL
  for(x in 1:ncol(pivot_pre_df)){
    current_colname <- colnames(pivot_pre_df)[x]
    current_colname_sym <- sym(current_colname)
    new_colname <- paste0(current_colname, c("F","T"))
    data_new <- pivot_pre_df %>% select(!!current_colname_sym) %>% 
      separate(!!current_colname_sym, into = c(new_colname[1], new_colname[2]), sep = " -> ") %>% as.data.frame()
    if(is.null(break_df)){
      break_df <- data_new
    }else{
      break_df <- as.data.frame(cbind(break_df, data_new))
    }
  }
  return(break_df)
}

automatic_update_daily_data <- function(last_date_data = as.Date("2023-07-13"), pp_overwrite = F){
  library(stringr)
  library(rvest)
  library(RSelenium)
  library(dplyr)
  library(tidyr)
  library(lubridate)
  
  diff_last_date <- abs(as.numeric(difftime(last_date_data, Sys.Date(), units="days")))
  writeLines("Fetching New Update data from Investing for every assets!")
  
  #----------------------------------------- UPDATE NEW DATA ASSETS, FETCH DIRECTLY FROM INVESTING.COM ---------------
  #----------------------------------------- DXY DATA ---------------------------------------------------------------------
  dxy_daily_add <- NULL
  if(diff_last_date > 30){
    dxy_daily_add <- investing_com_data_correction_2("https://www.investing.com/currencies/us-dollar-index-historical-data",
                                                     start_date = last_date_data, chromever = "108.0.5359.71", 
                                                     data_waiting_time = 10, timeframe_select = "daily",
                                                     extract_directly = TRUE)
  }else{
    dxy_daily_add <- read_html("https://www.investing.com/currencies/us-dollar-index-historical-data") %>% html_table()
  }
  
  dxy_daily_add <- as.data.frame(dxy_daily_add[[2]])
  dxy_daily_add <- dxy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(dxy_daily_add)[c(1,5)] <- c("Datetime","Close")
  dxy_daily_add$Datetime <- as.Date(dxy_daily_add$Datetime, tryFormats = c("%b %d, %Y"))
  dxy_daily_add$Open <- as.numeric(str_replace(dxy_daily_add$Open,",",""))
  dxy_daily_add$High <- as.numeric(str_replace(dxy_daily_add$High,",",""))
  dxy_daily_add$Low <- as.numeric(str_replace(dxy_daily_add$Low,",",""))
  dxy_daily_add$Close <- as.numeric(str_replace(dxy_daily_add$Close,",",""))
  dxy_daily_add <- dxy_daily_add %>% arrange(Datetime)
  
  dxy_daily <<- dxy_daily %>% select(Datetime, Open, High, Low, Close)
  dxy_daily <<- rbind(dxy_daily, dxy_daily_add)
  dxy_daily <<- dxy_daily[!duplicated(dxy_daily),]
  rownames(dxy_daily) <<- NULL
  writeLines("Successfully add DXY Data")
  
  #----------------------------------------- GOLD DATA --------------------------------------------------------------------
  xauusd_daily_add <- NULL
  if(diff_last_date > 30){
    xauusd_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/xau-usd-historical-data",
                                                      start_date = last_date_data, chromever = "108.0.5359.71", 
                                                      data_waiting_time = 10, timeframe_select = "daily",
                                                      extract_directly = TRUE)
  }else{
    xauusd_daily_add <- read_html("https://www.investing.com/currencies/xau-usd-historical-data") %>% html_table()
  }
  
  xauusd_daily_add <- as.data.frame(xauusd_daily_add[[2]])
  xauusd_daily_add <- xauusd_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(xauusd_daily_add)[c(1,5)] <- c("Datetime","Close")
  xauusd_daily_add$Datetime <- as.Date(xauusd_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  xauusd_daily_add$Open <- as.numeric(str_replace(xauusd_daily_add$Open,",",""))
  xauusd_daily_add$High <- as.numeric(str_replace(xauusd_daily_add$High,",",""))
  xauusd_daily_add$Low <- as.numeric(str_replace(xauusd_daily_add$Low,",",""))
  xauusd_daily_add$Close <- as.numeric(str_replace(xauusd_daily_add$Close,",",""))
  xauusd_daily_add <- xauusd_daily_add %>% arrange(Datetime)
  
  xauusd_daily <<- xauusd_daily %>% select(Datetime, Open, High, Low, Close)
  xauusd_daily <<- rbind(xauusd_daily, xauusd_daily_add)
  xauusd_daily <<- xauusd_daily[!duplicated(xauusd_daily),]
  rownames(xauusd_daily) <<- NULL
  writeLines("Successfully add XAUUSD Data")
  
  #----------------------------------------- FOREX DATA --------------------------------------------------------------------
  
  usdchf_daily_add <- NULL
  if(diff_last_date > 30){
    usdchf_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/usd-chf-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    usdchf_daily_add <- read_html("https://www.investing.com/currencies/usd-chf-historical-data") %>% html_table()
  }
  
  usdchf_daily_add <- as.data.frame(usdchf_daily_add[[2]])
  usdchf_daily_add <- usdchf_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(usdchf_daily_add)[c(1,5)] <- c("Datetime","Close")
  usdchf_daily_add$Datetime <- as.Date(usdchf_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  usdchf_daily_add$Open <- as.numeric(str_replace(usdchf_daily_add$Open,",",""))
  usdchf_daily_add$High <- as.numeric(str_replace(usdchf_daily_add$High,",",""))
  usdchf_daily_add$Low <- as.numeric(str_replace(usdchf_daily_add$Low,",",""))
  usdchf_daily_add$Close <- as.numeric(str_replace(usdchf_daily_add$Close,",",""))
  usdchf_daily_add <- usdchf_daily_add %>% arrange(Datetime)
  
  usdchf_daily <<- usdchf_daily %>% select(Datetime, Open, High, Low, Close)
  usdchf_daily <<- rbind(usdchf_daily, usdchf_daily_add)
  usdchf_daily <<- usdchf_daily[!duplicated(usdchf_daily),]
  rownames(usdchf_daily) <<- NULL
  writeLines("Successfully add USDCHF Data")
  
  gbpusd_daily_add <- NULL
  if(diff_last_date > 30){
    gbpusd_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/gbp-usd-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    gbpusd_daily_add <- read_html("https://www.investing.com/currencies/gbp-usd-historical-data") %>% html_table()
  }
  
  gbpusd_daily_add <- as.data.frame(gbpusd_daily_add[[2]])
  gbpusd_daily_add <- gbpusd_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(gbpusd_daily_add)[c(1,5)] <- c("Datetime","Close")
  gbpusd_daily_add$Datetime <- as.Date(gbpusd_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  gbpusd_daily_add$Open <- as.numeric(str_replace(gbpusd_daily_add$Open,",",""))
  gbpusd_daily_add$High <- as.numeric(str_replace(gbpusd_daily_add$High,",",""))
  gbpusd_daily_add$Low <- as.numeric(str_replace(gbpusd_daily_add$Low,",",""))
  gbpusd_daily_add$Close <- as.numeric(str_replace(gbpusd_daily_add$Close,",",""))
  gbpusd_daily_add <- gbpusd_daily_add %>% arrange(Datetime)
  
  gbpusd_daily <<- gbpusd_daily %>% select(Datetime, Open, High, Low, Close)
  gbpusd_daily <<- rbind(gbpusd_daily, gbpusd_daily_add)
  gbpusd_daily <<- gbpusd_daily[!duplicated(gbpusd_daily),]
  rownames(gbpusd_daily) <<- NULL
  writeLines("Successfully add GBPUSD Data")
  
  eurusd_daily_add <- NULL
  if(diff_last_date > 30){
    eurusd_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/eur-usd-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    eurusd_daily_add <- read_html("https://www.investing.com/currencies/eur-usd-historical-data") %>% html_table()
  }
  eurusd_daily_add <- as.data.frame(eurusd_daily_add[[2]])
  eurusd_daily_add <- eurusd_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(eurusd_daily_add)[c(1,5)] <- c("Datetime","Close")
  eurusd_daily_add$Datetime <- as.Date(eurusd_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  eurusd_daily_add$Open <- as.numeric(str_replace(eurusd_daily_add$Open,",",""))
  eurusd_daily_add$High <- as.numeric(str_replace(eurusd_daily_add$High,",",""))
  eurusd_daily_add$Low <- as.numeric(str_replace(eurusd_daily_add$Low,",",""))
  eurusd_daily_add$Close <- as.numeric(str_replace(eurusd_daily_add$Close,",",""))
  eurusd_daily_add <- eurusd_daily_add %>% arrange(Datetime)
  
  eurusd_daily <<- eurusd_daily %>% select(Datetime, Open, High, Low, Close)
  eurusd_daily <<- rbind(eurusd_daily, eurusd_daily_add)
  eurusd_daily <<- eurusd_daily[!duplicated(eurusd_daily),]
  rownames(eurusd_daily) <<- NULL
  writeLines("Successfully add EURUSD Data")
  
  usdcad_daily_add <- NULL
  if(diff_last_date > 30){
    usdcad_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/usd-cad-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    usdcad_daily_add <- read_html("https://www.investing.com/currencies/usd-cad-historical-data") %>% html_table()
  }
  usdcad_daily_add <- as.data.frame(usdcad_daily_add[[2]])
  usdcad_daily_add <- usdcad_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(usdcad_daily_add)[c(1,5)] <- c("Datetime","Close")
  usdcad_daily_add$Datetime <- as.Date(usdcad_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  usdcad_daily_add$Open <- as.numeric(str_replace(usdcad_daily_add$Open,",",""))
  usdcad_daily_add$High <- as.numeric(str_replace(usdcad_daily_add$High,",",""))
  usdcad_daily_add$Low <- as.numeric(str_replace(usdcad_daily_add$Low,",",""))
  usdcad_daily_add$Close <- as.numeric(str_replace(usdcad_daily_add$Close,",",""))
  usdcad_daily_add <- usdcad_daily_add %>% arrange(Datetime)
  
  usdcad_daily <<- usdcad_daily %>% select(Datetime, Open, High, Low, Close)
  usdcad_daily <<- rbind(usdcad_daily, usdcad_daily_add)
  usdcad_daily <<- usdcad_daily[!duplicated(usdcad_daily),]
  rownames(usdcad_daily) <<- NULL
  writeLines("Successfully add USDCAD Data")
  
  audusd_daily_add <- NULL
  if(diff_last_date > 30){
    audusd_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/aud-usd-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    audusd_daily_add <- read_html("https://www.investing.com/currencies/aud-usd-historical-data") %>% html_table()
  }
  audusd_daily_add <- as.data.frame(audusd_daily_add[[2]])
  audusd_daily_add <- audusd_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(audusd_daily_add)[c(1,5)] <- c("Datetime","Close")
  audusd_daily_add$Datetime <- as.Date(audusd_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  audusd_daily_add$Open <- as.numeric(str_replace(audusd_daily_add$Open,",",""))
  audusd_daily_add$High <- as.numeric(str_replace(audusd_daily_add$High,",",""))
  audusd_daily_add$Low <- as.numeric(str_replace(audusd_daily_add$Low,",",""))
  audusd_daily_add$Close <- as.numeric(str_replace(audusd_daily_add$Close,",",""))
  audusd_daily_add <- audusd_daily_add %>% arrange(Datetime)
  
  audusd_daily <<- audusd_daily %>% select(Datetime, Open, High, Low, Close)
  audusd_daily <<- rbind(audusd_daily, audusd_daily_add)
  audusd_daily <<- audusd_daily[!duplicated(audusd_daily),]
  rownames(audusd_daily) <<- NULL
  writeLines("Successfully add AUDUSD Data")
  
  usdjpy_daily_add <- NULL
  if(diff_last_date > 30){
    usdjpy_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/usd-jpy-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    usdjpy_daily_add <- read_html("https://www.investing.com/currencies/usd-jpy-historical-data") %>% html_table()
  }
  usdjpy_daily_add <- as.data.frame(usdjpy_daily_add[[2]])
  usdjpy_daily_add <- usdjpy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(usdjpy_daily_add)[c(1,5)] <- c("Datetime","Close")
  usdjpy_daily_add$Datetime <- as.Date(usdjpy_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  usdjpy_daily_add$Open <- as.numeric(str_replace(usdjpy_daily_add$Open,",",""))
  usdjpy_daily_add$High <- as.numeric(str_replace(usdjpy_daily_add$High,",",""))
  usdjpy_daily_add$Low <- as.numeric(str_replace(usdjpy_daily_add$Low,",",""))
  usdjpy_daily_add$Close <- as.numeric(str_replace(usdjpy_daily_add$Close,",",""))
  usdjpy_daily_add <- usdjpy_daily_add %>% arrange(Datetime)
  
  usdjpy_daily <<- usdjpy_daily %>% select(Datetime, Open, High, Low, Close)
  usdjpy_daily <<- rbind(usdjpy_daily, usdjpy_daily_add)
  usdjpy_daily <<- usdjpy_daily[!duplicated(usdjpy_daily),]
  rownames(usdjpy_daily) <<- NULL
  writeLines("Successfully add USDJPY Data")
  
  audnzd_daily_add <- NULL
  if(diff_last_date > 30){
    audnzd_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/aud-nzd-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    audnzd_daily_add <- read_html("https://www.investing.com/currencies/aud-nzd-historical-data") %>% html_table()
  }
  audnzd_daily_add <- as.data.frame(audnzd_daily_add[[2]])
  audnzd_daily_add <- audnzd_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(audnzd_daily_add)[c(1,5)] <- c("Datetime","Close")
  audnzd_daily_add$Datetime <- as.Date(audnzd_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  audnzd_daily_add$Open <- as.numeric(str_replace(audnzd_daily_add$Open,",",""))
  audnzd_daily_add$High <- as.numeric(str_replace(audnzd_daily_add$High,",",""))
  audnzd_daily_add$Low <- as.numeric(str_replace(audnzd_daily_add$Low,",",""))
  audnzd_daily_add$Close <- as.numeric(str_replace(audnzd_daily_add$Close,",",""))
  audnzd_daily_add <- audnzd_daily_add %>% arrange(Datetime)
  
  audnzd_daily <<- audnzd_daily %>% select(Datetime, Open, High, Low, Close)
  audnzd_daily <<- rbind(audnzd_daily, audnzd_daily_add)
  audnzd_daily <<- audnzd_daily[!duplicated(audnzd_daily),]
  rownames(audnzd_daily) <<- NULL
  writeLines("Successfully add AUDNZD Data")
  
  gbpjpy_daily_add <- NULL
  if(diff_last_date > 30){
    gbpjpy_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/gbp-jpy-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    gbpjpy_daily_add <- read_html("https://www.investing.com/currencies/gbp-jpy-historical-data") %>% html_table()
  }
  gbpjpy_daily_add <- as.data.frame(gbpjpy_daily_add[[2]])
  gbpjpy_daily_add <- gbpjpy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(gbpjpy_daily_add)[c(1,5)] <- c("Datetime","Close")
  gbpjpy_daily_add$Datetime <- as.Date(gbpjpy_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  gbpjpy_daily_add$Open <- as.numeric(str_replace(gbpjpy_daily_add$Open,",",""))
  gbpjpy_daily_add$High <- as.numeric(str_replace(gbpjpy_daily_add$High,",",""))
  gbpjpy_daily_add$Low <- as.numeric(str_replace(gbpjpy_daily_add$Low,",",""))
  gbpjpy_daily_add$Close <- as.numeric(str_replace(gbpjpy_daily_add$Close,",",""))
  gbpjpy_daily_add <- gbpjpy_daily_add %>% arrange(Datetime)
  
  gbpjpy_daily <<- gbpjpy_daily %>% select(Datetime, Open, High, Low, Close)
  gbpjpy_daily <<- rbind(gbpjpy_daily, gbpjpy_daily_add)
  gbpjpy_daily <<- gbpjpy_daily[!duplicated(gbpjpy_daily),]
  rownames(gbpjpy_daily) <<- NULL
  writeLines("Successfully add GBPJPY Data")
  
  euraud_daily_add <- NULL
  if(diff_last_date > 30){
    euraud_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/eur-aud-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    euraud_daily_add <- read_html("https://www.investing.com/currencies/eur-aud-historical-data") %>% html_table()
  }
  
  euraud_daily_add <- as.data.frame(euraud_daily_add[[2]])
  euraud_daily_add <- euraud_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(euraud_daily_add)[c(1,5)] <- c("Datetime","Close")
  euraud_daily_add$Datetime <- as.Date(euraud_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  euraud_daily_add$Open <- as.numeric(str_replace(euraud_daily_add$Open,",",""))
  euraud_daily_add$High <- as.numeric(str_replace(euraud_daily_add$High,",",""))
  euraud_daily_add$Low <- as.numeric(str_replace(euraud_daily_add$Low,",",""))
  euraud_daily_add$Close <- as.numeric(str_replace(euraud_daily_add$Close,",",""))
  euraud_daily_add <- euraud_daily_add %>% arrange(Datetime)
  
  euraud_daily <<- euraud_daily %>% select(Datetime, Open, High, Low, Close)
  euraud_daily <<- rbind(euraud_daily, euraud_daily_add)
  euraud_daily <<- euraud_daily[!duplicated(euraud_daily),]
  rownames(euraud_daily) <<- NULL
  writeLines("Successfully add EURAUD Data")
  
  eurjpy_daily_add <- NULL
  if(diff_last_date > 30){
    eurjpy_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/eur-jpy-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    eurjpy_daily_add <- read_html("https://www.investing.com/currencies/eur-jpy-historical-data") %>% html_table()
  }
  
  eurjpy_daily_add <- as.data.frame(eurjpy_daily_add[[2]])
  eurjpy_daily_add <- eurjpy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(eurjpy_daily_add)[c(1,5)] <- c("Datetime","Close")
  eurjpy_daily_add$Datetime <- as.Date(eurjpy_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  eurjpy_daily_add$Open <- as.numeric(str_replace(eurjpy_daily_add$Open,",",""))
  eurjpy_daily_add$High <- as.numeric(str_replace(eurjpy_daily_add$High,",",""))
  eurjpy_daily_add$Low <- as.numeric(str_replace(eurjpy_daily_add$Low,",",""))
  eurjpy_daily_add$Close <- as.numeric(str_replace(eurjpy_daily_add$Close,",",""))
  eurjpy_daily_add <- eurjpy_daily_add %>% arrange(Datetime)
  
  eurjpy_daily <<- eurjpy_daily %>% select(Datetime, Open, High, Low, Close)
  eurjpy_daily <<- rbind(eurjpy_daily, eurjpy_daily_add)
  eurjpy_daily <<- eurjpy_daily[!duplicated(eurjpy_daily),]
  rownames(eurjpy_daily) <<- NULL
  writeLines("Successfully add EURJPY Data")
  
  cadchf_daily_add <- NULL
  if(diff_last_date > 30){
    cadchf_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/cad-chf-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    cadchf_daily_add <- read_html("https://www.investing.com/currencies/cad-chf-historical-data") %>% html_table()
  }
  cadchf_daily_add <- as.data.frame(cadchf_daily_add[[2]])
  cadchf_daily_add <- cadchf_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(cadchf_daily_add)[c(1,5)] <- c("Datetime","Close")
  cadchf_daily_add$Datetime <- as.Date(cadchf_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  cadchf_daily_add$Open <- as.numeric(str_replace(cadchf_daily_add$Open,",",""))
  cadchf_daily_add$High <- as.numeric(str_replace(cadchf_daily_add$High,",",""))
  cadchf_daily_add$Low <- as.numeric(str_replace(cadchf_daily_add$Low,",",""))
  cadchf_daily_add$Close <- as.numeric(str_replace(cadchf_daily_add$Close,",",""))
  cadchf_daily_add <- cadchf_daily_add %>% arrange(Datetime)
  
  cadchf_daily <<- cadchf_daily %>% select(Datetime, Open, High, Low, Close)
  cadchf_daily <<- rbind(cadchf_daily, cadchf_daily_add)
  cadchf_daily <<- cadchf_daily[!duplicated(cadchf_daily),]
  rownames(cadchf_daily) <<- NULL
  writeLines("Successfully add CADCHF Data")
  
  audjpy_daily_add <- NULL
  if(diff_last_date > 30){
    audjpy_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/aud-jpy-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    audjpy_daily_add <- read_html("https://www.investing.com/currencies/aud-jpy-historical-data") %>% html_table()
  }
  audjpy_daily_add <- as.data.frame(audjpy_daily_add[[2]])
  audjpy_daily_add <- audjpy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(audjpy_daily_add)[c(1,5)] <- c("Datetime","Close")
  audjpy_daily_add$Datetime <- as.Date(audjpy_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  audjpy_daily_add$Open <- as.numeric(str_replace(audjpy_daily_add$Open,",",""))
  audjpy_daily_add$High <- as.numeric(str_replace(audjpy_daily_add$High,",",""))
  audjpy_daily_add$Low <- as.numeric(str_replace(audjpy_daily_add$Low,",",""))
  audjpy_daily_add$Close <- as.numeric(str_replace(audjpy_daily_add$Close,",",""))
  audjpy_daily_add <- audjpy_daily_add %>% arrange(Datetime)
  
  audjpy_daily <<- audjpy_daily %>% select(Datetime, Open, High, Low, Close)
  audjpy_daily <<- rbind(audjpy_daily, audjpy_daily_add)
  audjpy_daily <<- audjpy_daily[!duplicated(audjpy_daily),]
  rownames(audjpy_daily) <<- NULL
  writeLines("Successfully add AUDJPY Data")
  
  eurgbp_daily_add <- NULL
  if(diff_last_date > 30){
    eurgbp_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/eur-gbp-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    eurgbp_daily_add <- read_html("https://www.investing.com/currencies/eur-gbp-historical-data") %>% html_table()
  }
  eurgbp_daily_add <- as.data.frame(eurgbp_daily_add[[2]])
  eurgbp_daily_add <- eurgbp_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(eurgbp_daily_add)[c(1,5)] <- c("Datetime","Close")
  eurgbp_daily_add$Datetime <- as.Date(eurgbp_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  eurgbp_daily_add$Open <- as.numeric(str_replace(eurgbp_daily_add$Open,",",""))
  eurgbp_daily_add$High <- as.numeric(str_replace(eurgbp_daily_add$High,",",""))
  eurgbp_daily_add$Low <- as.numeric(str_replace(eurgbp_daily_add$Low,",",""))
  eurgbp_daily_add$Close <- as.numeric(str_replace(eurgbp_daily_add$Close,",",""))
  eurgbp_daily_add <- eurgbp_daily_add %>% arrange(Datetime)
  
  eurgbp_daily <<- eurgbp_daily %>% select(Datetime, Open, High, Low, Close)
  eurgbp_daily <<- rbind(eurgbp_daily, eurgbp_daily_add)
  eurgbp_daily <<- eurgbp_daily[!duplicated(eurgbp_daily),]
  rownames(eurgbp_daily) <<- NULL
  writeLines("Successfully add EURGBP Data")
  
  eurchf_daily_add <- NULL
  if(diff_last_date > 30){
    eurchf_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/eur-chf-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    eurchf_daily_add <- read_html("https://www.investing.com/currencies/eur-chf-historical-data") %>% html_table()
  }
  eurchf_daily_add <- as.data.frame(eurchf_daily_add[[2]])
  eurchf_daily_add <- eurchf_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(eurchf_daily_add)[c(1,5)] <- c("Datetime","Close")
  eurchf_daily_add$Datetime <- as.Date(eurchf_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  eurchf_daily_add$Open <- as.numeric(str_replace(eurchf_daily_add$Open,",",""))
  eurchf_daily_add$High <- as.numeric(str_replace(eurchf_daily_add$High,",",""))
  eurchf_daily_add$Low <- as.numeric(str_replace(eurchf_daily_add$Low,",",""))
  eurchf_daily_add$Close <- as.numeric(str_replace(eurchf_daily_add$Close,",",""))
  eurchf_daily_add <- eurchf_daily_add %>% arrange(Datetime)
  
  eurchf_daily <<- eurchf_daily %>% select(Datetime, Open, High, Low, Close)
  eurchf_daily <<- rbind(eurchf_daily, eurchf_daily_add)
  eurchf_daily <<- eurchf_daily[!duplicated(eurchf_daily),]
  rownames(eurchf_daily) <<- NULL
  writeLines("Successfully add EURCHF Data")
  
  gbpchf_daily_add <- NULL
  if(diff_last_date > 30){
    gbpchf_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/gbp-chf-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    gbpchf_daily_add <- read_html("https://www.investing.com/currencies/gbp-chf-historical-data") %>% html_table()
  }
  gbpchf_daily_add <- as.data.frame(gbpchf_daily_add[[2]])
  gbpchf_daily_add <- gbpchf_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(gbpchf_daily_add)[c(1,5)] <- c("Datetime","Close")
  gbpchf_daily_add$Datetime <- as.Date(gbpchf_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  gbpchf_daily_add$Open <- as.numeric(str_replace(gbpchf_daily_add$Open,",",""))
  gbpchf_daily_add$High <- as.numeric(str_replace(gbpchf_daily_add$High,",",""))
  gbpchf_daily_add$Low <- as.numeric(str_replace(gbpchf_daily_add$Low,",",""))
  gbpchf_daily_add$Close <- as.numeric(str_replace(gbpchf_daily_add$Close,",",""))
  gbpchf_daily_add <- gbpchf_daily_add %>% arrange(Datetime)
  
  gbpchf_daily <<- gbpchf_daily %>% select(Datetime, Open, High, Low, Close)
  gbpchf_daily <<- rbind(gbpchf_daily, gbpchf_daily_add)
  gbpchf_daily <<- gbpchf_daily[!duplicated(gbpchf_daily),]
  rownames(gbpchf_daily) <<- NULL
  writeLines("Successfully add GBPCHF Data")
  
  cadjpy_daily_add <- NULL
  if(diff_last_date > 30){
    cadjpy_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/cad-jpy-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    cadjpy_daily_add <- read_html("https://www.investing.com/currencies/cad-jpy-historical-data") %>% html_table()
  }
  cadjpy_daily_add <- as.data.frame(cadjpy_daily_add[[2]])
  cadjpy_daily_add <- cadjpy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(cadjpy_daily_add)[c(1,5)] <- c("Datetime","Close")
  cadjpy_daily_add$Datetime <- as.Date(cadjpy_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  cadjpy_daily_add$Open <- as.numeric(str_replace(cadjpy_daily_add$Open,",",""))
  cadjpy_daily_add$High <- as.numeric(str_replace(cadjpy_daily_add$High,",",""))
  cadjpy_daily_add$Low <- as.numeric(str_replace(cadjpy_daily_add$Low,",",""))
  cadjpy_daily_add$Close <- as.numeric(str_replace(cadjpy_daily_add$Close,",",""))
  cadjpy_daily_add <- cadjpy_daily_add %>% arrange(Datetime)
  
  cadjpy_daily <<- cadjpy_daily %>% select(Datetime, Open, High, Low, Close)
  cadjpy_daily <<- rbind(cadjpy_daily, cadjpy_daily_add)
  cadjpy_daily <<- cadjpy_daily[!duplicated(cadjpy_daily),]
  rownames(cadjpy_daily) <<- NULL
  writeLines("Successfully add CADJPY Data")
  
  audcad_daily_add <- NULL
  if(diff_last_date > 30){
    audcad_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/aud-cad-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    audcad_daily_add <- read_html("https://www.investing.com/currencies/aud-cad-historical-data") %>% html_table()
  }
  audcad_daily_add <- as.data.frame(audcad_daily_add[[2]])
  audcad_daily_add <- audcad_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(audcad_daily_add)[c(1,5)] <- c("Datetime","Close")
  audcad_daily_add$Datetime <- as.Date(audcad_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  audcad_daily_add$Open <- as.numeric(str_replace(audcad_daily_add$Open,",",""))
  audcad_daily_add$High <- as.numeric(str_replace(audcad_daily_add$High,",",""))
  audcad_daily_add$Low <- as.numeric(str_replace(audcad_daily_add$Low,",",""))
  audcad_daily_add$Close <- as.numeric(str_replace(audcad_daily_add$Close,",",""))
  audcad_daily_add <- audcad_daily_add %>% arrange(Datetime)
  
  audcad_daily <<- audcad_daily %>% select(Datetime, Open, High, Low, Close)
  audcad_daily <<- rbind(audcad_daily, audcad_daily_add)
  audcad_daily <<- audcad_daily[!duplicated(audcad_daily),]
  rownames(audcad_daily) <<- NULL
  writeLines("Successfully add AUDCAD Data")
  
  audchf_daily_add <- NULL
  if(diff_last_date > 30){
    audchf_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/aud-chf-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    audchf_daily_add <- read_html("https://www.investing.com/currencies/aud-chf-historical-data") %>% html_table()
  }
  audchf_daily_add <- as.data.frame(audchf_daily_add[[2]])
  audchf_daily_add <- audchf_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(audchf_daily_add)[c(1,5)] <- c("Datetime","Close")
  audchf_daily_add$Datetime <- as.Date(audchf_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  audchf_daily_add$Open <- as.numeric(str_replace(audchf_daily_add$Open,",",""))
  audchf_daily_add$High <- as.numeric(str_replace(audchf_daily_add$High,",",""))
  audchf_daily_add$Low <- as.numeric(str_replace(audchf_daily_add$Low,",",""))
  audchf_daily_add$Close <- as.numeric(str_replace(audchf_daily_add$Close,",",""))
  audchf_daily_add <- audchf_daily_add %>% arrange(Datetime)
  
  audchf_daily <<- audchf_daily %>% select(Datetime, Open, High, Low, Close)
  audchf_daily <<- rbind(audchf_daily, audchf_daily_add)
  audchf_daily <<- audchf_daily[!duplicated(audchf_daily),]
  rownames(audchf_daily) <<- NULL
  writeLines("Successfully add AUDCHF Data")
  
  chfjpy_daily_add <- NULL
  if(diff_last_date > 30){
    chfjpy_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/chf-jpy-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    chfjpy_daily_add <- read_html("https://www.investing.com/currencies/chf-jpy-historical-data") %>% html_table()
  }
  
  chfjpy_daily_add <- as.data.frame(chfjpy_daily_add[[2]])
  chfjpy_daily_add <- chfjpy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(chfjpy_daily_add)[c(1,5)] <- c("Datetime","Close")
  chfjpy_daily_add$Datetime <- as.Date(chfjpy_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  chfjpy_daily_add$Open <- as.numeric(str_replace(chfjpy_daily_add$Open,",",""))
  chfjpy_daily_add$High <- as.numeric(str_replace(chfjpy_daily_add$High,",",""))
  chfjpy_daily_add$Low <- as.numeric(str_replace(chfjpy_daily_add$Low,",",""))
  chfjpy_daily_add$Close <- as.numeric(str_replace(chfjpy_daily_add$Close,",",""))
  chfjpy_daily_add <- chfjpy_daily_add %>% arrange(Datetime)
  
  chfjpy_daily <<- chfjpy_daily %>% select(Datetime, Open, High, Low, Close)
  chfjpy_daily <<- rbind(chfjpy_daily, chfjpy_daily_add)
  chfjpy_daily <<- chfjpy_daily[!duplicated(chfjpy_daily),]
  rownames(chfjpy_daily) <<- NULL
  writeLines("Successfully add CHFJPY Data")
  
  eurnzd_daily_add <- NULL
  if(diff_last_date > 30){
    eurnzd_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/eur-nzd-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    eurnzd_daily_add <- read_html("https://www.investing.com/currencies/eur-nzd-historical-data") %>% html_table()
  }
  
  eurnzd_daily_add <- as.data.frame(eurnzd_daily_add[[2]])
  eurnzd_daily_add <- eurnzd_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(eurnzd_daily_add)[c(1,5)] <- c("Datetime","Close")
  eurnzd_daily_add$Datetime <- as.Date(eurnzd_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  eurnzd_daily_add$Open <- as.numeric(str_replace(eurnzd_daily_add$Open,",",""))
  eurnzd_daily_add$High <- as.numeric(str_replace(eurnzd_daily_add$High,",",""))
  eurnzd_daily_add$Low <- as.numeric(str_replace(eurnzd_daily_add$Low,",",""))
  eurnzd_daily_add$Close <- as.numeric(str_replace(eurnzd_daily_add$Close,",",""))
  eurnzd_daily_add <- eurnzd_daily_add %>% arrange(Datetime)
  
  eurnzd_daily <<- eurnzd_daily %>% select(Datetime, Open, High, Low, Close)
  eurnzd_daily <<- rbind(eurnzd_daily, eurnzd_daily_add)
  eurnzd_daily <<- eurnzd_daily[!duplicated(eurnzd_daily),]
  rownames(eurnzd_daily) <<- NULL
  writeLines("Successfully add EURNZD Data")
  
  eurcad_daily_add <- NULL
  if(diff_last_date > 30){
    eurcad_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/eur-cad-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    eurcad_daily_add <- read_html("https://www.investing.com/currencies/eur-cad-historical-data") %>% html_table()
  }
  eurcad_daily_add <- as.data.frame(eurcad_daily_add[[2]])
  eurcad_daily_add <- eurcad_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(eurcad_daily_add)[c(1,5)] <- c("Datetime","Close")
  eurcad_daily_add$Datetime <- as.Date(eurcad_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  eurcad_daily_add$Open <- as.numeric(str_replace(eurcad_daily_add$Open,",",""))
  eurcad_daily_add$High <- as.numeric(str_replace(eurcad_daily_add$High,",",""))
  eurcad_daily_add$Low <- as.numeric(str_replace(eurcad_daily_add$Low,",",""))
  eurcad_daily_add$Close <- as.numeric(str_replace(eurcad_daily_add$Close,",",""))
  eurcad_daily_add <- eurcad_daily_add %>% arrange(Datetime)
  
  eurcad_daily <<- eurcad_daily %>% select(Datetime, Open, High, Low, Close)
  eurcad_daily <<- rbind(eurcad_daily, eurcad_daily_add)
  eurcad_daily <<- eurcad_daily[!duplicated(eurcad_daily),]
  rownames(eurcad_daily) <<- NULL
  writeLines("Successfully add EURCAD Data")
  
  nzdjpy_daily_add <- NULL
  if(diff_last_date > 30){
    nzdjpy_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/nzd-jpy-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    nzdjpy_daily_add <- read_html("https://www.investing.com/currencies/nzd-jpy-historical-data") %>% html_table()
  }
  nzdjpy_daily_add <- as.data.frame(nzdjpy_daily_add[[2]])
  nzdjpy_daily_add <- nzdjpy_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(nzdjpy_daily_add)[c(1,5)] <- c("Datetime","Close")
  nzdjpy_daily_add$Datetime <- as.Date(nzdjpy_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  nzdjpy_daily_add$Open <- as.numeric(str_replace(nzdjpy_daily_add$Open,",",""))
  nzdjpy_daily_add$High <- as.numeric(str_replace(nzdjpy_daily_add$High,",",""))
  nzdjpy_daily_add$Low <- as.numeric(str_replace(nzdjpy_daily_add$Low,",",""))
  nzdjpy_daily_add$Close <- as.numeric(str_replace(nzdjpy_daily_add$Close,",",""))
  nzdjpy_daily_add <- nzdjpy_daily_add %>% arrange(Datetime)
  
  nzdjpy_daily <<- nzdjpy_daily %>% select(Datetime, Open, High, Low, Close)
  nzdjpy_daily <<- rbind(nzdjpy_daily, nzdjpy_daily_add)
  nzdjpy_daily <<- nzdjpy_daily[!duplicated(nzdjpy_daily),]
  rownames(nzdjpy_daily) <<- NULL
  writeLines("Successfully add NZDJPY Data")
  
  nzdusd_daily_add <- NULL
  if(diff_last_date > 30){
    nzdusd_daily_add <- investing_com_data_correction_1("https://www.investing.com/currencies/nzd-usd-historical-data",
                                                        start_date = last_date_data, chromever = "108.0.5359.71", 
                                                        data_waiting_time = 10, timeframe_select = "daily",
                                                        extract_directly = TRUE)
  }else{
    nzdusd_daily_add <- read_html("https://www.investing.com/currencies/nzd-usd-historical-data") %>% html_table()
  }
  nzdusd_daily_add <- as.data.frame(nzdusd_daily_add[[2]])
  nzdusd_daily_add <- nzdusd_daily_add %>% select(Date, Open, High, Low, Price)
  colnames(nzdusd_daily_add)[c(1,5)] <- c("Datetime","Close")
  nzdusd_daily_add$Datetime <- as.Date(nzdusd_daily_add$Datetime, tryFormats = c("%m/%d/%Y"))
  nzdusd_daily_add$Open <- as.numeric(str_replace(nzdusd_daily_add$Open,",",""))
  nzdusd_daily_add$High <- as.numeric(str_replace(nzdusd_daily_add$High,",",""))
  nzdusd_daily_add$Low <- as.numeric(str_replace(nzdusd_daily_add$Low,",",""))
  nzdusd_daily_add$Close <- as.numeric(str_replace(nzdusd_daily_add$Close,",",""))
  nzdusd_daily_add <- nzdusd_daily_add %>% arrange(Datetime)
  
  nzdusd_daily <<- nzdusd_daily %>% select(Datetime, Open, High, Low, Close)
  nzdusd_daily <<- rbind(nzdusd_daily, nzdusd_daily_add)
  nzdusd_daily <<- nzdusd_daily[!duplicated(nzdusd_daily),]
  rownames(nzdusd_daily) <<- NULL
  writeLines("Successfully add NZDUSD Data")
  
  #filter unwanted data
  audcad_daily <<- audcad_daily %>% filter(Datetime < Sys.Date())
  audchf_daily <<- audchf_daily %>% filter(Datetime < Sys.Date())
  audjpy_daily <<- audjpy_daily %>% filter(Datetime < Sys.Date())
  audnzd_daily <<- audnzd_daily %>% filter(Datetime < Sys.Date())
  audusd_daily <<- audusd_daily %>% filter(Datetime < Sys.Date())
  cadchf_daily <<- cadchf_daily %>% filter(Datetime < Sys.Date())
  cadjpy_daily <<- cadjpy_daily %>% filter(Datetime < Sys.Date())
  chfjpy_daily <<- chfjpy_daily %>% filter(Datetime < Sys.Date())
  dxy_daily <<- dxy_daily %>% filter(Datetime < Sys.Date())
  euraud_daily <<- euraud_daily %>% filter(Datetime < Sys.Date())
  eurcad_daily <<- eurcad_daily %>% filter(Datetime < Sys.Date())
  eurchf_daily <<- eurchf_daily %>% filter(Datetime < Sys.Date())
  eurgbp_daily <<- eurgbp_daily %>% filter(Datetime < Sys.Date())
  eurjpy_daily <<- eurjpy_daily %>% filter(Datetime < Sys.Date())
  eurnzd_daily <<- eurnzd_daily %>% filter(Datetime < Sys.Date())
  eurusd_daily <<- eurusd_daily %>% filter(Datetime < Sys.Date())
  gbpchf_daily <<- gbpchf_daily %>% filter(Datetime < Sys.Date())
  gbpjpy_daily <<- gbpjpy_daily %>% filter(Datetime < Sys.Date())
  gbpusd_daily <<- gbpusd_daily %>% filter(Datetime < Sys.Date())
  nzdusd_daily <<- nzdusd_daily %>% filter(Datetime < Sys.Date())
  nzdjpy_daily <<- nzdjpy_daily %>% filter(Datetime < Sys.Date())
  usdcad_daily <<- usdcad_daily %>% filter(Datetime < Sys.Date())
  usdchf_daily <<- usdchf_daily %>% filter(Datetime < Sys.Date())
  usdjpy_daily <<- usdjpy_daily %>% filter(Datetime < Sys.Date())
  xauusd_daily <<- xauusd_daily %>% filter(Datetime < Sys.Date())
  
  audcad_daily <<- audcad_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  audchf_daily <<- audchf_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  audjpy_daily <<- audjpy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  audnzd_daily <<- audnzd_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  audusd_daily <<- audusd_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  cadchf_daily <<- cadchf_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  cadjpy_daily <<- cadjpy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  chfjpy_daily <<- chfjpy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  dxy_daily <<- dxy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  euraud_daily <<- euraud_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  eurcad_daily <<- eurcad_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  eurchf_daily <<- eurchf_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  eurgbp_daily <<- eurgbp_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  eurjpy_daily <<- eurjpy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  eurnzd_daily <<- eurnzd_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  eurusd_daily <<- eurusd_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  gbpchf_daily <<- gbpchf_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  gbpjpy_daily <<- gbpjpy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  gbpusd_daily <<- gbpusd_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  nzdusd_daily <<- nzdusd_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  nzdjpy_daily <<- nzdjpy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  usdcad_daily <<- usdcad_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  usdchf_daily <<- usdchf_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  usdjpy_daily <<- usdjpy_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  xauusd_daily <<- xauusd_daily %>% mutate(weekdays = weekdays(Datetime)) %>% filter(!weekdays %in% c("Saturday","Sunday")) %>% select(-weekdays)
  
  audcad_multi_datetime <- table(audcad_daily$Datetime)
  if(any(audcad_multi_datetime > 1)){
    problem <- names(audcad_multi_datetime)[which(audcad_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(audcad_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        audcad_daily <<- audcad_daily[-rows_exist[y],] #keep last row exist
        rownames(audcad_daily) <<- NULL
      }
    }
  }
  
  audusd_multi_datetime <- table(audusd_daily$Datetime)
  if(any(audusd_multi_datetime > 1)){
    problem <- names(audusd_multi_datetime)[which(audusd_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(audusd_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        audusd_daily <<- audusd_daily[-rows_exist[y],] #keep last row exist
        rownames(audusd_daily) <<- NULL
      }
    }
  }
  
  audchf_multi_datetime <- table(audchf_daily$Datetime)
  if(any(audchf_multi_datetime > 1)){
    problem <- names(audchf_multi_datetime)[which(audchf_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(audchf_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        audchf_daily <<- audchf_daily[-rows_exist[y],] #keep last row exist
        rownames(audchf_daily) <<- NULL
      }
    }
  }
  
  audjpy_multi_datetime <- table(audjpy_daily$Datetime)
  if(any(audjpy_multi_datetime > 1)){
    problem <- names(audjpy_multi_datetime)[which(audjpy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(audjpy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        audjpy_daily <<- audjpy_daily[-rows_exist[y],] #keep last row exist
        rownames(audjpy_daily) <<- NULL
      }
    }
  }
  
  audnzd_multi_datetime <- table(audnzd_daily$Datetime)
  if(any(audnzd_multi_datetime > 1)){
    problem <- names(audnzd_multi_datetime)[which(audnzd_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(audnzd_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        audnzd_daily <<- audnzd_daily[-rows_exist[y],] #keep last row exist
        rownames(audnzd_daily) <<- NULL
      }
    }
  }
  
  cadchf_multi_datetime <- table(cadchf_daily$Datetime)
  if(any(cadchf_multi_datetime > 1)){
    problem <- names(cadchf_multi_datetime)[which(cadchf_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(cadchf_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        cadchf_daily <<- cadchf_daily[-rows_exist[y],] #keep last row exist
        rownames(cadchf_daily) <<- NULL
      }
    }
  }
  
  cadjpy_multi_datetime <- table(cadjpy_daily$Datetime)
  if(any(cadjpy_multi_datetime > 1)){
    problem <- names(cadjpy_multi_datetime)[which(cadjpy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(cadjpy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        cadjpy_daily <<- cadjpy_daily[-rows_exist[y],] #keep last row exist
        rownames(cadjpy_daily) <<- NULL
      }
    }
  }
  
  chfjpy_multi_datetime <- table(chfjpy_daily$Datetime)
  if(any(chfjpy_multi_datetime > 1)){
    problem <- names(chfjpy_multi_datetime)[which(chfjpy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(chfjpy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        chfjpy_daily <<- chfjpy_daily[-rows_exist[y],] #keep last row exist
        rownames(chfjpy_daily) <<- NULL
      }
    }
  }
  
  dxy_multi_datetime <- table(dxy_daily$Datetime)
  if(any(dxy_multi_datetime > 1)){
    problem <- names(dxy_multi_datetime)[which(dxy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(dxy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        dxy_daily <<- dxy_daily[-rows_exist[y],] #keep last row exist
        rownames(dxy_daily) <<- NULL
      }
    }
  }
  
  euraud_multi_datetime <- table(euraud_daily$Datetime)
  if(any(euraud_multi_datetime > 1)){
    problem <- names(euraud_multi_datetime)[which(euraud_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(euraud_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        euraud_daily <<- euraud_daily[-rows_exist[y],] #keep last row exist
        rownames(euraud_daily) <<- NULL
      }
    }
  }
  
  eurcad_multi_datetime <- table(eurcad_daily$Datetime)
  if(any(eurcad_multi_datetime > 1)){
    problem <- names(eurcad_multi_datetime)[which(eurcad_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(eurcad_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        eurcad_daily <<- eurcad_daily[-rows_exist[y],] #keep last row exist
        rownames(eurcad_daily) <<- NULL
      }
    }
  }
  
  eurchf_multi_datetime <- table(eurchf_daily$Datetime)
  if(any(eurchf_multi_datetime > 1)){
    problem <- names(eurchf_multi_datetime)[which(eurchf_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(eurchf_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        eurchf_daily <<- eurchf_daily[-rows_exist[y],] #keep last row exist
        rownames(eurchf_daily) <<- NULL
      }
    }
  }
  eurgbp_multi_datetime <- table(eurgbp_daily$Datetime)
  if(any(eurgbp_multi_datetime > 1)){
    problem <- names(eurgbp_multi_datetime)[which(eurgbp_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(eurgbp_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        eurgbp_daily <<- eurgbp_daily[-rows_exist[y],] #keep last row exist
        rownames(eurgbp_daily) <<- NULL
      }
    }
  }
  
  eurjpy_multi_datetime <- table(eurjpy_daily$Datetime)
  if(any(eurjpy_multi_datetime > 1)){
    problem <- names(eurjpy_multi_datetime)[which(eurjpy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(eurjpy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        eurjpy_daily <<- eurjpy_daily[-rows_exist[y],] #keep last row exist
        rownames(eurjpy_daily) <<- NULL
      }
    }
  }
  
  eurnzd_multi_datetime <- table(eurnzd_daily$Datetime)
  if(any(eurnzd_multi_datetime > 1)){
    problem <- names(eurnzd_multi_datetime)[which(eurnzd_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(eurnzd_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        eurnzd_daily <<- eurnzd_daily[-rows_exist[y],] #keep last row exist
        rownames(eurnzd_daily) <<- NULL
      }
    }
  }
  
  eurusd_multi_datetime <- table(eurusd_daily$Datetime)
  if(any(eurusd_multi_datetime > 1)){
    problem <- names(eurusd_multi_datetime)[which(eurusd_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(eurusd_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        eurusd_daily <<- eurusd_daily[-rows_exist[y],] #keep last row exist
        rownames(eurusd_daily) <<- NULL
      }
    }
  }
  
  gbpchf_multi_datetime <- table(gbpchf_daily$Datetime)
  if(any(gbpchf_multi_datetime > 1)){
    problem <- names(gbpchf_multi_datetime)[which(gbpchf_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(gbpchf_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        gbpchf_daily <<- gbpchf_daily[-rows_exist[y],] #keep last row exist
        rownames(gbpchf_daily) <<- NULL
      }
    }
  }
  
  gbpjpy_multi_datetime <- table(gbpjpy_daily$Datetime)
  if(any(gbpjpy_multi_datetime > 1)){
    problem <- names(gbpjpy_multi_datetime)[which(gbpjpy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(gbpjpy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        gbpjpy_daily <<- gbpjpy_daily[-rows_exist[y],] #keep last row exist
        rownames(gbpjpy_daily) <<- NULL
      }
    }
  }
  
  gbpusd_multi_datetime <- table(gbpusd_daily$Datetime)
  if(any(gbpusd_multi_datetime > 1)){
    problem <- names(gbpusd_multi_datetime)[which(gbpusd_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(gbpusd_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        gbpusd_daily <<- gbpusd_daily[-rows_exist[y],] #keep last row exist
        rownames(gbpusd_daily) <<- NULL
      }
    }
  }
  
  nzdjpy_multi_datetime <- table(nzdjpy_daily$Datetime)
  if(any(nzdjpy_multi_datetime > 1)){
    problem <- names(nzdjpy_multi_datetime)[which(nzdjpy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(nzdjpy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        nzdjpy_daily <<- nzdjpy_daily[-rows_exist[y],] #keep last row exist
        rownames(nzdjpy_daily) <<- NULL
      }
    }
  }
  
  nzdusd_multi_datetime <- table(nzdusd_daily$Datetime)
  if(any(nzdusd_multi_datetime > 1)){
    problem <- names(nzdusd_multi_datetime)[which(nzdusd_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(nzdusd_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        nzdusd_daily <<- nzdusd_daily[-rows_exist[y],] #keep last row exist
        rownames(nzdusd_daily) <<- NULL
      }
    }
  }
  
  usdcad_multi_datetime <- table(usdcad_daily$Datetime)
  if(any(usdcad_multi_datetime > 1)){
    problem <- names(usdcad_multi_datetime)[which(usdcad_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(usdcad_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        usdcad_daily <<- usdcad_daily[-rows_exist[y],] #keep last row exist
        rownames(usdcad_daily) <<- NULL
      }
    }
  }
  
  usdchf_multi_datetime <- table(usdchf_daily$Datetime)
  if(any(usdchf_multi_datetime > 1)){
    problem <- names(usdchf_multi_datetime)[which(usdchf_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(usdchf_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        usdchf_daily <<- usdchf_daily[-rows_exist[y],] #keep last row exist
        rownames(usdchf_daily) <<- NULL
      }
    }
  }
  
  usdjpy_multi_datetime <- table(usdjpy_daily$Datetime)
  if(any(usdjpy_multi_datetime > 1)){
    problem <- names(usdjpy_multi_datetime)[which(usdjpy_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(usdjpy_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        usdjpy_daily <<- usdjpy_daily[-rows_exist[y],] #keep last row exist
        rownames(usdjpy_daily) <<- NULL
      }
    }
  }
  
  xauusd_multi_datetime <- table(xauusd_daily$Datetime)
  if(any(xauusd_multi_datetime > 1)){
    problem <- names(xauusd_multi_datetime)[which(xauusd_multi_datetime > 1)]
    for(x in 1:length(problem)){
      rows_exist <- which(xauusd_daily$Datetime == as.Date(problem[x]))
      for(y in 1:(length(rows_exist) - 1)){
        xauusd_daily <<- xauusd_daily[-rows_exist[y],] #keep last row exist
        rownames(xauusd_daily) <<- NULL
      }
    }
  }
  
  writeLines("Updating Pivot Vectors of the latest data!")
  #----------------------------------------- UPDATE PP, VECTOR, AND PRECEDENCE VARIABLE FROM NEW DATA ----------------
  #----------------------------------------- 1) AUDUSD ---------------------------------------------------
  if(pp_overwrite){
    audusd_daily_pp <<- pivot_point_trading(audusd_daily, asset_name = "AUDUSD", save_value = T)
    audusd_daily_vector <<- pivot_point_vector(audusd_daily_pp, vector_type = "nearest_diff")
    audusd_daily_vector$vector_check <<- suppressWarnings(
      audusd_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audusd_daily_vector$weekday_trade <<- weekdays(audusd_daily_vector$Datetime)
    range_daily <- audusd_daily$High - audusd_daily$Low
    audusd_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    audusd_daily_vector <<- vector_clean_same_oc(audusd_daily_vector, see_results = F)
    writeLines("Successfully Updating AUDUSD PP Data")
    audusd_pre1 <- precedence_sequences(audusd_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audusd_pre1 <- pivot_precedence_break(audusd_pre1)
    audusd_prevar <<- as.data.frame(cbind(WK = audusd_daily_vector$weekday_trade[7:nrow(audusd_daily_vector)], 
                                          FIB = audusd_daily_vector$FIB_GAP[7:nrow(audusd_daily_vector)], 
                                          RNG = audusd_daily_vector$prev_range[7:nrow(audusd_daily_vector)],
                                          CPR = audusd_daily_vector$CPR_GAP[7:nrow(audusd_daily_vector)],
                                          audusd_pre1))
    audusd_prevar <<- quantile_cut_df_num(audusd_prevar)
    audusd_prevar <<- audusd_prevar %>% mutate_if(is.character, as.factor) 
    audusd_prevar$Datetime <<- audusd_daily_vector$Datetime[7:nrow(audusd_daily_vector)]
    writeLines("Successfully Updated AUDUSD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- audusd_daily_pp$Datetime[nrow(audusd_daily_pp)]
    audusd_daily_pp_add <- pivot_point_trading(audusd_daily[which(audusd_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "AUDUSD", save_value = T)
    audusd_daily_pp <<- rbind(audusd_daily_pp, audusd_daily_pp_add[-1,])
    audusd_daily_vector_add <- pivot_point_vector(audusd_daily_pp_add, vector_type = "nearest_diff")
    audusd_daily_vector_add$vector_check <- suppressWarnings(
      audusd_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audusd_daily_vector_add$weekday_trade <- weekdays(audusd_daily_vector_add$Datetime)
    range_daily <- audusd_daily$High[which(audusd_daily$Datetime >= last_pp_datetime)] - audusd_daily$Low[which(audusd_daily$Datetime >= last_pp_datetime)]
    audusd_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    audusd_daily_vector_add <- vector_clean_same_oc(audusd_daily_vector_add, see_results = F)
    audusd_daily_vector <<- rbind(audusd_daily_vector, audusd_daily_vector_add)
    writeLines("Successfully Updating AUDUSD PP Data")
    
    last_prevar_datetime <- audusd_prevar$Datetime[nrow(audusd_prevar)]
    which_audusd_rows <- which(audusd_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    audusd_pre1 <- precedence_sequences(audusd_daily_vector$vector_pivot[(which_audusd_rows - 6):nrow(audusd_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audusd_pre1 <- pivot_precedence_break(audusd_pre1)
    audusd_prevar_add <- as.data.frame(cbind(WK = audusd_daily_vector$weekday_trade[which_audusd_rows:nrow(audusd_daily_vector)], 
                                             FIB = audusd_daily_vector$FIB_GAP[which_audusd_rows:nrow(audusd_daily_vector)], 
                                             RNG = audusd_daily_vector$prev_range[which_audusd_rows:nrow(audusd_daily_vector)],
                                             CPR = audusd_daily_vector$CPR_GAP[which_audusd_rows:nrow(audusd_daily_vector)],
                                             audusd_pre1))
    audusd_prevar_add$Datetime <- audusd_daily_vector$Datetime[which_audusd_rows:nrow(audusd_daily_vector)]
    audusd_prevar_add$FIB <- NA
    audusd_prevar_add$CPR <- NA
    audusd_prevar_add$RNG <- NA
    audusd_prevar_add$FIB <- factor(audusd_prevar_add$FIB, levels = levels(audusd_prevar$FIB))
    audusd_prevar_add$CPR <- factor(audusd_prevar_add$CPR, levels = levels(audusd_prevar$CPR))
    audusd_prevar_add$RNG <- factor(audusd_prevar_add$RNG, levels = levels(audusd_prevar$RNG))
    audusd_prevar_add <- audusd_prevar_add %>% mutate_if(is.character, as.factor) 
    audusd_prevar <<- rbind(audusd_prevar, audusd_prevar_add)
    writeLines("Successfully Updating AUDUSD PP Precedence Variables")
  }
  
  #----------------------------------------- 2) EURUSD ---------------------------------------------------
  if(pp_overwrite){
    eurusd_daily_pp <<- pivot_point_trading(eurusd_daily, asset_name = "EURUSD", save_value = T)
    eurusd_daily_vector <<- pivot_point_vector(eurusd_daily_pp, vector_type = "nearest_diff")
    eurusd_daily_vector$vector_check <<- suppressWarnings(
      eurusd_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurusd_daily_vector$weekday_trade <<- weekdays(eurusd_daily_vector$Datetime)
    range_daily <- eurusd_daily$High - eurusd_daily$Low
    eurusd_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    eurusd_daily_vector <<- vector_clean_same_oc(eurusd_daily_vector, see_results = F)
    writeLines("Successfully Updating EURUSD PP Data")
    eurusd_pre1 <- precedence_sequences(eurusd_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurusd_pre1 <- pivot_precedence_break(eurusd_pre1)
    eurusd_prevar <<- as.data.frame(cbind(WK = eurusd_daily_vector$weekday_trade[7:nrow(eurusd_daily_vector)], 
                                          FIB = eurusd_daily_vector$FIB_GAP[7:nrow(eurusd_daily_vector)], 
                                          RNG = eurusd_daily_vector$prev_range[7:nrow(eurusd_daily_vector)],
                                          CPR = eurusd_daily_vector$CPR_GAP[7:nrow(eurusd_daily_vector)],
                                          eurusd_pre1))
    eurusd_prevar <<- quantile_cut_df_num(eurusd_prevar)
    eurusd_prevar <<- eurusd_prevar %>% mutate_if(is.character, as.factor) 
    eurusd_prevar$Datetime <<- eurusd_daily_vector$Datetime[7:nrow(eurusd_daily_vector)]
    writeLines("Successfully Updated EURUSD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- eurusd_daily_pp$Datetime[nrow(eurusd_daily_pp)]
    eurusd_daily_pp_add <- pivot_point_trading(eurusd_daily[which(eurusd_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "EURUSD", save_value = T)
    eurusd_daily_pp <<- rbind(eurusd_daily_pp, eurusd_daily_pp_add[-1,])
    eurusd_daily_vector_add <- pivot_point_vector(eurusd_daily_pp_add, vector_type = "nearest_diff")
    eurusd_daily_vector_add$vector_check <- suppressWarnings(
      eurusd_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurusd_daily_vector_add$weekday_trade <- weekdays(eurusd_daily_vector_add$Datetime)
    range_daily <- eurusd_daily$High[which(eurusd_daily$Datetime >= last_pp_datetime)] - eurusd_daily$Low[which(eurusd_daily$Datetime >= last_pp_datetime)]
    eurusd_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    eurusd_daily_vector_add <- vector_clean_same_oc(eurusd_daily_vector_add, see_results = F)
    eurusd_daily_vector <<- rbind(eurusd_daily_vector, eurusd_daily_vector_add)
    writeLines("Successfully Updating EURUSD PP Data")
    
    last_prevar_datetime <- eurusd_prevar$Datetime[nrow(eurusd_prevar)]
    which_eurusd_rows <- which(eurusd_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    eurusd_pre1 <- precedence_sequences(eurusd_daily_vector$vector_pivot[(which_eurusd_rows - 6):nrow(eurusd_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurusd_pre1 <- pivot_precedence_break(eurusd_pre1)
    eurusd_prevar_add <- as.data.frame(cbind(WK = eurusd_daily_vector$weekday_trade[which_eurusd_rows:nrow(eurusd_daily_vector)], 
                                             FIB = eurusd_daily_vector$FIB_GAP[which_eurusd_rows:nrow(eurusd_daily_vector)], 
                                             RNG = eurusd_daily_vector$prev_range[which_eurusd_rows:nrow(eurusd_daily_vector)],
                                             CPR = eurusd_daily_vector$CPR_GAP[which_eurusd_rows:nrow(eurusd_daily_vector)],
                                             eurusd_pre1))
    eurusd_prevar_add$Datetime <- eurusd_daily_vector$Datetime[which_eurusd_rows:nrow(eurusd_daily_vector)]
    eurusd_prevar_add$FIB <- NA
    eurusd_prevar_add$CPR <- NA
    eurusd_prevar_add$RNG <- NA
    eurusd_prevar_add$FIB <- factor(eurusd_prevar_add$FIB, levels = levels(eurusd_prevar$FIB))
    eurusd_prevar_add$CPR <- factor(eurusd_prevar_add$CPR, levels = levels(eurusd_prevar$CPR))
    eurusd_prevar_add$RNG <- factor(eurusd_prevar_add$RNG, levels = levels(eurusd_prevar$RNG))
    eurusd_prevar_add <- eurusd_prevar_add %>% mutate_if(is.character, as.factor) 
    eurusd_prevar <<- rbind(eurusd_prevar, eurusd_prevar_add)
    writeLines("Successfully Updating EURUSD PP Precedence Variables")
  }
  
  #----------------------------------------- 3) GBPUSD ---------------------------------------------------
  if(pp_overwrite){
    gbpusd_daily_pp <<- pivot_point_trading(gbpusd_daily, asset_name = "GBPUSD", save_value = T)
    gbpusd_daily_vector <<- pivot_point_vector(gbpusd_daily_pp, vector_type = "nearest_diff")
    gbpusd_daily_vector$vector_check <<- suppressWarnings(
      gbpusd_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    gbpusd_daily_vector$weekday_trade <<- weekdays(gbpusd_daily_vector$Datetime)
    range_daily <- gbpusd_daily$High - gbpusd_daily$Low
    gbpusd_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    gbpusd_daily_vector <<- vector_clean_same_oc(gbpusd_daily_vector, see_results = F)
    writeLines("Successfully Updating GBPUSD PP Data")
    gbpusd_pre1 <- precedence_sequences(gbpusd_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    gbpusd_pre1 <- pivot_precedence_break(gbpusd_pre1)
    gbpusd_prevar <<- as.data.frame(cbind(WK = gbpusd_daily_vector$weekday_trade[7:nrow(gbpusd_daily_vector)], 
                                          FIB = gbpusd_daily_vector$FIB_GAP[7:nrow(gbpusd_daily_vector)], 
                                          RNG = gbpusd_daily_vector$prev_range[7:nrow(gbpusd_daily_vector)],
                                          CPR = gbpusd_daily_vector$CPR_GAP[7:nrow(gbpusd_daily_vector)],
                                          gbpusd_pre1))
    gbpusd_prevar <<- quantile_cut_df_num(gbpusd_prevar)
    gbpusd_prevar <<- gbpusd_prevar %>% mutate_if(is.character, as.factor) 
    gbpusd_prevar$Datetime <<- gbpusd_daily_vector$Datetime[7:nrow(gbpusd_daily_vector)]
    writeLines("Successfully Updated GBPUSD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- gbpusd_daily_pp$Datetime[nrow(gbpusd_daily_pp)]
    gbpusd_daily_pp_add <- pivot_point_trading(gbpusd_daily[which(gbpusd_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "GBPUSD", save_value = T)
    gbpusd_daily_pp <<- rbind(gbpusd_daily_pp, gbpusd_daily_pp_add[-1,])
    gbpusd_daily_vector_add <- pivot_point_vector(gbpusd_daily_pp_add, vector_type = "nearest_diff")
    gbpusd_daily_vector_add$vector_check <- suppressWarnings(
      gbpusd_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    gbpusd_daily_vector_add$weekday_trade <- weekdays(gbpusd_daily_vector_add$Datetime)
    range_daily <- gbpusd_daily$High[which(gbpusd_daily$Datetime >= last_pp_datetime)] - gbpusd_daily$Low[which(gbpusd_daily$Datetime >= last_pp_datetime)]
    gbpusd_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    gbpusd_daily_vector_add <- vector_clean_same_oc(gbpusd_daily_vector_add, see_results = F)
    gbpusd_daily_vector <<- rbind(gbpusd_daily_vector, gbpusd_daily_vector_add)
    writeLines("Successfully Updating GBPUSD PP Data")
    
    last_prevar_datetime <- gbpusd_prevar$Datetime[nrow(gbpusd_prevar)]
    which_gbpusd_rows <- which(gbpusd_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    gbpusd_pre1 <- precedence_sequences(gbpusd_daily_vector$vector_pivot[(which_gbpusd_rows - 6):nrow(gbpusd_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    gbpusd_pre1 <- pivot_precedence_break(gbpusd_pre1)
    gbpusd_prevar_add <- as.data.frame(cbind(WK = gbpusd_daily_vector$weekday_trade[which_gbpusd_rows:nrow(gbpusd_daily_vector)], 
                                             FIB = gbpusd_daily_vector$FIB_GAP[which_gbpusd_rows:nrow(gbpusd_daily_vector)], 
                                             RNG = gbpusd_daily_vector$prev_range[which_gbpusd_rows:nrow(gbpusd_daily_vector)],
                                             CPR = gbpusd_daily_vector$CPR_GAP[which_gbpusd_rows:nrow(gbpusd_daily_vector)],
                                             gbpusd_pre1))
    gbpusd_prevar_add$Datetime <- gbpusd_daily_vector$Datetime[which_gbpusd_rows:nrow(gbpusd_daily_vector)]
    gbpusd_prevar_add$FIB <- NA
    gbpusd_prevar_add$CPR <- NA
    gbpusd_prevar_add$RNG <- NA
    gbpusd_prevar_add$FIB <- factor(gbpusd_prevar_add$FIB, levels = levels(gbpusd_prevar$FIB))
    gbpusd_prevar_add$CPR <- factor(gbpusd_prevar_add$CPR, levels = levels(gbpusd_prevar$CPR))
    gbpusd_prevar_add$RNG <- factor(gbpusd_prevar_add$RNG, levels = levels(gbpusd_prevar$RNG))
    gbpusd_prevar_add <- gbpusd_prevar_add %>% mutate_if(is.character, as.factor) 
    gbpusd_prevar <<- rbind(gbpusd_prevar, gbpusd_prevar_add)
    writeLines("Successfully Updating GBPUSD PP Precedence Variables")
  }
  
  #----------------------------------------- 4) USDCAD ---------------------------------------------------
  if(pp_overwrite){
    usdcad_daily_pp <<- pivot_point_trading(usdcad_daily, asset_name = "USDCAD", save_value = T)
    usdcad_daily_vector <<- pivot_point_vector(usdcad_daily_pp, vector_type = "nearest_diff")
    usdcad_daily_vector$vector_check <<- suppressWarnings(
      usdcad_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    usdcad_daily_vector$weekday_trade <<- weekdays(usdcad_daily_vector$Datetime)
    range_daily <- usdcad_daily$High - usdcad_daily$Low
    usdcad_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    usdcad_daily_vector <<- vector_clean_same_oc(usdcad_daily_vector, see_results = F)
    writeLines("Successfully Updating USDCAD PP Data")
    usdcad_pre1 <- precedence_sequences(usdcad_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    usdcad_pre1 <- pivot_precedence_break(usdcad_pre1)
    usdcad_prevar <<- as.data.frame(cbind(WK = usdcad_daily_vector$weekday_trade[7:nrow(usdcad_daily_vector)], 
                                          FIB = usdcad_daily_vector$FIB_GAP[7:nrow(usdcad_daily_vector)], 
                                          RNG = usdcad_daily_vector$prev_range[7:nrow(usdcad_daily_vector)],
                                          CPR = usdcad_daily_vector$CPR_GAP[7:nrow(usdcad_daily_vector)],
                                          usdcad_pre1))
    usdcad_prevar <<- quantile_cut_df_num(usdcad_prevar)
    usdcad_prevar <<- usdcad_prevar %>% mutate_if(is.character, as.factor) 
    usdcad_prevar$Datetime <<- usdcad_daily_vector$Datetime[7:nrow(usdcad_daily_vector)]
    writeLines("Successfully Updated USDCAD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- usdcad_daily_pp$Datetime[nrow(usdcad_daily_pp)]
    usdcad_daily_pp_add <- pivot_point_trading(usdcad_daily[which(usdcad_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "USDCAD", save_value = T)
    usdcad_daily_pp <<- rbind(usdcad_daily_pp, usdcad_daily_pp_add[-1,])
    usdcad_daily_vector_add <- pivot_point_vector(usdcad_daily_pp_add, vector_type = "nearest_diff")
    usdcad_daily_vector_add$vector_check <- suppressWarnings(
      usdcad_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    usdcad_daily_vector_add$weekday_trade <- weekdays(usdcad_daily_vector_add$Datetime)
    range_daily <- usdcad_daily$High[which(usdcad_daily$Datetime >= last_pp_datetime)] - usdcad_daily$Low[which(usdcad_daily$Datetime >= last_pp_datetime)]
    usdcad_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    usdcad_daily_vector_add <- vector_clean_same_oc(usdcad_daily_vector_add, see_results = F)
    usdcad_daily_vector <<- rbind(usdcad_daily_vector, usdcad_daily_vector_add)
    writeLines("Successfully Updating USDCAD PP Data")
    
    last_prevar_datetime <- usdcad_prevar$Datetime[nrow(usdcad_prevar)]
    which_usdcad_rows <- which(usdcad_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    usdcad_pre1 <- precedence_sequences(usdcad_daily_vector$vector_pivot[(which_usdcad_rows - 6):nrow(usdcad_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    usdcad_pre1 <- pivot_precedence_break(usdcad_pre1)
    usdcad_prevar_add <- as.data.frame(cbind(WK = usdcad_daily_vector$weekday_trade[which_usdcad_rows:nrow(usdcad_daily_vector)], 
                                             FIB = usdcad_daily_vector$FIB_GAP[which_usdcad_rows:nrow(usdcad_daily_vector)], 
                                             RNG = usdcad_daily_vector$prev_range[which_usdcad_rows:nrow(usdcad_daily_vector)],
                                             CPR = usdcad_daily_vector$CPR_GAP[which_usdcad_rows:nrow(usdcad_daily_vector)],
                                             usdcad_pre1))
    usdcad_prevar_add$Datetime <- usdcad_daily_vector$Datetime[which_usdcad_rows:nrow(usdcad_daily_vector)]
    usdcad_prevar_add$FIB <- NA
    usdcad_prevar_add$CPR <- NA
    usdcad_prevar_add$RNG <- NA
    usdcad_prevar_add$FIB <- factor(usdcad_prevar_add$FIB, levels = levels(usdcad_prevar$FIB))
    usdcad_prevar_add$CPR <- factor(usdcad_prevar_add$CPR, levels = levels(usdcad_prevar$CPR))
    usdcad_prevar_add$RNG <- factor(usdcad_prevar_add$RNG, levels = levels(usdcad_prevar$RNG))
    usdcad_prevar_add <- usdcad_prevar_add %>% mutate_if(is.character, as.factor) 
    usdcad_prevar <<- rbind(usdcad_prevar, usdcad_prevar_add)
    writeLines("Successfully Updating USDCAD PP Precedence Variables")
  }
  
  #----------------------------------------- 5) USDCHF ---------------------------------------------------
  if(pp_overwrite){
    usdchf_daily_pp <<- pivot_point_trading(usdchf_daily, asset_name = "USDCHF", save_value = T)
    usdchf_daily_vector <<- pivot_point_vector(usdchf_daily_pp, vector_type = "nearest_diff")
    usdchf_daily_vector$vector_check <<- suppressWarnings(
      usdchf_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    usdchf_daily_vector$weekday_trade <<- weekdays(usdchf_daily_vector$Datetime)
    range_daily <- usdchf_daily$High - usdchf_daily$Low
    usdchf_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    usdchf_daily_vector <<- vector_clean_same_oc(usdchf_daily_vector, see_results = F)
    writeLines("Successfully Updating USDCHF PP Data")
    usdchf_pre1 <- precedence_sequences(usdchf_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    usdchf_pre1 <- pivot_precedence_break(usdchf_pre1)
    usdchf_prevar <<- as.data.frame(cbind(WK = usdchf_daily_vector$weekday_trade[7:nrow(usdchf_daily_vector)], 
                                          FIB = usdchf_daily_vector$FIB_GAP[7:nrow(usdchf_daily_vector)], 
                                          RNG = usdchf_daily_vector$prev_range[7:nrow(usdchf_daily_vector)],
                                          CPR = usdchf_daily_vector$CPR_GAP[7:nrow(usdchf_daily_vector)],
                                          usdchf_pre1))
    usdchf_prevar <<- quantile_cut_df_num(usdchf_prevar)
    usdchf_prevar <<- usdchf_prevar %>% mutate_if(is.character, as.factor) 
    usdchf_prevar$Datetime <<- usdchf_daily_vector$Datetime[7:nrow(usdchf_daily_vector)]
    writeLines("Successfully Updated USDCHF Precedence Variables")
    
  }
  else{
    last_pp_datetime <- usdchf_daily_pp$Datetime[nrow(usdchf_daily_pp)]
    usdchf_daily_pp_add <- pivot_point_trading(usdchf_daily[which(usdchf_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "USDCHF", save_value = T)
    usdchf_daily_pp <<- rbind(usdchf_daily_pp, usdchf_daily_pp_add[-1,])
    usdchf_daily_vector_add <- pivot_point_vector(usdchf_daily_pp_add, vector_type = "nearest_diff")
    usdchf_daily_vector_add$vector_check <- suppressWarnings(
      usdchf_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    usdchf_daily_vector_add$weekday_trade <- weekdays(usdchf_daily_vector_add$Datetime)
    range_daily <- usdchf_daily$High[which(usdchf_daily$Datetime >= last_pp_datetime)] - usdchf_daily$Low[which(usdchf_daily$Datetime >= last_pp_datetime)]
    usdchf_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    usdchf_daily_vector_add <- vector_clean_same_oc(usdchf_daily_vector_add, see_results = F)
    usdchf_daily_vector <<- rbind(usdchf_daily_vector, usdchf_daily_vector_add)
    writeLines("Successfully Updating USDCHF PP Data")
    
    last_prevar_datetime <- usdchf_prevar$Datetime[nrow(usdchf_prevar)]
    which_usdchf_rows <- which(usdchf_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    usdchf_pre1 <- precedence_sequences(usdchf_daily_vector$vector_pivot[(which_usdchf_rows - 6):nrow(usdchf_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    usdchf_pre1 <- pivot_precedence_break(usdchf_pre1)
    usdchf_prevar_add <- as.data.frame(cbind(WK = usdchf_daily_vector$weekday_trade[which_usdchf_rows:nrow(usdchf_daily_vector)], 
                                             FIB = usdchf_daily_vector$FIB_GAP[which_usdchf_rows:nrow(usdchf_daily_vector)], 
                                             RNG = usdchf_daily_vector$prev_range[which_usdchf_rows:nrow(usdchf_daily_vector)],
                                             CPR = usdchf_daily_vector$CPR_GAP[which_usdchf_rows:nrow(usdchf_daily_vector)],
                                             usdchf_pre1))
    usdchf_prevar_add$Datetime <- usdchf_daily_vector$Datetime[which_usdchf_rows:nrow(usdchf_daily_vector)]
    usdchf_prevar_add$FIB <- NA
    usdchf_prevar_add$CPR <- NA
    usdchf_prevar_add$RNG <- NA
    usdchf_prevar_add$FIB <- factor(usdchf_prevar_add$FIB, levels = levels(usdchf_prevar$FIB))
    usdchf_prevar_add$CPR <- factor(usdchf_prevar_add$CPR, levels = levels(usdchf_prevar$CPR))
    usdchf_prevar_add$RNG <- factor(usdchf_prevar_add$RNG, levels = levels(usdchf_prevar$RNG))
    usdchf_prevar_add <- usdchf_prevar_add %>% mutate_if(is.character, as.factor) 
    usdchf_prevar <<- rbind(usdchf_prevar, usdchf_prevar_add)
    writeLines("Successfully Updating USDCHF PP Precedence Variables")
  }
  
  #----------------------------------------- 6) USDJPY ---------------------------------------------------
  if(pp_overwrite){
    usdjpy_daily_pp <<- pivot_point_trading(usdjpy_daily, asset_name = "USDJPY", save_value = T)
    usdjpy_daily_vector <<- pivot_point_vector(usdjpy_daily_pp, vector_type = "nearest_diff")
    usdjpy_daily_vector$vector_check <<- suppressWarnings(
      usdjpy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    usdjpy_daily_vector$weekday_trade <<- weekdays(usdjpy_daily_vector$Datetime)
    range_daily <- usdjpy_daily$High - usdjpy_daily$Low
    usdjpy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    usdjpy_daily_vector <<- vector_clean_same_oc(usdjpy_daily_vector, see_results = F)
    writeLines("Successfully Updating USDJPY PP Data")
    usdjpy_pre1 <- precedence_sequences(usdjpy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    usdjpy_pre1 <- pivot_precedence_break(usdjpy_pre1)
    usdjpy_prevar <<- as.data.frame(cbind(WK = usdjpy_daily_vector$weekday_trade[7:nrow(usdjpy_daily_vector)], 
                                          FIB = usdjpy_daily_vector$FIB_GAP[7:nrow(usdjpy_daily_vector)], 
                                          RNG = usdjpy_daily_vector$prev_range[7:nrow(usdjpy_daily_vector)],
                                          CPR = usdjpy_daily_vector$CPR_GAP[7:nrow(usdjpy_daily_vector)],
                                          usdjpy_pre1))
    usdjpy_prevar <<- quantile_cut_df_num(usdjpy_prevar)
    usdjpy_prevar <<- usdjpy_prevar %>% mutate_if(is.character, as.factor) 
    usdjpy_prevar$Datetime <<- usdjpy_daily_vector$Datetime[7:nrow(usdjpy_daily_vector)]
    writeLines("Successfully Updated USDJPY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- usdjpy_daily_pp$Datetime[nrow(usdjpy_daily_pp)]
    usdjpy_daily_pp_add <- pivot_point_trading(usdjpy_daily[which(usdjpy_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "USDJPY", save_value = T)
    usdjpy_daily_pp <<- rbind(usdjpy_daily_pp, usdjpy_daily_pp_add[-1,])
    usdjpy_daily_vector_add <- pivot_point_vector(usdjpy_daily_pp_add, vector_type = "nearest_diff")
    usdjpy_daily_vector_add$vector_check <- suppressWarnings(
      usdjpy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    usdjpy_daily_vector_add$weekday_trade <- weekdays(usdjpy_daily_vector_add$Datetime)
    range_daily <- usdjpy_daily$High[which(usdjpy_daily$Datetime >= last_pp_datetime)] - usdjpy_daily$Low[which(usdjpy_daily$Datetime >= last_pp_datetime)]
    usdjpy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    usdjpy_daily_vector_add <- vector_clean_same_oc(usdjpy_daily_vector_add, see_results = F)
    usdjpy_daily_vector <<- rbind(usdjpy_daily_vector, usdjpy_daily_vector_add)
    writeLines("Successfully Updating USDJPY PP Data")
    
    last_prevar_datetime <- usdjpy_prevar$Datetime[nrow(usdjpy_prevar)]
    which_usdjpy_rows <- which(usdjpy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    usdjpy_pre1 <- precedence_sequences(usdjpy_daily_vector$vector_pivot[(which_usdjpy_rows - 6):nrow(usdjpy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    usdjpy_pre1 <- pivot_precedence_break(usdjpy_pre1)
    usdjpy_prevar_add <- as.data.frame(cbind(WK = usdjpy_daily_vector$weekday_trade[which_usdjpy_rows:nrow(usdjpy_daily_vector)], 
                                             FIB = usdjpy_daily_vector$FIB_GAP[which_usdjpy_rows:nrow(usdjpy_daily_vector)], 
                                             RNG = usdjpy_daily_vector$prev_range[which_usdjpy_rows:nrow(usdjpy_daily_vector)],
                                             CPR = usdjpy_daily_vector$CPR_GAP[which_usdjpy_rows:nrow(usdjpy_daily_vector)],
                                             usdjpy_pre1))
    usdjpy_prevar_add$Datetime <- usdjpy_daily_vector$Datetime[which_usdjpy_rows:nrow(usdjpy_daily_vector)]
    usdjpy_prevar_add$FIB <- NA
    usdjpy_prevar_add$CPR <- NA
    usdjpy_prevar_add$RNG <- NA
    usdjpy_prevar_add$FIB <- factor(usdjpy_prevar_add$FIB, levels = levels(usdjpy_prevar$FIB))
    usdjpy_prevar_add$CPR <- factor(usdjpy_prevar_add$CPR, levels = levels(usdjpy_prevar$CPR))
    usdjpy_prevar_add$RNG <- factor(usdjpy_prevar_add$RNG, levels = levels(usdjpy_prevar$RNG))
    usdjpy_prevar_add <- usdjpy_prevar_add %>% mutate_if(is.character, as.factor) 
    usdjpy_prevar <<- rbind(usdjpy_prevar, usdjpy_prevar_add)
    writeLines("Successfully Updating USDJPY PP Precedence Variables")
  }
  
  #----------------------------------------- 7) NZDJPY ---------------------------------------------------
  if(pp_overwrite){
    nzdjpy_daily_pp <<- pivot_point_trading(nzdjpy_daily, asset_name = "NZDJPY", save_value = T)
    nzdjpy_daily_vector <<- pivot_point_vector(nzdjpy_daily_pp, vector_type = "nearest_diff")
    nzdjpy_daily_vector$vector_check <<- suppressWarnings(
      nzdjpy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    nzdjpy_daily_vector$weekday_trade <<- weekdays(nzdjpy_daily_vector$Datetime)
    range_daily <- nzdjpy_daily$High - nzdjpy_daily$Low
    nzdjpy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    nzdjpy_daily_vector <<- vector_clean_same_oc(nzdjpy_daily_vector, see_results = F)
    writeLines("Successfully Updating NZDJPY PP Data")
    nzdjpy_pre1 <- precedence_sequences(nzdjpy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    nzdjpy_pre1 <- pivot_precedence_break(nzdjpy_pre1)
    nzdjpy_prevar <<- as.data.frame(cbind(WK = nzdjpy_daily_vector$weekday_trade[7:nrow(nzdjpy_daily_vector)], 
                                          FIB = nzdjpy_daily_vector$FIB_GAP[7:nrow(nzdjpy_daily_vector)], 
                                          RNG = nzdjpy_daily_vector$prev_range[7:nrow(nzdjpy_daily_vector)],
                                          CPR = nzdjpy_daily_vector$CPR_GAP[7:nrow(nzdjpy_daily_vector)],
                                          nzdjpy_pre1))
    nzdjpy_prevar <<- quantile_cut_df_num(nzdjpy_prevar)
    nzdjpy_prevar <<- nzdjpy_prevar %>% mutate_if(is.character, as.factor) 
    nzdjpy_prevar$Datetime <<- nzdjpy_daily_vector$Datetime[7:nrow(nzdjpy_daily_vector)]
    writeLines("Successfully Updated NZDJPY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- nzdjpy_daily_pp$Datetime[nrow(nzdjpy_daily_pp)]
    nzdjpy_daily_pp_add <- pivot_point_trading(nzdjpy_daily[which(nzdjpy_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "NZDJPY", save_value = T)
    nzdjpy_daily_pp <<- rbind(nzdjpy_daily_pp, nzdjpy_daily_pp_add[-1,])
    nzdjpy_daily_vector_add <- pivot_point_vector(nzdjpy_daily_pp_add, vector_type = "nearest_diff")
    nzdjpy_daily_vector_add$vector_check <- suppressWarnings(
      nzdjpy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    nzdjpy_daily_vector_add$weekday_trade <- weekdays(nzdjpy_daily_vector_add$Datetime)
    range_daily <- nzdjpy_daily$High[which(nzdjpy_daily$Datetime >= last_pp_datetime)] - nzdjpy_daily$Low[which(nzdjpy_daily$Datetime >= last_pp_datetime)]
    nzdjpy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    nzdjpy_daily_vector_add <- vector_clean_same_oc(nzdjpy_daily_vector_add, see_results = F)
    nzdjpy_daily_vector <<- rbind(nzdjpy_daily_vector, nzdjpy_daily_vector_add)
    writeLines("Successfully Updating NZDJPY PP Data")
    
    last_prevar_datetime <- nzdjpy_prevar$Datetime[nrow(nzdjpy_prevar)]
    which_nzdjpy_rows <- which(nzdjpy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    nzdjpy_pre1 <- precedence_sequences(nzdjpy_daily_vector$vector_pivot[(which_nzdjpy_rows - 6):nrow(nzdjpy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    nzdjpy_pre1 <- pivot_precedence_break(nzdjpy_pre1)
    nzdjpy_prevar_add <- as.data.frame(cbind(WK = nzdjpy_daily_vector$weekday_trade[which_nzdjpy_rows:nrow(nzdjpy_daily_vector)], 
                                             FIB = nzdjpy_daily_vector$FIB_GAP[which_nzdjpy_rows:nrow(nzdjpy_daily_vector)], 
                                             RNG = nzdjpy_daily_vector$prev_range[which_nzdjpy_rows:nrow(nzdjpy_daily_vector)],
                                             CPR = nzdjpy_daily_vector$CPR_GAP[which_nzdjpy_rows:nrow(nzdjpy_daily_vector)],
                                             nzdjpy_pre1))
    nzdjpy_prevar_add$Datetime <- nzdjpy_daily_vector$Datetime[which_nzdjpy_rows:nrow(nzdjpy_daily_vector)]
    nzdjpy_prevar_add$FIB <- NA
    nzdjpy_prevar_add$CPR <- NA
    nzdjpy_prevar_add$RNG <- NA
    nzdjpy_prevar_add$FIB <- factor(nzdjpy_prevar_add$FIB, levels = levels(nzdjpy_prevar$FIB))
    nzdjpy_prevar_add$CPR <- factor(nzdjpy_prevar_add$CPR, levels = levels(nzdjpy_prevar$CPR))
    nzdjpy_prevar_add$RNG <- factor(nzdjpy_prevar_add$RNG, levels = levels(nzdjpy_prevar$RNG))
    nzdjpy_prevar_add <- nzdjpy_prevar_add %>% mutate_if(is.character, as.factor) 
    nzdjpy_prevar <<- rbind(nzdjpy_prevar, nzdjpy_prevar_add)
    writeLines("Successfully Updating NZDJPY PP Precedence Variables")
  }
  
  #----------------------------------------- 8) EURJPY ---------------------------------------------------
  if(pp_overwrite){
    eurjpy_daily_pp <<- pivot_point_trading(eurjpy_daily, asset_name = "EURJPY", save_value = T)
    eurjpy_daily_vector <<- pivot_point_vector(eurjpy_daily_pp, vector_type = "nearest_diff")
    eurjpy_daily_vector$vector_check <<- suppressWarnings(
      eurjpy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurjpy_daily_vector$weekday_trade <<- weekdays(eurjpy_daily_vector$Datetime)
    range_daily <- eurjpy_daily$High - eurjpy_daily$Low
    eurjpy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    eurjpy_daily_vector <<- vector_clean_same_oc(eurjpy_daily_vector, see_results = F)
    writeLines("Successfully Updating EURJPY PP Data")
    eurjpy_pre1 <- precedence_sequences(eurjpy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurjpy_pre1 <- pivot_precedence_break(eurjpy_pre1)
    eurjpy_prevar <<- as.data.frame(cbind(WK = eurjpy_daily_vector$weekday_trade[7:nrow(eurjpy_daily_vector)], 
                                          FIB = eurjpy_daily_vector$FIB_GAP[7:nrow(eurjpy_daily_vector)], 
                                          RNG = eurjpy_daily_vector$prev_range[7:nrow(eurjpy_daily_vector)],
                                          CPR = eurjpy_daily_vector$CPR_GAP[7:nrow(eurjpy_daily_vector)],
                                          eurjpy_pre1))
    eurjpy_prevar <<- quantile_cut_df_num(eurjpy_prevar)
    eurjpy_prevar <<- eurjpy_prevar %>% mutate_if(is.character, as.factor) 
    eurjpy_prevar$Datetime <<- eurjpy_daily_vector$Datetime[7:nrow(eurjpy_daily_vector)]
    writeLines("Successfully Updated EURJPY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- eurjpy_daily_pp$Datetime[nrow(eurjpy_daily_pp)]
    eurjpy_daily_pp_add <- pivot_point_trading(eurjpy_daily[which(eurjpy_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "EURJPY", save_value = T)
    eurjpy_daily_pp <<- rbind(eurjpy_daily_pp, eurjpy_daily_pp_add[-1,])
    eurjpy_daily_vector_add <- pivot_point_vector(eurjpy_daily_pp_add, vector_type = "nearest_diff")
    eurjpy_daily_vector_add$vector_check <- suppressWarnings(
      eurjpy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurjpy_daily_vector_add$weekday_trade <- weekdays(eurjpy_daily_vector_add$Datetime)
    range_daily <- eurjpy_daily$High[which(eurjpy_daily$Datetime >= last_pp_datetime)] - eurjpy_daily$Low[which(eurjpy_daily$Datetime >= last_pp_datetime)]
    eurjpy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    eurjpy_daily_vector_add <- vector_clean_same_oc(eurjpy_daily_vector_add, see_results = F)
    eurjpy_daily_vector <<- rbind(eurjpy_daily_vector, eurjpy_daily_vector_add)
    writeLines("Successfully Updating EURJPY PP Data")
    
    last_prevar_datetime <- eurjpy_prevar$Datetime[nrow(eurjpy_prevar)]
    which_eurjpy_rows <- which(eurjpy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    eurjpy_pre1 <- precedence_sequences(eurjpy_daily_vector$vector_pivot[(which_eurjpy_rows - 6):nrow(eurjpy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurjpy_pre1 <- pivot_precedence_break(eurjpy_pre1)
    eurjpy_prevar_add <- as.data.frame(cbind(WK = eurjpy_daily_vector$weekday_trade[which_eurjpy_rows:nrow(eurjpy_daily_vector)], 
                                             FIB = eurjpy_daily_vector$FIB_GAP[which_eurjpy_rows:nrow(eurjpy_daily_vector)], 
                                             RNG = eurjpy_daily_vector$prev_range[which_eurjpy_rows:nrow(eurjpy_daily_vector)],
                                             CPR = eurjpy_daily_vector$CPR_GAP[which_eurjpy_rows:nrow(eurjpy_daily_vector)],
                                             eurjpy_pre1))
    eurjpy_prevar_add$Datetime <- eurjpy_daily_vector$Datetime[which_eurjpy_rows:nrow(eurjpy_daily_vector)]
    eurjpy_prevar_add$FIB <- NA
    eurjpy_prevar_add$CPR <- NA
    eurjpy_prevar_add$RNG <- NA
    eurjpy_prevar_add$FIB <- factor(eurjpy_prevar_add$FIB, levels = levels(eurjpy_prevar$FIB))
    eurjpy_prevar_add$CPR <- factor(eurjpy_prevar_add$CPR, levels = levels(eurjpy_prevar$CPR))
    eurjpy_prevar_add$RNG <- factor(eurjpy_prevar_add$RNG, levels = levels(eurjpy_prevar$RNG))
    eurjpy_prevar_add <- eurjpy_prevar_add %>% mutate_if(is.character, as.factor) 
    eurjpy_prevar <<- rbind(eurjpy_prevar, eurjpy_prevar_add)
    writeLines("Successfully Updating EURJPY PP Precedence Variables")
  }
  
  #----------------------------------------- 9) AUDNZD ---------------------------------------------------
  if(pp_overwrite){
    audnzd_daily_pp <<- pivot_point_trading(audnzd_daily, asset_name = "AUDNZD", save_value = T)
    audnzd_daily_vector <<- pivot_point_vector(audnzd_daily_pp, vector_type = "nearest_diff")
    audnzd_daily_vector$vector_check <<- suppressWarnings(
      audnzd_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audnzd_daily_vector$weekday_trade <<- weekdays(audnzd_daily_vector$Datetime)
    range_daily <- audnzd_daily$High - audnzd_daily$Low
    audnzd_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    audnzd_daily_vector <<- vector_clean_same_oc(audnzd_daily_vector, see_results = F)
    writeLines("Successfully Updating AUDNZD PP Data")
    audnzd_pre1 <- precedence_sequences(audnzd_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audnzd_pre1 <- pivot_precedence_break(audnzd_pre1)
    audnzd_prevar <<- as.data.frame(cbind(WK = audnzd_daily_vector$weekday_trade[7:nrow(audnzd_daily_vector)], 
                                          FIB = audnzd_daily_vector$FIB_GAP[7:nrow(audnzd_daily_vector)], 
                                          RNG = audnzd_daily_vector$prev_range[7:nrow(audnzd_daily_vector)],
                                          CPR = audnzd_daily_vector$CPR_GAP[7:nrow(audnzd_daily_vector)],
                                          audnzd_pre1))
    audnzd_prevar <<- quantile_cut_df_num(audnzd_prevar)
    audnzd_prevar <<- audnzd_prevar %>% mutate_if(is.character, as.factor) 
    audnzd_prevar$Datetime <<- audnzd_daily_vector$Datetime[7:nrow(audnzd_daily_vector)]
    writeLines("Successfully Updated AUDNZD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- audnzd_daily_pp$Datetime[nrow(audnzd_daily_pp)]
    audnzd_daily_pp_add <- pivot_point_trading(audnzd_daily[which(audnzd_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "AUDNZD", save_value = T)
    audnzd_daily_pp <<- rbind(audnzd_daily_pp, audnzd_daily_pp_add[-1,])
    audnzd_daily_vector_add <- pivot_point_vector(audnzd_daily_pp_add, vector_type = "nearest_diff")
    audnzd_daily_vector_add$vector_check <- suppressWarnings(
      audnzd_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audnzd_daily_vector_add$weekday_trade <- weekdays(audnzd_daily_vector_add$Datetime)
    range_daily <- audnzd_daily$High[which(audnzd_daily$Datetime >= last_pp_datetime)] - audnzd_daily$Low[which(audnzd_daily$Datetime >= last_pp_datetime)]
    audnzd_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    audnzd_daily_vector_add <- vector_clean_same_oc(audnzd_daily_vector_add, see_results = F)
    audnzd_daily_vector <<- rbind(audnzd_daily_vector, audnzd_daily_vector_add)
    writeLines("Successfully Updating AUDNZD PP Data")
    
    last_prevar_datetime <- audnzd_prevar$Datetime[nrow(audnzd_prevar)]
    which_audnzd_rows <- which(audnzd_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    audnzd_pre1 <- precedence_sequences(audnzd_daily_vector$vector_pivot[(which_audnzd_rows - 6):nrow(audnzd_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audnzd_pre1 <- pivot_precedence_break(audnzd_pre1)
    audnzd_prevar_add <- as.data.frame(cbind(WK = audnzd_daily_vector$weekday_trade[which_audnzd_rows:nrow(audnzd_daily_vector)], 
                                             FIB = audnzd_daily_vector$FIB_GAP[which_audnzd_rows:nrow(audnzd_daily_vector)], 
                                             RNG = audnzd_daily_vector$prev_range[which_audnzd_rows:nrow(audnzd_daily_vector)],
                                             CPR = audnzd_daily_vector$CPR_GAP[which_audnzd_rows:nrow(audnzd_daily_vector)],
                                             audnzd_pre1))
    audnzd_prevar_add$Datetime <- audnzd_daily_vector$Datetime[which_audnzd_rows:nrow(audnzd_daily_vector)]
    unique_fib_prevar <- as.character(unique(na.omit(audnzd_prevar$FIB)))
    unique_cpr_prevar <- as.character(unique(na.omit(audnzd_prevar$CPR)))
    unique_rng_prevar <- as.character(unique(na.omit(audnzd_prevar$RNG)))
    audnzd_prevar_add$FIB <- NA
    audnzd_prevar_add$CPR <- NA
    audnzd_prevar_add$RNG <- NA
    audnzd_prevar_add$FIB <- factor(audnzd_prevar_add$FIB, levels = levels(audnzd_prevar$FIB))
    audnzd_prevar_add$CPR <- factor(audnzd_prevar_add$CPR, levels = levels(audnzd_prevar$CPR))
    audnzd_prevar_add$RNG <- factor(audnzd_prevar_add$RNG, levels = levels(audnzd_prevar$RNG))
    audnzd_prevar_add <- audnzd_prevar_add %>% mutate_if(is.character, as.factor) 
    audnzd_prevar <<- rbind(audnzd_prevar, audnzd_prevar_add)
    writeLines("Successfully Updating AUDNZD PP Precedence Variables")
  }
  
  #----------------------------------------- 10) GBPJPY ---------------------------------------------------
  if(pp_overwrite){
    gbpjpy_daily_pp <<- pivot_point_trading(gbpjpy_daily, asset_name = "GBPJPY", save_value = T)
    gbpjpy_daily_vector <<- pivot_point_vector(gbpjpy_daily_pp, vector_type = "nearest_diff")
    gbpjpy_daily_vector$vector_check <<- suppressWarnings(
      gbpjpy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    gbpjpy_daily_vector$weekday_trade <<- weekdays(gbpjpy_daily_vector$Datetime)
    range_daily <- gbpjpy_daily$High - gbpjpy_daily$Low
    gbpjpy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    gbpjpy_daily_vector <<- vector_clean_same_oc(gbpjpy_daily_vector, see_results = F)
    writeLines("Successfully Updating GBPJPY PP Data")
    gbpjpy_pre1 <- precedence_sequences(gbpjpy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    gbpjpy_pre1 <- pivot_precedence_break(gbpjpy_pre1)
    gbpjpy_prevar <<- as.data.frame(cbind(WK = gbpjpy_daily_vector$weekday_trade[7:nrow(gbpjpy_daily_vector)], 
                                          FIB = gbpjpy_daily_vector$FIB_GAP[7:nrow(gbpjpy_daily_vector)], 
                                          RNG = gbpjpy_daily_vector$prev_range[7:nrow(gbpjpy_daily_vector)],
                                          CPR = gbpjpy_daily_vector$CPR_GAP[7:nrow(gbpjpy_daily_vector)],
                                          gbpjpy_pre1))
    gbpjpy_prevar <<- quantile_cut_df_num(gbpjpy_prevar)
    gbpjpy_prevar <<- gbpjpy_prevar %>% mutate_if(is.character, as.factor) 
    gbpjpy_prevar$Datetime <<- gbpjpy_daily_vector$Datetime[7:nrow(gbpjpy_daily_vector)]
    writeLines("Successfully Updated GBPJPY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- gbpjpy_daily_pp$Datetime[nrow(gbpjpy_daily_pp)]
    gbpjpy_daily_pp_add <- pivot_point_trading(gbpjpy_daily[which(gbpjpy_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "GBPJPY", save_value = T)
    gbpjpy_daily_pp <<- rbind(gbpjpy_daily_pp, gbpjpy_daily_pp_add[-1,])
    gbpjpy_daily_vector_add <- pivot_point_vector(gbpjpy_daily_pp_add, vector_type = "nearest_diff")
    gbpjpy_daily_vector_add$vector_check <- suppressWarnings(
      gbpjpy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    gbpjpy_daily_vector_add$weekday_trade <- weekdays(gbpjpy_daily_vector_add$Datetime)
    range_daily <- gbpjpy_daily$High[which(gbpjpy_daily$Datetime >= last_pp_datetime)] - gbpjpy_daily$Low[which(gbpjpy_daily$Datetime >= last_pp_datetime)]
    gbpjpy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    gbpjpy_daily_vector_add <- vector_clean_same_oc(gbpjpy_daily_vector_add, see_results = F)
    gbpjpy_daily_vector <<- rbind(gbpjpy_daily_vector, gbpjpy_daily_vector_add)
    writeLines("Successfully Updating GBPJPY PP Data")
    
    last_prevar_datetime <- gbpjpy_prevar$Datetime[nrow(gbpjpy_prevar)]
    which_gbpjpy_rows <- which(gbpjpy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    gbpjpy_pre1 <- precedence_sequences(gbpjpy_daily_vector$vector_pivot[(which_gbpjpy_rows - 6):nrow(gbpjpy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    gbpjpy_pre1 <- pivot_precedence_break(gbpjpy_pre1)
    gbpjpy_prevar_add <- as.data.frame(cbind(WK = gbpjpy_daily_vector$weekday_trade[which_gbpjpy_rows:nrow(gbpjpy_daily_vector)], 
                                             FIB = gbpjpy_daily_vector$FIB_GAP[which_gbpjpy_rows:nrow(gbpjpy_daily_vector)], 
                                             RNG = gbpjpy_daily_vector$prev_range[which_gbpjpy_rows:nrow(gbpjpy_daily_vector)],
                                             CPR = gbpjpy_daily_vector$CPR_GAP[which_gbpjpy_rows:nrow(gbpjpy_daily_vector)],
                                             gbpjpy_pre1))
    gbpjpy_prevar_add$Datetime <- gbpjpy_daily_vector$Datetime[which_gbpjpy_rows:nrow(gbpjpy_daily_vector)]
    gbpjpy_prevar_add$FIB <- NA
    gbpjpy_prevar_add$CPR <- NA
    gbpjpy_prevar_add$RNG <- NA
    gbpjpy_prevar_add$FIB <- factor(gbpjpy_prevar_add$FIB, levels = levels(gbpjpy_prevar$FIB))
    gbpjpy_prevar_add$CPR <- factor(gbpjpy_prevar_add$CPR, levels = levels(gbpjpy_prevar$CPR))
    gbpjpy_prevar_add$RNG <- factor(gbpjpy_prevar_add$RNG, levels = levels(gbpjpy_prevar$RNG))
    gbpjpy_prevar_add <- gbpjpy_prevar_add %>% mutate_if(is.character, as.factor) 
    gbpjpy_prevar <<- rbind(gbpjpy_prevar, gbpjpy_prevar_add)
    writeLines("Successfully Updating GBPJPY PP Precedence Variables")
  }
  
  #----------------------------------------- 11) EURAUD ---------------------------------------------------
  if(pp_overwrite){
    euraud_daily_pp <<- pivot_point_trading(euraud_daily, asset_name = "EURAUD", save_value = T)
    euraud_daily_vector <<- pivot_point_vector(euraud_daily_pp, vector_type = "nearest_diff")
    euraud_daily_vector$vector_check <<- suppressWarnings(
      euraud_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    euraud_daily_vector$weekday_trade <<- weekdays(euraud_daily_vector$Datetime)
    range_daily <- euraud_daily$High - euraud_daily$Low
    euraud_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    euraud_daily_vector <<- vector_clean_same_oc(euraud_daily_vector, see_results = F)
    writeLines("Successfully Updating EURAUD PP Data")
    euraud_pre1 <- precedence_sequences(euraud_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    euraud_pre1 <- pivot_precedence_break(euraud_pre1)
    euraud_prevar <<- as.data.frame(cbind(WK = euraud_daily_vector$weekday_trade[7:nrow(euraud_daily_vector)], 
                                          FIB = euraud_daily_vector$FIB_GAP[7:nrow(euraud_daily_vector)], 
                                          RNG = euraud_daily_vector$prev_range[7:nrow(euraud_daily_vector)],
                                          CPR = euraud_daily_vector$CPR_GAP[7:nrow(euraud_daily_vector)],
                                          euraud_pre1))
    euraud_prevar <<- quantile_cut_df_num(euraud_prevar)
    euraud_prevar <<- euraud_prevar %>% mutate_if(is.character, as.factor) 
    euraud_prevar$Datetime <<- euraud_daily_vector$Datetime[7:nrow(euraud_daily_vector)]
    writeLines("Successfully Updated EURAUD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- euraud_daily_pp$Datetime[nrow(euraud_daily_pp)]
    euraud_daily_pp_add <- pivot_point_trading(euraud_daily[which(euraud_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "EURAUD", save_value = T)
    euraud_daily_pp <<- rbind(euraud_daily_pp, euraud_daily_pp_add[-1,])
    euraud_daily_vector_add <- pivot_point_vector(euraud_daily_pp_add, vector_type = "nearest_diff")
    euraud_daily_vector_add$vector_check <- suppressWarnings(
      euraud_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    euraud_daily_vector_add$weekday_trade <- weekdays(euraud_daily_vector_add$Datetime)
    range_daily <- euraud_daily$High[which(euraud_daily$Datetime >= last_pp_datetime)] - euraud_daily$Low[which(euraud_daily$Datetime >= last_pp_datetime)]
    euraud_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    euraud_daily_vector_add <- vector_clean_same_oc(euraud_daily_vector_add, see_results = F)
    euraud_daily_vector <<- rbind(euraud_daily_vector, euraud_daily_vector_add)
    writeLines("Successfully Updating EURAUD PP Data")
    
    last_prevar_datetime <- euraud_prevar$Datetime[nrow(euraud_prevar)]
    which_euraud_rows <- which(euraud_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    euraud_pre1 <- precedence_sequences(euraud_daily_vector$vector_pivot[(which_euraud_rows - 6):nrow(euraud_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    euraud_pre1 <- pivot_precedence_break(euraud_pre1)
    euraud_prevar_add <- as.data.frame(cbind(WK = euraud_daily_vector$weekday_trade[which_euraud_rows:nrow(euraud_daily_vector)], 
                                             FIB = euraud_daily_vector$FIB_GAP[which_euraud_rows:nrow(euraud_daily_vector)], 
                                             RNG = euraud_daily_vector$prev_range[which_euraud_rows:nrow(euraud_daily_vector)],
                                             CPR = euraud_daily_vector$CPR_GAP[which_euraud_rows:nrow(euraud_daily_vector)],
                                             euraud_pre1))
    euraud_prevar_add$Datetime <- euraud_daily_vector$Datetime[which_euraud_rows:nrow(euraud_daily_vector)]
    euraud_prevar_add$FIB <- NA
    euraud_prevar_add$CPR <- NA
    euraud_prevar_add$RNG <- NA
    euraud_prevar_add$FIB <- factor(euraud_prevar_add$FIB, levels = levels(euraud_prevar$FIB))
    euraud_prevar_add$CPR <- factor(euraud_prevar_add$CPR, levels = levels(euraud_prevar$CPR))
    euraud_prevar_add$RNG <- factor(euraud_prevar_add$RNG, levels = levels(euraud_prevar$RNG))
    euraud_prevar_add <- euraud_prevar_add %>% mutate_if(is.character, as.factor) 
    euraud_prevar <<- rbind(euraud_prevar, euraud_prevar_add)
    writeLines("Successfully Updating EURAUD PP Precedence Variables")
  }
  
  #----------------------------------------- 12) CADCHF ---------------------------------------------------
  if(pp_overwrite){
    cadchf_daily_pp <<- pivot_point_trading(cadchf_daily, asset_name = "CADCHF", save_value = T)
    cadchf_daily_vector <<- pivot_point_vector(cadchf_daily_pp, vector_type = "nearest_diff")
    cadchf_daily_vector$vector_check <<- suppressWarnings(
      cadchf_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    cadchf_daily_vector$weekday_trade <<- weekdays(cadchf_daily_vector$Datetime)
    range_daily <- cadchf_daily$High - cadchf_daily$Low
    cadchf_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    cadchf_daily_vector <<- vector_clean_same_oc(cadchf_daily_vector, see_results = F)
    writeLines("Successfully Updating CADCHF PP Data")
    cadchf_pre1 <- precedence_sequences(cadchf_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    cadchf_pre1 <- pivot_precedence_break(cadchf_pre1)
    cadchf_prevar <<- as.data.frame(cbind(WK = cadchf_daily_vector$weekday_trade[7:nrow(cadchf_daily_vector)], 
                                          FIB = cadchf_daily_vector$FIB_GAP[7:nrow(cadchf_daily_vector)], 
                                          RNG = cadchf_daily_vector$prev_range[7:nrow(cadchf_daily_vector)],
                                          CPR = cadchf_daily_vector$CPR_GAP[7:nrow(cadchf_daily_vector)],
                                          cadchf_pre1))
    cadchf_prevar <<- quantile_cut_df_num(cadchf_prevar)
    cadchf_prevar <<- cadchf_prevar %>% mutate_if(is.character, as.factor) 
    cadchf_prevar$Datetime <<- cadchf_daily_vector$Datetime[7:nrow(cadchf_daily_vector)]
    writeLines("Successfully Updated CADCHF Precedence Variables")
    
  }
  else{
    last_pp_datetime <- cadchf_daily_pp$Datetime[nrow(cadchf_daily_pp)]
    cadchf_daily_pp_add <- pivot_point_trading(cadchf_daily[which(cadchf_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "CADCHF", save_value = T)
    cadchf_daily_pp <<- rbind(cadchf_daily_pp, cadchf_daily_pp_add[-1,])
    cadchf_daily_vector_add <- pivot_point_vector(cadchf_daily_pp_add, vector_type = "nearest_diff")
    cadchf_daily_vector_add$vector_check <- suppressWarnings(
      cadchf_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    cadchf_daily_vector_add$weekday_trade <- weekdays(cadchf_daily_vector_add$Datetime)
    range_daily <- cadchf_daily$High[which(cadchf_daily$Datetime >= last_pp_datetime)] - cadchf_daily$Low[which(cadchf_daily$Datetime >= last_pp_datetime)]
    cadchf_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    cadchf_daily_vector_add <- vector_clean_same_oc(cadchf_daily_vector_add, see_results = F)
    cadchf_daily_vector <<- rbind(cadchf_daily_vector, cadchf_daily_vector_add)
    writeLines("Successfully Updating CADCHF PP Data")
    
    last_prevar_datetime <- cadchf_prevar$Datetime[nrow(cadchf_prevar)]
    which_cadchf_rows <- which(cadchf_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    cadchf_pre1 <- precedence_sequences(cadchf_daily_vector$vector_pivot[(which_cadchf_rows - 6):nrow(cadchf_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    cadchf_pre1 <- pivot_precedence_break(cadchf_pre1)
    cadchf_prevar_add <- as.data.frame(cbind(WK = cadchf_daily_vector$weekday_trade[which_cadchf_rows:nrow(cadchf_daily_vector)], 
                                             FIB = cadchf_daily_vector$FIB_GAP[which_cadchf_rows:nrow(cadchf_daily_vector)], 
                                             RNG = cadchf_daily_vector$prev_range[which_cadchf_rows:nrow(cadchf_daily_vector)],
                                             CPR = cadchf_daily_vector$CPR_GAP[which_cadchf_rows:nrow(cadchf_daily_vector)],
                                             cadchf_pre1))
    cadchf_prevar_add$Datetime <- cadchf_daily_vector$Datetime[which_cadchf_rows:nrow(cadchf_daily_vector)]
    cadchf_prevar_add$FIB <- NA
    cadchf_prevar_add$CPR <- NA
    cadchf_prevar_add$RNG <- NA
    cadchf_prevar_add$FIB <- factor(cadchf_prevar_add$FIB, levels = levels(cadchf_prevar$FIB))
    cadchf_prevar_add$CPR <- factor(cadchf_prevar_add$CPR, levels = levels(cadchf_prevar$CPR))
    cadchf_prevar_add$RNG <- factor(cadchf_prevar_add$RNG, levels = levels(cadchf_prevar$RNG))
    cadchf_prevar_add <- cadchf_prevar_add %>% mutate_if(is.character, as.factor) 
    cadchf_prevar <<- rbind(cadchf_prevar, cadchf_prevar_add)
    writeLines("Successfully Updating CADCHF PP Precedence Variables")
  }
  
  #----------------------------------------- 13) AUDJPY ---------------------------------------------------
  if(pp_overwrite){
    audjpy_daily_pp <<- pivot_point_trading(audjpy_daily, asset_name = "AUDJPY", save_value = T)
    audjpy_daily_vector <<- pivot_point_vector(audjpy_daily_pp, vector_type = "nearest_diff")
    audjpy_daily_vector$vector_check <<- suppressWarnings(
      audjpy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audjpy_daily_vector$weekday_trade <<- weekdays(audjpy_daily_vector$Datetime)
    range_daily <- audjpy_daily$High - audjpy_daily$Low
    audjpy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    audjpy_daily_vector <<- vector_clean_same_oc(audjpy_daily_vector, see_results = F)
    writeLines("Successfully Updating AUDJPY PP Data")
    audjpy_pre1 <- precedence_sequences(audjpy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audjpy_pre1 <- pivot_precedence_break(audjpy_pre1)
    audjpy_prevar <<- as.data.frame(cbind(WK = audjpy_daily_vector$weekday_trade[7:nrow(audjpy_daily_vector)], 
                                          FIB = audjpy_daily_vector$FIB_GAP[7:nrow(audjpy_daily_vector)], 
                                          RNG = audjpy_daily_vector$prev_range[7:nrow(audjpy_daily_vector)],
                                          CPR = audjpy_daily_vector$CPR_GAP[7:nrow(audjpy_daily_vector)],
                                          audjpy_pre1))
    audjpy_prevar <<- quantile_cut_df_num(audjpy_prevar)
    audjpy_prevar <<- audjpy_prevar %>% mutate_if(is.character, as.factor) 
    audjpy_prevar$Datetime <<- audjpy_daily_vector$Datetime[7:nrow(audjpy_daily_vector)]
    writeLines("Successfully Updated AUDJPY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- audjpy_daily_pp$Datetime[nrow(audjpy_daily_pp)]
    audjpy_daily_pp_add <- pivot_point_trading(audjpy_daily[which(audjpy_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "AUDJPY", save_value = T)
    audjpy_daily_pp <<- rbind(audjpy_daily_pp, audjpy_daily_pp_add[-1,])
    audjpy_daily_vector_add <- pivot_point_vector(audjpy_daily_pp_add, vector_type = "nearest_diff")
    audjpy_daily_vector_add$vector_check <- suppressWarnings(
      audjpy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audjpy_daily_vector_add$weekday_trade <- weekdays(audjpy_daily_vector_add$Datetime)
    range_daily <- audjpy_daily$High[which(audjpy_daily$Datetime >= last_pp_datetime)] - audjpy_daily$Low[which(audjpy_daily$Datetime >= last_pp_datetime)]
    audjpy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    audjpy_daily_vector_add <- vector_clean_same_oc(audjpy_daily_vector_add, see_results = F)
    audjpy_daily_vector <<- rbind(audjpy_daily_vector, audjpy_daily_vector_add)
    writeLines("Successfully Updating AUDJPY PP Data")
    
    last_prevar_datetime <- audjpy_prevar$Datetime[nrow(audjpy_prevar)]
    which_audjpy_rows <- which(audjpy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    audjpy_pre1 <- precedence_sequences(audjpy_daily_vector$vector_pivot[(which_audjpy_rows - 6):nrow(audjpy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audjpy_pre1 <- pivot_precedence_break(audjpy_pre1)
    audjpy_prevar_add <- as.data.frame(cbind(WK = audjpy_daily_vector$weekday_trade[which_audjpy_rows:nrow(audjpy_daily_vector)], 
                                             FIB = audjpy_daily_vector$FIB_GAP[which_audjpy_rows:nrow(audjpy_daily_vector)], 
                                             RNG = audjpy_daily_vector$prev_range[which_audjpy_rows:nrow(audjpy_daily_vector)],
                                             CPR = audjpy_daily_vector$CPR_GAP[which_audjpy_rows:nrow(audjpy_daily_vector)],
                                             audjpy_pre1))
    audjpy_prevar_add$Datetime <- audjpy_daily_vector$Datetime[which_audjpy_rows:nrow(audjpy_daily_vector)]
    audjpy_prevar_add$FIB <- NA
    audjpy_prevar_add$CPR <- NA
    audjpy_prevar_add$RNG <- NA
    audjpy_prevar_add$FIB <- factor(audjpy_prevar_add$FIB, levels = levels(audjpy_prevar$FIB))
    audjpy_prevar_add$CPR <- factor(audjpy_prevar_add$CPR, levels = levels(audjpy_prevar$CPR))
    audjpy_prevar_add$RNG <- factor(audjpy_prevar_add$RNG, levels = levels(audjpy_prevar$RNG))
    audjpy_prevar_add <- audjpy_prevar_add %>% mutate_if(is.character, as.factor) 
    audjpy_prevar <<- rbind(audjpy_prevar, audjpy_prevar_add)
    writeLines("Successfully Updating AUDJPY PP Precedence Variables")
  }
  
  #----------------------------------------- 14) EURGBP ---------------------------------------------------
  if(pp_overwrite){
    eurgbp_daily_pp <<- pivot_point_trading(eurgbp_daily, asset_name = "EURGBP", save_value = T)
    eurgbp_daily_vector <<- pivot_point_vector(eurgbp_daily_pp, vector_type = "nearest_diff")
    eurgbp_daily_vector$vector_check <<- suppressWarnings(
      eurgbp_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurgbp_daily_vector$weekday_trade <<- weekdays(eurgbp_daily_vector$Datetime)
    range_daily <- eurgbp_daily$High - eurgbp_daily$Low
    eurgbp_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    eurgbp_daily_vector <<- vector_clean_same_oc(eurgbp_daily_vector, see_results = F)
    writeLines("Successfully Updating EURGBP PP Data")
    eurgbp_pre1 <- precedence_sequences(eurgbp_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurgbp_pre1 <- pivot_precedence_break(eurgbp_pre1)
    eurgbp_prevar <<- as.data.frame(cbind(WK = eurgbp_daily_vector$weekday_trade[7:nrow(eurgbp_daily_vector)], 
                                          FIB = eurgbp_daily_vector$FIB_GAP[7:nrow(eurgbp_daily_vector)], 
                                          RNG = eurgbp_daily_vector$prev_range[7:nrow(eurgbp_daily_vector)],
                                          CPR = eurgbp_daily_vector$CPR_GAP[7:nrow(eurgbp_daily_vector)],
                                          eurgbp_pre1))
    eurgbp_prevar <<- quantile_cut_df_num(eurgbp_prevar)
    eurgbp_prevar <<- eurgbp_prevar %>% mutate_if(is.character, as.factor) 
    eurgbp_prevar$Datetime <<- eurgbp_daily_vector$Datetime[7:nrow(eurgbp_daily_vector)]
    writeLines("Successfully Updated EURGBP Precedence Variables")
    
  }
  else{
    last_pp_datetime <- eurgbp_daily_pp$Datetime[nrow(eurgbp_daily_pp)]
    eurgbp_daily_pp_add <- pivot_point_trading(eurgbp_daily[which(eurgbp_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "EURGBP", save_value = T)
    eurgbp_daily_pp <<- rbind(eurgbp_daily_pp, eurgbp_daily_pp_add[-1,])
    eurgbp_daily_vector_add <- pivot_point_vector(eurgbp_daily_pp_add, vector_type = "nearest_diff")
    eurgbp_daily_vector_add$vector_check <- suppressWarnings(
      eurgbp_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurgbp_daily_vector_add$weekday_trade <- weekdays(eurgbp_daily_vector_add$Datetime)
    range_daily <- eurgbp_daily$High[which(eurgbp_daily$Datetime >= last_pp_datetime)] - eurgbp_daily$Low[which(eurgbp_daily$Datetime >= last_pp_datetime)]
    eurgbp_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    eurgbp_daily_vector_add <- vector_clean_same_oc(eurgbp_daily_vector_add, see_results = F)
    eurgbp_daily_vector <<- rbind(eurgbp_daily_vector, eurgbp_daily_vector_add)
    writeLines("Successfully Updating EURGBP PP Data")
    
    last_prevar_datetime <- eurgbp_prevar$Datetime[nrow(eurgbp_prevar)]
    which_eurgbp_rows <- which(eurgbp_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    eurgbp_pre1 <- precedence_sequences(eurgbp_daily_vector$vector_pivot[(which_eurgbp_rows - 6):nrow(eurgbp_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurgbp_pre1 <- pivot_precedence_break(eurgbp_pre1)
    eurgbp_prevar_add <- as.data.frame(cbind(WK = eurgbp_daily_vector$weekday_trade[which_eurgbp_rows:nrow(eurgbp_daily_vector)], 
                                             FIB = eurgbp_daily_vector$FIB_GAP[which_eurgbp_rows:nrow(eurgbp_daily_vector)], 
                                             RNG = eurgbp_daily_vector$prev_range[which_eurgbp_rows:nrow(eurgbp_daily_vector)],
                                             CPR = eurgbp_daily_vector$CPR_GAP[which_eurgbp_rows:nrow(eurgbp_daily_vector)],
                                             eurgbp_pre1))
    eurgbp_prevar_add$Datetime <- eurgbp_daily_vector$Datetime[which_eurgbp_rows:nrow(eurgbp_daily_vector)]
    eurgbp_prevar_add$FIB <- NA
    eurgbp_prevar_add$CPR <- NA
    eurgbp_prevar_add$RNG <- NA
    eurgbp_prevar_add$FIB <- factor(eurgbp_prevar_add$FIB, levels = levels(eurgbp_prevar$FIB))
    eurgbp_prevar_add$CPR <- factor(eurgbp_prevar_add$CPR, levels = levels(eurgbp_prevar$CPR))
    eurgbp_prevar_add$RNG <- factor(eurgbp_prevar_add$RNG, levels = levels(eurgbp_prevar$RNG))
    eurgbp_prevar_add <- eurgbp_prevar_add %>% mutate_if(is.character, as.factor) 
    eurgbp_prevar <<- rbind(eurgbp_prevar, eurgbp_prevar_add)
    writeLines("Successfully Updating EURGBP PP Precedence Variables")
  }
  
  #----------------------------------------- 15) EURCHF ---------------------------------------------------
  if(pp_overwrite){
    eurchf_daily_pp <<- pivot_point_trading(eurchf_daily, asset_name = "EURCHF", save_value = T)
    eurchf_daily_vector <<- pivot_point_vector(eurchf_daily_pp, vector_type = "nearest_diff")
    eurchf_daily_vector$vector_check <<- suppressWarnings(
      eurchf_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurchf_daily_vector$weekday_trade <<- weekdays(eurchf_daily_vector$Datetime)
    range_daily <- eurchf_daily$High - eurchf_daily$Low
    eurchf_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    eurchf_daily_vector <<- vector_clean_same_oc(eurchf_daily_vector, see_results = F)
    writeLines("Successfully Updating EURCHF PP Data")
    eurchf_pre1 <- precedence_sequences(eurchf_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurchf_pre1 <- pivot_precedence_break(eurchf_pre1)
    eurchf_prevar <<- as.data.frame(cbind(WK = eurchf_daily_vector$weekday_trade[7:nrow(eurchf_daily_vector)], 
                                          FIB = eurchf_daily_vector$FIB_GAP[7:nrow(eurchf_daily_vector)], 
                                          RNG = eurchf_daily_vector$prev_range[7:nrow(eurchf_daily_vector)],
                                          CPR = eurchf_daily_vector$CPR_GAP[7:nrow(eurchf_daily_vector)],
                                          eurchf_pre1))
    eurchf_prevar <<- quantile_cut_df_num(eurchf_prevar)
    eurchf_prevar <<- eurchf_prevar %>% mutate_if(is.character, as.factor) 
    eurchf_prevar$Datetime <<- eurchf_daily_vector$Datetime[7:nrow(eurchf_daily_vector)]
    writeLines("Successfully Updated EURCHF Precedence Variables")
    
  }
  else{
    last_pp_datetime <- eurchf_daily_pp$Datetime[nrow(eurchf_daily_pp)]
    eurchf_daily_pp_add <- pivot_point_trading(eurchf_daily[which(eurchf_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "EURCHF", save_value = T)
    eurchf_daily_pp <<- rbind(eurchf_daily_pp, eurchf_daily_pp_add[-1,])
    eurchf_daily_vector_add <- pivot_point_vector(eurchf_daily_pp_add, vector_type = "nearest_diff")
    eurchf_daily_vector_add$vector_check <- suppressWarnings(
      eurchf_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurchf_daily_vector_add$weekday_trade <- weekdays(eurchf_daily_vector_add$Datetime)
    range_daily <- eurchf_daily$High[which(eurchf_daily$Datetime >= last_pp_datetime)] - eurchf_daily$Low[which(eurchf_daily$Datetime >= last_pp_datetime)]
    eurchf_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    eurchf_daily_vector_add <- vector_clean_same_oc(eurchf_daily_vector_add, see_results = F)
    eurchf_daily_vector <<- rbind(eurchf_daily_vector, eurchf_daily_vector_add)
    writeLines("Successfully Updating EURCHF PP Data")
    
    last_prevar_datetime <- eurchf_prevar$Datetime[nrow(eurchf_prevar)]
    which_eurchf_rows <- which(eurchf_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    eurchf_pre1 <- precedence_sequences(eurchf_daily_vector$vector_pivot[(which_eurchf_rows - 6):nrow(eurchf_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurchf_pre1 <- pivot_precedence_break(eurchf_pre1)
    eurchf_prevar_add <- as.data.frame(cbind(WK = eurchf_daily_vector$weekday_trade[which_eurchf_rows:nrow(eurchf_daily_vector)], 
                                             FIB = eurchf_daily_vector$FIB_GAP[which_eurchf_rows:nrow(eurchf_daily_vector)], 
                                             RNG = eurchf_daily_vector$prev_range[which_eurchf_rows:nrow(eurchf_daily_vector)],
                                             CPR = eurchf_daily_vector$CPR_GAP[which_eurchf_rows:nrow(eurchf_daily_vector)],
                                             eurchf_pre1))
    eurchf_prevar_add$Datetime <- eurchf_daily_vector$Datetime[which_eurchf_rows:nrow(eurchf_daily_vector)]
    eurchf_prevar_add$FIB <- NA
    eurchf_prevar_add$CPR <- NA
    eurchf_prevar_add$RNG <- NA
    eurchf_prevar_add$FIB <- factor(eurchf_prevar_add$FIB, levels = levels(eurchf_prevar$FIB))
    eurchf_prevar_add$CPR <- factor(eurchf_prevar_add$CPR, levels = levels(eurchf_prevar$CPR))
    eurchf_prevar_add$RNG <- factor(eurchf_prevar_add$RNG, levels = levels(eurchf_prevar$RNG))
    eurchf_prevar_add <- eurchf_prevar_add %>% mutate_if(is.character, as.factor) 
    eurchf_prevar <<- rbind(eurchf_prevar, eurchf_prevar_add)
    writeLines("Successfully Updating EURCHF PP Precedence Variables")
  }
  
  #----------------------------------------- 16) GBPCHF ---------------------------------------------------
  if(pp_overwrite){
    gbpchf_daily_pp <<- pivot_point_trading(gbpchf_daily, asset_name = "GBPCHF", save_value = T)
    gbpchf_daily_vector <<- pivot_point_vector(gbpchf_daily_pp, vector_type = "nearest_diff")
    gbpchf_daily_vector$vector_check <<- suppressWarnings(
      gbpchf_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    gbpchf_daily_vector$weekday_trade <<- weekdays(gbpchf_daily_vector$Datetime)
    range_daily <- gbpchf_daily$High - gbpchf_daily$Low
    gbpchf_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    gbpchf_daily_vector <<- vector_clean_same_oc(gbpchf_daily_vector, see_results = F)
    writeLines("Successfully Updating GBPCHF PP Data")
    gbpchf_pre1 <- precedence_sequences(gbpchf_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    gbpchf_pre1 <- pivot_precedence_break(gbpchf_pre1)
    gbpchf_prevar <<- as.data.frame(cbind(WK = gbpchf_daily_vector$weekday_trade[7:nrow(gbpchf_daily_vector)], 
                                          FIB = gbpchf_daily_vector$FIB_GAP[7:nrow(gbpchf_daily_vector)], 
                                          RNG = gbpchf_daily_vector$prev_range[7:nrow(gbpchf_daily_vector)],
                                          CPR = gbpchf_daily_vector$CPR_GAP[7:nrow(gbpchf_daily_vector)],
                                          gbpchf_pre1))
    gbpchf_prevar <<- quantile_cut_df_num(gbpchf_prevar)
    gbpchf_prevar <<- gbpchf_prevar %>% mutate_if(is.character, as.factor) 
    gbpchf_prevar$Datetime <<- gbpchf_daily_vector$Datetime[7:nrow(gbpchf_daily_vector)]
    writeLines("Successfully Updated GBPCHF Precedence Variables")
    
  }
  else{
    last_pp_datetime <- gbpchf_daily_pp$Datetime[nrow(gbpchf_daily_pp)]
    gbpchf_daily_pp_add <- pivot_point_trading(gbpchf_daily[which(gbpchf_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "GBPCHF", save_value = T)
    gbpchf_daily_pp <<- rbind(gbpchf_daily_pp, gbpchf_daily_pp_add[-1,])
    gbpchf_daily_vector_add <- pivot_point_vector(gbpchf_daily_pp_add, vector_type = "nearest_diff")
    gbpchf_daily_vector_add$vector_check <- suppressWarnings(
      gbpchf_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    gbpchf_daily_vector_add$weekday_trade <- weekdays(gbpchf_daily_vector_add$Datetime)
    range_daily <- gbpchf_daily$High[which(gbpchf_daily$Datetime >= last_pp_datetime)] - gbpchf_daily$Low[which(gbpchf_daily$Datetime >= last_pp_datetime)]
    gbpchf_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    gbpchf_daily_vector_add <- vector_clean_same_oc(gbpchf_daily_vector_add, see_results = F)
    gbpchf_daily_vector <<- rbind(gbpchf_daily_vector, gbpchf_daily_vector_add)
    writeLines("Successfully Updating GBPCHF PP Data")
    
    last_prevar_datetime <- gbpchf_prevar$Datetime[nrow(gbpchf_prevar)]
    which_gbpchf_rows <- which(gbpchf_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    gbpchf_pre1 <- precedence_sequences(gbpchf_daily_vector$vector_pivot[(which_gbpchf_rows - 6):nrow(gbpchf_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    gbpchf_pre1 <- pivot_precedence_break(gbpchf_pre1)
    gbpchf_prevar_add <- as.data.frame(cbind(WK = gbpchf_daily_vector$weekday_trade[which_gbpchf_rows:nrow(gbpchf_daily_vector)], 
                                             FIB = gbpchf_daily_vector$FIB_GAP[which_gbpchf_rows:nrow(gbpchf_daily_vector)], 
                                             RNG = gbpchf_daily_vector$prev_range[which_gbpchf_rows:nrow(gbpchf_daily_vector)],
                                             CPR = gbpchf_daily_vector$CPR_GAP[which_gbpchf_rows:nrow(gbpchf_daily_vector)],
                                             gbpchf_pre1))
    gbpchf_prevar_add$Datetime <- gbpchf_daily_vector$Datetime[which_gbpchf_rows:nrow(gbpchf_daily_vector)]
    gbpchf_prevar_add$FIB <- NA
    gbpchf_prevar_add$CPR <- NA
    gbpchf_prevar_add$RNG <- NA
    gbpchf_prevar_add$FIB <- factor(gbpchf_prevar_add$FIB, levels = levels(gbpchf_prevar$FIB))
    gbpchf_prevar_add$CPR <- factor(gbpchf_prevar_add$CPR, levels = levels(gbpchf_prevar$CPR))
    gbpchf_prevar_add$RNG <- factor(gbpchf_prevar_add$RNG, levels = levels(gbpchf_prevar$RNG))
    gbpchf_prevar_add <- gbpchf_prevar_add %>% mutate_if(is.character, as.factor) 
    gbpchf_prevar <<- rbind(gbpchf_prevar, gbpchf_prevar_add)
    writeLines("Successfully Updating GBPCHF PP Precedence Variables")
  }
  
  #----------------------------------------- 17) CADJPY ---------------------------------------------------
  if(pp_overwrite){
    cadjpy_daily_pp <<- pivot_point_trading(cadjpy_daily, asset_name = "CADJPY", save_value = T)
    cadjpy_daily_vector <<- pivot_point_vector(cadjpy_daily_pp, vector_type = "nearest_diff")
    cadjpy_daily_vector$vector_check <<- suppressWarnings(
      cadjpy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    cadjpy_daily_vector$weekday_trade <<- weekdays(cadjpy_daily_vector$Datetime)
    range_daily <- cadjpy_daily$High - cadjpy_daily$Low
    cadjpy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    cadjpy_daily_vector <<- vector_clean_same_oc(cadjpy_daily_vector, see_results = F)
    writeLines("Successfully Updating CADJPY PP Data")
    cadjpy_pre1 <- precedence_sequences(cadjpy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    cadjpy_pre1 <- pivot_precedence_break(cadjpy_pre1)
    cadjpy_prevar <<- as.data.frame(cbind(WK = cadjpy_daily_vector$weekday_trade[7:nrow(cadjpy_daily_vector)], 
                                          FIB = cadjpy_daily_vector$FIB_GAP[7:nrow(cadjpy_daily_vector)], 
                                          RNG = cadjpy_daily_vector$prev_range[7:nrow(cadjpy_daily_vector)],
                                          CPR = cadjpy_daily_vector$CPR_GAP[7:nrow(cadjpy_daily_vector)],
                                          cadjpy_pre1))
    cadjpy_prevar <<- quantile_cut_df_num(cadjpy_prevar)
    cadjpy_prevar <<- cadjpy_prevar %>% mutate_if(is.character, as.factor) 
    cadjpy_prevar$Datetime <<- cadjpy_daily_vector$Datetime[7:nrow(cadjpy_daily_vector)]
    writeLines("Successfully Updated CADJPY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- cadjpy_daily_pp$Datetime[nrow(cadjpy_daily_pp)]
    cadjpy_daily_pp_add <- pivot_point_trading(cadjpy_daily[which(cadjpy_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "CADJPY", save_value = T)
    cadjpy_daily_pp <<- rbind(cadjpy_daily_pp, cadjpy_daily_pp_add[-1,])
    cadjpy_daily_vector_add <- pivot_point_vector(cadjpy_daily_pp_add, vector_type = "nearest_diff")
    cadjpy_daily_vector_add$vector_check <- suppressWarnings(
      cadjpy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    cadjpy_daily_vector_add$weekday_trade <- weekdays(cadjpy_daily_vector_add$Datetime)
    range_daily <- cadjpy_daily$High[which(cadjpy_daily$Datetime >= last_pp_datetime)] - cadjpy_daily$Low[which(cadjpy_daily$Datetime >= last_pp_datetime)]
    cadjpy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    cadjpy_daily_vector_add <- vector_clean_same_oc(cadjpy_daily_vector_add, see_results = F)
    cadjpy_daily_vector <<- rbind(cadjpy_daily_vector, cadjpy_daily_vector_add)
    writeLines("Successfully Updating CADJPY PP Data")
    
    last_prevar_datetime <- cadjpy_prevar$Datetime[nrow(cadjpy_prevar)]
    which_cadjpy_rows <- which(cadjpy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    cadjpy_pre1 <- precedence_sequences(cadjpy_daily_vector$vector_pivot[(which_cadjpy_rows - 6):nrow(cadjpy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    cadjpy_pre1 <- pivot_precedence_break(cadjpy_pre1)
    cadjpy_prevar_add <- as.data.frame(cbind(WK = cadjpy_daily_vector$weekday_trade[which_cadjpy_rows:nrow(cadjpy_daily_vector)], 
                                             FIB = cadjpy_daily_vector$FIB_GAP[which_cadjpy_rows:nrow(cadjpy_daily_vector)], 
                                             RNG = cadjpy_daily_vector$prev_range[which_cadjpy_rows:nrow(cadjpy_daily_vector)],
                                             CPR = cadjpy_daily_vector$CPR_GAP[which_cadjpy_rows:nrow(cadjpy_daily_vector)],
                                             cadjpy_pre1))
    cadjpy_prevar_add$Datetime <- cadjpy_daily_vector$Datetime[which_cadjpy_rows:nrow(cadjpy_daily_vector)]
    cadjpy_prevar_add$FIB <- NA
    cadjpy_prevar_add$CPR <- NA
    cadjpy_prevar_add$RNG <- NA
    cadjpy_prevar_add$FIB <- factor(cadjpy_prevar_add$FIB, levels = levels(cadjpy_prevar$FIB))
    cadjpy_prevar_add$CPR <- factor(cadjpy_prevar_add$CPR, levels = levels(cadjpy_prevar$CPR))
    cadjpy_prevar_add$RNG <- factor(cadjpy_prevar_add$RNG, levels = levels(cadjpy_prevar$RNG))
    cadjpy_prevar_add <- cadjpy_prevar_add %>% mutate_if(is.character, as.factor) 
    cadjpy_prevar <<- rbind(cadjpy_prevar, cadjpy_prevar_add)
    writeLines("Successfully Updating CADJPY PP Precedence Variables")
  }
  
  #----------------------------------------- 18) AUDCAD ---------------------------------------------------
  if(pp_overwrite){
    audcad_daily_pp <<- pivot_point_trading(audcad_daily, asset_name = "AUDCAD", save_value = T)
    audcad_daily_vector <<- pivot_point_vector(audcad_daily_pp, vector_type = "nearest_diff")
    audcad_daily_vector$vector_check <<- suppressWarnings(
      audcad_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audcad_daily_vector$weekday_trade <<- weekdays(audcad_daily_vector$Datetime)
    range_daily <- audcad_daily$High - audcad_daily$Low
    audcad_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    audcad_daily_vector <<- vector_clean_same_oc(audcad_daily_vector, see_results = F)
    writeLines("Successfully Updating AUDCAD PP Data")
    audcad_pre1 <- precedence_sequences(audcad_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audcad_pre1 <- pivot_precedence_break(audcad_pre1)
    audcad_prevar <<- as.data.frame(cbind(WK = audcad_daily_vector$weekday_trade[7:nrow(audcad_daily_vector)], 
                                          FIB = audcad_daily_vector$FIB_GAP[7:nrow(audcad_daily_vector)], 
                                          RNG = audcad_daily_vector$prev_range[7:nrow(audcad_daily_vector)],
                                          CPR = audcad_daily_vector$CPR_GAP[7:nrow(audcad_daily_vector)],
                                          audcad_pre1))
    audcad_prevar <<- quantile_cut_df_num(audcad_prevar)
    audcad_prevar <<- audcad_prevar %>% mutate_if(is.character, as.factor) 
    audcad_prevar$Datetime <<- audcad_daily_vector$Datetime[7:nrow(audcad_daily_vector)]
    writeLines("Successfully Updated AUDCAD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- audcad_daily_pp$Datetime[nrow(audcad_daily_pp)]
    audcad_daily_pp_add <- pivot_point_trading(audcad_daily[which(audcad_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "AUDCAD", save_value = T)
    audcad_daily_pp <<- rbind(audcad_daily_pp, audcad_daily_pp_add[-1,])
    audcad_daily_vector_add <- pivot_point_vector(audcad_daily_pp_add, vector_type = "nearest_diff")
    audcad_daily_vector_add$vector_check <- suppressWarnings(
      audcad_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audcad_daily_vector_add$weekday_trade <- weekdays(audcad_daily_vector_add$Datetime)
    range_daily <- audcad_daily$High[which(audcad_daily$Datetime >= last_pp_datetime)] - audcad_daily$Low[which(audcad_daily$Datetime >= last_pp_datetime)]
    audcad_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    audcad_daily_vector_add <- vector_clean_same_oc(audcad_daily_vector_add, see_results = F)
    audcad_daily_vector <<- rbind(audcad_daily_vector, audcad_daily_vector_add)
    writeLines("Successfully Updating AUDCAD PP Data")
    
    last_prevar_datetime <- audcad_prevar$Datetime[nrow(audcad_prevar)]
    which_audcad_rows <- which(audcad_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    audcad_pre1 <- precedence_sequences(audcad_daily_vector$vector_pivot[(which_audcad_rows - 6):nrow(audcad_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audcad_pre1 <- pivot_precedence_break(audcad_pre1)
    audcad_prevar_add <- as.data.frame(cbind(WK = audcad_daily_vector$weekday_trade[which_audcad_rows:nrow(audcad_daily_vector)], 
                                             FIB = audcad_daily_vector$FIB_GAP[which_audcad_rows:nrow(audcad_daily_vector)], 
                                             RNG = audcad_daily_vector$prev_range[which_audcad_rows:nrow(audcad_daily_vector)],
                                             CPR = audcad_daily_vector$CPR_GAP[which_audcad_rows:nrow(audcad_daily_vector)],
                                             audcad_pre1))
    audcad_prevar_add$Datetime <- audcad_daily_vector$Datetime[which_audcad_rows:nrow(audcad_daily_vector)]
    audcad_prevar_add$FIB <- NA
    audcad_prevar_add$CPR <- NA
    audcad_prevar_add$RNG <- NA
    audcad_prevar_add$FIB <- factor(audcad_prevar_add$FIB, levels = levels(audcad_prevar$FIB))
    audcad_prevar_add$CPR <- factor(audcad_prevar_add$CPR, levels = levels(audcad_prevar$CPR))
    audcad_prevar_add$RNG <- factor(audcad_prevar_add$RNG, levels = levels(audcad_prevar$RNG))
    audcad_prevar_add <- audcad_prevar_add %>% mutate_if(is.character, as.factor) 
    audcad_prevar <<- rbind(audcad_prevar, audcad_prevar_add)
    writeLines("Successfully Updating AUDCAD PP Precedence Variables")
  }
  
  #----------------------------------------- 19) AUDCHF ---------------------------------------------------
  if(pp_overwrite){
    audchf_daily_pp <<- pivot_point_trading(audchf_daily, asset_name = "AUDCHF", save_value = T)
    audchf_daily_vector <<- pivot_point_vector(audchf_daily_pp, vector_type = "nearest_diff")
    audchf_daily_vector$vector_check <<- suppressWarnings(
      audchf_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audchf_daily_vector$weekday_trade <<- weekdays(audchf_daily_vector$Datetime)
    range_daily <- audchf_daily$High - audchf_daily$Low
    audchf_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    audchf_daily_vector <<- vector_clean_same_oc(audchf_daily_vector, see_results = F)
    writeLines("Successfully Updating AUDCHF PP Data")
    audchf_pre1 <- precedence_sequences(audchf_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audchf_pre1 <- pivot_precedence_break(audchf_pre1)
    audchf_prevar <<- as.data.frame(cbind(WK = audchf_daily_vector$weekday_trade[7:nrow(audchf_daily_vector)], 
                                          FIB = audchf_daily_vector$FIB_GAP[7:nrow(audchf_daily_vector)], 
                                          RNG = audchf_daily_vector$prev_range[7:nrow(audchf_daily_vector)],
                                          CPR = audchf_daily_vector$CPR_GAP[7:nrow(audchf_daily_vector)],
                                          audchf_pre1))
    audchf_prevar <<- quantile_cut_df_num(audchf_prevar)
    audchf_prevar <<- audchf_prevar %>% mutate_if(is.character, as.factor) 
    audchf_prevar$Datetime <<- audchf_daily_vector$Datetime[7:nrow(audchf_daily_vector)]
    writeLines("Successfully Updated AUDCHF Precedence Variables")
    
  }
  else{
    last_pp_datetime <- audchf_daily_pp$Datetime[nrow(audchf_daily_pp)]
    audchf_daily_pp_add <- pivot_point_trading(audchf_daily[which(audchf_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "AUDCHF", save_value = T)
    audchf_daily_pp <<- rbind(audchf_daily_pp, audchf_daily_pp_add[-1,])
    audchf_daily_vector_add <- pivot_point_vector(audchf_daily_pp_add, vector_type = "nearest_diff")
    audchf_daily_vector_add$vector_check <- suppressWarnings(
      audchf_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    audchf_daily_vector_add$weekday_trade <- weekdays(audchf_daily_vector_add$Datetime)
    range_daily <- audchf_daily$High[which(audchf_daily$Datetime >= last_pp_datetime)] - audchf_daily$Low[which(audchf_daily$Datetime >= last_pp_datetime)]
    audchf_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    audchf_daily_vector_add <- vector_clean_same_oc(audchf_daily_vector_add, see_results = F)
    audchf_daily_vector <<- rbind(audchf_daily_vector, audchf_daily_vector_add)
    writeLines("Successfully Updating AUDCHF PP Data")
    
    last_prevar_datetime <- audchf_prevar$Datetime[nrow(audchf_prevar)]
    which_audchf_rows <- which(audchf_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    audchf_pre1 <- precedence_sequences(audchf_daily_vector$vector_pivot[(which_audchf_rows - 6):nrow(audchf_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    audchf_pre1 <- pivot_precedence_break(audchf_pre1)
    audchf_prevar_add <- as.data.frame(cbind(WK = audchf_daily_vector$weekday_trade[which_audchf_rows:nrow(audchf_daily_vector)], 
                                             FIB = audchf_daily_vector$FIB_GAP[which_audchf_rows:nrow(audchf_daily_vector)], 
                                             RNG = audchf_daily_vector$prev_range[which_audchf_rows:nrow(audchf_daily_vector)],
                                             CPR = audchf_daily_vector$CPR_GAP[which_audchf_rows:nrow(audchf_daily_vector)],
                                             audchf_pre1))
    audchf_prevar_add$Datetime <- audchf_daily_vector$Datetime[which_audchf_rows:nrow(audchf_daily_vector)]
    audchf_prevar_add$FIB <- NA
    audchf_prevar_add$CPR <- NA
    audchf_prevar_add$RNG <- NA
    audchf_prevar_add$FIB <- factor(audchf_prevar_add$FIB, levels = levels(audchf_prevar$FIB))
    audchf_prevar_add$CPR <- factor(audchf_prevar_add$CPR, levels = levels(audchf_prevar$CPR))
    audchf_prevar_add$RNG <- factor(audchf_prevar_add$RNG, levels = levels(audchf_prevar$RNG))
    audchf_prevar_add <- audchf_prevar_add %>% mutate_if(is.character, as.factor) 
    audchf_prevar <<- rbind(audchf_prevar, audchf_prevar_add)
    writeLines("Successfully Updating AUDCHF PP Precedence Variables")
  }
  
  #----------------------------------------- 20) CHFJPY ---------------------------------------------------
  if(pp_overwrite){
    chfjpy_daily_pp <<- pivot_point_trading(chfjpy_daily, asset_name = "CHFJPY", save_value = T)
    chfjpy_daily_vector <<- pivot_point_vector(chfjpy_daily_pp, vector_type = "nearest_diff")
    chfjpy_daily_vector$vector_check <<- suppressWarnings(
      chfjpy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    chfjpy_daily_vector$weekday_trade <<- weekdays(chfjpy_daily_vector$Datetime)
    range_daily <- chfjpy_daily$High - chfjpy_daily$Low
    chfjpy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    chfjpy_daily_vector <<- vector_clean_same_oc(chfjpy_daily_vector, see_results = F)
    writeLines("Successfully Updating CHFJPY PP Data")
    chfjpy_pre1 <- precedence_sequences(chfjpy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    chfjpy_pre1 <- pivot_precedence_break(chfjpy_pre1)
    chfjpy_prevar <<- as.data.frame(cbind(WK = chfjpy_daily_vector$weekday_trade[7:nrow(chfjpy_daily_vector)], 
                                          FIB = chfjpy_daily_vector$FIB_GAP[7:nrow(chfjpy_daily_vector)], 
                                          RNG = chfjpy_daily_vector$prev_range[7:nrow(chfjpy_daily_vector)],
                                          CPR = chfjpy_daily_vector$CPR_GAP[7:nrow(chfjpy_daily_vector)],
                                          chfjpy_pre1))
    chfjpy_prevar <<- quantile_cut_df_num(chfjpy_prevar)
    chfjpy_prevar <<- chfjpy_prevar %>% mutate_if(is.character, as.factor) 
    chfjpy_prevar$Datetime <<- chfjpy_daily_vector$Datetime[7:nrow(chfjpy_daily_vector)]
    writeLines("Successfully Updated CHFJPY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- chfjpy_daily_pp$Datetime[nrow(chfjpy_daily_pp)]
    chfjpy_daily_pp_add <- pivot_point_trading(chfjpy_daily[which(chfjpy_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "CHFJPY", save_value = T)
    chfjpy_daily_pp <<- rbind(chfjpy_daily_pp, chfjpy_daily_pp_add[-1,])
    chfjpy_daily_vector_add <- pivot_point_vector(chfjpy_daily_pp_add, vector_type = "nearest_diff")
    chfjpy_daily_vector_add$vector_check <- suppressWarnings(
      chfjpy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    chfjpy_daily_vector_add$weekday_trade <- weekdays(chfjpy_daily_vector_add$Datetime)
    range_daily <- chfjpy_daily$High[which(chfjpy_daily$Datetime >= last_pp_datetime)] - chfjpy_daily$Low[which(chfjpy_daily$Datetime >= last_pp_datetime)]
    chfjpy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    chfjpy_daily_vector_add <- vector_clean_same_oc(chfjpy_daily_vector_add, see_results = F)
    chfjpy_daily_vector <<- rbind(chfjpy_daily_vector, chfjpy_daily_vector_add)
    writeLines("Successfully Updating CHFJPY PP Data")
    
    last_prevar_datetime <- chfjpy_prevar$Datetime[nrow(chfjpy_prevar)]
    which_chfjpy_rows <- which(chfjpy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    chfjpy_pre1 <- precedence_sequences(chfjpy_daily_vector$vector_pivot[(which_chfjpy_rows - 6):nrow(chfjpy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    chfjpy_pre1 <- pivot_precedence_break(chfjpy_pre1)
    chfjpy_prevar_add <- as.data.frame(cbind(WK = chfjpy_daily_vector$weekday_trade[which_chfjpy_rows:nrow(chfjpy_daily_vector)], 
                                             FIB = chfjpy_daily_vector$FIB_GAP[which_chfjpy_rows:nrow(chfjpy_daily_vector)], 
                                             RNG = chfjpy_daily_vector$prev_range[which_chfjpy_rows:nrow(chfjpy_daily_vector)],
                                             CPR = chfjpy_daily_vector$CPR_GAP[which_chfjpy_rows:nrow(chfjpy_daily_vector)],
                                             chfjpy_pre1))
    chfjpy_prevar_add$Datetime <- chfjpy_daily_vector$Datetime[which_chfjpy_rows:nrow(chfjpy_daily_vector)]
    chfjpy_prevar_add$FIB <- NA
    chfjpy_prevar_add$CPR <- NA
    chfjpy_prevar_add$RNG <- NA
    chfjpy_prevar_add$FIB <- factor(chfjpy_prevar_add$FIB, levels = levels(chfjpy_prevar$FIB))
    chfjpy_prevar_add$CPR <- factor(chfjpy_prevar_add$CPR, levels = levels(chfjpy_prevar$CPR))
    chfjpy_prevar_add$RNG <- factor(chfjpy_prevar_add$RNG, levels = levels(chfjpy_prevar$RNG))
    chfjpy_prevar_add <- chfjpy_prevar_add %>% mutate_if(is.character, as.factor) 
    chfjpy_prevar <<- rbind(chfjpy_prevar, chfjpy_prevar_add)
    writeLines("Successfully Updating CHFJPY PP Precedence Variables")
  }
  
  #----------------------------------------- 21) EURNZD ---------------------------------------------------
  if(pp_overwrite){
    eurnzd_daily_pp <<- pivot_point_trading(eurnzd_daily, asset_name = "EURNZD", save_value = T)
    eurnzd_daily_vector <<- pivot_point_vector(eurnzd_daily_pp, vector_type = "nearest_diff")
    eurnzd_daily_vector$vector_check <<- suppressWarnings(
      eurnzd_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurnzd_daily_vector$weekday_trade <<- weekdays(eurnzd_daily_vector$Datetime)
    range_daily <- eurnzd_daily$High - eurnzd_daily$Low
    eurnzd_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    eurnzd_daily_vector <<- vector_clean_same_oc(eurnzd_daily_vector, see_results = F)
    writeLines("Successfully Updating EURNZD PP Data")
    eurnzd_pre1 <- precedence_sequences(eurnzd_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurnzd_pre1 <- pivot_precedence_break(eurnzd_pre1)
    eurnzd_prevar <<- as.data.frame(cbind(WK = eurnzd_daily_vector$weekday_trade[7:nrow(eurnzd_daily_vector)], 
                                          FIB = eurnzd_daily_vector$FIB_GAP[7:nrow(eurnzd_daily_vector)], 
                                          RNG = eurnzd_daily_vector$prev_range[7:nrow(eurnzd_daily_vector)],
                                          CPR = eurnzd_daily_vector$CPR_GAP[7:nrow(eurnzd_daily_vector)],
                                          eurnzd_pre1))
    eurnzd_prevar <<- quantile_cut_df_num(eurnzd_prevar)
    eurnzd_prevar <<- eurnzd_prevar %>% mutate_if(is.character, as.factor) 
    eurnzd_prevar$Datetime <<- eurnzd_daily_vector$Datetime[7:nrow(eurnzd_daily_vector)]
    writeLines("Successfully Updated EURNZD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- eurnzd_daily_pp$Datetime[nrow(eurnzd_daily_pp)]
    eurnzd_daily_pp_add <- pivot_point_trading(eurnzd_daily[which(eurnzd_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "EURNZD", save_value = T)
    eurnzd_daily_pp <<- rbind(eurnzd_daily_pp, eurnzd_daily_pp_add[-1,])
    eurnzd_daily_vector_add <- pivot_point_vector(eurnzd_daily_pp_add, vector_type = "nearest_diff")
    eurnzd_daily_vector_add$vector_check <- suppressWarnings(
      eurnzd_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurnzd_daily_vector_add$weekday_trade <- weekdays(eurnzd_daily_vector_add$Datetime)
    range_daily <- eurnzd_daily$High[which(eurnzd_daily$Datetime >= last_pp_datetime)] - eurnzd_daily$Low[which(eurnzd_daily$Datetime >= last_pp_datetime)]
    eurnzd_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    eurnzd_daily_vector_add <- vector_clean_same_oc(eurnzd_daily_vector_add, see_results = F)
    eurnzd_daily_vector <<- rbind(eurnzd_daily_vector, eurnzd_daily_vector_add)
    writeLines("Successfully Updating EURNZD PP Data")
    
    last_prevar_datetime <- eurnzd_prevar$Datetime[nrow(eurnzd_prevar)]
    which_eurnzd_rows <- which(eurnzd_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    eurnzd_pre1 <- precedence_sequences(eurnzd_daily_vector$vector_pivot[(which_eurnzd_rows - 6):nrow(eurnzd_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurnzd_pre1 <- pivot_precedence_break(eurnzd_pre1)
    eurnzd_prevar_add <- as.data.frame(cbind(WK = eurnzd_daily_vector$weekday_trade[which_eurnzd_rows:nrow(eurnzd_daily_vector)], 
                                             FIB = eurnzd_daily_vector$FIB_GAP[which_eurnzd_rows:nrow(eurnzd_daily_vector)], 
                                             RNG = eurnzd_daily_vector$prev_range[which_eurnzd_rows:nrow(eurnzd_daily_vector)],
                                             CPR = eurnzd_daily_vector$CPR_GAP[which_eurnzd_rows:nrow(eurnzd_daily_vector)],
                                             eurnzd_pre1))
    eurnzd_prevar_add$Datetime <- eurnzd_daily_vector$Datetime[which_eurnzd_rows:nrow(eurnzd_daily_vector)]
    
    eurnzd_prevar_add$FIB <- NA
    eurnzd_prevar_add$CPR <- NA
    eurnzd_prevar_add$RNG <- NA
    eurnzd_prevar_add$FIB <- factor(eurnzd_prevar_add$FIB, levels = levels(eurnzd_prevar$FIB))
    eurnzd_prevar_add$CPR <- factor(eurnzd_prevar_add$CPR, levels = levels(eurnzd_prevar$CPR))
    eurnzd_prevar_add$RNG <- factor(eurnzd_prevar_add$RNG, levels = levels(eurnzd_prevar$RNG))
    eurnzd_prevar_add <- eurnzd_prevar_add %>% mutate_if(is.character, as.factor) 
    eurnzd_prevar <<- rbind(eurnzd_prevar, eurnzd_prevar_add)
    writeLines("Successfully Updating EURNZD PP Precedence Variables")
  }
  
  #----------------------------------------- 22) EURCAD ---------------------------------------------------
  if(pp_overwrite){
    eurcad_daily_pp <<- pivot_point_trading(eurcad_daily, asset_name = "EURCAD", save_value = T)
    eurcad_daily_vector <<- pivot_point_vector(eurcad_daily_pp, vector_type = "nearest_diff")
    eurcad_daily_vector$vector_check <<- suppressWarnings(
      eurcad_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurcad_daily_vector$weekday_trade <<- weekdays(eurcad_daily_vector$Datetime)
    range_daily <- eurcad_daily$High - eurcad_daily$Low
    eurcad_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    eurcad_daily_vector <<- vector_clean_same_oc(eurcad_daily_vector, see_results = F)
    writeLines("Successfully Updating EURCAD PP Data")
    eurcad_pre1 <- precedence_sequences(eurcad_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurcad_pre1 <- pivot_precedence_break(eurcad_pre1)
    eurcad_prevar <<- as.data.frame(cbind(WK = eurcad_daily_vector$weekday_trade[7:nrow(eurcad_daily_vector)], 
                                          FIB = eurcad_daily_vector$FIB_GAP[7:nrow(eurcad_daily_vector)], 
                                          RNG = eurcad_daily_vector$prev_range[7:nrow(eurcad_daily_vector)],
                                          CPR = eurcad_daily_vector$CPR_GAP[7:nrow(eurcad_daily_vector)],
                                          eurcad_pre1))
    eurcad_prevar <<- quantile_cut_df_num(eurcad_prevar)
    eurcad_prevar <<- eurcad_prevar %>% mutate_if(is.character, as.factor) 
    eurcad_prevar$Datetime <<- eurcad_daily_vector$Datetime[7:nrow(eurcad_daily_vector)]
    writeLines("Successfully Updated EURCAD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- eurcad_daily_pp$Datetime[nrow(eurcad_daily_pp)]
    eurcad_daily_pp_add <- pivot_point_trading(eurcad_daily[which(eurcad_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "EURCAD", save_value = T)
    eurcad_daily_pp <<- rbind(eurcad_daily_pp, eurcad_daily_pp_add[-1,])
    eurcad_daily_vector_add <- pivot_point_vector(eurcad_daily_pp_add, vector_type = "nearest_diff")
    eurcad_daily_vector_add$vector_check <- suppressWarnings(
      eurcad_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    eurcad_daily_vector_add$weekday_trade <- weekdays(eurcad_daily_vector_add$Datetime)
    range_daily <- eurcad_daily$High[which(eurcad_daily$Datetime >= last_pp_datetime)] - eurcad_daily$Low[which(eurcad_daily$Datetime >= last_pp_datetime)]
    eurcad_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    eurcad_daily_vector_add <- vector_clean_same_oc(eurcad_daily_vector_add, see_results = F)
    eurcad_daily_vector <<- rbind(eurcad_daily_vector, eurcad_daily_vector_add)
    writeLines("Successfully Updating EURCAD PP Data")
    
    last_prevar_datetime <- eurcad_prevar$Datetime[nrow(eurcad_prevar)]
    which_eurcad_rows <- which(eurcad_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    eurcad_pre1 <- precedence_sequences(eurcad_daily_vector$vector_pivot[(which_eurcad_rows - 6):nrow(eurcad_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    eurcad_pre1 <- pivot_precedence_break(eurcad_pre1)
    eurcad_prevar_add <- as.data.frame(cbind(WK = eurcad_daily_vector$weekday_trade[which_eurcad_rows:nrow(eurcad_daily_vector)], 
                                             FIB = eurcad_daily_vector$FIB_GAP[which_eurcad_rows:nrow(eurcad_daily_vector)], 
                                             RNG = eurcad_daily_vector$prev_range[which_eurcad_rows:nrow(eurcad_daily_vector)],
                                             CPR = eurcad_daily_vector$CPR_GAP[which_eurcad_rows:nrow(eurcad_daily_vector)],
                                             eurcad_pre1))
    eurcad_prevar_add$Datetime <- eurcad_daily_vector$Datetime[which_eurcad_rows:nrow(eurcad_daily_vector)]
    eurcad_prevar_add$FIB <- NA
    eurcad_prevar_add$CPR <- NA
    eurcad_prevar_add$RNG <- NA
    eurcad_prevar_add$FIB <- factor(eurcad_prevar_add$FIB, levels = levels(eurcad_prevar$FIB))
    eurcad_prevar_add$CPR <- factor(eurcad_prevar_add$CPR, levels = levels(eurcad_prevar$CPR))
    eurcad_prevar_add$RNG <- factor(eurcad_prevar_add$RNG, levels = levels(eurcad_prevar$RNG))
    eurcad_prevar_add <- eurcad_prevar_add %>% mutate_if(is.character, as.factor) 
    eurcad_prevar <<- rbind(eurcad_prevar, eurcad_prevar_add)
    writeLines("Successfully Updating EURCAD PP Precedence Variables")
  }
  
  #----------------------------------------- 23) NZDUSD ---------------------------------------------------
  if(pp_overwrite){
    nzdusd_daily_pp <<- pivot_point_trading(nzdusd_daily, asset_name = "NZDUSD", save_value = T)
    nzdusd_daily_vector <<- pivot_point_vector(nzdusd_daily_pp, vector_type = "nearest_diff")
    nzdusd_daily_vector$vector_check <<- suppressWarnings(
      nzdusd_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    nzdusd_daily_vector$weekday_trade <<- weekdays(nzdusd_daily_vector$Datetime)
    range_daily <- nzdusd_daily$High - nzdusd_daily$Low
    nzdusd_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    nzdusd_daily_vector <<- vector_clean_same_oc(nzdusd_daily_vector, see_results = F)
    writeLines("Successfully Updating NZDUSD PP Data")
    nzdusd_pre1 <- precedence_sequences(nzdusd_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    nzdusd_pre1 <- pivot_precedence_break(nzdusd_pre1)
    nzdusd_prevar <<- as.data.frame(cbind(WK = nzdusd_daily_vector$weekday_trade[7:nrow(nzdusd_daily_vector)], 
                                          FIB = nzdusd_daily_vector$FIB_GAP[7:nrow(nzdusd_daily_vector)], 
                                          RNG = nzdusd_daily_vector$prev_range[7:nrow(nzdusd_daily_vector)],
                                          CPR = nzdusd_daily_vector$CPR_GAP[7:nrow(nzdusd_daily_vector)],
                                          nzdusd_pre1))
    nzdusd_prevar <<- quantile_cut_df_num(nzdusd_prevar)
    nzdusd_prevar <<- nzdusd_prevar %>% mutate_if(is.character, as.factor) 
    nzdusd_prevar$Datetime <<- nzdusd_daily_vector$Datetime[7:nrow(nzdusd_daily_vector)]
    writeLines("Successfully Updated NZDUSD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- nzdusd_daily_pp$Datetime[nrow(nzdusd_daily_pp)]
    nzdusd_daily_pp_add <- pivot_point_trading(nzdusd_daily[which(nzdusd_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "NZDUSD", save_value = T)
    nzdusd_daily_pp <<- rbind(nzdusd_daily_pp, nzdusd_daily_pp_add[-1,])
    nzdusd_daily_vector_add <- pivot_point_vector(nzdusd_daily_pp_add, vector_type = "nearest_diff")
    nzdusd_daily_vector_add$vector_check <- suppressWarnings(
      nzdusd_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    nzdusd_daily_vector_add$weekday_trade <- weekdays(nzdusd_daily_vector_add$Datetime)
    range_daily <- nzdusd_daily$High[which(nzdusd_daily$Datetime >= last_pp_datetime)] - nzdusd_daily$Low[which(nzdusd_daily$Datetime >= last_pp_datetime)]
    nzdusd_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    nzdusd_daily_vector_add <- vector_clean_same_oc(nzdusd_daily_vector_add, see_results = F)
    nzdusd_daily_vector <<- rbind(nzdusd_daily_vector, nzdusd_daily_vector_add)
    writeLines("Successfully Updating NZDUSD PP Data")
    
    last_prevar_datetime <- nzdusd_prevar$Datetime[nrow(nzdusd_prevar)]
    which_nzdusd_rows <- which(nzdusd_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    nzdusd_pre1 <- precedence_sequences(nzdusd_daily_vector$vector_pivot[(which_nzdusd_rows - 6):nrow(nzdusd_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    nzdusd_pre1 <- pivot_precedence_break(nzdusd_pre1)
    nzdusd_prevar_add <- as.data.frame(cbind(WK = nzdusd_daily_vector$weekday_trade[which_nzdusd_rows:nrow(nzdusd_daily_vector)], 
                                             FIB = nzdusd_daily_vector$FIB_GAP[which_nzdusd_rows:nrow(nzdusd_daily_vector)], 
                                             RNG = nzdusd_daily_vector$prev_range[which_nzdusd_rows:nrow(nzdusd_daily_vector)],
                                             CPR = nzdusd_daily_vector$CPR_GAP[which_nzdusd_rows:nrow(nzdusd_daily_vector)],
                                             nzdusd_pre1))
    nzdusd_prevar_add$Datetime <- nzdusd_daily_vector$Datetime[which_nzdusd_rows:nrow(nzdusd_daily_vector)]
    nzdusd_prevar_add$FIB <- NA
    nzdusd_prevar_add$CPR <- NA
    nzdusd_prevar_add$RNG <- NA
    nzdusd_prevar_add$FIB <- factor(nzdusd_prevar_add$FIB, levels = levels(nzdusd_prevar$FIB))
    nzdusd_prevar_add$CPR <- factor(nzdusd_prevar_add$CPR, levels = levels(nzdusd_prevar$CPR))
    nzdusd_prevar_add$RNG <- factor(nzdusd_prevar_add$RNG, levels = levels(nzdusd_prevar$RNG))
    nzdusd_prevar_add <- nzdusd_prevar_add %>% mutate_if(is.character, as.factor) 
    nzdusd_prevar <<- rbind(nzdusd_prevar, nzdusd_prevar_add)
    writeLines("Successfully Updating NZDUSD PP Precedence Variables")
  }
  
  #----------------------------------------- 24) XAUUSD ---------------------------------------------------
  if(pp_overwrite){
    xauusd_daily_pp <<- pivot_point_trading(xauusd_daily, asset_name = "XAUUSD", save_value = T)
    xauusd_daily_vector <<- pivot_point_vector(xauusd_daily_pp, vector_type = "nearest_diff")
    xauusd_daily_vector$vector_check <<- suppressWarnings(
      xauusd_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    xauusd_daily_vector$weekday_trade <<- weekdays(xauusd_daily_vector$Datetime)
    range_daily <- xauusd_daily$High - xauusd_daily$Low
    xauusd_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    xauusd_daily_vector <<- vector_clean_same_oc(xauusd_daily_vector, see_results = F)
    writeLines("Successfully Updating XAUUSD PP Data")
    xauusd_pre1 <- precedence_sequences(xauusd_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    xauusd_pre1 <- pivot_precedence_break(xauusd_pre1)
    xauusd_prevar <<- as.data.frame(cbind(WK = xauusd_daily_vector$weekday_trade[7:nrow(xauusd_daily_vector)], 
                                          FIB = xauusd_daily_vector$FIB_GAP[7:nrow(xauusd_daily_vector)], 
                                          RNG = xauusd_daily_vector$prev_range[7:nrow(xauusd_daily_vector)],
                                          CPR = xauusd_daily_vector$CPR_GAP[7:nrow(xauusd_daily_vector)],
                                          xauusd_pre1))
    xauusd_prevar <<- quantile_cut_df_num(xauusd_prevar)
    xauusd_prevar <<- xauusd_prevar %>% mutate_if(is.character, as.factor)
    xauusd_prevar$Datetime <<- xauusd_daily_vector$Datetime[7:nrow(xauusd_daily_vector)]
    writeLines("Successfully Updated XAUUSD Precedence Variables")
    
  }
  else{
    last_pp_datetime <- xauusd_daily_pp$Datetime[nrow(xauusd_daily_pp)]
    xauusd_daily_pp_add <- pivot_point_trading(xauusd_daily[which(xauusd_daily$Datetime >= last_pp_datetime),], 
                                               asset_name = "XAUUSD", save_value = T)
    xauusd_daily_pp <<- rbind(xauusd_daily_pp, xauusd_daily_pp_add[-1,])
    xauusd_daily_vector_add <- pivot_point_vector(xauusd_daily_pp_add, vector_type = "nearest_diff")
    xauusd_daily_vector_add$vector_check <- suppressWarnings(
      xauusd_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    xauusd_daily_vector_add$weekday_trade <- weekdays(xauusd_daily_vector_add$Datetime)
    range_daily <- xauusd_daily$High[which(xauusd_daily$Datetime >= last_pp_datetime)] - xauusd_daily$Low[which(xauusd_daily$Datetime >= last_pp_datetime)]
    xauusd_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    xauusd_daily_vector_add <- vector_clean_same_oc(xauusd_daily_vector_add, see_results = F)
    xauusd_daily_vector <<- rbind(xauusd_daily_vector, xauusd_daily_vector_add)
    writeLines("Successfully Updating XAUUSD PP Data")
    
    last_prevar_datetime <- xauusd_prevar$Datetime[nrow(xauusd_prevar)]
    which_xauusd_rows <- which(xauusd_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    xauusd_pre1 <- precedence_sequences(xauusd_daily_vector$vector_pivot[(which_xauusd_rows - 6):nrow(xauusd_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    xauusd_pre1 <- pivot_precedence_break(xauusd_pre1)
    xauusd_prevar_add <- as.data.frame(cbind(WK = xauusd_daily_vector$weekday_trade[which_xauusd_rows:nrow(xauusd_daily_vector)], 
                                             FIB = xauusd_daily_vector$FIB_GAP[which_xauusd_rows:nrow(xauusd_daily_vector)], 
                                             RNG = xauusd_daily_vector$prev_range[which_xauusd_rows:nrow(xauusd_daily_vector)],
                                             CPR = xauusd_daily_vector$CPR_GAP[which_xauusd_rows:nrow(xauusd_daily_vector)],
                                             xauusd_pre1))
    xauusd_prevar_add$Datetime <- xauusd_daily_vector$Datetime[which_xauusd_rows:nrow(xauusd_daily_vector)]
    xauusd_prevar_add$FIB <- NA
    xauusd_prevar_add$CPR <- NA
    xauusd_prevar_add$RNG <- NA
    xauusd_prevar_add$FIB <- factor(xauusd_prevar_add$FIB, levels = levels(xauusd_prevar$FIB))
    xauusd_prevar_add$CPR <- factor(xauusd_prevar_add$CPR, levels = levels(xauusd_prevar$CPR))
    xauusd_prevar_add$RNG <- factor(xauusd_prevar_add$RNG, levels = levels(xauusd_prevar$RNG))
    xauusd_prevar_add <- xauusd_prevar_add %>% mutate_if(is.character, as.factor) 
    xauusd_prevar <<- rbind(xauusd_prevar, xauusd_prevar_add)
    writeLines("Successfully Updating XAUUSD PP Precedence Variables")
  }
  
  #----------------------------------------- 25) DXY ---------------------------------------------------
  if(pp_overwrite){
    dxy_daily_pp <<- pivot_point_trading(dxy_daily, asset_name = "DXY", save_value = T)
    dxy_daily_vector <<- pivot_point_vector(dxy_daily_pp, vector_type = "nearest_diff")
    dxy_daily_vector$vector_check <<- suppressWarnings(
      dxy_daily_vector %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    dxy_daily_vector$weekday_trade <<- weekdays(dxy_daily_vector$Datetime)
    range_daily <- dxy_daily$High - dxy_daily$Low
    dxy_daily_vector$prev_range <<- range_daily[1:(length(range_daily) - 1)]
    dxy_daily_vector <<- vector_clean_same_oc(dxy_daily_vector, see_results = F)
    writeLines("Successfully Updating DXY PP Data")
    dxy_pre1 <- precedence_sequences(dxy_daily_vector$vector_pivot, precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    dxy_pre1 <- pivot_precedence_break(dxy_pre1)
    dxy_prevar <<- as.data.frame(cbind(WK = dxy_daily_vector$weekday_trade[7:nrow(dxy_daily_vector)], 
                                       FIB = dxy_daily_vector$FIB_GAP[7:nrow(dxy_daily_vector)], 
                                       RNG = dxy_daily_vector$prev_range[7:nrow(dxy_daily_vector)],
                                       CPR = dxy_daily_vector$CPR_GAP[7:nrow(dxy_daily_vector)],
                                       dxy_pre1))
    dxy_prevar <<- quantile_cut_df_num(dxy_prevar)
    dxy_prevar <<- dxy_prevar %>% mutate_if(is.character, as.factor) 
    dxy_prevar$Datetime <<- dxy_daily_vector$Datetime[7:nrow(dxy_daily_vector)]
    writeLines("Successfully Updated DXY Precedence Variables")
    
  }
  else{
    last_pp_datetime <- dxy_daily_pp$Datetime[nrow(dxy_daily_pp)]
    dxy_daily_pp_add <- pivot_point_trading(dxy_daily[which(dxy_daily$Datetime >= last_pp_datetime),], 
                                            asset_name = "DXY", save_value = T)
    dxy_daily_pp <<- rbind(dxy_daily_pp, dxy_daily_pp_add[-1,])
    dxy_daily_vector_add <- pivot_point_vector(dxy_daily_pp_add, vector_type = "nearest_diff")
    dxy_daily_vector_add$vector_check <- suppressWarnings(
      dxy_daily_vector_add %>%
        mutate(vector_pivot_sub = vector_pivot) %>%
        separate(vector_pivot_sub, into = c("from","to"), sep = " -> ") %>%
        mutate(vector_check = (from == to)) %>% select(vector_check) %>% unlist() %>% as.vector())
    dxy_daily_vector_add$weekday_trade <- weekdays(dxy_daily_vector_add$Datetime)
    range_daily <- dxy_daily$High[which(dxy_daily$Datetime >= last_pp_datetime)] - dxy_daily$Low[which(dxy_daily$Datetime >= last_pp_datetime)]
    dxy_daily_vector_add$prev_range <- range_daily[1:(length(range_daily) - 1)]
    dxy_daily_vector_add <- vector_clean_same_oc(dxy_daily_vector_add, see_results = F)
    dxy_daily_vector <<- rbind(dxy_daily_vector, dxy_daily_vector_add)
    writeLines("Successfully Updating DXY PP Data")
    
    last_prevar_datetime <- dxy_prevar$Datetime[nrow(dxy_prevar)]
    which_dxy_rows <- which(dxy_daily_vector$Datetime == last_prevar_datetime)[1] + 1
    dxy_pre1 <- precedence_sequences(dxy_daily_vector$vector_pivot[(which_dxy_rows - 6):nrow(dxy_daily_vector)], precedence_sequence = 7, exact_sequence = T, var_name = "VECP")[[1]][,-8]
    dxy_pre1 <- pivot_precedence_break(dxy_pre1)
    dxy_prevar_add <- as.data.frame(cbind(WK = dxy_daily_vector$weekday_trade[which_dxy_rows:nrow(dxy_daily_vector)], 
                                          FIB = dxy_daily_vector$FIB_GAP[which_dxy_rows:nrow(dxy_daily_vector)], 
                                          RNG = dxy_daily_vector$prev_range[which_dxy_rows:nrow(dxy_daily_vector)],
                                          CPR = dxy_daily_vector$CPR_GAP[which_dxy_rows:nrow(dxy_daily_vector)],
                                          dxy_pre1))
    dxy_prevar_add$Datetime <- dxy_daily_vector$Datetime[which_dxy_rows:nrow(dxy_daily_vector)]
    unique_fib_prevar <- as.character(unique(na.omit(dxy_prevar$FIB)))
    unique_cpr_prevar <- as.character(unique(na.omit(dxy_prevar$CPR)))
    unique_rng_prevar <- as.character(unique(na.omit(dxy_prevar$RNG)))
    fib_split <- strsplit(unique_fib_prevar, "-")
    cpr_split <- strsplit(unique_cpr_prevar, "-")
    rng_split <- strsplit(unique_rng_prevar, "-")
    
    fib_from_unique <- as.numeric(unlist(lapply(fib_split, FUN = function(x) x[[1]])))
    cpr_from_unique <- as.numeric(unlist(lapply(cpr_split, FUN = function(x) x[[1]])))
    rng_from_unique <- as.numeric(unlist(lapply(rng_split, FUN = function(x) x[[1]])))
    fib_to_unique <- as.numeric(unlist(lapply(fib_split, FUN = function(x) x[[2]])))
    cpr_to_unique <- as.numeric(unlist(lapply(cpr_split, FUN = function(x) x[[2]])))
    rng_to_unique <- as.numeric(unlist(lapply(rng_split, FUN = function(x) x[[2]])))
    
    FIB_replacement <- dxy_prevar_add$FIB
    CPR_replacement <- dxy_prevar_add$CPR
    RNG_replacement <- dxy_prevar_add$RNG
    minmax_fib <- c(min(fib_from_unique), max(fib_to_unique))
    minmax_cpr <- c(min(cpr_from_unique), max(cpr_to_unique))
    minmax_rng <- c(min(rng_from_unique), max(rng_to_unique))
    where_minmax_fib <- c(which(fib_from_unique == min(fib_from_unique)), which(fib_to_unique == max(fib_to_unique)))
    where_minmax_cpr <- c(which(cpr_from_unique == min(cpr_from_unique)), which(cpr_to_unique == max(cpr_to_unique)))
    where_minmax_rng <- c(which(rng_from_unique == min(rng_from_unique)), which(rng_to_unique == max(rng_to_unique)))
    for(x in 1:length(fib_from_unique)){
      FIB_replacement <- ifelse(dxy_prevar_add$FIB >= fib_from_unique[x] & dxy_prevar_add$FIB <= fib_to_unique[x], unique_fib_prevar[x], 
                                ifelse(x == where_minmax_fib[1] & dxy_prevar_add$FIB < fib_from_unique[x], unique_fib_prevar[x], 
                                       ifelse(x == where_minmax_fib[2] & dxy_prevar_add$FIB > fib_to_unique[x], unique_fib_prevar[x], FIB_replacement)))
    }
    for(y in 1:length(cpr_from_unique)){
      CPR_replacement <- ifelse(dxy_prevar_add$CPR >= cpr_from_unique[x] & dxy_prevar_add$CPR <= cpr_to_unique[x], unique_cpr_prevar[x], 
                                ifelse(x == where_minmax_cpr[1] & dxy_prevar_add$CPR < cpr_from_unique[x], unique_cpr_prevar[x], 
                                       ifelse(x == where_minmax_cpr[2] & dxy_prevar_add$CPR > cpr_to_unique[x], unique_cpr_prevar[x], CPR_replacement)))
    }
    for(z in 1:length(rng_from_unique)){
      RNG_replacement <- ifelse(dxy_prevar_add$RNG >= rng_from_unique[x] & dxy_prevar_add$RNG <= rng_to_unique[x], unique_rng_prevar[x], 
                                ifelse(x == where_minmax_rng[1] & dxy_prevar_add$RNG < rng_from_unique[x], unique_rng_prevar[x], 
                                       ifelse(x == where_minmax_rng[2] & dxy_prevar_add$RNG > rng_to_unique[x], unique_rng_prevar[x], RNG_replacement)))
    }
    dxy_prevar_add$FIB <- FIB_replacement
    dxy_prevar_add$CPR <- CPR_replacement
    dxy_prevar_add$RNG <- RNG_replacement
    dxy_prevar_add$FIB <- factor(dxy_prevar_add$FIB, levels = levels(dxy_prevar$FIB))
    dxy_prevar_add$CPR <- factor(dxy_prevar_add$CPR, levels = levels(dxy_prevar$CPR))
    dxy_prevar_add$RNG <- factor(dxy_prevar_add$RNG, levels = levels(dxy_prevar$RNG))
    dxy_prevar_add <- dxy_prevar_add %>% mutate_if(is.character, as.factor) 
    dxy_prevar <<- rbind(dxy_prevar, dxy_prevar_add)
    writeLines("Successfully Updating DXY PP Precedence Variables")
  }
}

automatic_fix_cpr_rng_fib <- function(asset_daily_vector, asset_prevar, debug = F){
  fib_na_datetime <- asset_prevar$Datetime[which(is.na(asset_prevar$FIB))]
  cpr_na_datetime <- asset_prevar$Datetime[which(is.na(asset_prevar$CPR))]
  rng_na_datetime <- asset_prevar$Datetime[which(is.na(asset_prevar$RNG))]
  
  if(length(fib_na_datetime) > 0){
    if(debug) print(asset_prevar[which(is.na(asset_prevar$FIB)),])
    fib_to_fill <- asset_daily_vector[which(asset_daily_vector$Datetime %in% fib_na_datetime),]$FIB_GAP
    if(debug) print(fib_to_fill)
    unique_fib_prevar <- as.character(unique(na.omit(asset_prevar$FIB)))
    fib_split <- strsplit(unique_fib_prevar, "-")
    fib_from_unique <- as.numeric(unlist(lapply(fib_split, FUN = function(x) x[[1]])))
    fib_to_unique <- as.numeric(unlist(lapply(fib_split, FUN = function(x) x[[2]])))
    if(debug) print(unique_fib_prevar)
    if(debug) print(fib_from_unique)
    if(debug) print(fib_to_unique)
    w_min_ff <- which(fib_from_unique == min(fib_from_unique))
    w_max_ft <- which(fib_to_unique == max(fib_to_unique))
    fib_to_fill_factor <- ifelse(fib_to_fill < min(fib_from_unique), unique_fib_prevar[w_min_ff],
                                 ifelse(fib_to_fill > max(fib_to_unique), unique_fib_prevar[w_max_ft], NA))
    if(debug) print(fib_to_fill_factor)
    for(a in 1:length(fib_from_unique)){
      for(b in 1:length(fib_to_fill)){
        if(is.na(fib_to_fill_factor[b]) && (fib_to_fill[b] >= fib_from_unique[a] && fib_to_fill[b] <= fib_to_unique[a])){
          fib_to_fill_factor[b] <- unique_fib_prevar[a]
        }
      }
    }
    t1 <- which(is.na(asset_prevar$FIB))
    asset_prevar$FIB[which(is.na(asset_prevar$FIB))] <- fib_to_fill_factor
    if(debug) print(asset_prevar[t1,])
  }
  if(length(cpr_na_datetime) > 0){
    if(debug) print(asset_prevar[which(is.na(asset_prevar$CPR)),])
    cpr_to_fill <- asset_daily_vector[which(asset_daily_vector$Datetime %in% cpr_na_datetime),]$CPR_GAP
    if(debug) print(cpr_to_fill)
    unique_cpr_prevar <- as.character(unique(na.omit(asset_prevar$CPR)))
    cpr_split <- strsplit(unique_cpr_prevar, "-")
    cpr_from_unique <- as.numeric(unlist(lapply(cpr_split, FUN = function(x) x[[1]])))
    cpr_to_unique <- as.numeric(unlist(lapply(cpr_split, FUN = function(x) x[[2]])))
    if(debug) print(unique_cpr_prevar)
    if(debug) print(cpr_from_unique)
    if(debug) print(cpr_to_unique)
    w_min_cf <- which(cpr_from_unique == min(cpr_from_unique))
    w_max_ct <- which(cpr_to_unique == max(cpr_to_unique))
    cpr_to_fill_factor <- ifelse(cpr_to_fill < min(cpr_from_unique), unique_cpr_prevar[w_min_cf],
                                 ifelse(cpr_to_fill > max(cpr_to_unique), unique_cpr_prevar[w_max_ct], NA))
    if(debug) print(cpr_to_fill_factor)
    for(a in 1:length(cpr_from_unique)){
      for(b in 1:length(cpr_to_fill)){
        if(is.na(cpr_to_fill_factor[b]) && (cpr_to_fill[b] >= cpr_from_unique[a] && cpr_to_fill[b] <= cpr_to_unique[a])){
          cpr_to_fill_factor[b] <- unique_cpr_prevar[a]
        }
      }
    }
    t2 <- which(is.na(asset_prevar$CPR))
    asset_prevar$CPR[which(is.na(asset_prevar$CPR))] <- cpr_to_fill_factor
    if(debug) print(asset_prevar[t2,])
  }
  if(length(rng_na_datetime) > 0){
    if(debug) print(asset_prevar[which(is.na(asset_prevar$RNG)),])
    rng_to_fill <- asset_daily_vector[which(asset_daily_vector$Datetime %in% rng_na_datetime),]$prev_range
    if(debug) print(rng_to_fill)
    unique_rng_prevar <- as.character(unique(na.omit(asset_prevar$RNG)))
    rng_split <- strsplit(unique_rng_prevar, "-")
    rng_from_unique <- as.numeric(unlist(lapply(rng_split, FUN = function(x) x[[1]])))
    rng_to_unique <- as.numeric(unlist(lapply(rng_split, FUN = function(x) x[[2]])))
    if(debug) print(unique_rng_prevar)
    if(debug) print(rng_from_unique)
    if(debug) print(rng_to_unique)
    w_min_rf <- which(rng_from_unique == min(rng_from_unique))
    w_max_rt <- which(rng_to_unique == max(rng_to_unique))
    rng_to_fill_factor <- ifelse(rng_to_fill < min(rng_from_unique), unique_rng_prevar[w_min_rf],
                                 ifelse(rng_to_fill > max(rng_to_unique), unique_rng_prevar[w_max_rt], NA))
    if(debug) print(rng_to_fill_factor)
    for(a in 1:length(rng_from_unique)){
      for(b in 1:length(rng_to_fill)){
        if(is.na(rng_to_fill_factor[b]) && (rng_to_fill[b] >= rng_from_unique[a] && rng_to_fill[b] <= rng_to_unique[a])){
          rng_to_fill_factor[b] <- unique_rng_prevar[a]
        }
      }
    }
    t3 <- which(is.na(asset_prevar$RNG))
    asset_prevar$RNG[which(is.na(asset_prevar$RNG))] <- rng_to_fill_factor
    if(debug) print(asset_prevar[t3,])
  }
  
  return(asset_prevar)
}

#rules generated from apriori arules to pivot point can use this to find the matched rules for any given time
find_match_rules <- function(rules_df, rules_input, filter_rules_target = c("BC","Fib_R1","Fib_S1")){
  library(dplyr, warn.conflicts = FALSE)
  options(dplyr.summarise.inform = FALSE)
  
  library(stringr)
  library(tictoc)
  matched_rules_df <- NULL
  source_rules_n <- length(colnames(rules_df)[grepl("Source_", colnames(rules_df))])
  
  writeLines("1) ------------------------------------ individual_rules ------------------------------------------")
  tic()
  fib_unique_values_eval <- "fib_unique_values <- unique(c("
  cpr_unique_values_eval <- "cpr_unique_values <- unique(c("
  rng_unique_values_eval <- "rng_unique_values <- unique(c("
  for(v in 1:source_rules_n){
    if(v == source_rules_n){
      fib_unique_values_eval <- paste0(fib_unique_values_eval, 'rules_df$Result_',v,'[which(rules_df$Source_',v,' == "FIB")]))')
      cpr_unique_values_eval <- paste0(cpr_unique_values_eval, 'rules_df$Result_',v,'[which(rules_df$Source_',v,' == "CPR")]))')
      rng_unique_values_eval <- paste0(rng_unique_values_eval, 'rules_df$Result_',v,'[which(rules_df$Source_',v,' == "RNG")]))')
    }else{
      fib_unique_values_eval <- paste0(fib_unique_values_eval, 'rules_df$Result_',v,'[which(rules_df$Source_',v,' == "FIB")],')
      cpr_unique_values_eval <- paste0(cpr_unique_values_eval, 'rules_df$Result_',v,'[which(rules_df$Source_',v,' == "CPR")],')
      rng_unique_values_eval <- paste0(rng_unique_values_eval, 'rules_df$Result_',v,'[which(rules_df$Source_',v,' == "RNG")],')
    }
  }
  fib_input_adjusted <- "-"
  cpr_input_adjusted <- "-"
  rng_input_adjusted <- "-"
  eval(parse(text = fib_unique_values_eval))
  eval(parse(text = cpr_unique_values_eval))
  eval(parse(text = rng_unique_values_eval))
  
  if(length(fib_unique_values) > 0) fib_strsplit <- strsplit(as.character(fib_unique_values), "-")
  if(length(cpr_unique_values) > 0) cpr_strsplit <- strsplit(as.character(cpr_unique_values), "-")
  if(length(rng_unique_values) > 0) rng_strsplit <- strsplit(as.character(rng_unique_values), "-")
  if(length(fib_unique_values) > 0) fib_from_to <- as.data.frame(matrix(unlist(fib_strsplit), nrow = length(fib_strsplit), byrow=TRUE))
  if(length(cpr_unique_values) > 0) cpr_from_to <- as.data.frame(matrix(unlist(cpr_strsplit), nrow = length(cpr_strsplit), byrow=TRUE))
  if(length(rng_unique_values) > 0) rng_from_to <- as.data.frame(matrix(unlist(rng_strsplit), nrow = length(rng_strsplit), byrow=TRUE))
  if(length(fib_unique_values) > 0) fib_from_to <- fib_from_to %>% mutate_if(is.character, as.numeric)
  if(length(cpr_unique_values) > 0) cpr_from_to <- cpr_from_to %>% mutate_if(is.character, as.numeric)
  if(length(rng_unique_values) > 0) rng_from_to <- rng_from_to %>% mutate_if(is.character, as.numeric)
  if(length(fib_unique_values) > 0) colnames(fib_from_to) <- c("from","to")
  if(length(cpr_unique_values) > 0) colnames(cpr_from_to) <- c("from","to")
  if(length(rng_unique_values) > 0) colnames(rng_from_to) <- c("from","to")
  if(length(fib_unique_values) > 0) rules_input_fib <- as.numeric(unlist(strsplit(as.character(rules_input$FIB), "-")))
  if(length(cpr_unique_values) > 0) rules_input_cpr <- as.numeric(unlist(strsplit(as.character(rules_input$CPR), "-")))
  if(length(rng_unique_values) > 0) rules_input_rng <- as.numeric(unlist(strsplit(as.character(rules_input$RNG), "-")))
  if(length(fib_unique_values) > 0) diff_fib_input_from <- which.min(abs(fib_from_to$from - rules_input_fib[1]))
  if(length(cpr_unique_values) > 0) diff_cpr_input_from <- which.min(abs(cpr_from_to$from - rules_input_cpr[1]))
  if(length(rng_unique_values) > 0) diff_rng_input_from <- which.min(abs(rng_from_to$from - rules_input_rng[1]))
  if(length(fib_unique_values) > 0 && rules_input$FIB != "-") fib_input_adjusted <- fib_unique_values[diff_fib_input_from]
  if(length(cpr_unique_values) > 0 && rules_input$CPR != "-") cpr_input_adjusted <- cpr_unique_values[diff_cpr_input_from]
  if(length(rng_unique_values) > 0 && rules_input$RNG != "-") rng_input_adjusted <- rng_unique_values[diff_rng_input_from]
  
  #print(fib_unique_values)
  #print(cpr_unique_values)
  #print(rng_unique_values)
  #print(fib_input_adjusted)
  #print(rng_input_adjusted)
  #print(cpr_input_adjusted)
  WK_eval_rules <- "WK_rules <- rules_df[which("
  FIB_eval_rules <- "FIB_rules <- rules_df[which("
  CPR_eval_rules <- "CPR_rules <- rules_df[which("
  RNG_eval_rules <- "RNG_rules <- rules_df[which("
  VECP6F_eval_rules <- "VECP6F_rules <- rules_df[which("
  VECP6T_eval_rules <- "VECP6T_rules <- rules_df[which("
  VECP5F_eval_rules <- "VECP5F_rules <- rules_df[which("
  VECP5T_eval_rules <- "VECP5T_rules <- rules_df[which("
  VECP4F_eval_rules <- "VECP4F_rules <- rules_df[which("
  VECP4T_eval_rules <- "VECP4T_rules <- rules_df[which("
  VECP3F_eval_rules <- "VECP3F_rules <- rules_df[which("
  VECP3T_eval_rules <- "VECP3T_rules <- rules_df[which("
  VECP2F_eval_rules <- "VECP2F_rules <- rules_df[which("
  VECP2T_eval_rules <- "VECP2T_rules <- rules_df[which("
  VECP1F_eval_rules <- "VECP1F_rules <- rules_df[which("
  VECP1T_eval_rules <- "VECP1T_rules <- rules_df[which("
  
  for(v in 1:source_rules_n){
    if(v == source_rules_n){
      WK_eval_rules <- paste0(WK_eval_rules, "(rules_df$Source_",v, " == ", '"WK" & rules_df$Result_',v, ' == "', as.character(rules_input$WK), '")),]')
      FIB_eval_rules <- paste0(FIB_eval_rules, "(rules_df$Source_",v, " == ", '"FIB" & rules_df$Result_',v, ' == "', as.character(fib_input_adjusted), '")),]')
      CPR_eval_rules <- paste0(CPR_eval_rules, "(rules_df$Source_",v, " == ", '"CPR" & rules_df$Result_',v, ' == "', as.character(cpr_input_adjusted), '")),]')
      RNG_eval_rules <- paste0(RNG_eval_rules, "(rules_df$Source_",v, " == ", '"RNG" & rules_df$Result_',v, ' == "', as.character(rng_input_adjusted), '")),]')
      VECP6F_eval_rules <- paste0(VECP6F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_6F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_6F), '")),]')
      VECP6T_eval_rules <- paste0(VECP6T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_6T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_6T), '")),]')
      VECP5F_eval_rules <- paste0(VECP5F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_5F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_5F), '")),]')
      VECP5T_eval_rules <- paste0(VECP5T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_5T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_5T), '")),]')
      VECP4F_eval_rules <- paste0(VECP4F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_4F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_4F), '")),]')
      VECP4T_eval_rules <- paste0(VECP4T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_4T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_4T), '")),]')
      VECP3F_eval_rules <- paste0(VECP3F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_3F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_3F), '")),]')
      VECP3T_eval_rules <- paste0(VECP3T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_3T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_3T), '")),]')
      VECP2F_eval_rules <- paste0(VECP2F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_2F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_2F), '")),]')
      VECP2T_eval_rules <- paste0(VECP2T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_2T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_2T), '")),]')
      VECP1F_eval_rules <- paste0(VECP1F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_1F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_1F), '")),]')
      VECP1T_eval_rules <- paste0(VECP1T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_1T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_1T), '")),]')
    }else{
      WK_eval_rules <- paste0(WK_eval_rules, "(rules_df$Source_",v, " == ", '"WK" & rules_df$Result_',v, ' == "', as.character(rules_input$WK), '") |')
      FIB_eval_rules <- paste0(FIB_eval_rules, "(rules_df$Source_",v, " == ", '"FIB" & rules_df$Result_',v, ' == "', as.character(fib_input_adjusted), '") |')
      CPR_eval_rules <- paste0(CPR_eval_rules, "(rules_df$Source_",v, " == ", '"CPR" & rules_df$Result_',v, ' == "', as.character(cpr_input_adjusted), '") |')
      RNG_eval_rules <- paste0(RNG_eval_rules, "(rules_df$Source_",v, " == ", '"RNG" & rules_df$Result_',v, ' == "', as.character(rng_input_adjusted), '") |')
      VECP6F_eval_rules <- paste0(VECP6F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_6F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_6F), '") |')
      VECP6T_eval_rules <- paste0(VECP6T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_6T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_6T), '") |')
      VECP5F_eval_rules <- paste0(VECP5F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_5F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_5F), '") |')
      VECP5T_eval_rules <- paste0(VECP5T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_5T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_5T), '") |')
      VECP4F_eval_rules <- paste0(VECP4F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_4F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_4F), '") |')
      VECP4T_eval_rules <- paste0(VECP4T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_4T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_4T), '") |')
      VECP3F_eval_rules <- paste0(VECP3F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_3F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_3F), '") |')
      VECP3T_eval_rules <- paste0(VECP3T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_3T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_3T), '") |')
      VECP2F_eval_rules <- paste0(VECP2F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_2F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_2F), '") |')
      VECP2T_eval_rules <- paste0(VECP2T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_2T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_2T), '") |')
      VECP1F_eval_rules <- paste0(VECP1F_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_1F" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_1F), '") |')
      VECP1T_eval_rules <- paste0(VECP1T_eval_rules, "(rules_df$Source_",v, " == ", '"VECP_1T" & rules_df$Result_',v, ' == "', as.character(rules_input$VECP_1T), '") |')
    }
  }
  
  eval(parse(text = WK_eval_rules))
  eval(parse(text = FIB_eval_rules))
  eval(parse(text = CPR_eval_rules))
  eval(parse(text = RNG_eval_rules))
  eval(parse(text = VECP6F_eval_rules))
  eval(parse(text = VECP6T_eval_rules))
  eval(parse(text = VECP5F_eval_rules))
  eval(parse(text = VECP5T_eval_rules))
  eval(parse(text = VECP4F_eval_rules))
  eval(parse(text = VECP4T_eval_rules))
  eval(parse(text = VECP3F_eval_rules))
  eval(parse(text = VECP3T_eval_rules))
  eval(parse(text = VECP2F_eval_rules))
  eval(parse(text = VECP2T_eval_rules))
  eval(parse(text = VECP1F_eval_rules))
  eval(parse(text = VECP1T_eval_rules))
  
  if(nrow(WK_rules) > 0) WK_rules$note <- "Individual WK Rules"
  if(nrow(FIB_rules) > 0) FIB_rules$note <- "Individual FIB Rules"
  if(nrow(CPR_rules) > 0) CPR_rules$note <- "Individual CPR Rules"
  if(nrow(RNG_rules) > 0) RNG_rules$note <- "Individual RNG Rules"
  if(nrow(VECP6F_rules) > 0) VECP6F_rules$note <- "Individual VECP6F Rules"
  if(nrow(VECP6T_rules) > 0) VECP6T_rules$note <- "Individual VECP5T Rules"
  if(nrow(VECP5F_rules) > 0) VECP5F_rules$note <- "Individual VECP5F Rules"
  if(nrow(VECP5T_rules) > 0) VECP5T_rules$note <- "Individual VECP5T Rules"
  if(nrow(VECP4F_rules) > 0) VECP4F_rules$note <- "Individual VECP4F Rules"
  if(nrow(VECP4T_rules) > 0) VECP4T_rules$note <- "Individual VECP4T Rules"
  if(nrow(VECP3F_rules) > 0) VECP3F_rules$note <- "Individual VECP3F Rules"
  if(nrow(VECP3T_rules) > 0) VECP3T_rules$note <- "Individual VECP3T Rules"
  if(nrow(VECP2F_rules) > 0) VECP2F_rules$note <- "Individual VECP2F Rules"
  if(nrow(VECP2T_rules) > 0) VECP2T_rules$note <- "Individual VECP2T Rules"
  if(nrow(VECP1F_rules) > 0) VECP1F_rules$note <- "Individual VECP1F Rules"
  if(nrow(VECP1T_rules) > 0) VECP1T_rules$note <- "Individual VECP1T Rules"
  
  individual_rules <- rbind(WK_rules, FIB_rules, CPR_rules, RNG_rules, VECP6F_rules, VECP6T_rules,
                            VECP5F_rules, VECP5T_rules, VECP4F_rules, VECP4T_rules, VECP3F_rules, VECP3T_rules,
                            VECP2F_rules, VECP2T_rules, VECP1F_rules, VECP1T_rules)
  
  if(length(filter_rules_target) > 0){
    for(b in 1:length(filter_rules_target)){
      individual_rules <- individual_rules %>% filter(target != filter_rules_target[b])
    }
    writeLines(paste0("Found Filtered ",nrow(individual_rules), " Unique Individual Rule Matches with Input !"))
  }else{
    writeLines(paste0("Found ",nrow(individual_rules), " Unique Individual Rule Matches with Input !"))
  }
  
  toc()
  writeLines("2) ------------------------------------ composite_rules ------------------------------------------")
  tic()
  
  composite_rules <- NULL
  composite_select_eval <- "composite_rules <- composite_rules %>% select("
  batch_n <- source_rules_n - 1
  
  for(f in 1:batch_n){
    composite_rules_eval <- "composite_rules_add <- rules_df[which("
    for(v in 2:(source_rules_n - f + 1)){
      if(v == (source_rules_n - f + 1)){
        composite_rules_eval <- paste0(composite_rules_eval, 'rules_df$Source_',v,' != "-"),]')
      }else{
        composite_rules_eval <- paste0(composite_rules_eval, 'rules_df$Source_',v,' != "-" & ')
      }
    }
    if(is.null(composite_rules)){
      eval(parse(text = composite_rules_eval))
      composite_rules <- composite_rules_add
    }else{
      eval(parse(text = composite_rules_eval))
      composite_rules <- rbind(composite_rules, composite_rules_add)
    }
  }
  
  for(v in 1:source_rules_n){
    if(v == source_rules_n){
      composite_select_eval <- paste0(composite_select_eval, 'Source_',v,')')
    }else{
      composite_select_eval <- paste0(composite_select_eval, 'Source_',v,',')
    }
  }
  
  eval(parse(text = composite_select_eval))
  
  no_duplicate_ignore_column_order <- function(df, columns_to_consider=c()){
    return(df[!duplicated(apply(df[,columns_to_consider], 1, function(row) paste(sort(row), collapse=""))),])
  }
  
  composite_rules <- composite_rules[!duplicated(composite_rules),]
  composite_rules <- no_duplicate_ignore_column_order(composite_rules, 1:source_rules_n)
  composite_all_rules <- NULL
  
  initial_eval_composite <- "composite_check_rule <- rules_df[which("
  for(v in 1:nrow(composite_rules)){
    #print(composite_rules[v,])
    composite_execute <- ""
    blank_composite_count <- 0
    for(w in 1:source_rules_n){
      source_name <- paste0("Source_",w)
      composite_value <- "-"
      composite_source <- composite_rules[[source_name]][v]
      if(composite_source == "FIB"){
        composite_value <- as.character(fib_input_adjusted)
      }else if(composite_source == "CPR"){
        composite_value <- as.character(cpr_input_adjusted)
      }else if(composite_source == "RNG"){
        composite_value <- as.character(rng_input_adjusted)
      }else if(composite_source != "-"){
        composite_value <- as.character(rules_input[[composite_source]])
      }
      if(w == source_rules_n){
        if(composite_execute == ""){
          composite_execute <- paste0(initial_eval_composite, "(rules_df$Source_",w, ' == "',composite_source,'" & rules_df$Result_',w, ' == "', composite_value, '")),]')
        }else{
          composite_execute <- paste0(composite_execute, "(rules_df$Source_",w, ' == "',composite_source,'" & rules_df$Result_',w, ' == "', composite_value, '")),]')
        }
        if(composite_value == "-") blank_composite_count <- blank_composite_count + 1
      }else{
        if(composite_execute == ""){
          composite_execute <- paste0(initial_eval_composite, "(rules_df$Source_",w, ' == "',composite_source,'" & rules_df$Result_',w, ' == "', composite_value, '") & ')
        }else{
          composite_execute <- paste0(composite_execute, "(rules_df$Source_",w, ' == "',composite_source,'" & rules_df$Result_',w, ' == "', composite_value, '") & ')
        }
        if(composite_value == "-") blank_composite_count <- blank_composite_count + 1
      }
    }
    
    #writeLines(paste0(v,") composite eval = ", composite_execute))
    eval(parse(text = composite_execute))
    composite_check_rule <- na.omit(composite_check_rule)
    total_fill <- source_rules_n - blank_composite_count
    if(nrow(composite_check_rule) > 0) composite_check_rule$note <- paste0(total_fill, " Rules Composite")
    
    if(is.null(composite_all_rules)){
      if(nrow(composite_check_rule) > 0) composite_all_rules <- composite_check_rule
    }else{
      if(nrow(composite_check_rule) > 0) composite_all_rules <- rbind(composite_all_rules, composite_check_rule)
    }
  }
  
  individual_rules <- individual_rules %>% arrange(desc(count))
  composite_all_rules <- composite_all_rules %>% arrange(desc(count))
  composite_all_rules <- composite_all_rules[!duplicated(composite_rules),]
  composite_all_rules <- no_duplicate_ignore_column_order(composite_all_rules, 1:(2*source_rules_n + 1))
  composite_all_rules <- na.omit(composite_all_rules)
  rownames(individual_rules) <- NULL
  rownames(composite_all_rules) <- NULL
  
  if(length(filter_rules_target) > 0){
    for(b in 1:length(filter_rules_target)){
      composite_all_rules <- composite_all_rules %>% filter(target != filter_rules_target[b])
    }
    writeLines(paste0("Found Filtered ",nrow(composite_all_rules), " Unique Composite Rule Matches with Input !"))
  }else{
    writeLines(paste0("Found ",nrow(composite_all_rules), " Unique Composite Rule Matches with Input !"))
  }
  
  if(nrow(composite_all_rules) != 0){
    grouped_individual_target <- individual_rules %>% group_by(note, target) %>% 
      summarise(total_count = sum(count)) %>% arrange(desc(total_count)) %>% as.data.frame()
    grouped_composite_target <- composite_all_rules %>% group_by(note, target) %>% 
      summarise(total_count = sum(count)) %>% arrange(desc(total_count)) %>% as.data.frame()
    unique_composite_target <- unique(grouped_composite_target$target)
    
    if(length(filter_rules_target) > 0){
      for(b in 1:length(filter_rules_target)){
        grouped_individual_target <- grouped_individual_target %>% filter(target != filter_rules_target[b])
        grouped_composite_target <- grouped_composite_target %>% filter(target != filter_rules_target[b])
      }
    }
    
    grouped_individual_VECPF_target <- NULL
    grouped_composite_VECPF_target <- NULL
    for(w in 1:length(unique_composite_target)){
      grouped_individual_VECPF_target_eval <- "grouped_individual_VECPF_target_add <- individual_rules %>% filter("
      grouped_composite_VECPF_target_eval <- "grouped_composite_VECPF_target_add <- composite_all_rules %>% filter("
      for(v in 1:source_rules_n){
        if(v == source_rules_n){
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Source_',v,' == "VECPF" )')
          grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, 'Source_',v,' == "VECPF" )')
        }else{
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Source_',v,' == "VECPF" |')
          grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, 'Source_',v,' == "VECPF" | ')
        }
      }
      grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, ' %>% filter(target == "',unique_composite_target[w],'")')
      grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, " %>% group_by( ")
      grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, ' %>% filter(target == "',unique_composite_target[w],'")')
      grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, " %>% group_by( ")
      for(v in 1:(2*source_rules_n)){
        if(v == (2*source_rules_n)){
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Result_',v - source_rules_n,',target)')
          grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, 'Result_',v - source_rules_n,',target)')
        }else if(v <= source_rules_n){
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Source_',v,',')
          grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, 'Source_',v,',')
        }else{
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Result_',v - source_rules_n,',')
          grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, 'Result_',v - source_rules_n,',')
        }
      }
      grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, " %>% summarise(total_count = sum(count)) %>% arrange(desc(total_count)) %>% as.data.frame()")
      grouped_composite_VECPF_target_eval <- paste0(grouped_composite_VECPF_target_eval, " %>% summarise(total_count = sum(count)) %>% arrange(desc(total_count)) %>% as.data.frame()")
      eval(parse(text = grouped_individual_VECPF_target_eval))
      eval(parse(text = grouped_composite_VECPF_target_eval))
      if(is.null(grouped_individual_VECPF_target)){
        grouped_individual_VECPF_target <- grouped_individual_VECPF_target_add
      }else{
        grouped_individual_VECPF_target <- rbind(grouped_individual_VECPF_target, grouped_individual_VECPF_target_add)
      }
      if(is.null(grouped_composite_VECPF_target)){
        grouped_composite_VECPF_target <- grouped_composite_VECPF_target_add
      }else{
        grouped_composite_VECPF_target <- rbind(grouped_composite_VECPF_target, grouped_composite_VECPF_target_add)
      }
    }
    
    if(length(filter_rules_target) > 0){
      for(b in 1:length(filter_rules_target)){
        grouped_individual_VECPF_target <- grouped_individual_VECPF_target %>% filter(target != filter_rules_target[b])
        grouped_composite_VECPF_target <- grouped_composite_VECPF_target %>% filter(target != filter_rules_target[b])
      }
    }
    
    add_general_individual_VECPF <- "grouped_individual_VECPF_target$real_VECPF <-"
    add_general_composite_VECPF <- "grouped_composite_VECPF_target$real_VECPF <-"
    for(v in 1:source_rules_n){
      if(v == source_rules_n){
        add_general_individual_VECPF <- paste0(add_general_individual_VECPF, ' ifelse(grouped_individual_VECPF_target$Source_',v,' == "VECPF", grouped_individual_VECPF_target$Result_',v, ', "-"')
        add_general_composite_VECPF <- paste0(add_general_composite_VECPF, ' ifelse(grouped_composite_VECPF_target$Source_',v,' == "VECPF", grouped_composite_VECPF_target$Result_',v, ', "-"')
        add_closing <- paste0(rep(")", source_rules_n), collapse="")
        add_general_individual_VECPF <- paste0(add_general_individual_VECPF, add_closing)
        add_general_composite_VECPF <- paste0(add_general_composite_VECPF, add_closing)
      }else{
        add_general_individual_VECPF <- paste0(add_general_individual_VECPF, ' ifelse(grouped_individual_VECPF_target$Source_',v,' == "VECPF", grouped_individual_VECPF_target$Result_',v, ",")
        add_general_composite_VECPF <- paste0(add_general_composite_VECPF, ' ifelse(grouped_composite_VECPF_target$Source_',v,' == "VECPF", grouped_composite_VECPF_target$Result_',v, ",")
      }
    }
    eval(parse(text = add_general_individual_VECPF))
    eval(parse(text = add_general_composite_VECPF))
    
    simplified_individual_VECPF <- grouped_individual_VECPF_target %>% group_by(real_VECPF, target) %>% 
      summarise(total_count = sum(total_count)) %>% arrange(desc(total_count)) %>% as.data.frame()
    simplified_composite_VECPF <- grouped_composite_VECPF_target %>% group_by(real_VECPF, target) %>% 
      summarise(total_count = sum(total_count)) %>% arrange(desc(total_count)) %>% as.data.frame()
    
  }
  else{
    writeLines("Doesn't Found any composite rules!")
    composite_all_rules <- "No Grouped Composite Found!"
    rownames(individual_rules) <- NULL
    grouped_individual_target <- individual_rules %>% group_by(note, target) %>% 
      summarise(total_count = sum(count)) %>% arrange(desc(total_count)) %>% as.data.frame()
    unique_individual_target <- unique(grouped_individual_target$target)
    grouped_composite_target <- "No Grouped Composite Found!"
    
    grouped_individual_VECPF_target <- NULL
    grouped_composite_VECPF_target <- "No Grouped Composite Found!"
    
    if(length(filter_rules_target) > 0){
      for(b in 1:length(filter_rules_target)){
        grouped_individual_target <- grouped_individual_target %>% filter(target != filter_rules_target[b])
      }
    }
    
    for(w in 1:length(unique_individual_target)){
      grouped_individual_VECPF_target_eval <- "grouped_individual_VECPF_target_add <- individual_rules %>% filter("
      for(v in 1:source_rules_n){
        if(v == source_rules_n){
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Source_',v,' == "VECPF" )')
        }else{
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Source_',v,' == "VECPF" |')
        }
      }
      grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, ' %>% filter(target == "',unique_individual_target[w],'")')
      grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, " %>% group_by( ")
      for(v in 1:(2*source_rules_n)){
        if(v == (2*source_rules_n)){
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Result_',v - source_rules_n,',target)')
        }else if(v <= source_rules_n){
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Source_',v,',')
        }else{
          grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, 'Result_',v - source_rules_n,',')
        }
      }
      
      grouped_individual_VECPF_target_eval <- paste0(grouped_individual_VECPF_target_eval, " %>% summarise(total_count = sum(count)) %>% arrange(desc(total_count)) %>% as.data.frame()")
      eval(parse(text = grouped_individual_VECPF_target_eval))
      
      if(is.null(grouped_individual_VECPF_target)){
        grouped_individual_VECPF_target <- grouped_individual_VECPF_target_add
      }else{
        grouped_individual_VECPF_target <- rbind(grouped_individual_VECPF_target, grouped_individual_VECPF_target_add)
      }
    }
    
    if(length(filter_rules_target) > 0){
      for(b in 1:length(filter_rules_target)){
        grouped_individual_VECPF_target <- grouped_individual_VECPF_target %>% filter(target != filter_rules_target[b])
      }
    }
    
    add_general_individual_VECPF <- "grouped_individual_VECPF_target$real_VECPF <- "
    for(v in 1:source_rules_n){
      if(v == source_rules_n){
        add_general_individual_VECPF <- paste0(add_general_individual_VECPF, ' ifelse(grouped_individual_VECPF_target$Source_',v,' == "VECPF", grouped_individual_VECPF_target$Result_',v, ', "-"')
        add_closing <- paste0(rep(")", source_rules_n), collapse="")
        add_general_individual_VECPF <- paste0(add_general_individual_VECPF, add_closing)
      }else{
        add_general_individual_VECPF <- paste0(add_general_individual_VECPF, 'ifelse(grouped_individual_VECPF_target$Source_',v,' == "VECPF", grouped_individual_VECPF_target$Result_',v, ",")
      }
    }
    eval(parse(text = add_general_individual_VECPF))
    
    simplified_individual_VECPF <- grouped_individual_VECPF_target %>% group_by(real_VECPF, target) %>% 
      summarise(total_count = sum(total_count)) %>% arrange(desc(total_count)) %>% as.data.frame()
    simplified_composite_VECPF <- "No Grouped Composite Found!"
    
  }
  toc()
  
  return(list(individual_rules = individual_rules, composite_rules = composite_rules, composite_all_rules = composite_all_rules, 
              grouped_individual_target = grouped_individual_target, 
              grouped_composite_target = grouped_composite_target,
              grouped_individual_VECPF_target = grouped_individual_VECPF_target, 
              grouped_composite_VECPF_target = grouped_composite_VECPF_target,
              simplified_individual_VECPF = simplified_individual_VECPF,
              simplified_composite_VECPF = simplified_composite_VECPF))
}

rule_matching <- function(audcad_possible_VECPF_tomorrow = c(),
                          audchf_possible_VECPF_tomorrow = c(),
                          audjpy_possible_VECPF_tomorrow = c(),
                          audnzd_possible_VECPF_tomorrow = c(),
                          audusd_possible_VECPF_tomorrow = c(),
                          cadchf_possible_VECPF_tomorrow = c(),
                          cadjpy_possible_VECPF_tomorrow = c(),
                          chfjpy_possible_VECPF_tomorrow = c(),
                          euraud_possible_VECPF_tomorrow = c(),
                          eurcad_possible_VECPF_tomorrow = c(),
                          eurchf_possible_VECPF_tomorrow = c(),
                          eurgbp_possible_VECPF_tomorrow = c(),
                          eurjpy_possible_VECPF_tomorrow = c(),
                          eurnzd_possible_VECPF_tomorrow = c(),
                          eurusd_possible_VECPF_tomorrow = c(),
                          gbpchf_possible_VECPF_tomorrow = c(),
                          gbpjpy_possible_VECPF_tomorrow = c(),
                          gbpusd_possible_VECPF_tomorrow = c(),
                          nzdjpy_possible_VECPF_tomorrow = c(),
                          nzdusd_possible_VECPF_tomorrow = c(),
                          usdcad_possible_VECPF_tomorrow = c(),
                          usdchf_possible_VECPF_tomorrow = c(),
                          usdjpy_possible_VECPF_tomorrow = c(),
                          xauusd_possible_VECPF_tomorrow = c()){
  
  calculate_fib_cpr_rng_tom <- function(asset_name, data_daily, data_prevar){
    unique_fib_prevar <- as.character(unique(na.omit(data_prevar$FIB)))
    unique_cpr_prevar <- as.character(unique(na.omit(data_prevar$CPR)))
    unique_rng_prevar <- as.character(unique(na.omit(data_prevar$RNG)))
    fib_split <- strsplit(unique_fib_prevar, "-")
    cpr_split <- strsplit(unique_cpr_prevar, "-")
    rng_split <- strsplit(unique_rng_prevar, "-")
    
    fib_from_unique <- as.numeric(unlist(lapply(fib_split, FUN = function(x) x[[1]])))
    cpr_from_unique <- as.numeric(unlist(lapply(cpr_split, FUN = function(x) x[[1]])))
    rng_from_unique <- as.numeric(unlist(lapply(rng_split, FUN = function(x) x[[1]])))
    
    pp <- (data_daily[["Close"]] + data_daily[["High"]] + data_daily[["Low"]]) / 3
    tc <- (data_daily[["High"]] + data_daily[["Low"]]) / 2
    bc <- pp - tc + pp
    
    fib_vec <- c(0.236, 0.382, 0.5, 0.618, 0.786)
    fib_r1 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[1]]
    fib_r2 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[2]]
    fib_r3 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[3]]
    fib_r4 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[4]]
    fib_r5 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[5]]
    fib_s1 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[1]]
    fib_s2 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[2]]
    fib_s3 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[3]]
    fib_s4 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[4]]
    fib_s5 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[5]]
    range <- (data_daily[["High"]] - data_daily[["Low"]])
    
    asset_decimal <- 0
    if(asset_name == "XAUUSD"){
      asset_decimal <- 2
    }else if(grepl("JPY",asset_name)){
      asset_decimal <- 2
    }else{
      asset_decimal <- 4
    }
    
    CPR <- round(abs(bc - pp) * 10^asset_decimal,2)
    FIB <- round((((fib_r2 - fib_r1) + (fib_r3 - fib_r2) + (fib_r4 - fib_r3) + (fib_r5 - fib_r4)) / 4) * 10^asset_decimal, 2)
    
    nearest_fib_unique <- unique_fib_prevar[which.min(abs(fib_from_unique - FIB))]
    nearest_cpr_unique <- unique_cpr_prevar[which.min(abs(cpr_from_unique - CPR))]
    nearest_rng_unique <- unique_rng_prevar[which.min(abs(rng_from_unique - range))]
    
    return(c(nearest_fib_unique, nearest_cpr_unique, nearest_rng_unique))
  }
  
  #Form every variable prevar for today and tomorrow, and match it to the underlying rules of pivot 
  #prepare for tomorrow prevar setup
  library(lubridate)
  #--------------------------------- 1) AUDCAD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 1) AUDCAD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  audcad_rules_list <- list()
  audcad_current_prevar <- audcad_prevar[nrow(audcad_prevar),]
  audcad_today_rules <- find_match_rules(audcad_rules, audcad_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDCAD Today Property Variables --------------------------------")
  writeLines("")
  print(audcad_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(audcad_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(audcad_today_rules[[5]])
  writeLines("")
  audcad_rules_list <- c(audcad_rules_list, list(audcad_today_rules))
  names(audcad_rules_list)[length(audcad_rules_list)] <- "AUDCAD_TODAY_VECPF_RULES"
  
  audcad_tom_prevar <- audcad_prevar[nrow(audcad_prevar),]
  audcad_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(audcad_tom_prevar$WK == "Saturday")) audcad_tom_prevar$WK <- "Monday"
  audcad_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "AUDCAD", 
                                                  audcad_daily[nrow(audcad_daily),], audcad_prevar)
  audcad_tom_prevar$FIB <- audcad_fib_cpr_rng[1]
  audcad_tom_prevar$RNG <- audcad_fib_cpr_rng[3]
  audcad_tom_prevar$CPR <- audcad_fib_cpr_rng[2]
  audcad_tom_prevar$VECP_6F <- audcad_tom_prevar$VECP_5F
  audcad_tom_prevar$VECP_6T <- audcad_tom_prevar$VECP_5T
  audcad_tom_prevar$VECP_5F <- audcad_tom_prevar$VECP_4F
  audcad_tom_prevar$VECP_5T <- audcad_tom_prevar$VECP_4T
  audcad_tom_prevar$VECP_4F <- audcad_tom_prevar$VECP_3F
  audcad_tom_prevar$VECP_4T <- audcad_tom_prevar$VECP_3T
  audcad_tom_prevar$VECP_3F <- audcad_tom_prevar$VECP_2F
  audcad_tom_prevar$VECP_3T <- audcad_tom_prevar$VECP_2T
  audcad_tom_prevar$VECP_2F <- audcad_tom_prevar$VECP_1F
  audcad_tom_prevar$VECP_2T <- audcad_tom_prevar$VECP_1T
  audcad_tom_prevar$VECP_1F <- audcad_tom_prevar$VECPF
  audcad_tom_prevar$VECP_1T <- audcad_tom_prevar$VECPT
  audcad_tom_prevar$VECPF <- "-"
  audcad_tom_prevar$VECPT <- "-"
  audcad_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(audcad_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(audcad_possible_VECPF_tomorrow)){
      audcad_tom_prevar$VECPF <- audcad_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- AUDCAD Tomorrow Property Variables if VECPF = ",audcad_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(audcad_tom_prevar)
      writeLines("")
      audcad_tom_rules <- find_match_rules(audcad_rules, audcad_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",audcad_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(audcad_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",audcad_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(audcad_tom_rules[[5]])
      writeLines("")
      audcad_rules_list <- c(audcad_rules_list, list(audcad_tom_rules))
      names(audcad_rules_list)[length(audcad_rules_list)] <- paste0("AUDCAD_TOM_VECPF_", audcad_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  audcad_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDCAD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(audcad_tom_prevar)
  writeLines("")
  audcad_tom_rules <- find_match_rules(audcad_rules, audcad_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(audcad_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(audcad_tom_rules[[5]])
  writeLines("")
  audcad_rules_list <- c(audcad_rules_list, list(audcad_tom_rules))
  names(audcad_rules_list)[length(audcad_rules_list)] <- "AUDCAD_TOM_ORDINARY_RULES"
  
  #--------------------------------- 2) AUDCHF Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 2) AUDCHF Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  audchf_rules_list <- list()
  audchf_current_prevar <- audchf_prevar[nrow(audchf_prevar),]
  audchf_today_rules <- find_match_rules(audchf_rules, audchf_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDCHF Today Property Variables --------------------------------")
  writeLines("")
  print(audchf_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(audchf_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(audchf_today_rules[[5]])
  writeLines("")
  audchf_rules_list <- c(audchf_rules_list, list(audchf_today_rules))
  names(audchf_rules_list)[length(audchf_rules_list)] <- "AUDCHF_TODAY_VECPF_RULES"
  
  audchf_tom_prevar <- audchf_prevar[nrow(audchf_prevar),]
  audchf_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(audchf_tom_prevar$WK == "Saturday")) audchf_tom_prevar$WK <- "Monday"
  audchf_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "AUDCHF", 
                                                  audchf_daily[nrow(audchf_daily),], audchf_prevar)
  audchf_tom_prevar$FIB <- audchf_fib_cpr_rng[1]
  audchf_tom_prevar$RNG <- audchf_fib_cpr_rng[3]
  audchf_tom_prevar$CPR <- audchf_fib_cpr_rng[2]
  audchf_tom_prevar$VECP_6F <- audchf_tom_prevar$VECP_5F
  audchf_tom_prevar$VECP_6T <- audchf_tom_prevar$VECP_5T
  audchf_tom_prevar$VECP_5F <- audchf_tom_prevar$VECP_4F
  audchf_tom_prevar$VECP_5T <- audchf_tom_prevar$VECP_4T
  audchf_tom_prevar$VECP_4F <- audchf_tom_prevar$VECP_3F
  audchf_tom_prevar$VECP_4T <- audchf_tom_prevar$VECP_3T
  audchf_tom_prevar$VECP_3F <- audchf_tom_prevar$VECP_2F
  audchf_tom_prevar$VECP_3T <- audchf_tom_prevar$VECP_2T
  audchf_tom_prevar$VECP_2F <- audchf_tom_prevar$VECP_1F
  audchf_tom_prevar$VECP_2T <- audchf_tom_prevar$VECP_1T
  audchf_tom_prevar$VECP_1F <- audchf_tom_prevar$VECPF
  audchf_tom_prevar$VECP_1T <- audchf_tom_prevar$VECPT
  audchf_tom_prevar$VECPF <- "-"
  audchf_tom_prevar$VECPT <- "-"
  audchf_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(audchf_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(audchf_possible_VECPF_tomorrow)){
      audchf_tom_prevar$VECPF <- audchf_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- AUDCHF Tomorrow Property Variables if VECPF = ",audchf_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(audchf_tom_prevar)
      writeLines("")
      audchf_tom_rules <- find_match_rules(audchf_rules, audchf_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",audchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(audchf_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",audchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(audchf_tom_rules[[5]])
      writeLines("")
      audchf_rules_list <- c(audchf_rules_list, list(audchf_tom_rules))
      names(audchf_rules_list)[length(audchf_rules_list)] <- paste0("AUDCHF_TOM_VECPF_", audchf_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  audchf_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDCHF Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(audchf_tom_prevar)
  writeLines("")
  audchf_tom_rules <- find_match_rules(audchf_rules, audchf_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(audchf_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(audchf_tom_rules[[5]])
  writeLines("")
  audchf_rules_list <- c(audchf_rules_list, list(audchf_tom_rules))
  names(audchf_rules_list)[length(audchf_rules_list)] <- "AUDCHF_TOM_ORDINARY_RULES"
  
  #--------------------------------- 3) AUDJPY Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 3) AUDJPY Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  audjpy_rules_list <- list()
  audjpy_current_prevar <- audjpy_prevar[nrow(audjpy_prevar),]
  audjpy_today_rules <- find_match_rules(audjpy_rules, audjpy_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDJPY Today Property Variables --------------------------------")
  writeLines("")
  print(audjpy_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(audjpy_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(audjpy_today_rules[[5]])
  writeLines("")
  audjpy_rules_list <- c(audjpy_rules_list, list(audjpy_today_rules))
  names(audjpy_rules_list)[length(audjpy_rules_list)] <- "AUDJPY_TODAY_VECPF_RULES"
  
  audjpy_tom_prevar <- audjpy_prevar[nrow(audjpy_prevar),]
  audjpy_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(audjpy_tom_prevar$WK == "Saturday")) audjpy_tom_prevar$WK <- "Monday"
  audjpy_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "AUDJPY", 
                                                  audjpy_daily[nrow(audjpy_daily),], audjpy_prevar)
  audjpy_tom_prevar$FIB <- audjpy_fib_cpr_rng[1]
  audjpy_tom_prevar$RNG <- audjpy_fib_cpr_rng[3]
  audjpy_tom_prevar$CPR <- audjpy_fib_cpr_rng[2]
  audjpy_tom_prevar$VECP_6F <- audjpy_tom_prevar$VECP_5F
  audjpy_tom_prevar$VECP_6T <- audjpy_tom_prevar$VECP_5T
  audjpy_tom_prevar$VECP_5F <- audjpy_tom_prevar$VECP_4F
  audjpy_tom_prevar$VECP_5T <- audjpy_tom_prevar$VECP_4T
  audjpy_tom_prevar$VECP_4F <- audjpy_tom_prevar$VECP_3F
  audjpy_tom_prevar$VECP_4T <- audjpy_tom_prevar$VECP_3T
  audjpy_tom_prevar$VECP_3F <- audjpy_tom_prevar$VECP_2F
  audjpy_tom_prevar$VECP_3T <- audjpy_tom_prevar$VECP_2T
  audjpy_tom_prevar$VECP_2F <- audjpy_tom_prevar$VECP_1F
  audjpy_tom_prevar$VECP_2T <- audjpy_tom_prevar$VECP_1T
  audjpy_tom_prevar$VECP_1F <- audjpy_tom_prevar$VECPF
  audjpy_tom_prevar$VECP_1T <- audjpy_tom_prevar$VECPT
  audjpy_tom_prevar$VECPF <- "-"
  audjpy_tom_prevar$VECPT <- "-"
  audjpy_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(audjpy_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(audjpy_possible_VECPF_tomorrow)){
      audjpy_tom_prevar$VECPF <- audjpy_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- AUDJPY Tomorrow Property Variables if VECPF = ",audjpy_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(audjpy_tom_prevar)
      writeLines("")
      audjpy_tom_rules <- find_match_rules(audjpy_rules, audjpy_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",audjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(audjpy_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",audjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(audjpy_tom_rules[[5]])
      writeLines("")
      audjpy_rules_list <- c(audjpy_rules_list, list(audjpy_tom_rules))
      names(audjpy_rules_list)[length(audjpy_rules_list)] <- paste0("AUDJPY_TOM_VECPF_", audjpy_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  audjpy_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDJPY Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(audjpy_tom_prevar)
  writeLines("")
  audjpy_tom_rules <- find_match_rules(audjpy_rules, audjpy_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(audjpy_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(audjpy_tom_rules[[5]])
  writeLines("")
  audjpy_rules_list <- c(audjpy_rules_list, list(audjpy_tom_rules))
  names(audjpy_rules_list)[length(audjpy_rules_list)] <- "AUDJPY_TOM_ORDINARY_RULES"
  
  #--------------------------------- 4) AUDNZD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 4) AUDNZD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  audnzd_rules_list <- list()
  audnzd_current_prevar <- audnzd_prevar[nrow(audnzd_prevar),]
  audnzd_today_rules <- find_match_rules(audnzd_rules, audnzd_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDNZD Today Property Variables --------------------------------")
  writeLines("")
  print(audnzd_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(audnzd_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(audnzd_today_rules[[5]])
  writeLines("")
  audnzd_rules_list <- c(audnzd_rules_list, list(audnzd_today_rules))
  names(audnzd_rules_list)[length(audnzd_rules_list)] <- "AUDNZD_TODAY_VECPF_RULES"
  
  audnzd_tom_prevar <- audnzd_prevar[nrow(audnzd_prevar),]
  audnzd_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(audnzd_tom_prevar$WK == "Saturday")) audnzd_tom_prevar$WK <- "Monday"
  audnzd_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "AUDNZD", 
                                                  audnzd_daily[nrow(audnzd_daily),], audnzd_prevar)
  audnzd_tom_prevar$FIB <- audnzd_fib_cpr_rng[1]
  audnzd_tom_prevar$RNG <- audnzd_fib_cpr_rng[3]
  audnzd_tom_prevar$CPR <- audnzd_fib_cpr_rng[2]
  audnzd_tom_prevar$VECP_6F <- audnzd_tom_prevar$VECP_5F
  audnzd_tom_prevar$VECP_6T <- audnzd_tom_prevar$VECP_5T
  audnzd_tom_prevar$VECP_5F <- audnzd_tom_prevar$VECP_4F
  audnzd_tom_prevar$VECP_5T <- audnzd_tom_prevar$VECP_4T
  audnzd_tom_prevar$VECP_4F <- audnzd_tom_prevar$VECP_3F
  audnzd_tom_prevar$VECP_4T <- audnzd_tom_prevar$VECP_3T
  audnzd_tom_prevar$VECP_3F <- audnzd_tom_prevar$VECP_2F
  audnzd_tom_prevar$VECP_3T <- audnzd_tom_prevar$VECP_2T
  audnzd_tom_prevar$VECP_2F <- audnzd_tom_prevar$VECP_1F
  audnzd_tom_prevar$VECP_2T <- audnzd_tom_prevar$VECP_1T
  audnzd_tom_prevar$VECP_1F <- audnzd_tom_prevar$VECPF
  audnzd_tom_prevar$VECP_1T <- audnzd_tom_prevar$VECPT
  audnzd_tom_prevar$VECPF <- "-"
  audnzd_tom_prevar$VECPT <- "-"
  audnzd_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(audnzd_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(audnzd_possible_VECPF_tomorrow)){
      audnzd_tom_prevar$VECPF <- audnzd_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- AUDNZD Tomorrow Property Variables if VECPF = ",audnzd_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(audnzd_tom_prevar)
      writeLines("")
      audnzd_tom_rules <- find_match_rules(audnzd_rules, audnzd_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",audnzd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(audnzd_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",audnzd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(audnzd_tom_rules[[5]])
      writeLines("")
      audnzd_rules_list <- c(audnzd_rules_list, list(audnzd_tom_rules))
      names(audnzd_rules_list)[length(audnzd_rules_list)] <- paste0("AUDNZD_TOM_VECPF_", audnzd_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  audnzd_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDNZD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(audnzd_tom_prevar)
  writeLines("")
  audnzd_tom_rules <- find_match_rules(audnzd_rules, audnzd_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(audnzd_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(audnzd_tom_rules[[5]])
  writeLines("")
  audnzd_rules_list <- c(audnzd_rules_list, list(audnzd_tom_rules))
  names(audnzd_rules_list)[length(audnzd_rules_list)] <- "AUDNZD_TOM_ORDINARY_RULES"
  
  #--------------------------------- 5) AUDUSD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 5) AUDUSD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  audusd_rules_list <- list()
  audusd_current_prevar <- audusd_prevar[nrow(audusd_prevar),]
  audusd_today_rules <- find_match_rules(audusd_rules, audusd_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDUSD Today Property Variables --------------------------------")
  writeLines("")
  print(audusd_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(audusd_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(audusd_today_rules[[5]])
  writeLines("")
  audusd_rules_list <- c(audusd_rules_list, list(audusd_today_rules))
  names(audusd_rules_list)[length(audusd_rules_list)] <- "AUDUSD_TODAY_VECPF_RULES"
  
  audusd_tom_prevar <- audusd_prevar[nrow(audusd_prevar),]
  audusd_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(audusd_tom_prevar$WK == "Saturday")) audusd_tom_prevar$WK <- "Monday"
  audusd_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "AUDUSD", 
                                                  audusd_daily[nrow(audusd_daily),], audusd_prevar)
  audusd_tom_prevar$FIB <- audusd_fib_cpr_rng[1]
  audusd_tom_prevar$RNG <- audusd_fib_cpr_rng[3]
  audusd_tom_prevar$CPR <- audusd_fib_cpr_rng[2]
  audusd_tom_prevar$VECP_6F <- audusd_tom_prevar$VECP_5F
  audusd_tom_prevar$VECP_6T <- audusd_tom_prevar$VECP_5T
  audusd_tom_prevar$VECP_5F <- audusd_tom_prevar$VECP_4F
  audusd_tom_prevar$VECP_5T <- audusd_tom_prevar$VECP_4T
  audusd_tom_prevar$VECP_4F <- audusd_tom_prevar$VECP_3F
  audusd_tom_prevar$VECP_4T <- audusd_tom_prevar$VECP_3T
  audusd_tom_prevar$VECP_3F <- audusd_tom_prevar$VECP_2F
  audusd_tom_prevar$VECP_3T <- audusd_tom_prevar$VECP_2T
  audusd_tom_prevar$VECP_2F <- audusd_tom_prevar$VECP_1F
  audusd_tom_prevar$VECP_2T <- audusd_tom_prevar$VECP_1T
  audusd_tom_prevar$VECP_1F <- audusd_tom_prevar$VECPF
  audusd_tom_prevar$VECP_1T <- audusd_tom_prevar$VECPT
  audusd_tom_prevar$VECPF <- "-"
  audusd_tom_prevar$VECPT <- "-"
  audusd_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(audusd_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(audusd_possible_VECPF_tomorrow)){
      audusd_tom_prevar$VECPF <- audusd_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- AUDUSD Tomorrow Property Variables if VECPF = ",audusd_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(audusd_tom_prevar)
      writeLines("")
      audusd_tom_rules <- find_match_rules(audusd_rules, audusd_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",audusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(audusd_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",audusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(audusd_tom_rules[[5]])
      writeLines("")
      audusd_rules_list <- c(audusd_rules_list, list(audusd_tom_rules))
      names(audusd_rules_list)[length(audusd_rules_list)] <- paste0("AUDUSD_TOM_VECPF_", audusd_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  audusd_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- AUDUSD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(audusd_tom_prevar)
  writeLines("")
  audusd_tom_rules <- find_match_rules(audusd_rules, audusd_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(audusd_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(audusd_tom_rules[[5]])
  writeLines("")
  audusd_rules_list <- c(audusd_rules_list, list(audusd_tom_rules))
  names(audusd_rules_list)[length(audusd_rules_list)] <- "AUDUSD_TOM_ORDINARY_RULES"
  #--------------------------------- 6) CADCHF Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 6) CADCHF Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  cadchf_rules_list <- list()
  cadchf_current_prevar <- cadchf_prevar[nrow(cadchf_prevar),]
  cadchf_today_rules <- find_match_rules(cadchf_rules, cadchf_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- CADCHF Today Property Variables --------------------------------")
  writeLines("")
  print(cadchf_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(cadchf_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(cadchf_today_rules[[5]])
  writeLines("")
  cadchf_rules_list <- c(cadchf_rules_list, list(cadchf_today_rules))
  names(cadchf_rules_list)[length(cadchf_rules_list)] <- "CADCHF_TODAY_VECPF_RULES"
  
  cadchf_tom_prevar <- cadchf_prevar[nrow(cadchf_prevar),]
  cadchf_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(cadchf_tom_prevar$WK == "Saturday")) cadchf_tom_prevar$WK <- "Monday"
  cadchf_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "CADCHF", 
                                                  cadchf_daily[nrow(cadchf_daily),], cadchf_prevar)
  cadchf_tom_prevar$FIB <- cadchf_fib_cpr_rng[1]
  cadchf_tom_prevar$RNG <- cadchf_fib_cpr_rng[3]
  cadchf_tom_prevar$CPR <- cadchf_fib_cpr_rng[2]
  cadchf_tom_prevar$VECP_6F <- cadchf_tom_prevar$VECP_5F
  cadchf_tom_prevar$VECP_6T <- cadchf_tom_prevar$VECP_5T
  cadchf_tom_prevar$VECP_5F <- cadchf_tom_prevar$VECP_4F
  cadchf_tom_prevar$VECP_5T <- cadchf_tom_prevar$VECP_4T
  cadchf_tom_prevar$VECP_4F <- cadchf_tom_prevar$VECP_3F
  cadchf_tom_prevar$VECP_4T <- cadchf_tom_prevar$VECP_3T
  cadchf_tom_prevar$VECP_3F <- cadchf_tom_prevar$VECP_2F
  cadchf_tom_prevar$VECP_3T <- cadchf_tom_prevar$VECP_2T
  cadchf_tom_prevar$VECP_2F <- cadchf_tom_prevar$VECP_1F
  cadchf_tom_prevar$VECP_2T <- cadchf_tom_prevar$VECP_1T
  cadchf_tom_prevar$VECP_1F <- cadchf_tom_prevar$VECPF
  cadchf_tom_prevar$VECP_1T <- cadchf_tom_prevar$VECPT
  cadchf_tom_prevar$VECPF <- "-"
  cadchf_tom_prevar$VECPT <- "-"
  cadchf_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(cadchf_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(cadchf_possible_VECPF_tomorrow)){
      cadchf_tom_prevar$VECPF <- cadchf_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- CADCHF Tomorrow Property Variables if VECPF = ",cadchf_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(cadchf_tom_prevar)
      writeLines("")
      cadchf_tom_rules <- find_match_rules(cadchf_rules, cadchf_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",cadchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(cadchf_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",cadchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(cadchf_tom_rules[[5]])
      writeLines("")
      cadchf_rules_list <- c(cadchf_rules_list, list(cadchf_tom_rules))
      names(cadchf_rules_list)[length(cadchf_rules_list)] <- paste0("CADCHF_TOM_VECPF_", cadchf_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  cadchf_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- CADCHF Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(cadchf_tom_prevar)
  writeLines("")
  cadchf_tom_rules <- find_match_rules(cadchf_rules, cadchf_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(cadchf_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(cadchf_tom_rules[[5]])
  writeLines("")
  cadchf_rules_list <- c(cadchf_rules_list, list(cadchf_tom_rules))
  names(cadchf_rules_list)[length(cadchf_rules_list)] <- "CADCHF_TOM_ORDINARY_RULES"
  #--------------------------------- 7) CADJPY Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 7) CADJPY Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  cadjpy_rules_list <- list()
  cadjpy_current_prevar <- cadjpy_prevar[nrow(cadjpy_prevar),]
  cadjpy_today_rules <- find_match_rules(cadjpy_rules, cadjpy_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- CADJPY Today Property Variables --------------------------------")
  writeLines("")
  print(cadjpy_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(cadjpy_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(cadjpy_today_rules[[5]])
  writeLines("")
  cadjpy_rules_list <- c(cadjpy_rules_list, list(cadjpy_today_rules))
  names(cadjpy_rules_list)[length(cadjpy_rules_list)] <- "CADJPY_TODAY_VECPF_RULES"
  
  cadjpy_tom_prevar <- cadjpy_prevar[nrow(cadjpy_prevar),]
  cadjpy_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(cadjpy_tom_prevar$WK == "Saturday")) cadjpy_tom_prevar$WK <- "Monday"
  cadjpy_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "CADJPY", 
                                                  cadjpy_daily[nrow(cadjpy_daily),], cadjpy_prevar)
  cadjpy_tom_prevar$FIB <- cadjpy_fib_cpr_rng[1]
  cadjpy_tom_prevar$RNG <- cadjpy_fib_cpr_rng[3]
  cadjpy_tom_prevar$CPR <- cadjpy_fib_cpr_rng[2]
  cadjpy_tom_prevar$VECP_6F <- cadjpy_tom_prevar$VECP_5F
  cadjpy_tom_prevar$VECP_6T <- cadjpy_tom_prevar$VECP_5T
  cadjpy_tom_prevar$VECP_5F <- cadjpy_tom_prevar$VECP_4F
  cadjpy_tom_prevar$VECP_5T <- cadjpy_tom_prevar$VECP_4T
  cadjpy_tom_prevar$VECP_4F <- cadjpy_tom_prevar$VECP_3F
  cadjpy_tom_prevar$VECP_4T <- cadjpy_tom_prevar$VECP_3T
  cadjpy_tom_prevar$VECP_3F <- cadjpy_tom_prevar$VECP_2F
  cadjpy_tom_prevar$VECP_3T <- cadjpy_tom_prevar$VECP_2T
  cadjpy_tom_prevar$VECP_2F <- cadjpy_tom_prevar$VECP_1F
  cadjpy_tom_prevar$VECP_2T <- cadjpy_tom_prevar$VECP_1T
  cadjpy_tom_prevar$VECP_1F <- cadjpy_tom_prevar$VECPF
  cadjpy_tom_prevar$VECP_1T <- cadjpy_tom_prevar$VECPT
  cadjpy_tom_prevar$VECPF <- "-"
  cadjpy_tom_prevar$VECPT <- "-"
  cadjpy_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(cadjpy_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(cadjpy_possible_VECPF_tomorrow)){
      cadjpy_tom_prevar$VECPF <- cadjpy_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- CADJPY Tomorrow Property Variables if VECPF = ",cadjpy_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(cadjpy_tom_prevar)
      writeLines("")
      cadjpy_tom_rules <- find_match_rules(cadjpy_rules, cadjpy_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",cadjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(cadjpy_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",cadjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(cadjpy_tom_rules[[5]])
      writeLines("")
      cadjpy_rules_list <- c(cadjpy_rules_list, list(cadjpy_tom_rules))
      names(cadjpy_rules_list)[length(cadjpy_rules_list)] <- paste0("CADJPY_TOM_VECPF_", cadjpy_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  cadjpy_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- CADJPY Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(cadjpy_tom_prevar)
  writeLines("")
  cadjpy_tom_rules <- find_match_rules(cadjpy_rules, cadjpy_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(cadjpy_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(cadjpy_tom_rules[[5]])
  writeLines("")
  cadjpy_rules_list <- c(cadjpy_rules_list, list(cadjpy_tom_rules))
  names(cadjpy_rules_list)[length(cadjpy_rules_list)] <- "CADJPY_TOM_ORDINARY_RULES"
  
  #--------------------------------- 8) CHFJPY Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 8) CHFJPY Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  chfjpy_rules_list <- list()
  chfjpy_current_prevar <- chfjpy_prevar[nrow(chfjpy_prevar),]
  chfjpy_today_rules <- find_match_rules(chfjpy_rules, chfjpy_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- CHFJPY Today Property Variables --------------------------------")
  writeLines("")
  print(chfjpy_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(chfjpy_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(chfjpy_today_rules[[5]])
  writeLines("")
  chfjpy_rules_list <- c(chfjpy_rules_list, list(chfjpy_today_rules))
  names(chfjpy_rules_list)[length(chfjpy_rules_list)] <- "CHFJPY_TODAY_VECPF_RULES"
  
  chfjpy_tom_prevar <- chfjpy_prevar[nrow(chfjpy_prevar),]
  chfjpy_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(chfjpy_tom_prevar$WK == "Saturday")) chfjpy_tom_prevar$WK <- "Monday"
  chfjpy_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "CHFJPY", 
                                                  chfjpy_daily[nrow(chfjpy_daily),], chfjpy_prevar)
  chfjpy_tom_prevar$FIB <- chfjpy_fib_cpr_rng[1]
  chfjpy_tom_prevar$RNG <- chfjpy_fib_cpr_rng[3]
  chfjpy_tom_prevar$CPR <- chfjpy_fib_cpr_rng[2]
  chfjpy_tom_prevar$VECP_6F <- chfjpy_tom_prevar$VECP_5F
  chfjpy_tom_prevar$VECP_6T <- chfjpy_tom_prevar$VECP_5T
  chfjpy_tom_prevar$VECP_5F <- chfjpy_tom_prevar$VECP_4F
  chfjpy_tom_prevar$VECP_5T <- chfjpy_tom_prevar$VECP_4T
  chfjpy_tom_prevar$VECP_4F <- chfjpy_tom_prevar$VECP_3F
  chfjpy_tom_prevar$VECP_4T <- chfjpy_tom_prevar$VECP_3T
  chfjpy_tom_prevar$VECP_3F <- chfjpy_tom_prevar$VECP_2F
  chfjpy_tom_prevar$VECP_3T <- chfjpy_tom_prevar$VECP_2T
  chfjpy_tom_prevar$VECP_2F <- chfjpy_tom_prevar$VECP_1F
  chfjpy_tom_prevar$VECP_2T <- chfjpy_tom_prevar$VECP_1T
  chfjpy_tom_prevar$VECP_1F <- chfjpy_tom_prevar$VECPF
  chfjpy_tom_prevar$VECP_1T <- chfjpy_tom_prevar$VECPT
  chfjpy_tom_prevar$VECPF <- "-"
  chfjpy_tom_prevar$VECPT <- "-"
  chfjpy_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(chfjpy_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(chfjpy_possible_VECPF_tomorrow)){
      chfjpy_tom_prevar$VECPF <- chfjpy_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- CHFJPY Tomorrow Property Variables if VECPF = ",chfjpy_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(chfjpy_tom_prevar)
      writeLines("")
      chfjpy_tom_rules <- find_match_rules(chfjpy_rules, chfjpy_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",chfjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(chfjpy_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",chfjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(chfjpy_tom_rules[[5]])
      writeLines("")
      chfjpy_rules_list <- c(chfjpy_rules_list, list(chfjpy_tom_rules))
      names(chfjpy_rules_list)[length(chfjpy_rules_list)] <- paste0("CHFJPY_TOM_VECPF_", chfjpy_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  chfjpy_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- CHFJPY Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(chfjpy_tom_prevar)
  writeLines("")
  chfjpy_tom_rules <- find_match_rules(chfjpy_rules, chfjpy_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(chfjpy_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(chfjpy_tom_rules[[5]])
  writeLines("")
  chfjpy_rules_list <- c(chfjpy_rules_list, list(chfjpy_tom_rules))
  names(chfjpy_rules_list)[length(chfjpy_rules_list)] <- "CHFJPY_TOM_ORDINARY_RULES"
  #--------------------------------- 9) EURAUD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 9) EURAUD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  euraud_rules_list <- list()
  euraud_current_prevar <- euraud_prevar[nrow(euraud_prevar),]
  euraud_today_rules <- find_match_rules(euraud_rules, euraud_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURAUD Today Property Variables --------------------------------")
  writeLines("")
  print(euraud_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(euraud_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(euraud_today_rules[[5]])
  writeLines("")
  euraud_rules_list <- c(euraud_rules_list, list(euraud_today_rules))
  names(euraud_rules_list)[length(euraud_rules_list)] <- "EURAUD_TODAY_VECPF_RULES"
  
  euraud_tom_prevar <- euraud_prevar[nrow(euraud_prevar),]
  euraud_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(euraud_tom_prevar$WK == "Saturday")) euraud_tom_prevar$WK <- "Monday"
  euraud_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "EURAUD", 
                                                  euraud_daily[nrow(euraud_daily),], euraud_prevar)
  euraud_tom_prevar$FIB <- euraud_fib_cpr_rng[1]
  euraud_tom_prevar$RNG <- euraud_fib_cpr_rng[3]
  euraud_tom_prevar$CPR <- euraud_fib_cpr_rng[2]
  euraud_tom_prevar$VECP_6F <- euraud_tom_prevar$VECP_5F
  euraud_tom_prevar$VECP_6T <- euraud_tom_prevar$VECP_5T
  euraud_tom_prevar$VECP_5F <- euraud_tom_prevar$VECP_4F
  euraud_tom_prevar$VECP_5T <- euraud_tom_prevar$VECP_4T
  euraud_tom_prevar$VECP_4F <- euraud_tom_prevar$VECP_3F
  euraud_tom_prevar$VECP_4T <- euraud_tom_prevar$VECP_3T
  euraud_tom_prevar$VECP_3F <- euraud_tom_prevar$VECP_2F
  euraud_tom_prevar$VECP_3T <- euraud_tom_prevar$VECP_2T
  euraud_tom_prevar$VECP_2F <- euraud_tom_prevar$VECP_1F
  euraud_tom_prevar$VECP_2T <- euraud_tom_prevar$VECP_1T
  euraud_tom_prevar$VECP_1F <- euraud_tom_prevar$VECPF
  euraud_tom_prevar$VECP_1T <- euraud_tom_prevar$VECPT
  euraud_tom_prevar$VECPF <- "-"
  euraud_tom_prevar$VECPT <- "-"
  euraud_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(euraud_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(euraud_possible_VECPF_tomorrow)){
      euraud_tom_prevar$VECPF <- euraud_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- EURAUD Tomorrow Property Variables if VECPF = ",euraud_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(euraud_tom_prevar)
      writeLines("")
      euraud_tom_rules <- find_match_rules(euraud_rules, euraud_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",euraud_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(euraud_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",euraud_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(euraud_tom_rules[[5]])
      writeLines("")
      euraud_rules_list <- c(euraud_rules_list, list(euraud_tom_rules))
      names(euraud_rules_list)[length(euraud_rules_list)] <- paste0("EURAUD_TOM_VECPF_", euraud_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  euraud_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURAUD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(euraud_tom_prevar)
  writeLines("")
  euraud_tom_rules <- find_match_rules(euraud_rules, euraud_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(euraud_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(euraud_tom_rules[[5]])
  writeLines("")
  euraud_rules_list <- c(euraud_rules_list, list(euraud_tom_rules))
  names(euraud_rules_list)[length(euraud_rules_list)] <- "EURAUD_TOM_ORDINARY_RULES"
  #--------------------------------- 10) EURCAD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 10) EURCAD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  eurcad_rules_list <- list()
  eurcad_current_prevar <- eurcad_prevar[nrow(eurcad_prevar),]
  eurcad_today_rules <- find_match_rules(eurcad_rules, eurcad_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURCAD Today Property Variables --------------------------------")
  writeLines("")
  print(eurcad_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(eurcad_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(eurcad_today_rules[[5]])
  writeLines("")
  eurcad_rules_list <- c(eurcad_rules_list, list(eurcad_today_rules))
  names(eurcad_rules_list)[length(eurcad_rules_list)] <- "EURCAD_TODAY_VECPF_RULES"
  
  eurcad_tom_prevar <- eurcad_prevar[nrow(eurcad_prevar),]
  eurcad_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(eurcad_tom_prevar$WK == "Saturday")) eurcad_tom_prevar$WK <- "Monday"
  eurcad_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "EURCAD", 
                                                  eurcad_daily[nrow(eurcad_daily),], eurcad_prevar)
  eurcad_tom_prevar$FIB <- eurcad_fib_cpr_rng[1]
  eurcad_tom_prevar$RNG <- eurcad_fib_cpr_rng[3]
  eurcad_tom_prevar$CPR <- eurcad_fib_cpr_rng[2]
  eurcad_tom_prevar$VECP_6F <- eurcad_tom_prevar$VECP_5F
  eurcad_tom_prevar$VECP_6T <- eurcad_tom_prevar$VECP_5T
  eurcad_tom_prevar$VECP_5F <- eurcad_tom_prevar$VECP_4F
  eurcad_tom_prevar$VECP_5T <- eurcad_tom_prevar$VECP_4T
  eurcad_tom_prevar$VECP_4F <- eurcad_tom_prevar$VECP_3F
  eurcad_tom_prevar$VECP_4T <- eurcad_tom_prevar$VECP_3T
  eurcad_tom_prevar$VECP_3F <- eurcad_tom_prevar$VECP_2F
  eurcad_tom_prevar$VECP_3T <- eurcad_tom_prevar$VECP_2T
  eurcad_tom_prevar$VECP_2F <- eurcad_tom_prevar$VECP_1F
  eurcad_tom_prevar$VECP_2T <- eurcad_tom_prevar$VECP_1T
  eurcad_tom_prevar$VECP_1F <- eurcad_tom_prevar$VECPF
  eurcad_tom_prevar$VECP_1T <- eurcad_tom_prevar$VECPT
  eurcad_tom_prevar$VECPF <- "-"
  eurcad_tom_prevar$VECPT <- "-"
  eurcad_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(eurcad_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(eurcad_possible_VECPF_tomorrow)){
      eurcad_tom_prevar$VECPF <- eurcad_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- EURCAD Tomorrow Property Variables if VECPF = ",eurcad_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(eurcad_tom_prevar)
      writeLines("")
      eurcad_tom_rules <- find_match_rules(eurcad_rules, eurcad_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",eurcad_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(eurcad_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",eurcad_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(eurcad_tom_rules[[5]])
      writeLines("")
      eurcad_rules_list <- c(eurcad_rules_list, list(eurcad_tom_rules))
      names(eurcad_rules_list)[length(eurcad_rules_list)] <- paste0("EURCAD_TOM_VECPF_", eurcad_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  eurcad_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURCAD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(eurcad_tom_prevar)
  writeLines("")
  eurcad_tom_rules <- find_match_rules(eurcad_rules, eurcad_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(eurcad_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(eurcad_tom_rules[[5]])
  writeLines("")
  eurcad_rules_list <- c(eurcad_rules_list, list(eurcad_tom_rules))
  names(eurcad_rules_list)[length(eurcad_rules_list)] <- "EURCAD_TOM_ORDINARY_RULES"
  #--------------------------------- 11) EURCHF Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 11) EURCHF Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  eurchf_rules_list <- list()
  eurchf_current_prevar <- eurchf_prevar[nrow(eurchf_prevar),]
  eurchf_today_rules <- find_match_rules(eurchf_rules, eurchf_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURCHF Today Property Variables --------------------------------")
  writeLines("")
  print(eurchf_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(eurchf_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(eurchf_today_rules[[5]])
  writeLines("")
  eurchf_rules_list <- c(eurchf_rules_list, list(eurchf_today_rules))
  names(eurchf_rules_list)[length(eurchf_rules_list)] <- "EURCHF_TODAY_VECPF_RULES"
  
  eurchf_tom_prevar <- eurchf_prevar[nrow(eurchf_prevar),]
  eurchf_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(eurchf_tom_prevar$WK == "Saturday")) eurchf_tom_prevar$WK <- "Monday"
  eurchf_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "EURCHF", 
                                                  eurchf_daily[nrow(eurchf_daily),], eurchf_prevar)
  eurchf_tom_prevar$FIB <- eurchf_fib_cpr_rng[1]
  eurchf_tom_prevar$RNG <- eurchf_fib_cpr_rng[3]
  eurchf_tom_prevar$CPR <- eurchf_fib_cpr_rng[2]
  eurchf_tom_prevar$VECP_6F <- eurchf_tom_prevar$VECP_5F
  eurchf_tom_prevar$VECP_6T <- eurchf_tom_prevar$VECP_5T
  eurchf_tom_prevar$VECP_5F <- eurchf_tom_prevar$VECP_4F
  eurchf_tom_prevar$VECP_5T <- eurchf_tom_prevar$VECP_4T
  eurchf_tom_prevar$VECP_4F <- eurchf_tom_prevar$VECP_3F
  eurchf_tom_prevar$VECP_4T <- eurchf_tom_prevar$VECP_3T
  eurchf_tom_prevar$VECP_3F <- eurchf_tom_prevar$VECP_2F
  eurchf_tom_prevar$VECP_3T <- eurchf_tom_prevar$VECP_2T
  eurchf_tom_prevar$VECP_2F <- eurchf_tom_prevar$VECP_1F
  eurchf_tom_prevar$VECP_2T <- eurchf_tom_prevar$VECP_1T
  eurchf_tom_prevar$VECP_1F <- eurchf_tom_prevar$VECPF
  eurchf_tom_prevar$VECP_1T <- eurchf_tom_prevar$VECPT
  eurchf_tom_prevar$VECPF <- "-"
  eurchf_tom_prevar$VECPT <- "-"
  eurchf_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(eurchf_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(eurchf_possible_VECPF_tomorrow)){
      eurchf_tom_prevar$VECPF <- eurchf_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- EURCHF Tomorrow Property Variables if VECPF = ",eurchf_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(eurchf_tom_prevar)
      writeLines("")
      eurchf_tom_rules <- find_match_rules(eurchf_rules, eurchf_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",eurchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(eurchf_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",eurchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(eurchf_tom_rules[[5]])
      writeLines("")
      eurchf_rules_list <- c(eurchf_rules_list, list(eurchf_tom_rules))
      names(eurchf_rules_list)[length(eurchf_rules_list)] <- paste0("EURCHF_TOM_VECPF_", eurchf_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  eurchf_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURCHF Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(eurchf_tom_prevar)
  writeLines("")
  eurchf_tom_rules <- find_match_rules(eurchf_rules, eurchf_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(eurchf_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(eurchf_tom_rules[[5]])
  writeLines("")
  eurchf_rules_list <- c(eurchf_rules_list, list(eurchf_tom_rules))
  names(eurchf_rules_list)[length(eurchf_rules_list)] <- "EURCHF_TOM_ORDINARY_RULES"
  #--------------------------------- 12) EURGBP Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 12) EURGBP Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  eurgbp_rules_list <- list()
  eurgbp_current_prevar <- eurgbp_prevar[nrow(eurgbp_prevar),]
  eurgbp_today_rules <- find_match_rules(eurgbp_rules, eurgbp_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURGBP Today Property Variables --------------------------------")
  writeLines("")
  print(eurgbp_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(eurgbp_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(eurgbp_today_rules[[5]])
  writeLines("")
  eurgbp_rules_list <- c(eurgbp_rules_list, list(eurgbp_today_rules))
  names(eurgbp_rules_list)[length(eurgbp_rules_list)] <- "EURGBP_TODAY_VECPF_RULES"
  
  eurgbp_tom_prevar <- eurgbp_prevar[nrow(eurgbp_prevar),]
  eurgbp_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(eurgbp_tom_prevar$WK == "Saturday")) eurgbp_tom_prevar$WK <- "Monday"
  eurgbp_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "EURGBP", 
                                                  eurgbp_daily[nrow(eurgbp_daily),], eurgbp_prevar)
  eurgbp_tom_prevar$FIB <- eurgbp_fib_cpr_rng[1]
  eurgbp_tom_prevar$RNG <- eurgbp_fib_cpr_rng[3]
  eurgbp_tom_prevar$CPR <- eurgbp_fib_cpr_rng[2]
  eurgbp_tom_prevar$VECP_6F <- eurgbp_tom_prevar$VECP_5F
  eurgbp_tom_prevar$VECP_6T <- eurgbp_tom_prevar$VECP_5T
  eurgbp_tom_prevar$VECP_5F <- eurgbp_tom_prevar$VECP_4F
  eurgbp_tom_prevar$VECP_5T <- eurgbp_tom_prevar$VECP_4T
  eurgbp_tom_prevar$VECP_4F <- eurgbp_tom_prevar$VECP_3F
  eurgbp_tom_prevar$VECP_4T <- eurgbp_tom_prevar$VECP_3T
  eurgbp_tom_prevar$VECP_3F <- eurgbp_tom_prevar$VECP_2F
  eurgbp_tom_prevar$VECP_3T <- eurgbp_tom_prevar$VECP_2T
  eurgbp_tom_prevar$VECP_2F <- eurgbp_tom_prevar$VECP_1F
  eurgbp_tom_prevar$VECP_2T <- eurgbp_tom_prevar$VECP_1T
  eurgbp_tom_prevar$VECP_1F <- eurgbp_tom_prevar$VECPF
  eurgbp_tom_prevar$VECP_1T <- eurgbp_tom_prevar$VECPT
  eurgbp_tom_prevar$VECPF <- "-"
  eurgbp_tom_prevar$VECPT <- "-"
  eurgbp_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(eurgbp_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(eurgbp_possible_VECPF_tomorrow)){
      eurgbp_tom_prevar$VECPF <- eurgbp_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- EURGBP Tomorrow Property Variables if VECPF = ",eurgbp_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(eurgbp_tom_prevar)
      writeLines("")
      eurgbp_tom_rules <- find_match_rules(eurgbp_rules, eurgbp_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",eurgbp_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(eurgbp_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",eurgbp_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(eurgbp_tom_rules[[5]])
      writeLines("")
      eurgbp_rules_list <- c(eurgbp_rules_list, list(eurgbp_tom_rules))
      names(eurgbp_rules_list)[length(eurgbp_rules_list)] <- paste0("EURGBP_TOM_VECPF_", eurgbp_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  eurgbp_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURGBP Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(eurgbp_tom_prevar)
  writeLines("")
  eurgbp_tom_rules <- find_match_rules(eurgbp_rules, eurgbp_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(eurgbp_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(eurgbp_tom_rules[[5]])
  writeLines("")
  eurgbp_rules_list <- c(eurgbp_rules_list, list(eurgbp_tom_rules))
  names(eurgbp_rules_list)[length(eurgbp_rules_list)] <- "EURGBP_TOM_ORDINARY_RULES"
  #--------------------------------- 13) EURJPY Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 13) EURJPY Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  eurjpy_rules_list <- list()
  eurjpy_current_prevar <- eurjpy_prevar[nrow(eurjpy_prevar),]
  eurjpy_today_rules <- find_match_rules(eurjpy_rules, eurjpy_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURJPY Today Property Variables --------------------------------")
  writeLines("")
  print(eurjpy_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(eurjpy_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(eurjpy_today_rules[[5]])
  writeLines("")
  eurjpy_rules_list <- c(eurjpy_rules_list, list(eurjpy_today_rules))
  names(eurjpy_rules_list)[length(eurjpy_rules_list)] <- "EURJPY_TODAY_VECPF_RULES"
  
  eurjpy_tom_prevar <- eurjpy_prevar[nrow(eurjpy_prevar),]
  eurjpy_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(eurjpy_tom_prevar$WK == "Saturday")) eurjpy_tom_prevar$WK <- "Monday"
  eurjpy_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "EURJPY", 
                                                  eurjpy_daily[nrow(eurjpy_daily),], eurjpy_prevar)
  eurjpy_tom_prevar$FIB <- eurjpy_fib_cpr_rng[1]
  eurjpy_tom_prevar$RNG <- eurjpy_fib_cpr_rng[3]
  eurjpy_tom_prevar$CPR <- eurjpy_fib_cpr_rng[2]
  eurjpy_tom_prevar$VECP_6F <- eurjpy_tom_prevar$VECP_5F
  eurjpy_tom_prevar$VECP_6T <- eurjpy_tom_prevar$VECP_5T
  eurjpy_tom_prevar$VECP_5F <- eurjpy_tom_prevar$VECP_4F
  eurjpy_tom_prevar$VECP_5T <- eurjpy_tom_prevar$VECP_4T
  eurjpy_tom_prevar$VECP_4F <- eurjpy_tom_prevar$VECP_3F
  eurjpy_tom_prevar$VECP_4T <- eurjpy_tom_prevar$VECP_3T
  eurjpy_tom_prevar$VECP_3F <- eurjpy_tom_prevar$VECP_2F
  eurjpy_tom_prevar$VECP_3T <- eurjpy_tom_prevar$VECP_2T
  eurjpy_tom_prevar$VECP_2F <- eurjpy_tom_prevar$VECP_1F
  eurjpy_tom_prevar$VECP_2T <- eurjpy_tom_prevar$VECP_1T
  eurjpy_tom_prevar$VECP_1F <- eurjpy_tom_prevar$VECPF
  eurjpy_tom_prevar$VECP_1T <- eurjpy_tom_prevar$VECPT
  eurjpy_tom_prevar$VECPF <- "-"
  eurjpy_tom_prevar$VECPT <- "-"
  eurjpy_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(eurjpy_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(eurjpy_possible_VECPF_tomorrow)){
      eurjpy_tom_prevar$VECPF <- eurjpy_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- EURJPY Tomorrow Property Variables if VECPF = ",eurjpy_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(eurjpy_tom_prevar)
      writeLines("")
      eurjpy_tom_rules <- find_match_rules(eurjpy_rules, eurjpy_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",eurjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(eurjpy_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",eurjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(eurjpy_tom_rules[[5]])
      writeLines("")
      eurjpy_rules_list <- c(eurjpy_rules_list, list(eurjpy_tom_rules))
      names(eurjpy_rules_list)[length(eurjpy_rules_list)] <- paste0("EURJPY_TOM_VECPF_", eurjpy_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  eurjpy_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURJPY Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(eurjpy_tom_prevar)
  writeLines("")
  eurjpy_tom_rules <- find_match_rules(eurjpy_rules, eurjpy_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(eurjpy_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(eurjpy_tom_rules[[5]])
  writeLines("")
  eurjpy_rules_list <- c(eurjpy_rules_list, list(eurjpy_tom_rules))
  names(eurjpy_rules_list)[length(eurjpy_rules_list)] <- "EURJPY_TOM_ORDINARY_RULES"
  #--------------------------------- 14) EURNZD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 14) EURNZD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  eurnzd_rules_list <- list()
  eurnzd_current_prevar <- eurnzd_prevar[nrow(eurnzd_prevar),]
  eurnzd_today_rules <- find_match_rules(eurnzd_rules, eurnzd_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURNZD Today Property Variables --------------------------------")
  writeLines("")
  print(eurnzd_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(eurnzd_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(eurnzd_today_rules[[5]])
  writeLines("")
  eurnzd_rules_list <- c(eurnzd_rules_list, list(eurnzd_today_rules))
  names(eurnzd_rules_list)[length(eurnzd_rules_list)] <- "EURNZD_TODAY_VECPF_RULES"
  
  eurnzd_tom_prevar <- eurnzd_prevar[nrow(eurnzd_prevar),]
  eurnzd_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(eurnzd_tom_prevar$WK == "Saturday")) eurnzd_tom_prevar$WK <- "Monday"
  eurnzd_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "EURNZD", 
                                                  eurnzd_daily[nrow(eurnzd_daily),], eurnzd_prevar)
  eurnzd_tom_prevar$FIB <- eurnzd_fib_cpr_rng[1]
  eurnzd_tom_prevar$RNG <- eurnzd_fib_cpr_rng[3]
  eurnzd_tom_prevar$CPR <- eurnzd_fib_cpr_rng[2]
  eurnzd_tom_prevar$VECP_6F <- eurnzd_tom_prevar$VECP_5F
  eurnzd_tom_prevar$VECP_6T <- eurnzd_tom_prevar$VECP_5T
  eurnzd_tom_prevar$VECP_5F <- eurnzd_tom_prevar$VECP_4F
  eurnzd_tom_prevar$VECP_5T <- eurnzd_tom_prevar$VECP_4T
  eurnzd_tom_prevar$VECP_4F <- eurnzd_tom_prevar$VECP_3F
  eurnzd_tom_prevar$VECP_4T <- eurnzd_tom_prevar$VECP_3T
  eurnzd_tom_prevar$VECP_3F <- eurnzd_tom_prevar$VECP_2F
  eurnzd_tom_prevar$VECP_3T <- eurnzd_tom_prevar$VECP_2T
  eurnzd_tom_prevar$VECP_2F <- eurnzd_tom_prevar$VECP_1F
  eurnzd_tom_prevar$VECP_2T <- eurnzd_tom_prevar$VECP_1T
  eurnzd_tom_prevar$VECP_1F <- eurnzd_tom_prevar$VECPF
  eurnzd_tom_prevar$VECP_1T <- eurnzd_tom_prevar$VECPT
  eurnzd_tom_prevar$VECPF <- "-"
  eurnzd_tom_prevar$VECPT <- "-"
  eurnzd_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(eurnzd_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(eurnzd_possible_VECPF_tomorrow)){
      eurnzd_tom_prevar$VECPF <- eurnzd_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- EURNZD Tomorrow Property Variables if VECPF = ",eurnzd_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(eurnzd_tom_prevar)
      writeLines("")
      eurnzd_tom_rules <- find_match_rules(eurnzd_rules, eurnzd_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",eurnzd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(eurnzd_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",eurnzd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(eurnzd_tom_rules[[5]])
      writeLines("")
      eurnzd_rules_list <- c(eurnzd_rules_list, list(eurnzd_tom_rules))
      names(eurnzd_rules_list)[length(eurnzd_rules_list)] <- paste0("EURNZD_TOM_VECPF_", eurnzd_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  eurnzd_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURNZD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(eurnzd_tom_prevar)
  writeLines("")
  eurnzd_tom_rules <- find_match_rules(eurnzd_rules, eurnzd_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(eurnzd_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(eurnzd_tom_rules[[5]])
  writeLines("")
  eurnzd_rules_list <- c(eurnzd_rules_list, list(eurnzd_tom_rules))
  names(eurnzd_rules_list)[length(eurnzd_rules_list)] <- "EURNZD_TOM_ORDINARY_RULES"
  #--------------------------------- 15) EURUSD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 15) EURUSD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  eurusd_rules_list <- list()
  eurusd_current_prevar <- eurusd_prevar[nrow(eurusd_prevar),]
  eurusd_today_rules <- find_match_rules(eurusd_rules, eurusd_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURUSD Today Property Variables --------------------------------")
  writeLines("")
  print(eurusd_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(eurusd_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(eurusd_today_rules[[5]])
  writeLines("")
  eurusd_rules_list <- c(eurusd_rules_list, list(eurusd_today_rules))
  names(eurusd_rules_list)[length(eurusd_rules_list)] <- "EURUSD_TODAY_VECPF_RULES"
  
  eurusd_tom_prevar <- eurusd_prevar[nrow(eurusd_prevar),]
  eurusd_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(eurusd_tom_prevar$WK == "Saturday")) eurusd_tom_prevar$WK <- "Monday"
  eurusd_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "EURUSD", 
                                                  eurusd_daily[nrow(eurusd_daily),], eurusd_prevar)
  eurusd_tom_prevar$FIB <- eurusd_fib_cpr_rng[1]
  eurusd_tom_prevar$RNG <- eurusd_fib_cpr_rng[3]
  eurusd_tom_prevar$CPR <- eurusd_fib_cpr_rng[2]
  eurusd_tom_prevar$VECP_6F <- eurusd_tom_prevar$VECP_5F
  eurusd_tom_prevar$VECP_6T <- eurusd_tom_prevar$VECP_5T
  eurusd_tom_prevar$VECP_5F <- eurusd_tom_prevar$VECP_4F
  eurusd_tom_prevar$VECP_5T <- eurusd_tom_prevar$VECP_4T
  eurusd_tom_prevar$VECP_4F <- eurusd_tom_prevar$VECP_3F
  eurusd_tom_prevar$VECP_4T <- eurusd_tom_prevar$VECP_3T
  eurusd_tom_prevar$VECP_3F <- eurusd_tom_prevar$VECP_2F
  eurusd_tom_prevar$VECP_3T <- eurusd_tom_prevar$VECP_2T
  eurusd_tom_prevar$VECP_2F <- eurusd_tom_prevar$VECP_1F
  eurusd_tom_prevar$VECP_2T <- eurusd_tom_prevar$VECP_1T
  eurusd_tom_prevar$VECP_1F <- eurusd_tom_prevar$VECPF
  eurusd_tom_prevar$VECP_1T <- eurusd_tom_prevar$VECPT
  eurusd_tom_prevar$VECPF <- "-"
  eurusd_tom_prevar$VECPT <- "-"
  eurusd_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(eurusd_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(eurusd_possible_VECPF_tomorrow)){
      eurusd_tom_prevar$VECPF <- eurusd_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- EURUSD Tomorrow Property Variables if VECPF = ",eurusd_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(eurusd_tom_prevar)
      writeLines("")
      eurusd_tom_rules <- find_match_rules(eurusd_rules, eurusd_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",eurusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(eurusd_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",eurusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(eurusd_tom_rules[[5]])
      writeLines("")
      eurusd_rules_list <- c(eurusd_rules_list, list(eurusd_tom_rules))
      names(eurusd_rules_list)[length(eurusd_rules_list)] <- paste0("EURUSD_TOM_VECPF_", eurusd_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  eurusd_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- EURUSD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(eurusd_tom_prevar)
  writeLines("")
  eurusd_tom_rules <- find_match_rules(eurusd_rules, eurusd_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(eurusd_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(eurusd_tom_rules[[5]])
  writeLines("")
  eurusd_rules_list <- c(eurusd_rules_list, list(eurusd_tom_rules))
  names(eurusd_rules_list)[length(eurusd_rules_list)] <- "EURUSD_TOM_ORDINARY_RULES"
  #--------------------------------- 16) GBPCHF Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 16) GBPCHF Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  gbpchf_rules_list <- list()
  gbpchf_current_prevar <- gbpchf_prevar[nrow(gbpchf_prevar),]
  gbpchf_today_rules <- find_match_rules(gbpchf_rules, gbpchf_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- GBPCHF Today Property Variables --------------------------------")
  writeLines("")
  print(gbpchf_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(gbpchf_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(gbpchf_today_rules[[5]])
  writeLines("")
  gbpchf_rules_list <- c(gbpchf_rules_list, list(gbpchf_today_rules))
  names(gbpchf_rules_list)[length(gbpchf_rules_list)] <- "GBPCHF_TODAY_VECPF_RULES"
  
  gbpchf_tom_prevar <- gbpchf_prevar[nrow(gbpchf_prevar),]
  gbpchf_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(gbpchf_tom_prevar$WK == "Saturday")) gbpchf_tom_prevar$WK <- "Monday"
  gbpchf_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "GBPCHF", 
                                                  gbpchf_daily[nrow(gbpchf_daily),], gbpchf_prevar)
  gbpchf_tom_prevar$FIB <- gbpchf_fib_cpr_rng[1]
  gbpchf_tom_prevar$RNG <- gbpchf_fib_cpr_rng[3]
  gbpchf_tom_prevar$CPR <- gbpchf_fib_cpr_rng[2]
  gbpchf_tom_prevar$VECP_6F <- gbpchf_tom_prevar$VECP_5F
  gbpchf_tom_prevar$VECP_6T <- gbpchf_tom_prevar$VECP_5T
  gbpchf_tom_prevar$VECP_5F <- gbpchf_tom_prevar$VECP_4F
  gbpchf_tom_prevar$VECP_5T <- gbpchf_tom_prevar$VECP_4T
  gbpchf_tom_prevar$VECP_4F <- gbpchf_tom_prevar$VECP_3F
  gbpchf_tom_prevar$VECP_4T <- gbpchf_tom_prevar$VECP_3T
  gbpchf_tom_prevar$VECP_3F <- gbpchf_tom_prevar$VECP_2F
  gbpchf_tom_prevar$VECP_3T <- gbpchf_tom_prevar$VECP_2T
  gbpchf_tom_prevar$VECP_2F <- gbpchf_tom_prevar$VECP_1F
  gbpchf_tom_prevar$VECP_2T <- gbpchf_tom_prevar$VECP_1T
  gbpchf_tom_prevar$VECP_1F <- gbpchf_tom_prevar$VECPF
  gbpchf_tom_prevar$VECP_1T <- gbpchf_tom_prevar$VECPT
  gbpchf_tom_prevar$VECPF <- "-"
  gbpchf_tom_prevar$VECPT <- "-"
  gbpchf_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(gbpchf_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(gbpchf_possible_VECPF_tomorrow)){
      gbpchf_tom_prevar$VECPF <- gbpchf_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- GBPCHF Tomorrow Property Variables if VECPF = ",gbpchf_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(gbpchf_tom_prevar)
      writeLines("")
      gbpchf_tom_rules <- find_match_rules(gbpchf_rules, gbpchf_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",gbpchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(gbpchf_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",gbpchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(gbpchf_tom_rules[[5]])
      writeLines("")
      gbpchf_rules_list <- c(gbpchf_rules_list, list(gbpchf_tom_rules))
      names(gbpchf_rules_list)[length(gbpchf_rules_list)] <- paste0("GBPCHF_TOM_VECPF_", gbpchf_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  gbpchf_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- GBPCHF Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(gbpchf_tom_prevar)
  writeLines("")
  gbpchf_tom_rules <- find_match_rules(gbpchf_rules, gbpchf_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(gbpchf_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(gbpchf_tom_rules[[5]])
  writeLines("")
  gbpchf_rules_list <- c(gbpchf_rules_list, list(gbpchf_tom_rules))
  names(gbpchf_rules_list)[length(gbpchf_rules_list)] <- "GBPCHF_TOM_ORDINARY_RULES"
  #--------------------------------- 17) GBPJPY Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 17) GBPJPY Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  gbpjpy_rules_list <- list()
  gbpjpy_current_prevar <- gbpjpy_prevar[nrow(gbpjpy_prevar),]
  gbpjpy_today_rules <- find_match_rules(gbpjpy_rules, gbpjpy_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- GBPJPY Today Property Variables --------------------------------")
  writeLines("")
  print(gbpjpy_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(gbpjpy_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(gbpjpy_today_rules[[5]])
  writeLines("")
  gbpjpy_rules_list <- c(gbpjpy_rules_list, list(gbpjpy_today_rules))
  names(gbpjpy_rules_list)[length(gbpjpy_rules_list)] <- "GBPJPY_TODAY_VECPF_RULES"
  
  gbpjpy_tom_prevar <- gbpjpy_prevar[nrow(gbpjpy_prevar),]
  gbpjpy_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(gbpjpy_tom_prevar$WK == "Saturday")) gbpjpy_tom_prevar$WK <- "Monday"
  gbpjpy_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "GBPJPY", 
                                                  gbpjpy_daily[nrow(gbpjpy_daily),], gbpjpy_prevar)
  gbpjpy_tom_prevar$FIB <- gbpjpy_fib_cpr_rng[1]
  gbpjpy_tom_prevar$RNG <- gbpjpy_fib_cpr_rng[3]
  gbpjpy_tom_prevar$CPR <- gbpjpy_fib_cpr_rng[2]
  gbpjpy_tom_prevar$VECP_6F <- gbpjpy_tom_prevar$VECP_5F
  gbpjpy_tom_prevar$VECP_6T <- gbpjpy_tom_prevar$VECP_5T
  gbpjpy_tom_prevar$VECP_5F <- gbpjpy_tom_prevar$VECP_4F
  gbpjpy_tom_prevar$VECP_5T <- gbpjpy_tom_prevar$VECP_4T
  gbpjpy_tom_prevar$VECP_4F <- gbpjpy_tom_prevar$VECP_3F
  gbpjpy_tom_prevar$VECP_4T <- gbpjpy_tom_prevar$VECP_3T
  gbpjpy_tom_prevar$VECP_3F <- gbpjpy_tom_prevar$VECP_2F
  gbpjpy_tom_prevar$VECP_3T <- gbpjpy_tom_prevar$VECP_2T
  gbpjpy_tom_prevar$VECP_2F <- gbpjpy_tom_prevar$VECP_1F
  gbpjpy_tom_prevar$VECP_2T <- gbpjpy_tom_prevar$VECP_1T
  gbpjpy_tom_prevar$VECP_1F <- gbpjpy_tom_prevar$VECPF
  gbpjpy_tom_prevar$VECP_1T <- gbpjpy_tom_prevar$VECPT
  gbpjpy_tom_prevar$VECPF <- "-"
  gbpjpy_tom_prevar$VECPT <- "-"
  gbpjpy_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(gbpjpy_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(gbpjpy_possible_VECPF_tomorrow)){
      gbpjpy_tom_prevar$VECPF <- gbpjpy_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- GBPJPY Tomorrow Property Variables if VECPF = ",gbpjpy_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(gbpjpy_tom_prevar)
      writeLines("")
      gbpjpy_tom_rules <- find_match_rules(gbpjpy_rules, gbpjpy_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",gbpjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(gbpjpy_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",gbpjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(gbpjpy_tom_rules[[5]])
      writeLines("")
      gbpjpy_rules_list <- c(gbpjpy_rules_list, list(gbpjpy_tom_rules))
      names(gbpjpy_rules_list)[length(gbpjpy_rules_list)] <- paste0("GBPJPY_TOM_VECPF_", gbpjpy_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  gbpjpy_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- GBPJPY Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(gbpjpy_tom_prevar)
  writeLines("")
  gbpjpy_tom_rules <- find_match_rules(gbpjpy_rules, gbpjpy_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(gbpjpy_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(gbpjpy_tom_rules[[5]])
  writeLines("")
  gbpjpy_rules_list <- c(gbpjpy_rules_list, list(gbpjpy_tom_rules))
  names(gbpjpy_rules_list)[length(gbpjpy_rules_list)] <- "GBPJPY_TOM_ORDINARY_RULES"
  #--------------------------------- 18) GBPUSD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 18) GBPUSD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  gbpusd_rules_list <- list()
  gbpusd_current_prevar <- gbpusd_prevar[nrow(gbpusd_prevar),]
  gbpusd_today_rules <- find_match_rules(gbpusd_rules, gbpusd_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- GBPUSD Today Property Variables --------------------------------")
  writeLines("")
  print(gbpusd_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(gbpusd_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(gbpusd_today_rules[[5]])
  writeLines("")
  gbpusd_rules_list <- c(gbpusd_rules_list, list(gbpusd_today_rules))
  names(gbpusd_rules_list)[length(gbpusd_rules_list)] <- "GBPUSD_TODAY_VECPF_RULES"
  
  gbpusd_tom_prevar <- gbpusd_prevar[nrow(gbpusd_prevar),]
  gbpusd_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(gbpusd_tom_prevar$WK == "Saturday")) gbpusd_tom_prevar$WK <- "Monday"
  gbpusd_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "GBPUSD", 
                                                  gbpusd_daily[nrow(gbpusd_daily),], gbpusd_prevar)
  gbpusd_tom_prevar$FIB <- gbpusd_fib_cpr_rng[1]
  gbpusd_tom_prevar$RNG <- gbpusd_fib_cpr_rng[3]
  gbpusd_tom_prevar$CPR <- gbpusd_fib_cpr_rng[2]
  gbpusd_tom_prevar$VECP_6F <- gbpusd_tom_prevar$VECP_5F
  gbpusd_tom_prevar$VECP_6T <- gbpusd_tom_prevar$VECP_5T
  gbpusd_tom_prevar$VECP_5F <- gbpusd_tom_prevar$VECP_4F
  gbpusd_tom_prevar$VECP_5T <- gbpusd_tom_prevar$VECP_4T
  gbpusd_tom_prevar$VECP_4F <- gbpusd_tom_prevar$VECP_3F
  gbpusd_tom_prevar$VECP_4T <- gbpusd_tom_prevar$VECP_3T
  gbpusd_tom_prevar$VECP_3F <- gbpusd_tom_prevar$VECP_2F
  gbpusd_tom_prevar$VECP_3T <- gbpusd_tom_prevar$VECP_2T
  gbpusd_tom_prevar$VECP_2F <- gbpusd_tom_prevar$VECP_1F
  gbpusd_tom_prevar$VECP_2T <- gbpusd_tom_prevar$VECP_1T
  gbpusd_tom_prevar$VECP_1F <- gbpusd_tom_prevar$VECPF
  gbpusd_tom_prevar$VECP_1T <- gbpusd_tom_prevar$VECPT
  gbpusd_tom_prevar$VECPF <- "-"
  gbpusd_tom_prevar$VECPT <- "-"
  gbpusd_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(gbpusd_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(gbpusd_possible_VECPF_tomorrow)){
      gbpusd_tom_prevar$VECPF <- gbpusd_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- GBPUSD Tomorrow Property Variables if VECPF = ",gbpusd_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(gbpusd_tom_prevar)
      writeLines("")
      gbpusd_tom_rules <- find_match_rules(gbpusd_rules, gbpusd_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",gbpusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(gbpusd_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",gbpusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(gbpusd_tom_rules[[5]])
      writeLines("")
      gbpusd_rules_list <- c(gbpusd_rules_list, list(gbpusd_tom_rules))
      names(gbpusd_rules_list)[length(gbpusd_rules_list)] <- paste0("GBPUSD_TOM_VECPF_", gbpusd_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  gbpusd_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- GBPUSD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(gbpusd_tom_prevar)
  writeLines("")
  gbpusd_tom_rules <- find_match_rules(gbpusd_rules, gbpusd_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(gbpusd_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(gbpusd_tom_rules[[5]])
  writeLines("")
  gbpusd_rules_list <- c(gbpusd_rules_list, list(gbpusd_tom_rules))
  names(gbpusd_rules_list)[length(gbpusd_rules_list)] <- "GBPUSD_TOM_ORDINARY_RULES"
  #--------------------------------- 19) NZDJPY Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 19) NZDJPY Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  nzdjpy_rules_list <- list()
  nzdjpy_current_prevar <- nzdjpy_prevar[nrow(nzdjpy_prevar),]
  nzdjpy_today_rules <- find_match_rules(nzdjpy_rules, nzdjpy_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- NZDJPY Today Property Variables --------------------------------")
  writeLines("")
  print(nzdjpy_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(nzdjpy_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(nzdjpy_today_rules[[5]])
  writeLines("")
  nzdjpy_rules_list <- c(nzdjpy_rules_list, list(nzdjpy_today_rules))
  names(nzdjpy_rules_list)[length(nzdjpy_rules_list)] <- "NZDJPY_TODAY_VECPF_RULES"
  
  nzdjpy_tom_prevar <- nzdjpy_prevar[nrow(nzdjpy_prevar),]
  nzdjpy_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(nzdjpy_tom_prevar$WK == "Saturday")) nzdjpy_tom_prevar$WK <- "Monday"
  nzdjpy_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "NZDJPY", 
                                                  nzdjpy_daily[nrow(nzdjpy_daily),], nzdjpy_prevar)
  nzdjpy_tom_prevar$FIB <- nzdjpy_fib_cpr_rng[1]
  nzdjpy_tom_prevar$RNG <- nzdjpy_fib_cpr_rng[3]
  nzdjpy_tom_prevar$CPR <- nzdjpy_fib_cpr_rng[2]
  nzdjpy_tom_prevar$VECP_6F <- nzdjpy_tom_prevar$VECP_5F
  nzdjpy_tom_prevar$VECP_6T <- nzdjpy_tom_prevar$VECP_5T
  nzdjpy_tom_prevar$VECP_5F <- nzdjpy_tom_prevar$VECP_4F
  nzdjpy_tom_prevar$VECP_5T <- nzdjpy_tom_prevar$VECP_4T
  nzdjpy_tom_prevar$VECP_4F <- nzdjpy_tom_prevar$VECP_3F
  nzdjpy_tom_prevar$VECP_4T <- nzdjpy_tom_prevar$VECP_3T
  nzdjpy_tom_prevar$VECP_3F <- nzdjpy_tom_prevar$VECP_2F
  nzdjpy_tom_prevar$VECP_3T <- nzdjpy_tom_prevar$VECP_2T
  nzdjpy_tom_prevar$VECP_2F <- nzdjpy_tom_prevar$VECP_1F
  nzdjpy_tom_prevar$VECP_2T <- nzdjpy_tom_prevar$VECP_1T
  nzdjpy_tom_prevar$VECP_1F <- nzdjpy_tom_prevar$VECPF
  nzdjpy_tom_prevar$VECP_1T <- nzdjpy_tom_prevar$VECPT
  nzdjpy_tom_prevar$VECPF <- "-"
  nzdjpy_tom_prevar$VECPT <- "-"
  nzdjpy_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(nzdjpy_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(nzdjpy_possible_VECPF_tomorrow)){
      nzdjpy_tom_prevar$VECPF <- nzdjpy_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- NZDJPY Tomorrow Property Variables if VECPF = ",nzdjpy_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(nzdjpy_tom_prevar)
      writeLines("")
      nzdjpy_tom_rules <- find_match_rules(nzdjpy_rules, nzdjpy_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",nzdjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(nzdjpy_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",nzdjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(nzdjpy_tom_rules[[5]])
      writeLines("")
      nzdjpy_rules_list <- c(nzdjpy_rules_list, list(nzdjpy_tom_rules))
      names(nzdjpy_rules_list)[length(nzdjpy_rules_list)] <- paste0("NZDJPY_TOM_VECPF_", nzdjpy_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  nzdjpy_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- NZDJPY Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(nzdjpy_tom_prevar)
  writeLines("")
  nzdjpy_tom_rules <- find_match_rules(nzdjpy_rules, nzdjpy_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(nzdjpy_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(nzdjpy_tom_rules[[5]])
  writeLines("")
  nzdjpy_rules_list <- c(nzdjpy_rules_list, list(nzdjpy_tom_rules))
  names(nzdjpy_rules_list)[length(nzdjpy_rules_list)] <- "NZDJPY_TOM_ORDINARY_RULES"
  #--------------------------------- 20) NZDUSD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 20) NZDUSD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  nzdusd_rules_list <- list()
  nzdusd_current_prevar <- nzdusd_prevar[nrow(nzdusd_prevar),]
  nzdusd_today_rules <- find_match_rules(nzdusd_rules, nzdusd_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- NZDUSD Today Property Variables --------------------------------")
  writeLines("")
  print(nzdusd_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(nzdusd_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(nzdusd_today_rules[[5]])
  writeLines("")
  nzdusd_rules_list <- c(nzdusd_rules_list, list(nzdusd_today_rules))
  names(nzdusd_rules_list)[length(nzdusd_rules_list)] <- "NZDUSD_TODAY_VECPF_RULES"
  
  nzdusd_tom_prevar <- nzdusd_prevar[nrow(nzdusd_prevar),]
  nzdusd_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(nzdusd_tom_prevar$WK == "Saturday")) nzdusd_tom_prevar$WK <- "Monday"
  nzdusd_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "NZDUSD", 
                                                  nzdusd_daily[nrow(nzdusd_daily),], nzdusd_prevar)
  nzdusd_tom_prevar$FIB <- nzdusd_fib_cpr_rng[1]
  nzdusd_tom_prevar$RNG <- nzdusd_fib_cpr_rng[3]
  nzdusd_tom_prevar$CPR <- nzdusd_fib_cpr_rng[2]
  nzdusd_tom_prevar$VECP_6F <- nzdusd_tom_prevar$VECP_5F
  nzdusd_tom_prevar$VECP_6T <- nzdusd_tom_prevar$VECP_5T
  nzdusd_tom_prevar$VECP_5F <- nzdusd_tom_prevar$VECP_4F
  nzdusd_tom_prevar$VECP_5T <- nzdusd_tom_prevar$VECP_4T
  nzdusd_tom_prevar$VECP_4F <- nzdusd_tom_prevar$VECP_3F
  nzdusd_tom_prevar$VECP_4T <- nzdusd_tom_prevar$VECP_3T
  nzdusd_tom_prevar$VECP_3F <- nzdusd_tom_prevar$VECP_2F
  nzdusd_tom_prevar$VECP_3T <- nzdusd_tom_prevar$VECP_2T
  nzdusd_tom_prevar$VECP_2F <- nzdusd_tom_prevar$VECP_1F
  nzdusd_tom_prevar$VECP_2T <- nzdusd_tom_prevar$VECP_1T
  nzdusd_tom_prevar$VECP_1F <- nzdusd_tom_prevar$VECPF
  nzdusd_tom_prevar$VECP_1T <- nzdusd_tom_prevar$VECPT
  nzdusd_tom_prevar$VECPF <- "-"
  nzdusd_tom_prevar$VECPT <- "-"
  nzdusd_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(nzdusd_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(nzdusd_possible_VECPF_tomorrow)){
      nzdusd_tom_prevar$VECPF <- nzdusd_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- NZDUSD Tomorrow Property Variables if VECPF = ",nzdusd_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(nzdusd_tom_prevar)
      writeLines("")
      nzdusd_tom_rules <- find_match_rules(nzdusd_rules, nzdusd_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",nzdusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(nzdusd_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",nzdusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(nzdusd_tom_rules[[5]])
      writeLines("")
      nzdusd_rules_list <- c(nzdusd_rules_list, list(nzdusd_tom_rules))
      names(nzdusd_rules_list)[length(nzdusd_rules_list)] <- paste0("NZDUSD_TOM_VECPF_", nzdusd_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  nzdusd_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- NZDUSD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(nzdusd_tom_prevar)
  writeLines("")
  nzdusd_tom_rules <- find_match_rules(nzdusd_rules, nzdusd_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(nzdusd_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(nzdusd_tom_rules[[5]])
  writeLines("")
  nzdusd_rules_list <- c(nzdusd_rules_list, list(nzdusd_tom_rules))
  names(nzdusd_rules_list)[length(nzdusd_rules_list)] <- "NZDUSD_TOM_ORDINARY_RULES"
  #--------------------------------- 21) USDCAD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 21) USDCAD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  usdcad_rules_list <- list()
  usdcad_current_prevar <- usdcad_prevar[nrow(usdcad_prevar),]
  usdcad_today_rules <- find_match_rules(usdcad_rules, usdcad_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- USDCAD Today Property Variables --------------------------------")
  writeLines("")
  print(usdcad_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(usdcad_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(usdcad_today_rules[[5]])
  writeLines("")
  usdcad_rules_list <- c(usdcad_rules_list, list(usdcad_today_rules))
  names(usdcad_rules_list)[length(usdcad_rules_list)] <- "USDCAD_TODAY_VECPF_RULES"
  
  usdcad_tom_prevar <- usdcad_prevar[nrow(usdcad_prevar),]
  usdcad_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(usdcad_tom_prevar$WK == "Saturday")) usdcad_tom_prevar$WK <- "Monday"
  usdcad_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "USDCAD", 
                                                  usdcad_daily[nrow(usdcad_daily),], usdcad_prevar)
  usdcad_tom_prevar$FIB <- usdcad_fib_cpr_rng[1]
  usdcad_tom_prevar$RNG <- usdcad_fib_cpr_rng[3]
  usdcad_tom_prevar$CPR <- usdcad_fib_cpr_rng[2]
  usdcad_tom_prevar$VECP_6F <- usdcad_tom_prevar$VECP_5F
  usdcad_tom_prevar$VECP_6T <- usdcad_tom_prevar$VECP_5T
  usdcad_tom_prevar$VECP_5F <- usdcad_tom_prevar$VECP_4F
  usdcad_tom_prevar$VECP_5T <- usdcad_tom_prevar$VECP_4T
  usdcad_tom_prevar$VECP_4F <- usdcad_tom_prevar$VECP_3F
  usdcad_tom_prevar$VECP_4T <- usdcad_tom_prevar$VECP_3T
  usdcad_tom_prevar$VECP_3F <- usdcad_tom_prevar$VECP_2F
  usdcad_tom_prevar$VECP_3T <- usdcad_tom_prevar$VECP_2T
  usdcad_tom_prevar$VECP_2F <- usdcad_tom_prevar$VECP_1F
  usdcad_tom_prevar$VECP_2T <- usdcad_tom_prevar$VECP_1T
  usdcad_tom_prevar$VECP_1F <- usdcad_tom_prevar$VECPF
  usdcad_tom_prevar$VECP_1T <- usdcad_tom_prevar$VECPT
  usdcad_tom_prevar$VECPF <- "-"
  usdcad_tom_prevar$VECPT <- "-"
  usdcad_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(usdcad_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(usdcad_possible_VECPF_tomorrow)){
      usdcad_tom_prevar$VECPF <- usdcad_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- USDCAD Tomorrow Property Variables if VECPF = ",usdcad_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(usdcad_tom_prevar)
      writeLines("")
      usdcad_tom_rules <- find_match_rules(usdcad_rules, usdcad_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",usdcad_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(usdcad_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",usdcad_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(usdcad_tom_rules[[5]])
      writeLines("")
      usdcad_rules_list <- c(usdcad_rules_list, list(usdcad_tom_rules))
      names(usdcad_rules_list)[length(usdcad_rules_list)] <- paste0("USDCAD_TOM_VECPF_", usdcad_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  usdcad_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- USDCAD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(usdcad_tom_prevar)
  writeLines("")
  usdcad_tom_rules <- find_match_rules(usdcad_rules, usdcad_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(usdcad_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(usdcad_tom_rules[[5]])
  writeLines("")
  usdcad_rules_list <- c(usdcad_rules_list, list(usdcad_tom_rules))
  names(usdcad_rules_list)[length(usdcad_rules_list)] <- "USDCAD_TOM_ORDINARY_RULES"
  #--------------------------------- 22) USDCHF Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 22) USDCHF Rules Matching -----------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  usdchf_rules_list <- list()
  usdchf_current_prevar <- usdchf_prevar[nrow(usdchf_prevar),]
  usdchf_today_rules <- find_match_rules(usdchf_rules, usdchf_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- USDCHF Today Property Variables --------------------------------")
  writeLines("")
  print(usdchf_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(usdchf_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(usdchf_today_rules[[5]])
  writeLines("")
  usdchf_rules_list <- c(usdchf_rules_list, list(usdchf_today_rules))
  names(usdchf_rules_list)[length(usdchf_rules_list)] <- "USDCHF_TODAY_VECPF_RULES"
  
  usdchf_tom_prevar <- usdchf_prevar[nrow(usdchf_prevar),]
  usdchf_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(usdchf_tom_prevar$WK == "Saturday")) usdchf_tom_prevar$WK <- "Monday"
  usdchf_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "USDCHF", 
                                                  usdchf_daily[nrow(usdchf_daily),], usdchf_prevar)
  usdchf_tom_prevar$FIB <- usdchf_fib_cpr_rng[1]
  usdchf_tom_prevar$RNG <- usdchf_fib_cpr_rng[3]
  usdchf_tom_prevar$CPR <- usdchf_fib_cpr_rng[2]
  usdchf_tom_prevar$VECP_6F <- usdchf_tom_prevar$VECP_5F
  usdchf_tom_prevar$VECP_6T <- usdchf_tom_prevar$VECP_5T
  usdchf_tom_prevar$VECP_5F <- usdchf_tom_prevar$VECP_4F
  usdchf_tom_prevar$VECP_5T <- usdchf_tom_prevar$VECP_4T
  usdchf_tom_prevar$VECP_4F <- usdchf_tom_prevar$VECP_3F
  usdchf_tom_prevar$VECP_4T <- usdchf_tom_prevar$VECP_3T
  usdchf_tom_prevar$VECP_3F <- usdchf_tom_prevar$VECP_2F
  usdchf_tom_prevar$VECP_3T <- usdchf_tom_prevar$VECP_2T
  usdchf_tom_prevar$VECP_2F <- usdchf_tom_prevar$VECP_1F
  usdchf_tom_prevar$VECP_2T <- usdchf_tom_prevar$VECP_1T
  usdchf_tom_prevar$VECP_1F <- usdchf_tom_prevar$VECPF
  usdchf_tom_prevar$VECP_1T <- usdchf_tom_prevar$VECPT
  usdchf_tom_prevar$VECPF <- "-"
  usdchf_tom_prevar$VECPT <- "-"
  usdchf_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(usdchf_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(usdchf_possible_VECPF_tomorrow)){
      usdchf_tom_prevar$VECPF <- usdchf_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- USDCHF Tomorrow Property Variables if VECPF = ",usdchf_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(usdchf_tom_prevar)
      writeLines("")
      usdchf_tom_rules <- find_match_rules(usdchf_rules, usdchf_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",usdchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(usdchf_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",usdchf_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(usdchf_tom_rules[[5]])
      writeLines("")
      usdchf_rules_list <- c(usdchf_rules_list, list(usdchf_tom_rules))
      names(usdchf_rules_list)[length(usdchf_rules_list)] <- paste0("USDCHF_TOM_VECPF_", usdchf_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  usdchf_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- USDCHF Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(usdchf_tom_prevar)
  writeLines("")
  usdchf_tom_rules <- find_match_rules(usdchf_rules, usdchf_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(usdchf_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(usdchf_tom_rules[[5]])
  writeLines("")
  usdchf_rules_list <- c(usdchf_rules_list, list(usdchf_tom_rules))
  names(usdchf_rules_list)[length(usdchf_rules_list)] <- "USDCHF_TOM_ORDINARY_RULES"
  #--------------------------------- 23) USDJPY Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 23) USDJPY Rules Matching -----------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  usdjpy_rules_list <- list()
  usdjpy_current_prevar <- usdjpy_prevar[nrow(usdjpy_prevar),]
  usdjpy_today_rules <- find_match_rules(usdjpy_rules, usdjpy_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- USDJPY Today Property Variables --------------------------------")
  writeLines("")
  print(usdjpy_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(usdjpy_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(usdjpy_today_rules[[5]])
  writeLines("")
  usdjpy_rules_list <- c(usdjpy_rules_list, list(usdjpy_today_rules))
  names(usdjpy_rules_list)[length(usdjpy_rules_list)] <- "USDJPY_TODAY_VECPF_RULES"
  
  usdjpy_tom_prevar <- usdjpy_prevar[nrow(usdjpy_prevar),]
  usdjpy_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(usdjpy_tom_prevar$WK == "Saturday")) usdjpy_tom_prevar$WK <- "Monday"
  usdjpy_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "USDJPY", 
                                                  usdjpy_daily[nrow(usdjpy_daily),], usdjpy_prevar)
  usdjpy_tom_prevar$FIB <- usdjpy_fib_cpr_rng[1]
  usdjpy_tom_prevar$RNG <- usdjpy_fib_cpr_rng[3]
  usdjpy_tom_prevar$CPR <- usdjpy_fib_cpr_rng[2]
  usdjpy_tom_prevar$VECP_6F <- usdjpy_tom_prevar$VECP_5F
  usdjpy_tom_prevar$VECP_6T <- usdjpy_tom_prevar$VECP_5T
  usdjpy_tom_prevar$VECP_5F <- usdjpy_tom_prevar$VECP_4F
  usdjpy_tom_prevar$VECP_5T <- usdjpy_tom_prevar$VECP_4T
  usdjpy_tom_prevar$VECP_4F <- usdjpy_tom_prevar$VECP_3F
  usdjpy_tom_prevar$VECP_4T <- usdjpy_tom_prevar$VECP_3T
  usdjpy_tom_prevar$VECP_3F <- usdjpy_tom_prevar$VECP_2F
  usdjpy_tom_prevar$VECP_3T <- usdjpy_tom_prevar$VECP_2T
  usdjpy_tom_prevar$VECP_2F <- usdjpy_tom_prevar$VECP_1F
  usdjpy_tom_prevar$VECP_2T <- usdjpy_tom_prevar$VECP_1T
  usdjpy_tom_prevar$VECP_1F <- usdjpy_tom_prevar$VECPF
  usdjpy_tom_prevar$VECP_1T <- usdjpy_tom_prevar$VECPT
  usdjpy_tom_prevar$VECPF <- "-"
  usdjpy_tom_prevar$VECPT <- "-"
  usdjpy_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(usdjpy_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(usdjpy_possible_VECPF_tomorrow)){
      usdjpy_tom_prevar$VECPF <- usdjpy_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- USDJPY Tomorrow Property Variables if VECPF = ",usdjpy_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(usdjpy_tom_prevar)
      writeLines("")
      usdjpy_tom_rules <- find_match_rules(usdjpy_rules, usdjpy_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",usdjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(usdjpy_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",usdjpy_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(usdjpy_tom_rules[[5]])
      writeLines("")
      usdjpy_rules_list <- c(usdjpy_rules_list, list(usdjpy_tom_rules))
      names(usdjpy_rules_list)[length(usdjpy_rules_list)] <- paste0("USDJPY_TOM_VECPF_", usdjpy_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  usdjpy_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- USDJPY Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(usdjpy_tom_prevar)
  writeLines("")
  usdjpy_tom_rules <- find_match_rules(usdjpy_rules, usdjpy_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(usdjpy_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(usdjpy_tom_rules[[5]])
  writeLines("")
  usdjpy_rules_list <- c(usdjpy_rules_list, list(usdjpy_tom_rules))
  names(usdjpy_rules_list)[length(usdjpy_rules_list)] <- "USDJPY_TOM_ORDINARY_RULES"
  #--------------------------------- 24) XAUUSD Rules Matching ---------------------------------------------------
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("------------------------------- 24) XAUUSD Rules Matching ------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  xauusd_rules_list <- list()
  xauusd_current_prevar <- xauusd_prevar[nrow(xauusd_prevar),]
  xauusd_today_rules <- find_match_rules(xauusd_rules, xauusd_current_prevar)
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- XAUUSD Today Property Variables --------------------------------")
  writeLines("")
  print(xauusd_current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(xauusd_today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(xauusd_today_rules[[5]])
  writeLines("")
  xauusd_rules_list <- c(xauusd_rules_list, list(xauusd_today_rules))
  names(xauusd_rules_list)[length(xauusd_rules_list)] <- "XAUUSD_TODAY_VECPF_RULES"
  
  xauusd_tom_prevar <- xauusd_prevar[nrow(xauusd_prevar),]
  xauusd_tom_prevar$WK <- weekdays(Sys.Date())
  if(as.character(xauusd_tom_prevar$WK == "Saturday")) xauusd_tom_prevar$WK <- "Monday"
  xauusd_fib_cpr_rng <- calculate_fib_cpr_rng_tom(asset_name = "XAUUSD", 
                                                  xauusd_daily[nrow(xauusd_daily),], xauusd_prevar)
  xauusd_tom_prevar$FIB <- xauusd_fib_cpr_rng[1]
  xauusd_tom_prevar$RNG <- xauusd_fib_cpr_rng[3]
  xauusd_tom_prevar$CPR <- xauusd_fib_cpr_rng[2]
  xauusd_tom_prevar$VECP_6F <- xauusd_tom_prevar$VECP_5F
  xauusd_tom_prevar$VECP_6T <- xauusd_tom_prevar$VECP_5T
  xauusd_tom_prevar$VECP_5F <- xauusd_tom_prevar$VECP_4F
  xauusd_tom_prevar$VECP_5T <- xauusd_tom_prevar$VECP_4T
  xauusd_tom_prevar$VECP_4F <- xauusd_tom_prevar$VECP_3F
  xauusd_tom_prevar$VECP_4T <- xauusd_tom_prevar$VECP_3T
  xauusd_tom_prevar$VECP_3F <- xauusd_tom_prevar$VECP_2F
  xauusd_tom_prevar$VECP_3T <- xauusd_tom_prevar$VECP_2T
  xauusd_tom_prevar$VECP_2F <- xauusd_tom_prevar$VECP_1F
  xauusd_tom_prevar$VECP_2T <- xauusd_tom_prevar$VECP_1T
  xauusd_tom_prevar$VECP_1F <- xauusd_tom_prevar$VECPF
  xauusd_tom_prevar$VECP_1T <- xauusd_tom_prevar$VECPT
  xauusd_tom_prevar$VECPF <- "-"
  xauusd_tom_prevar$VECPT <- "-"
  xauusd_tom_prevar$Datetime <- (Sys.Date())
  
  if(length(xauusd_possible_VECPF_tomorrow) > 0){
    for(b in 1:length(xauusd_possible_VECPF_tomorrow)){
      xauusd_tom_prevar$VECPF <- xauusd_possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("---------------------------------- XAUUSD Tomorrow Property Variables if VECPF = ",xauusd_possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(xauusd_tom_prevar)
      writeLines("")
      xauusd_tom_rules <- find_match_rules(xauusd_rules, xauusd_tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",xauusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(xauusd_tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",xauusd_possible_VECPF_tomorrow[b]))
      writeLines("")
      print(xauusd_tom_rules[[5]])
      writeLines("")
      xauusd_rules_list <- c(xauusd_rules_list, list(xauusd_tom_rules))
      names(xauusd_rules_list)[length(xauusd_rules_list)] <- paste0("XAUUSD_TOM_VECPF_", xauusd_possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  xauusd_tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("---------------------------------- XAUUSD Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(xauusd_tom_prevar)
  writeLines("")
  xauusd_tom_rules <- find_match_rules(xauusd_rules, xauusd_tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(xauusd_tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(xauusd_tom_rules[[5]])
  writeLines("")
  xauusd_rules_list <- c(xauusd_rules_list, list(xauusd_tom_rules))
  names(xauusd_rules_list)[length(xauusd_rules_list)] <- "XAUUSD_TOM_ORDINARY_RULES"
  
  #--------------------------------------- Return Results ---------------------------------------------------
  return(list(audcad_rule_match = audcad_rules_list,
              audchf_rule_match = audchf_rules_list,
              audjpy_rule_match = audjpy_rules_list,
              audnzd_rule_match = audnzd_rules_list,
              audusd_rule_match = audusd_rules_list,
              cadchf_rule_match = cadchf_rules_list, 
              cadjpy_rule_match = cadjpy_rules_list, 
              chfjpy_rule_match = chfjpy_rules_list, 
              euraud_rule_match = euraud_rules_list, 
              eurcad_rule_match = eurcad_rules_list, 
              eurchf_rule_match = eurchf_rules_list, 
              eurgbp_rule_match = eurgbp_rules_list, 
              eurjpy_rule_match = eurjpy_rules_list, 
              eurnzd_rule_match = eurnzd_rules_list, 
              eurusd_rule_match = eurusd_rules_list, 
              gbpchf_rule_match = gbpchf_rules_list, 
              gbpjpy_rule_match = gbpjpy_rules_list, 
              gbpusd_rule_match = gbpusd_rules_list, 
              nzdjpy_rule_match = nzdjpy_rules_list, 
              nzdusd_rule_match = nzdusd_rules_list, 
              usdcad_rule_match = usdcad_rules_list, 
              usdchf_rule_match = usdchf_rules_list, 
              usdjpy_rule_match = usdjpy_rules_list, 
              xauusd_rule_match = xauusd_rules_list))
}

specific_asset_rule_matching <- function(rules, prevar, possible_VECPF_tomorrow = c(), asset_name = ""){
  
  calculate_fib_cpr_rng_tom <- function(asset_name, data_daily, data_prevar){
    unique_fib_prevar <- as.character(unique(na.omit(data_prevar$FIB)))
    unique_cpr_prevar <- as.character(unique(na.omit(data_prevar$CPR)))
    unique_rng_prevar <- as.character(unique(na.omit(data_prevar$RNG)))
    fib_split <- strsplit(unique_fib_prevar, "-")
    cpr_split <- strsplit(unique_cpr_prevar, "-")
    rng_split <- strsplit(unique_rng_prevar, "-")
    
    fib_from_unique <- as.numeric(unlist(lapply(fib_split, FUN = function(x) x[[1]])))
    cpr_from_unique <- as.numeric(unlist(lapply(cpr_split, FUN = function(x) x[[1]])))
    rng_from_unique <- as.numeric(unlist(lapply(rng_split, FUN = function(x) x[[1]])))
    
    pp <- (data_daily[["Close"]] + data_daily[["High"]] + data_daily[["Low"]]) / 3
    tc <- (data_daily[["High"]] + data_daily[["Low"]]) / 2
    bc <- pp - tc + pp
    
    fib_vec <- c(0.236, 0.382, 0.5, 0.618, 0.786)
    fib_r1 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[1]]
    fib_r2 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[2]]
    fib_r3 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[3]]
    fib_r4 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[4]]
    fib_r5 <- pp + (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[5]]
    fib_s1 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[1]]
    fib_s2 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[2]]
    fib_s3 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[3]]
    fib_s4 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[4]]
    fib_s5 <- pp - (data_daily[["High"]] - data_daily[["Low"]]) * fib_vec[[5]]
    range <- (data_daily[["High"]] - data_daily[["Low"]])
    
    asset_decimal <- 0
    if(asset_name == "XAUUSD"){
      asset_decimal <- 2
    }else if(grepl("JPY",asset_name)){
      asset_decimal <- 2
    }else{
      asset_decimal <- 4
    }
    
    CPR <- round(abs(bc - pp) * 10^asset_decimal,2)
    FIB <- round((((fib_r2 - fib_r1) + (fib_r3 - fib_r2) + (fib_r4 - fib_r3) + (fib_r5 - fib_r4)) / 4) * 10^asset_decimal, 2)
    
    nearest_fib_unique <- unique_fib_prevar[which.min(abs(fib_from_unique - FIB))]
    nearest_cpr_unique <- unique_cpr_prevar[which.min(abs(cpr_from_unique - CPR))]
    nearest_rng_unique <- unique_rng_prevar[which.min(abs(rng_from_unique - range))]
    
    return(c(nearest_fib_unique, nearest_cpr_unique, nearest_rng_unique))
  }
  writeLines("---------------------------------------------------------------------------------------------------")
  writeLines("-------------------------------   Rules Matching   ------------------------------------------------")
  writeLines("---------------------------------------------------------------------------------------------------")
  rules_list <- list()
  current_prevar <- prevar[nrow(prevar),]
  today_rules <- find_match_rules(rules, current_prevar)
  writeLines("")
  writeLines("")
  writeLines("----------------------------------  Today Property Variables --------------------------------")
  writeLines("")
  print(current_prevar)
  writeLines("")
  writeLines("Individual Rules for today")
  writeLines("")
  print(head(today_rules[[4]],30))
  writeLines("")
  writeLines("Composite Rules for today")
  writeLines("")
  print(today_rules[[5]])
  writeLines("")
  rules_list <- c(rules_list, list(today_rules))
  names(rules_list[[length(rules_list)]]) <- "TODAY_VECPF_RULES"
  
  tom_prevar <- prevar[nrow(prevar),]
  tom_prevar$WK <- weekdays(Sys.Date() + days(1))
  if(as.character(tom_prevar$WK == "Saturday")) tom_prevar$WK <- "Monday"
  tom_fib_cpr_rng_eval <- paste0('tom_fib_cpr_rng <- calculate_fib_cpr_rng_tom("',asset_name,'",',asset_name,"_daily[nrow(",asset_name,"_daily),],",asset_name,"_prevar",")")
  eval(parse(text = tom_fib_cpr_rng_eval))
  tom_prevar$FIB <- "-"
  tom_prevar$RNG <- "-"
  tom_prevar$CPR <- "-"
  tom_prevar$VECPF <- "-"
  tom_prevar$VECPT <- "-"
  tom_prevar$VECP_6F <- tom_prevar$VECP_5F
  tom_prevar$VECP_6T <- tom_prevar$VECP_5T
  tom_prevar$VECP_5F <- tom_prevar$VECP_4F
  tom_prevar$VECP_5T <- tom_prevar$VECP_4T
  tom_prevar$VECP_4F <- tom_prevar$VECP_3F
  tom_prevar$VECP_4T <- tom_prevar$VECP_3T
  tom_prevar$VECP_3F <- tom_prevar$VECP_2F
  tom_prevar$VECP_3T <- tom_prevar$VECP_2T
  tom_prevar$VECP_2F <- tom_prevar$VECP_1F
  tom_prevar$VECP_2T <- tom_prevar$VECP_1T
  tom_prevar$VECP_1F <- tom_prevar$VECPF
  tom_prevar$VECP_1T <- tom_prevar$VECPT
  tom_prevar$Datetime <- (Sys.Date() + days(1))
  
  if(length(possible_VECPF_tomorrow) > 0){
    for(b in 1:length(possible_VECPF_tomorrow)){
      tom_prevar$VECPF <- possible_VECPF_tomorrow[b]
      writeLines("")
      writeLines("")
      writeLines(paste0("----------------------------------  Tomorrow Property Variables if VECPF = ",possible_VECPF_tomorrow[b], " ---------------------------------"))
      writeLines("")
      print(tom_prevar)
      writeLines("")
      tom_rules <- find_match_rules(rules, tom_prevar)
      writeLines("")
      writeLines(paste0("Individual Rules for tomorrow if VECPF = ",possible_VECPF_tomorrow[b]))
      writeLines("")
      print(head(tom_rules[[4]],30))
      writeLines("")
      writeLines(paste0("Composite Rules for tomorrow if VECPF = ",possible_VECPF_tomorrow[b]))
      writeLines("")
      print(tom_rules[[5]])
      writeLines("")
      rules_list <- c(rules_list, list(tom_rules))
      names(rules_list[[length(rules_list)]]) <- paste0("TOM_VECPF_", possible_VECPF_tomorrow[b], "_RULES")
    }
  }
  tom_prevar$VECPF <- "-"
  writeLines("")
  writeLines("")
  writeLines("----------------------------------  Tomorrow Property Variables ----------------------------------")
  writeLines("")
  print(tom_prevar)
  writeLines("")
  tom_rules <- find_match_rules(rules, tom_prevar)
  writeLines("")
  writeLines("Individual Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(head(tom_rules[[4]], 30))
  writeLines("")
  writeLines("Composite Rules for tomorrow with Unknown VECPF")
  writeLines("")
  print(tom_rules[[5]])
  writeLines("")
  rules_list <- c(rules_list, list(tom_rules))
  names(rules_list[[length(rules_list)]]) <- "TOM_ORDINARY_RULES"
}

#--------------------------------------------------------------------------------------------------------------------------
################################### 40) Scraping various Economic Websites ################################################
#--------------------------------------------------------------------------------------------------------------------------

binman::list_versions("chromedriver") # to check the chromedriver list available for RSelenium

#type 1 investing.com for a page that only have a date scroller on datepicker
investing_com_data_correction_1 <- function(link = "https://www.investing.com/commodities/copper-historical-data",
                                            start_date = as.Date("2007-01-01"), chromever = "108.0.5359.22", 
                                            timeframe_select = "daily", extract_directly = TRUE, data_waiting_time = 10){
  library(RSelenium)
  library(rvest)
  library(tidyverse)
  library(stringr)
  library(lubridate)
  
  system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
  diff_date <- round(abs(as.numeric(difftime(start_date, Sys.Date(), units="weeks"))))
  all_jump <- diff_date
  
  tryCatch({
    rD <- rsDriver(browser="chrome", verbose=F, chromever = "108.0.5359.71")
    remDr <- rD[["client"]]
    remDr$navigate(link)
    webElem <- remDr$findElement("css", "body")
    for(x in 1:21){
      webElem$sendKeysToElement(list(key = "down_arrow"))
    }
    
    #close ads
    try({
      ads1 <- remDr$findElement(using = 'id', value = 'closeIconHit')
      ads1$clickElement()
    })
    
    try({
      html <- remDr$getPageSource()[[1]] 
      html_obj <- read_html(html) %>% html_nodes(".relative")
      inner_obj <- html_obj[[1]] %>% html_children() %>% html_children() %>% html_children() %>% html_children() %>% html_children() %>% html_children() 
      target_obj <- inner_obj[[1]] %>% html_children()
      class_name <- target_obj[[2]] %>% html_attr("class") %>% strsplit(" ") %>% unlist()
      class_name[2]
      ads2 <- remDr$findElement(using = 'class', value = class_name[2])
      ads2$clickElement()
    })
    
    try({
      time_frame <- remDr$findElement(using = 'class', value = 'css-tlfecz-indicatorContainer')
      time_frame$clickElement()
      html <- remDr$getPageSource()[[1]] 
      html_obj <- read_html(html) %>% html_nodes(".inv-select-container")
      html_obj2 <- html_obj %>% html_children
      html_obj3 <- html_obj2[[2]] %>% html_children
      html_obj4 <- html_obj3[[3]] %>% html_children
      id_frame <- html_obj4 %>% html_children()
      id_frame1 <- id_frame[[1]] %>% html_attr("id")
      id_frame2 <- id_frame[[2]] %>% html_attr("id")
      id_frame3 <- id_frame[[3]] %>% html_attr("id")
      hidden_frame_daily <- remDr$findElement(using = 'id', value = id_frame1)
      hidden_frame_weekly <- remDr$findElement(using = 'id', value = id_frame2)
      hidden_frame_monthly <- remDr$findElement(using = 'id', value = id_frame3)
      if(timeframe_select == "daily"){
        hidden_frame_daily$clickElement()
      }else if(timeframe_select == "weekly"){
        hidden_frame_weekly$clickElement()
      }else if(timeframe_select == "monthly"){
        hidden_frame_monthly$clickElement()
      }
    })
    
    writeLines('Done Selection Time Frame')
    
    html <- remDr$getPageSource()[[1]] 
    html_obj <- read_html(html) %>% html_nodes(".flex-row-reverse") %>% html_children
    html_obj2 <- html_obj[[2]] %>% html_children %>% html_children 
    click_history <- html_obj2[[2]] %>% html_children %>% html_attr("class")
    history_frame <- remDr$findElement(using = 'class', value = click_history)
    history_frame$clickElement()
    Sys.sleep(1)
    
    html <- remDr$getPageSource()[[1]] 
    html_obj <- read_html(html) %>% html_nodes(".flex-row-reverse") %>% html_children
    html_obj2 <- html_obj[[2]] %>% html_children 
    html_obj3 <- html_obj2[[2]] 
    html_obj4 <- html_obj3 %>% html_children 
    html_obj5 <- html_obj4[[1]] %>% html_children %>% html_children 
    before_date_class <- html_obj5[[1]] %>% html_attr("class")
    history_filler <- remDr$findElements(using = 'class', value = before_date_class)
    history_filler[[1]]$clickElement()
    Sys.sleep(1)
    
    input_dates <- remDr$findElements(using = 'xpath', value = './/input[@type="date"]')
    for(up in 1:all_jump){
      input_dates[[1]]$sendKeysToElement(list(key = "up_arrow"))
    }
    input_dates[[1]]$sendKeysToElement(list(key = "enter"))
    
    writeLines('Done Selection Date Picker')
    
    submiter_class <- html_obj4[[2]] %>% html_children %>% html_attr("class") 
    submiter_class <- unlist(strsplit(submiter_class, " "))[2]
    submiter <- remDr$findElement(using = 'class', value = submiter_class)
    submiter$clickElement()
    
    Sys.sleep(data_waiting_time)
    html <- remDr$getPageSource()[[1]] 
    html_table_obj <- read_html(html) %>% html_table()
    
    if(extract_directly){
      html_table_desired <- as.data.frame(html_table_obj[[2]])
      html_table_desired$Date <- as.Date(html_table_desired$Date, tryFormats = c("%b %d, %Y"))
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      return(html_table_desired)
    }else{
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      return(html_table_obj)
    }
    
  },error = function(cond){
    message(cond)
    system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
  })
}

#type 2 investing.com for a page that can input dates on datepicker
investing_com_data_correction_2 <- function(link = "https://www.investing.com/currencies/us-dollar-index-historical-data",
                                            start_date = as.Date("2007-01-01"), chromever = "108.0.5359.22", 
                                            data_waiting_time = 60, timeframe_select = "daily",
                                            extract_directly = FALSE){
  library(RSelenium)
  library(rvest)
  library(tidyverse)
  library(stringr)
  library(lubridate)
  
  system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
  diff_date <- abs(as.numeric(difftime(start_date, Sys.Date())))
  month_diff <- ceiling(diff_date / 30) 
  jump_in_1_month <- 4
  all_jump <- month_diff * jump_in_1_month
  
  tryCatch({
    rD <- rsDriver(browser="chrome", verbose=F, chromever = chromever)
    remDr <- rD[["client"]]
    remDr$navigate(link)
    webElem <- remDr$findElement("css", "body")
    Sys.sleep(10)
    
    try({
      ads_main <- remDr$findElement(using = 'id', value = 'promoAncmtClose')
      ads_main$clickElement()
    })
    
    try({
      ads_main2 <- remDr$findElement(using = 'class', value = 'popupCloseIcon')
      ads_main2$clickElement()
    })
    
    for(x in 1:12){
      webElem$sendKeysToElement(list(key = "down_arrow"))
    }
    
    #close ads
    try({
      ads1 <- remDr$findElement(using = 'class', value = 'bugCloseIcon')
      ads1$clickElement()
    })
    
    try({
      ads2 <- remDr$findElement(using = 'id', value = 'closeIconHit')
      ads2$clickElement()
    })
    
    try({
      ads3 <- remDr$findElement(using = 'id', value = '_7TLHU')
      ads3$clickElement()
    })
    
    time_frame <- remDr$findElement(using = 'xpath', value = './/select[@id = "data_interval"]')
    time_frame$clickElement()
    
    if(timeframe_select == "weekly"){
      time_frame$sendKeysToElement(list(key = "down_arrow"))
    }else if(timeframe_select == "monthly"){
      time_frame$sendKeysToElement(list(key = "down_arrow"))
      time_frame$sendKeysToElement(list(key = "down_arrow"))
    }
    
    time_frame$sendKeysToElement(list(key = "enter"))
    
    writeLines('Done Selection Time Frame')
    
    history_frame <- remDr$findElement(using = 'id', value = 'flatDatePickerCanvasHol')
    history_frame$clickElement()
    
    input_dates <- remDr$findElement(using = 'xpath', value = './/input[@id="startDate"]')
    char_input <- format(start_date, "%m/%d/%Y")
    for(clear in 1:11){
      input_dates$sendKeysToElement(list(key = "backspace"))
    }
    for(a in 1:nchar(char_input)){
      input_dates$sendKeysToElement(list(substr(char_input, a, a)))
    }
    
    writeLines('Done Selection Date Picker')
    
    submiter <- remDr$findElement(using = 'id', value = 'applyBtn')
    submiter$clickElement()
    
    Sys.sleep(data_waiting_time)
    html <- remDr$getPageSource()[[1]] 
    html_table_obj <- read_html(html) %>% html_table()
    
    
    
    if(extract_directly){
      html_table_desired <- as.data.frame(html_table_obj[[2]])
      html_table_desired$Date <- as.Date(html_table_desired$Date, tryFormats = c("%b %d, %Y"))
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      return(html_table_desired)
    }else{
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      return(html_table_obj)
    }
  },error = function(cond){
    message(cond)
    system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
  })
}

#type 3 investing.com for a page that holds economic calendar
investing_com_data_correction_3 <- function(link = "https://www.investing.com/economic-calendar/eia-crude-oil-inventories-75",
                                            hover_down = 21, show_more_click = 100, chromever = "108.0.5359.22"){
  library(RSelenium)
  library(rvest)
  library(tidyverse)
  library(stringr)
  
  system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
  
  tryCatch({
    rD <- rsDriver(browser="chrome", verbose=F, chromever = chromever)
    remDr <- rD[["client"]]
    remDr$navigate(link)
    webElem <- remDr$findElement("css", "body")
    Sys.sleep(20)
    
    try({
      ads_main <- remDr$findElement(using = 'id', value = 'promoAncmtClose')
      ads_main$clickElement()
    })
    
    try({
      ads_main2 <- remDr$findElement(using = 'class', value = 'popupCloseIcon')
      ads_main2$clickElement()
    })
    
    for(x in 1:hover_down){
      webElem$sendKeysToElement(list(key = "down_arrow"))
    }
    
    try({
      ads_main3 <- remDr$findElement(using = 'id', value = 'closeIconHit')
      ads_main3$clickElement()
    })
    
    id_page <- unlist(strsplit(link,"\\-"))
    id_page <- id_page[length(id_page)]
    button_value <- paste0("showMoreHistory",id_page)
    
    show_more_button <- remDr$findElement(using = 'id', value = button_value)
    if(show_more_click == Inf){
      while(1){
        tryCatch({
          show_more_button$clickElement()
          webElem$sendKeysToElement(list(key = "down_arrow"))
          webElem$sendKeysToElement(list(key = "down_arrow"))
          webElem$sendKeysToElement(list(key = "down_arrow"))
        },error = function(cond){
          writeLines("No Show More Button to be clicked!")
          break
        })
      }
    }else{
      for(x in 1:show_more_click){
        tryCatch({
          show_more_button$clickElement()
          webElem$sendKeysToElement(list(key = "down_arrow"))
          webElem$sendKeysToElement(list(key = "down_arrow"))
          webElem$sendKeysToElement(list(key = "down_arrow"))
        },error = function(cond){
          writeLines("No Show More Button to be clicked!")
          break
        })
      }
    }
    
    library(XML)
    doc <- htmlParse(remDr$getPageSource()[[1]])
    html_data <- readHTMLTable(doc)
    data_extract <- html_data[[1]]
    
    data_extract$`Release Date` <- as.Date(str_replace(unlist(lapply(strsplit(data_extract$`Release Date`," \\("), 
                                                                  FUN = function(x) x[[1]]))," $",""), tryFormats=c("%b %d, %Y"))
    data_extract$Actual <- str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(data_extract$Actual, "M",""),"K",""),"%",""),",",""),"\\$", "")," $",""),"B",""),"T","")
    data_extract$Forecast <- str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(data_extract$Forecast, "M",""),"K",""),"%",""),",",""),"\\$", "")," $",""),"B",""),"T","")
    data_extract$Previous <- str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(str_replace(data_extract$Previous, "M",""),"K",""),"%",""),",",""),"\\$", "")," $",""),"B",""),"T","")
    data_extract$Actual <- as.numeric(data_extract$Actual)
    data_extract$Forecast <- as.numeric(data_extract$Forecast)
    data_extract$Previous <- as.numeric(data_extract$Previous)
    
    remDr$close()
    rD$server$stop()
    rm(rD, remDr)
    gc()
    
    system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
    return(data_extract)
    
  },error = function(cond){
    message(cond)
    system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
  })
}

#need vpn to unblock sites
myfxbook_lot_sentiment <- function(simplify_cols = T){
  sentiment_df <- data.frame()
  link <- c("https://www.myfxbook.com/community/outlook/USDCHF",
            "https://www.myfxbook.com/community/outlook/GBPUSD",
            "https://www.myfxbook.com/community/outlook/EURUSD",
            "https://www.myfxbook.com/community/outlook/USDJPY",
            "https://www.myfxbook.com/community/outlook/USDCAD",
            "https://www.myfxbook.com/community/outlook/AUDUSD",
            "https://www.myfxbook.com/community/outlook/EURGBP",
            "https://www.myfxbook.com/community/outlook/EURAUD",
            "https://www.myfxbook.com/community/outlook/EURCHF",
            "https://www.myfxbook.com/community/outlook/EURJPY",
            "https://www.myfxbook.com/community/outlook/GBPCHF",
            "https://www.myfxbook.com/community/outlook/CADJPY",
            "https://www.myfxbook.com/community/outlook/GBPJPY",
            "https://www.myfxbook.com/community/outlook/AUDNZD",
            "https://www.myfxbook.com/community/outlook/AUDCAD",
            "https://www.myfxbook.com/community/outlook/AUDCHF",
            "https://www.myfxbook.com/community/outlook/AUDJPY",
            "https://www.myfxbook.com/community/outlook/CHFJPY",
            "https://www.myfxbook.com/community/outlook/EURNZD",
            "https://www.myfxbook.com/community/outlook/EURCAD",
            "https://www.myfxbook.com/community/outlook/CADCHF",
            "https://www.myfxbook.com/community/outlook/NZDJPY",
            "https://www.myfxbook.com/community/outlook/NZDUSD",
            "https://www.myfxbook.com/community/outlook/XAUUSD")
  
  library(dplyr)
  library(rvest)
  for(a in 1:length(link)){
    while(1){
      bool <- c(F,F,F,F)
      extract <- read_html(link[a]) %>% 
        html_node(".portlet-body") %>% html_node("#currentMetricsTable") %>% html_children()
      extract_asset <- unlist(strsplit(html_text(extract[[2]]), "\n"))
      extract_asset <- extract_asset[which(extract_asset != "")] 
      extract_short <- unlist(strsplit(html_text(extract[[3]]), "\n"))
      extract_short <- extract_short[which(extract_short != "")] 
      extract_long <- unlist(strsplit(html_text(extract[[4]]), "\n"))
      extract_long <- extract_long[which(extract_long != "")] 
      extract_long[2] <- as.numeric(extract_long[2])
      extract_short[2] <- as.numeric(extract_short[2])
      #print(extract_long[2])
      #print(extract_short[2])
      #print(extract_long[4])
      #print(extract_short[4])
      if(extract_long[2] == "NA" || is.na(extract_long[2])){
        bool[1] <- T
      }
      if(extract_short[2] == "NA" || is.na(extract_short[2])){
        bool[2] <- T
      }
      if(extract_long[4] == "%" || is.na(extract_long[4]) || extract_long[4] == " "){
        bool[3] <- T
      }
      if(extract_short[4] == "%" || is.na(extract_short[4])  || extract_short[4] == " "){
        bool[4] <- T
      }
      if(all(bool == F)){
        break 
      }else{
        writeLines(paste0("Retrying Link ",link[a]))
      }
    }
    
    
    if(nrow(sentiment_df) == 0){
      sentiment_df <- data.frame(time_extracted = Sys.time(),
                                 asset_name = extract_asset, 
                                 short_pct = extract_short[2],
                                 short_size = extract_short[4],
                                 long_pct = extract_long[2],
                                 long_size = extract_long[4])
    }else{
      sentiment_add <- data.frame(time_extracted = Sys.time(),
                                  asset_name = extract_asset, 
                                  short_pct = extract_short[2],
                                  short_size = extract_short[4],
                                  long_pct = extract_long[2],
                                  long_size = extract_long[4])
      sentiment_df <- rbind(sentiment_df, sentiment_add)
    }
  }
  
  sentiment_df$short_pct <- as.numeric(sentiment_df$short_pct)
  sentiment_df$long_pct <- as.numeric(sentiment_df$long_pct)
  sentiment_df$current_market_domination <- ifelse(sentiment_df$long_pct > sentiment_df$short_pct, paste0("Seller Domination"," (+",50 - sentiment_df$short_pct,")"), 
                                                   ifelse(sentiment_df$long_pct == sentiment_df$short_pct, "Equal Domination", paste0("Buyer Domination"," (+",50 - sentiment_df$long_pct,")")))
  library(stringr)
  short_lots <- as.numeric(unlist(strsplit(sentiment_df$short_size," lots")))
  long_lots <- as.numeric(unlist(strsplit(sentiment_df$long_size," lots")))
  sentiment_df$lots_ratio <- ifelse(short_lots > long_lots, 
                                    paste0("1:", round(short_lots / long_lots, 2)," Short Press/Long Adv"), 
                                    paste0("1:", round(long_lots / short_lots, 2)," Long Press/Short Adv"))
  bools_domination <- ifelse(grepl("\\+",sentiment_df$current_market_domination), TRUE, FALSE)
  sentiment_df$dominance <- NULL
  true_value <- str_replace(unlist(lapply(strsplit(sentiment_df$current_market_domination[bools_domination],"\\+"), 
                                          FUN = function(x) x[[2]])), "\\)", "")
  idx <- 1
  for(v in 1:length(bools_domination)){
    if(bools_domination[v]){
      sentiment_df$dominance[v] <- true_value[idx]
      idx <- idx + 1
    }else{
      sentiment_df$dominance[v] <- "0"
    }
  }
                                   
  sentiment_df$dominance <- as.numeric(sentiment_df$dominance)
  sentiment_df <- sentiment_df %>% arrange(desc(dominance))
  sentiment_df <- sentiment_df[,-which(colnames(sentiment_df) == "dominance")]
  sentiment_df$cheap_spread <- ifelse(sentiment_df$asset_name %in% c("EURUSD","USDCHF","GBPUSD","USDJPY","USDCAD","AUDUSD"), T, F)
  if(simplify_cols){
    sentiment_df <- sentiment_df %>% select(-short_pct, -long_pct, -lots_ratio)
    return(sentiment_df)
  }else{
    return(sentiment_df)
  }
}

stream_dailyfx_sentiment <- function(chromever = "108.0.5359.71"){
  library(RSelenium)
  library(rvest)
  library(tidyverse)
  library(stringr)
  
  rd <- NULL
  remDr <- NULL
  while(1){
    try({
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      rD <- rsDriver(browser="chrome", verbose=F, chromever = chromever)
      remDr <- rD[["client"]]
      break
    })
  }
  
  while(1){
    tryCatch({
      remDr$navigate("https://www.dailyfx.com/sentiment")
      Sys.sleep(10) # give the page time to fully load
      html <- remDr$getPageSource()[[1]]
      html_obj <- read_html(html)
      
      all_assets <- html_obj %>%
        html_nodes(".dfx-technicalSentimentComponent") %>% html_nodes(".flex-column") %>% html_children()
      
      sentiment_df <- data.frame()
      for(all_item in 1:length(all_assets)){
        asset_info <- all_assets[all_item] %>% html_children()
        asset_name <- unlist(strsplit(html_text(asset_info[[1]]),"\n"))[3]
        asset_signal <- unlist(strsplit(html_text(asset_info[[1]]),"\n"))[6]
        asset_net_long <- asset_info[[2]] %>% 
          html_nodes(".dfx-technicalSentimentCard__netLongContainer") %>% 
          html_children() %>% html_attr("data-value")
        asset_net_short <- asset_info[[2]] %>% 
          html_nodes(".dfx-technicalSentimentCard__netShortContainer") %>% 
          html_children() %>% html_attr("data-value")
        if(nrow(sentiment_df) == 0){
          sentiment_df <- data.frame(time_extracted = Sys.time(), asset_name, asset_signal, asset_net_long, asset_net_short)
        }else{
          sentiment_add <- data.frame(time_extracted = Sys.time(), asset_name, asset_signal, asset_net_long, asset_net_short)
          sentiment_df <- rbind(sentiment_df, sentiment_add)
        }
      }
      
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      return(sentiment_df)
      
    },error = function(cond){
      message("error happened!")
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
    })
  }
}

forex_factory_calendar_scrape <- function(link = "https://www.forexfactory.com/calendar"){
  library(rvest)
  library(stringr)
  forex_calendar <- read_html("https://www.forexfactory.com/calendar")
  forex_event <- forex_calendar %>% html_node(".calendar__table") %>% html_children()
  
  calendar_date <- forex_event %>% html_node(".calendar__date") %>% html_text()
  calendar_time <- forex_event %>% html_node(".calendar__time") %>% html_text()
  calendar_currency <- forex_event %>% html_node(".calendar__currency") %>% html_text()
  calendar_event <- forex_event %>% html_node(".calendar__event") %>% html_text()
  calendar_actual <- forex_event %>% html_node(".calendar__actual") %>% html_text()
  calendar_forecast <- forex_event %>% html_node(".calendar__forecast") %>% html_text()
  calendar_previous <- forex_event %>% html_node(".calendar__previous") %>% html_text()
  
  calendar_date <- calendar_date[which(!is.na(calendar_date))]
  calendar_time <- calendar_time[which(!is.na(calendar_time))]
  calendar_currency <- calendar_currency[which(!is.na(calendar_currency))]
  calendar_event <- calendar_event[which(!is.na(calendar_event))]
  calendar_actual <- calendar_actual[which(!is.na(calendar_actual))]
  calendar_forecast <- calendar_forecast[which(!is.na(calendar_forecast))]
  calendar_previous <- calendar_previous[which(!is.na(calendar_previous))]
  
  calendar_date <- str_replace_all(str_replace_all(calendar_date, "^[ ]{1,}", ""), "[ ]{1,}$", "")
  calendar_time <- str_replace_all(str_replace_all(str_replace_all(calendar_time, "\n", ""),"^[ ]{1,}", ""), "[ ]{1,}$", "")
  calendar_currency <- str_replace_all(calendar_currency, "\n", "")
  calendar_event <- str_replace_all(str_replace_all(calendar_event, "^[ ]{1,}", ""), "[ ]{1,}$", "")
  calendar_actual <- str_replace_all(calendar_actual, "^[ ]{1,}", "")
  calendar_forecast <- str_replace_all(str_replace_all(calendar_forecast, "\n", ""),"[ ]{1,}$", "")
  calendar_previous <- str_replace_all(str_replace_all(str_replace_all(calendar_previous, "\n", ""),"^[ ]{1,}", ""), "[ ]{1,}$", "")
  
  unique_calendar_date <- which(calendar_date != "")[-1]
  for(v in 1:(length(unique_calendar_date) - 1)){
    from_index <- unique_calendar_date[v]
    to_index <- unique_calendar_date[v + 1]
    for(fill in (from_index + 1):(to_index-1)){
      calendar_date[fill] <- calendar_date[from_index]
    }
  }
  for(fill in 1:length(calendar_time)){
    if(calendar_time[fill] == ""){
      calendar_time[fill] <- calendar_time[fill-1]
    }
  }
  
  if(length(which(calendar_event == "")) > 0 || length(which(calendar_event == " ")) > 0) calendar_event[which(calendar_event == "" || calendar_event == " ")] <- "N/A"
  if(length(which(calendar_actual == "")) > 0) calendar_actual[which(calendar_actual == "")] <- "N/A"
  if(length(which(calendar_forecast == "  ")) > 0) calendar_forecast[which(calendar_forecast == "  ")] <- "N/A" 
  if(length(which(calendar_previous == " ")) > 0) calendar_previous[which(calendar_previous == " ")] <- "N/A"
  
  #the calendar time from forexfactory default is hour - 11 from GMT + 7
  event_fundamental_clean <- as.data.frame(cbind(calendar_date = calendar_date[-1],
                                                 calendar_time = calendar_time[-1],
                                                 calendar_currency = calendar_currency[-1],
                                                 calendar_event = calendar_event[-1],
                                                 calendar_actual = calendar_actual[-1],
                                                 calendar_forecast = calendar_forecast[-1],
                                                 calendar_previous = calendar_previous[-1]))
  
  event_fundamental_clean$calendar_date <- paste0("2023 ", event_fundamental_clean$calendar_date)
  
  library(DescTools)
  for(a in 1:length(day.abb)){
    event_fundamental_clean$calendar_date <- str_replace_all(event_fundamental_clean$calendar_date, str_to_title(day.abb)[a], "")
  }
  event_fundamental_clean$calendar_date <- as.Date(event_fundamental_clean$calendar_date, tryFormats = c("%Y %b %d"))
  event_fundamental_clean$weekdays <- weekdays(event_fundamental_clean$calendar_date)
  
  time_transformer_string <- function(df, date_column, hour_column_string, convert_gmt = T){
    library(stringr)
    library(lubridate)
    hour_min_string <- df[[hour_column_string]]
    which_valid_hour <- which(grepl("am", hour_min_string) | grepl("pm", hour_min_string))
    hour_min_string_to_transform <- hour_min_string[which_valid_hour]
    hour_string <- unlist(lapply(strsplit(hour_min_string_to_transform, "\\:"), FUN = function(x) x[[1]]))
    minute_string <- unlist(lapply(strsplit(hour_min_string_to_transform, "\\:"), FUN = function(x) x[[2]]))
    minute_string <- str_replace_all(str_replace_all(minute_string, "am", ""),"pm","")
    ampm_string <- unlist(lapply(strsplit(hour_min_string_to_transform, "\\:"), FUN = function(x) x[[2]]))
    ampm_string <- str_replace_all(ampm_string, "^[0-9]{1,}", "")
    hour_string <- as.numeric(hour_string)
    hour_string <- ifelse(ampm_string == "pm", hour_string + 12, hour_string)
    if(convert_gmt){
      #convert hour to gmt + 7
      hour_string <- hour_string + 11
    }
    for(b in 1:length(which_valid_hour)){
      if(hour_string[b] > 24){
        df[[date_column]][which_valid_hour[b]] <- df[[date_column]][which_valid_hour[b]] + days(1) 
      }
    }
    hour_string <- ifelse(hour_string >= 24, hour_string - 24, hour_string)
    hourly_format <- paste0(hour_string,":",minute_string,":00")
    hour_min_string[which_valid_hour] <- hourly_format
    df[[hour_column_string]] <- hour_min_string
    return(df)
  }
  event_fundamental_clean <- time_transformer_string(event_fundamental_clean, "calendar_date", "calendar_time")
  colnames(event_fundamental_clean) <- c("date","time","currency","event",
                                         "actual","forecast","previous","weekdays")
  return(event_fundamental_clean)
}

technical_investing <- function(){
  library(rvest)
  audcad_tech_analysis <- read_html("https://www.investing.com/currencies/aud-cad-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  audcad_moving_average <- audcad_tech_analysis[[2]] %>% html_children()
  audcad_moving_average <- c(audcad_moving_average[[3]] %>% html_text(), audcad_moving_average[[4]] %>% html_text())
  audcad_tech_indicator <- audcad_tech_analysis[[3]] %>% html_children()
  audcad_tech_indicator <- c(audcad_tech_indicator[[3]] %>% html_text(), audcad_tech_indicator[[4]] %>% html_text())
  audcad_summary <- as.data.frame(rbind(audcad_moving_average, audcad_tech_indicator))
  
  audchf_tech_analysis <- read_html("https://www.investing.com/currencies/aud-chf-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  audchf_moving_average <- audchf_tech_analysis[[2]] %>% html_children()
  audchf_moving_average <- c(audchf_moving_average[[3]] %>% html_text(), audchf_moving_average[[4]] %>% html_text())
  audchf_tech_indicator <- audchf_tech_analysis[[3]] %>% html_children()
  audchf_tech_indicator <- c(audchf_tech_indicator[[3]] %>% html_text(), audchf_tech_indicator[[4]] %>% html_text())
  audchf_summary <- as.data.frame(rbind(audchf_moving_average, audchf_tech_indicator))
  
  audjpy_tech_analysis <- read_html("https://www.investing.com/currencies/aud-jpy-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  audjpy_moving_average <- audjpy_tech_analysis[[2]] %>% html_children()
  audjpy_moving_average <- c(audjpy_moving_average[[3]] %>% html_text(), audjpy_moving_average[[4]] %>% html_text())
  audjpy_tech_indicator <- audjpy_tech_analysis[[3]] %>% html_children()
  audjpy_tech_indicator <- c(audjpy_tech_indicator[[3]] %>% html_text(), audjpy_tech_indicator[[4]] %>% html_text())
  audjpy_summary <- as.data.frame(rbind(audjpy_moving_average, audjpy_tech_indicator))
  
  audnzd_tech_analysis <- read_html("https://www.investing.com/currencies/aud-nzd-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  audnzd_moving_average <- audnzd_tech_analysis[[2]] %>% html_children()
  audnzd_moving_average <- c(audnzd_moving_average[[3]] %>% html_text(), audnzd_moving_average[[4]] %>% html_text())
  audnzd_tech_indicator <- audnzd_tech_analysis[[3]] %>% html_children()
  audnzd_tech_indicator <- c(audnzd_tech_indicator[[3]] %>% html_text(), audnzd_tech_indicator[[4]] %>% html_text())
  audnzd_summary <- as.data.frame(rbind(audnzd_moving_average, audnzd_tech_indicator))
  
  audusd_tech_analysis <- read_html("https://www.investing.com/currencies/aud-usd-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  audusd_moving_average <- audusd_tech_analysis[[2]] %>% html_children()
  audusd_moving_average <- c(audusd_moving_average[[3]] %>% html_text(), audusd_moving_average[[4]] %>% html_text())
  audusd_tech_indicator <- audusd_tech_analysis[[3]] %>% html_children()
  audusd_tech_indicator <- c(audusd_tech_indicator[[3]] %>% html_text(), audusd_tech_indicator[[4]] %>% html_text())
  audusd_summary <- as.data.frame(rbind(audusd_moving_average, audusd_tech_indicator))
  
  cadchf_tech_analysis <- read_html("https://www.investing.com/currencies/cad-chf-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  cadchf_moving_average <- cadchf_tech_analysis[[2]] %>% html_children()
  cadchf_moving_average <- c(cadchf_moving_average[[3]] %>% html_text(), cadchf_moving_average[[4]] %>% html_text())
  cadchf_tech_indicator <- cadchf_tech_analysis[[3]] %>% html_children()
  cadchf_tech_indicator <- c(cadchf_tech_indicator[[3]] %>% html_text(), cadchf_tech_indicator[[4]] %>% html_text())
  cadchf_summary <- as.data.frame(rbind(cadchf_moving_average, cadchf_tech_indicator))
  
  cadjpy_tech_analysis <- read_html("https://www.investing.com/currencies/cad-jpy-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  cadjpy_moving_average <- cadjpy_tech_analysis[[2]] %>% html_children()
  cadjpy_moving_average <- c(cadjpy_moving_average[[3]] %>% html_text(), cadjpy_moving_average[[4]] %>% html_text())
  cadjpy_tech_indicator <- cadjpy_tech_analysis[[3]] %>% html_children()
  cadjpy_tech_indicator <- c(cadjpy_tech_indicator[[3]] %>% html_text(), cadjpy_tech_indicator[[4]] %>% html_text())
  cadjpy_summary <- as.data.frame(rbind(cadjpy_moving_average, cadjpy_tech_indicator))
  
  chfjpy_tech_analysis <- read_html("https://www.investing.com/currencies/chf-jpy-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  chfjpy_moving_average <- chfjpy_tech_analysis[[2]] %>% html_children()
  chfjpy_moving_average <- c(chfjpy_moving_average[[3]] %>% html_text(), chfjpy_moving_average[[4]] %>% html_text())
  chfjpy_tech_indicator <- chfjpy_tech_analysis[[3]] %>% html_children()
  chfjpy_tech_indicator <- c(chfjpy_tech_indicator[[3]] %>% html_text(), chfjpy_tech_indicator[[4]] %>% html_text())
  chfjpy_summary <- as.data.frame(rbind(chfjpy_moving_average, chfjpy_tech_indicator))
  
  dxy_tech_analysis <- read_html("https://www.investing.com/indices/usdollar-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  dxy_moving_average <- dxy_tech_analysis[[2]] %>% html_children()
  dxy_moving_average <- c(dxy_moving_average[[3]] %>% html_text(), dxy_moving_average[[4]] %>% html_text())
  dxy_tech_indicator <- dxy_tech_analysis[[3]] %>% html_children()
  dxy_tech_indicator <- c(dxy_tech_indicator[[3]] %>% html_text(), dxy_tech_indicator[[4]] %>% html_text())
  dxy_summary <- as.data.frame(rbind(dxy_moving_average, dxy_tech_indicator))
  
  euraud_tech_analysis <- read_html("https://www.investing.com/currencies/eur-aud-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  euraud_moving_average <- euraud_tech_analysis[[2]] %>% html_children()
  euraud_moving_average <- c(euraud_moving_average[[3]] %>% html_text(), euraud_moving_average[[4]] %>% html_text())
  euraud_tech_indicator <- euraud_tech_analysis[[3]] %>% html_children()
  euraud_tech_indicator <- c(euraud_tech_indicator[[3]] %>% html_text(), euraud_tech_indicator[[4]] %>% html_text())
  euraud_summary <- as.data.frame(rbind(euraud_moving_average, euraud_tech_indicator))
  
  eurcad_tech_analysis <- read_html("https://www.investing.com/currencies/eur-cad-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  eurcad_moving_average <- eurcad_tech_analysis[[2]] %>% html_children()
  eurcad_moving_average <- c(eurcad_moving_average[[3]] %>% html_text(), eurcad_moving_average[[4]] %>% html_text())
  eurcad_tech_indicator <- eurcad_tech_analysis[[3]] %>% html_children()
  eurcad_tech_indicator <- c(eurcad_tech_indicator[[3]] %>% html_text(), eurcad_tech_indicator[[4]] %>% html_text())
  eurcad_summary <- as.data.frame(rbind(eurcad_moving_average, eurcad_tech_indicator))
  
  eurchf_tech_analysis <- read_html("https://www.investing.com/currencies/eur-chf-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  eurchf_moving_average <- eurchf_tech_analysis[[2]] %>% html_children()
  eurchf_moving_average <- c(eurchf_moving_average[[3]] %>% html_text(), eurchf_moving_average[[4]] %>% html_text())
  eurchf_tech_indicator <- eurchf_tech_analysis[[3]] %>% html_children()
  eurchf_tech_indicator <- c(eurchf_tech_indicator[[3]] %>% html_text(), eurchf_tech_indicator[[4]] %>% html_text())
  eurchf_summary <- as.data.frame(rbind(eurchf_moving_average, eurchf_tech_indicator))
  
  eurgbp_tech_analysis <- read_html("https://www.investing.com/currencies/eur-gbp-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  eurgbp_moving_average <- eurgbp_tech_analysis[[2]] %>% html_children()
  eurgbp_moving_average <- c(eurgbp_moving_average[[3]] %>% html_text(), eurgbp_moving_average[[4]] %>% html_text())
  eurgbp_tech_indicator <- eurgbp_tech_analysis[[3]] %>% html_children()
  eurgbp_tech_indicator <- c(eurgbp_tech_indicator[[3]] %>% html_text(), eurgbp_tech_indicator[[4]] %>% html_text())
  eurgbp_summary <- as.data.frame(rbind(eurgbp_moving_average, eurgbp_tech_indicator))
  
  eurjpy_tech_analysis <- read_html("https://www.investing.com/currencies/eur-jpy-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  eurjpy_moving_average <- eurjpy_tech_analysis[[2]] %>% html_children()
  eurjpy_moving_average <- c(eurjpy_moving_average[[3]] %>% html_text(), eurjpy_moving_average[[4]] %>% html_text())
  eurjpy_tech_indicator <- eurjpy_tech_analysis[[3]] %>% html_children()
  eurjpy_tech_indicator <- c(eurjpy_tech_indicator[[3]] %>% html_text(), eurjpy_tech_indicator[[4]] %>% html_text())
  eurjpy_summary <- as.data.frame(rbind(eurjpy_moving_average, eurjpy_tech_indicator))
  
  eurnzd_tech_analysis <- read_html("https://www.investing.com/currencies/eur-nzd-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  eurnzd_moving_average <- eurnzd_tech_analysis[[2]] %>% html_children()
  eurnzd_moving_average <- c(eurnzd_moving_average[[3]] %>% html_text(), eurnzd_moving_average[[4]] %>% html_text())
  eurnzd_tech_indicator <- eurnzd_tech_analysis[[3]] %>% html_children()
  eurnzd_tech_indicator <- c(eurnzd_tech_indicator[[3]] %>% html_text(), eurnzd_tech_indicator[[4]] %>% html_text())
  eurnzd_summary <- as.data.frame(rbind(eurnzd_moving_average, eurnzd_tech_indicator))
  
  eurusd_tech_analysis <- read_html("https://www.investing.com/currencies/eur-usd-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  eurusd_moving_average <- eurusd_tech_analysis[[2]] %>% html_children()
  eurusd_moving_average <- c(eurusd_moving_average[[3]] %>% html_text(), eurusd_moving_average[[4]] %>% html_text())
  eurusd_tech_indicator <- eurusd_tech_analysis[[3]] %>% html_children()
  eurusd_tech_indicator <- c(eurusd_tech_indicator[[3]] %>% html_text(), eurusd_tech_indicator[[4]] %>% html_text())
  eurusd_summary <- as.data.frame(rbind(eurusd_moving_average, eurusd_tech_indicator))
  
  gbpchf_tech_analysis <- read_html("https://www.investing.com/currencies/gbp-chf-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  gbpchf_moving_average <- gbpchf_tech_analysis[[2]] %>% html_children()
  gbpchf_moving_average <- c(gbpchf_moving_average[[3]] %>% html_text(), gbpchf_moving_average[[4]] %>% html_text())
  gbpchf_tech_indicator <- gbpchf_tech_analysis[[3]] %>% html_children()
  gbpchf_tech_indicator <- c(gbpchf_tech_indicator[[3]] %>% html_text(), gbpchf_tech_indicator[[4]] %>% html_text())
  gbpchf_summary <- as.data.frame(rbind(gbpchf_moving_average, gbpchf_tech_indicator))
  
  gbpjpy_tech_analysis <- read_html("https://www.investing.com/currencies/gbp-jpy-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  gbpjpy_moving_average <- gbpjpy_tech_analysis[[2]] %>% html_children()
  gbpjpy_moving_average <- c(gbpjpy_moving_average[[3]] %>% html_text(), gbpjpy_moving_average[[4]] %>% html_text())
  gbpjpy_tech_indicator <- gbpjpy_tech_analysis[[3]] %>% html_children()
  gbpjpy_tech_indicator <- c(gbpjpy_tech_indicator[[3]] %>% html_text(), gbpjpy_tech_indicator[[4]] %>% html_text())
  gbpjpy_summary <- as.data.frame(rbind(gbpjpy_moving_average, gbpjpy_tech_indicator))
  
  gbpusd_tech_analysis <- read_html("https://www.investing.com/currencies/gbp-usd-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  gbpusd_moving_average <- gbpusd_tech_analysis[[2]] %>% html_children()
  gbpusd_moving_average <- c(gbpusd_moving_average[[3]] %>% html_text(), gbpusd_moving_average[[4]] %>% html_text())
  gbpusd_tech_indicator <- gbpusd_tech_analysis[[3]] %>% html_children()
  gbpusd_tech_indicator <- c(gbpusd_tech_indicator[[3]] %>% html_text(), gbpusd_tech_indicator[[4]] %>% html_text())
  gbpusd_summary <- as.data.frame(rbind(gbpusd_moving_average, gbpusd_tech_indicator))
  
  nzdjpy_tech_analysis <- read_html("https://www.investing.com/currencies/nzd-jpy-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  nzdjpy_moving_average <- nzdjpy_tech_analysis[[2]] %>% html_children()
  nzdjpy_moving_average <- c(nzdjpy_moving_average[[3]] %>% html_text(), nzdjpy_moving_average[[4]] %>% html_text())
  nzdjpy_tech_indicator <- nzdjpy_tech_analysis[[3]] %>% html_children()
  nzdjpy_tech_indicator <- c(nzdjpy_tech_indicator[[3]] %>% html_text(), nzdjpy_tech_indicator[[4]] %>% html_text())
  nzdjpy_summary <- as.data.frame(rbind(nzdjpy_moving_average, nzdjpy_tech_indicator))
  
  nzdusd_tech_analysis <- read_html("https://www.investing.com/currencies/nzd-usd-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  nzdusd_moving_average <- nzdusd_tech_analysis[[2]] %>% html_children()
  nzdusd_moving_average <- c(nzdusd_moving_average[[3]] %>% html_text(), nzdusd_moving_average[[4]] %>% html_text())
  nzdusd_tech_indicator <- nzdusd_tech_analysis[[3]] %>% html_children()
  nzdusd_tech_indicator <- c(nzdusd_tech_indicator[[3]] %>% html_text(), nzdusd_tech_indicator[[4]] %>% html_text())
  nzdusd_summary <- as.data.frame(rbind(nzdusd_moving_average, nzdusd_tech_indicator))
  
  usdcad_tech_analysis <- read_html("https://www.investing.com/currencies/usd-cad-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  usdcad_moving_average <- usdcad_tech_analysis[[2]] %>% html_children()
  usdcad_moving_average <- c(usdcad_moving_average[[3]] %>% html_text(), usdcad_moving_average[[4]] %>% html_text())
  usdcad_tech_indicator <- usdcad_tech_analysis[[3]] %>% html_children()
  usdcad_tech_indicator <- c(usdcad_tech_indicator[[3]] %>% html_text(), usdcad_tech_indicator[[4]] %>% html_text())
  usdcad_summary <- as.data.frame(rbind(usdcad_moving_average, usdcad_tech_indicator))
  
  usdchf_tech_analysis <- read_html("https://www.investing.com/currencies/usd-chf-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  usdchf_moving_average <- usdchf_tech_analysis[[2]] %>% html_children()
  usdchf_moving_average <- c(usdchf_moving_average[[3]] %>% html_text(), usdchf_moving_average[[4]] %>% html_text())
  usdchf_tech_indicator <- usdchf_tech_analysis[[3]] %>% html_children()
  usdchf_tech_indicator <- c(usdchf_tech_indicator[[3]] %>% html_text(), usdchf_tech_indicator[[4]] %>% html_text())
  usdchf_summary <- as.data.frame(rbind(usdchf_moving_average, usdchf_tech_indicator))
  
  usdjpy_tech_analysis <- read_html("https://www.investing.com/currencies/usd-jpy-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  usdjpy_moving_average <- usdjpy_tech_analysis[[2]] %>% html_children()
  usdjpy_moving_average <- c(usdjpy_moving_average[[3]] %>% html_text(), usdjpy_moving_average[[4]] %>% html_text())
  usdjpy_tech_indicator <- usdjpy_tech_analysis[[3]] %>% html_children()
  usdjpy_tech_indicator <- c(usdjpy_tech_indicator[[3]] %>% html_text(), usdjpy_tech_indicator[[4]] %>% html_text())
  usdjpy_summary <- as.data.frame(rbind(usdjpy_moving_average, usdjpy_tech_indicator))
  
  xauusd_tech_analysis <- read_html("https://www.investing.com/currencies/xau-usd-technical") %>% 
    html_node("#techStudiesInnerWrap") %>% html_children()
  xauusd_moving_average <- xauusd_tech_analysis[[2]] %>% html_children()
  xauusd_moving_average <- c(xauusd_moving_average[[3]] %>% html_text(), xauusd_moving_average[[4]] %>% html_text())
  xauusd_tech_indicator <- xauusd_tech_analysis[[3]] %>% html_children()
  xauusd_tech_indicator <- c(xauusd_tech_indicator[[3]] %>% html_text(), xauusd_tech_indicator[[4]] %>% html_text())
  xauusd_summary <- as.data.frame(rbind(xauusd_moving_average, xauusd_tech_indicator))
  
  summary_assets <- rbind(audcad_summary,audchf_summary,audjpy_summary,audnzd_summary,audusd_summary,
                          cadchf_summary,cadjpy_summary,chfjpy_summary,dxy_summary,euraud_summary,
                          eurcad_summary,eurchf_summary,eurgbp_summary,eurjpy_summary,eurnzd_summary,
                          eurusd_summary,gbpchf_summary,gbpjpy_summary,gbpusd_summary,nzdjpy_summary,
                          nzdusd_summary,usdcad_summary,usdchf_summary,usdjpy_summary,xauusd_summary)
  colnames(summary_assets) <- c("Buy Power","Sell Power")
  
  library(dplyr)
  library(stringr)
  summary_assets$type <- rownames(summary_assets)
  rownames(summary_assets) <- NULL
  
  summary_assets1 <- summary_assets %>% filter(grepl("moving_average", type)) %>%
    mutate(buy_num = as.numeric(str_replace(str_replace(`Buy Power`, "Buy \\(",""),"\\)",""))) %>%
    mutate(sell_num = as.numeric(str_replace(str_replace(`Sell Power`, "Sell \\(",""),"\\)","")))
  summary_assets1$max_power <- apply(summary_assets1[,which(colnames(summary_assets1) %in% c("buy_num", "sell_num"))], 
                                     FUN = function(x) max(x), MARGIN = 1)
  summary_assets1 <- arrange(summary_assets1, desc(max_power)) %>% select(-buy_num, -sell_num)
  summary_assets1 <- summary_assets1 %>% mutate(time_now = Sys.time())
  
  summary_assets2 <- summary_assets %>% filter(grepl("tech_indicator", type)) %>%
    mutate(buy_num = as.numeric(str_replace(str_replace(`Buy Power`, "Buy \\(",""),"\\)",""))) %>%
    mutate(sell_num = as.numeric(str_replace(str_replace(`Sell Power`, "Sell \\(",""),"\\)","")))
  summary_assets2$max_power <- apply(summary_assets2[,which(colnames(summary_assets2) %in% c("buy_num", "sell_num"))], 
                                     FUN = function(x) max(x), MARGIN = 1)
  summary_assets2 <- arrange(summary_assets2, desc(max_power)) %>% select(-buy_num, -sell_num)
  summary_assets2 <- summary_assets2 %>% mutate(time_now = Sys.time())
  
  max_power_asset2 <- c(max(summary_assets2$max_power), max(summary_assets2$max_power) - 1)
  which_type_max_power_asset2 <- summary_assets2$type[which(summary_assets2$max_power %in% max_power_asset2)]
  asset_max_power <- unlist(lapply(strsplit(which_type_max_power_asset2, "_"), FUN = function(x) x[[1]]))
  filter_eval <- "summary_assets3 <- summary_assets1 %>% filter("
  for(b in 1:length(asset_max_power)){
    if(b == length(asset_max_power)){
      filter_eval <- paste0(filter_eval, "grepl('",asset_max_power[b],"', type))")
    }else{
      filter_eval <- paste0(filter_eval, "grepl('",asset_max_power[b],"', type) | ")
    }
  }
  eval(parse(text = filter_eval))
  return(list(summary_assets1, summary_assets2, summary_assets3))
}

myfxbook_pip_calculator <- function(chromever = "108.0.5359.71", input_lots_num = 1){
  library(RSelenium)
  library(rvest)
  library(tidyverse)
  library(stringr)
  
  rd <- NULL
  remDr <- NULL
  while(1){
    try({
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      rD <- rsDriver(browser="chrome", verbose=F, chromever = chromever)
      remDr <- rD[["client"]]
      break
    })
  }
  
  while(1){
    tryCatch({
      remDr$navigate("https://www.myfxbook.com/forex-calculators/pip-calculator")
      Sys.sleep(30)
      try({
        writeLines("waiting for annoying ads..")
        ads_close <- remDr$findElement(using = "class", "color-white")
        ads_close$clickElement()
      })
      
      webElem <- remDr$findElement("css", "body")
      for(a in 1:7){
        webElem$sendKeysToElement(list(key = "down_arrow"))
      }
      
      input_lots <- remDr$findElement(using = 'class', value = 'inputValueFormValidation')
      input_lots$clickElement()
      input_lots$sendKeysToElement(list(as.character(input_lots_num)))
      
      calculate <- remDr$findElement(using = 'class', value = 'yellow-casablanca')
      calculate$clickElement()
      
      Sys.sleep(5) # give the page time to fully load
      html <- remDr$getPageSource()[[1]]
      html_obj <- read_html(html)
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      
      html_obj_table <- html_obj %>% html_table()
      html_obj_table <- html_obj_table[[2]] %>% as.data.frame()
      html_obj_table <- html_obj_table[-which(html_obj_table$X1 == ""),]
      colnames(html_obj_table) <- as.character(html_obj_table[1,])
      html_obj_table <- html_obj_table[-1,]
      return(html_obj_table)
      
      
      
    },error = function(cond){
      message("error happened!")
      message(cond)
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
    })
  }
}

news_investing <- function(){
  library(RSelenium)
  library(rvest)
  rD <- rsDriver(browser="chrome", verbose=F, chromever = "108.0.5359.71")
  remDr <- rD[["client"]]
  remDr$navigate("https://www.investing.com/currencies/xau-usd-news")
  html_obj <- remDr$getPageSource()
  
  xauusd_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  xauusd_news_df <- NULL
  
  for(v in 1:length(xauusd_news)){
    title <- xauusd_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- xauusd_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- xauusd_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(xauusd_news_df)){
      xauusd_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      xauusd_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      xauusd_news_df <- rbind(xauusd_news_df, xauusd_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/aud-cad-news")
  html_obj <- remDr$getPageSource()
  audcad_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  audcad_news_df <- NULL
  for(v in 1:length(audcad_news)){
    title <- audcad_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- audcad_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- audcad_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(audcad_news_df)){
      audcad_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      audcad_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      audcad_news_df <- rbind(audcad_news_df, audcad_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/aud-chf-news")
  html_obj <- remDr$getPageSource()
  audchf_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  audchf_news_df <- NULL
  for(v in 1:length(audchf_news)){
    title <- audchf_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- audchf_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- audchf_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(audchf_news_df)){
      audchf_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      audchf_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      audchf_news_df <- rbind(audchf_news_df, audchf_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/aud-jpy-news")
  html_obj <- remDr$getPageSource()
  audjpy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  audjpy_news_df <- NULL
  for(v in 1:length(audjpy_news)){
    title <- audjpy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- audjpy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- audjpy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(audjpy_news_df)){
      audjpy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      audjpy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      audjpy_news_df <- rbind(audjpy_news_df, audjpy_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/aud-nzd-news")
  html_obj <- remDr$getPageSource()
  audnzd_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  audnzd_news_df <- NULL
  for(v in 1:length(audnzd_news)){
    title <- audnzd_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- audnzd_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- audnzd_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(audnzd_news_df)){
      audnzd_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      audnzd_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      audnzd_news_df <- rbind(audnzd_news_df, audnzd_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/aud-usd-news")
  html_obj <- remDr$getPageSource()
  audusd_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  audusd_news_df <- NULL
  for(v in 1:length(audusd_news)){
    title <- audusd_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- audusd_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- audusd_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(audusd_news_df)){
      audusd_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      audusd_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      audusd_news_df <- rbind(audusd_news_df, audusd_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/cad-chf-news")
  html_obj <- remDr$getPageSource()
  cadchf_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  cadchf_news_df <- NULL
  for(v in 1:length(cadchf_news)){
    title <- cadchf_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- cadchf_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- cadchf_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(cadchf_news_df)){
      cadchf_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      cadchf_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      cadchf_news_df <- rbind(cadchf_news_df, cadchf_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/cad-jpy-news")
  html_obj <- remDr$getPageSource()
  cadjpy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  cadjpy_news_df <- NULL
  for(v in 1:length(cadjpy_news)){
    title <- cadjpy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- cadjpy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- cadjpy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(cadjpy_news_df)){
      cadjpy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      cadjpy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      cadjpy_news_df <- rbind(cadjpy_news_df, cadjpy_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/chf-jpy-news")
  html_obj <- remDr$getPageSource()
  chfjpy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  chfjpy_news_df <- NULL
  for(v in 1:length(chfjpy_news)){
    title <- chfjpy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- chfjpy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- chfjpy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(chfjpy_news_df)){
      chfjpy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      chfjpy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      chfjpy_news_df <- rbind(chfjpy_news_df, chfjpy_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/indices/usdollar-news")
  html_obj <- remDr$getPageSource()
  dxy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  dxy_news_df <- NULL
  for(v in 1:length(dxy_news)){
    title <- dxy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- dxy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- dxy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(dxy_news_df)){
      dxy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      dxy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      dxy_news_df <- rbind(dxy_news_df, dxy_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/eur-aud-news")
  html_obj <- remDr$getPageSource()
  euraud_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  euraud_news_df <- NULL
  for(v in 1:length(euraud_news)){
    title <- euraud_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- euraud_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- euraud_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(euraud_news_df)){
      euraud_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      euraud_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      euraud_news_df <- rbind(euraud_news_df, euraud_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/eur-cad-news")
  html_obj <- remDr$getPageSource()
  eurcad_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  eurcad_news_df <- NULL
  for(v in 1:length(eurcad_news)){
    title <- eurcad_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- eurcad_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- eurcad_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(eurcad_news_df)){
      eurcad_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      eurcad_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      eurcad_news_df <- rbind(eurcad_news_df, eurcad_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/eur-chf-news")
  html_obj <- remDr$getPageSource()
  eurchf_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  eurchf_news_df <- NULL
  for(v in 1:length(eurchf_news)){
    title <- eurchf_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- eurchf_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- eurchf_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(eurchf_news_df)){
      eurchf_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      eurchf_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      eurchf_news_df <- rbind(eurchf_news_df, eurchf_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/eur-gbp-news")
  html_obj <- remDr$getPageSource()
  eurgbp_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  eurgbp_news_df <- NULL
  for(v in 1:length(eurgbp_news)){
    title <- eurgbp_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- eurgbp_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- eurgbp_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(eurgbp_news_df)){
      eurgbp_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      eurgbp_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      eurgbp_news_df <- rbind(eurgbp_news_df, eurgbp_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/eur-jpy-news")
  html_obj <- remDr$getPageSource()
  eurjpy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  eurjpy_news_df <- NULL
  for(v in 1:length(eurjpy_news)){
    title <- eurjpy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- eurjpy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- eurjpy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(eurjpy_news_df)){
      eurjpy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      eurjpy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      eurjpy_news_df <- rbind(eurjpy_news_df, eurjpy_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/eur-nzd-news")
  html_obj <- remDr$getPageSource()
  eurnzd_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  eurnzd_news_df <- NULL
  for(v in 1:length(eurnzd_news)){
    title <- eurnzd_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- eurnzd_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- eurnzd_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(eurnzd_news_df)){
      eurnzd_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      eurnzd_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      eurnzd_news_df <- rbind(eurnzd_news_df, eurnzd_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/eur-usd-news")
  html_obj <- remDr$getPageSource()
  eurusd_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  eurusd_news_df <- NULL
  for(v in 1:length(eurusd_news)){
    title <- eurusd_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- eurusd_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- eurusd_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(eurusd_news_df)){
      eurusd_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      eurusd_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      eurusd_news_df <- rbind(eurusd_news_df, eurusd_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/gbp-chf-news")
  html_obj <- remDr$getPageSource()
  gbpchf_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  gbpchf_news_df <- NULL
  for(v in 1:length(gbpchf_news)){
    title <- gbpchf_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- gbpchf_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- gbpchf_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(gbpchf_news_df)){
      gbpchf_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      gbpchf_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      gbpchf_news_df <- rbind(gbpchf_news_df, gbpchf_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/gbp-jpy-news")
  html_obj <- remDr$getPageSource()
  gbpjpy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  gbpjpy_news_df <- NULL
  for(v in 1:length(gbpjpy_news)){
    title <- gbpjpy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- gbpjpy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- gbpjpy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(gbpjpy_news_df)){
      gbpjpy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      gbpjpy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      gbpjpy_news_df <- rbind(gbpjpy_news_df, gbpjpy_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/gbp-usd-news")
  html_obj <- remDr$getPageSource()
  gbpusd_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  gbpusd_news_df <- NULL
  for(v in 1:length(gbpusd_news)){
    title <- gbpusd_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- gbpusd_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- gbpusd_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(gbpusd_news_df)){
      gbpusd_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      gbpusd_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      gbpusd_news_df <- rbind(gbpusd_news_df, gbpusd_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/nzd-jpy-news")
  html_obj <- remDr$getPageSource()
  nzdjpy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  nzdjpy_news_df <- NULL
  for(v in 1:length(nzdjpy_news)){
    title <- nzdjpy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- nzdjpy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- nzdjpy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(nzdjpy_news_df)){
      nzdjpy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      nzdjpy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      nzdjpy_news_df <- rbind(nzdjpy_news_df, nzdjpy_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/nzd-usd-news")
  html_obj <- remDr$getPageSource()
  nzdusd_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  nzdusd_news_df <- NULL
  for(v in 1:length(nzdusd_news)){
    title <- nzdusd_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- nzdusd_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- nzdusd_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(nzdusd_news_df)){
      nzdusd_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      nzdusd_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      nzdusd_news_df <- rbind(nzdusd_news_df, nzdusd_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/usd-cad-news")
  html_obj <- remDr$getPageSource()
  usdcad_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  usdcad_news_df <- NULL
  for(v in 1:length(usdcad_news)){
    title <- usdcad_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- usdcad_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- usdcad_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(usdcad_news_df)){
      usdcad_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      usdcad_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      usdcad_news_df <- rbind(usdcad_news_df, usdcad_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/usd-chf-news")
  html_obj <- remDr$getPageSource()
  usdchf_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  usdchf_news_df <- NULL
  for(v in 1:length(usdchf_news)){
    title <- usdchf_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- usdchf_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- usdchf_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(usdchf_news_df)){
      usdchf_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      usdchf_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      usdchf_news_df <- rbind(usdchf_news_df, usdchf_news_add)
    }
  }
  
  remDr$navigate("https://www.investing.com/currencies/usd-jpy-news")
  html_obj <- remDr$getPageSource()
  usdjpy_news <- read_html(html_obj[[1]]) %>% 
    html_node(xpath='//*[@data-test="news-list"]') %>% html_children()
  usdjpy_news_df <- NULL
  for(v in 1:length(usdjpy_news)){
    title <- usdjpy_news[[v]] %>% html_node('.inv-link') %>% html_text()
    release_date <- usdjpy_news[[v]] %>% html_node('.list_list--horizontal__IzbiT') %>% 
      html_node('.layout_row__cHSAs') %>% html_children()
    release_date <- release_date[[4]] %>% html_text()
    description <- usdjpy_news[[v]] %>% html_node('.news-analysis_description__fErKF') %>% html_text()
    if(is.null(usdjpy_news_df)){
      usdjpy_news_df <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
    }else{
      usdjpy_news_add <- as.data.frame(cbind(title = title, release_date = release_date, description = description))
      usdjpy_news_df <- rbind(usdjpy_news_df, usdjpy_news_add)
    }
  }
  
  xauusd_news_df$asset_news <- "xauusd_news"
  audcad_news_df$asset_news <- "audcad_news"
  audchf_news_df$asset_news <- "audchf_news"
  audjpy_news_df$asset_news <- "audjpy_news"
  audnzd_news_df$asset_news <- "audnzd_news"
  audusd_news_df$asset_news <- "audusd_news"
  cadchf_news_df$asset_news <- "cadchf_news"
  cadjpy_news_df$asset_news <- "cadjpy_news"
  chfjpy_news_df$asset_news <- "chfjpy_news"
  dxy_news_df$asset_news <- "dxy_news"
  euraud_news_df$asset_news <- "euraud_news"
  eurcad_news_df$asset_news <- "eurcad_news"
  eurchf_news_df$asset_news <- "eurchf_news"
  eurgbp_news_df$asset_news <- "eurgbp_news"
  eurjpy_news_df$asset_news <- "eurjpy_news"
  eurnzd_news_df$asset_news <- "eurnzd_news"
  eurusd_news_df$asset_news <- "eurusd_news"
  gbpchf_news_df$asset_news <- "gbpchf_news"
  gbpjpy_news_df$asset_news <- "gbpjpy_news"
  gbpusd_news_df$asset_news <- "gbpusd_news"
  nzdjpy_news_df$asset_news <- "nzdjpy_news"
  nzdusd_news_df$asset_news <- "nzdusd_news"
  usdcad_news_df$asset_news <- "usdcad_news"
  usdchf_news_df$asset_news <- "usdchf_news"
  usdjpy_news_df$asset_news <- "usdjpy_news"
  news_df <- rbind(xauusd_news_df, audcad_news_df, audchf_news_df, audjpy_news_df,audnzd_news_df,audusd_news_df,
                   cadchf_news_df, cadjpy_news_df, chfjpy_news_df, dxy_news_df, euraud_news_df, eurcad_news_df,
                   eurchf_news_df, eurgbp_news_df, eurjpy_news_df, eurnzd_news_df, eurusd_news_df, gbpchf_news_df,
                   gbpjpy_news_df, gbpusd_news_df, nzdjpy_news_df, nzdusd_news_df, usdcad_news_df, usdchf_news_df,
                   usdjpy_news_df)
  
  remDr$close()
  rD$server$stop()
  rm(rD, remDr)
  gc()
  return(news_df)
}